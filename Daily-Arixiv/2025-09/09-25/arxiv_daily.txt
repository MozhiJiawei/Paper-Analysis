Gmail	李嘉维 <qetwe0000@gmail.com>
cs daily Subj-class mailing 8004a1 1
send mail ONLY to cs <no-reply@arxiv.org>	2025年9月29日 12:35
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Computer Vision and Pattern Recognition
Distributed, Parallel, and Cluster Computing
Multiagent Systems
 received from  Thu 25 Sep 25 18:00:00 GMT  to  Fri 26 Sep 25 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2509.21344
Date: Tue, 16 Sep 2025 19:09:27 GMT   (881kb)

Title: Towards mitigating information leakage when evaluating safety monitors
Authors: Gerard Boxo, Aman Neelappa, Shivam Raval
Categories: cs.AI cs.CL cs.LG
Comments: 14 pages, 4 figures
\\
  White box monitors that analyze model internals offer promising advantages
for detecting potentially harmful behaviors in large language models, including
lower computational costs and integration into layered defense systems.However,
training and evaluating these monitors requires response exemplars that exhibit
the target behaviors, typically elicited through prompting or fine-tuning. This
presents a challenge when the information used to elicit behaviors inevitably
leaks into the data that monitors ingest, inflating their effectiveness. We
present a systematic framework for evaluating a monitor's performance in terms
of its ability to detect genuine model behavior rather than superficial
elicitation artifacts. Furthermore, we propose three novel strategies to
evaluate the monitor: content filtering (removing deception-related text from
inputs), score filtering (aggregating only over task-relevant tokens), and
prompt distilled fine-tuned model organisms (models trained to exhibit
deceptive behavior without explicit prompting). Using deception detection as a
representative case study, we identify two forms of leakage that inflate
monitor performance: elicitation leakage from prompts that explicitly request
harmful behavior, and reasoning leakage from models that verbalize their
deceptive actions. Through experiments on multiple deception benchmarks, we
apply our proposed mitigation strategies and measure performance retention. Our
evaluation of the monitors reveal three crucial findings: (1) Content filtering
is a good mitigation strategy that allows for a smooth removal of elicitation
signal and can decrease probe AUROC by 30\% (2) Score filtering was found to
reduce AUROC by 15\% but is not as straightforward to attribute to (3) A
finetuned model organism improves monitor evaluations but reduces their
performance by upto 40\%, even when re-trained.
\\ ( https://arxiv.org/abs/2509.21344 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21549
Date: Thu, 25 Sep 2025 20:34:45 GMT   (292kb)

Title: Correct Reasoning Paths Visit Shared Decision Pivots
Authors: Dongkyu Cho and Amy B.Z. Zhang and Bilel Fehri and Sheng Wang and Rumi
  Chunara and Rui Song and Hengrui Cai
Categories: cs.AI
Comments: 18 pages, 10 figures
\\
  Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of
large language models (LLMs), yet verifying those traces at scale remains
unsolved. In response, we introduce the idea of decision pivots-minimal,
verifiable checkpoints that any correct reasoning path must visit. We
hypothesize that correct reasoning, though stylistically diverse, converge on
the same pivot set, while incorrect ones violate at least one pivot. Leveraging
this property, we propose a self-training pipeline that (i) samples diverse
reasoning paths and mines shared decision pivots, (ii) compresses each trace
into pivot-focused short-path reasoning using an auxiliary verifier, and (iii)
post-trains the model using its self-generated outputs. The proposed method
aligns reasoning without ground truth reasoning data or external metrics.
Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the
effectiveness of our method.
\\ ( https://arxiv.org/abs/2509.21549 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21553
Date: Thu, 25 Sep 2025 20:38:23 GMT   (1877kb)

Title: AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All
  You Need
Authors: Ahmed Jaber, Wangshu Zhu, Karthick Jayavelu, Justin Downes, Sameer
  Mohamed, Candace Agonafir, Linnia Hawkins, Tian Zheng
Categories: cs.AI cs.CE cs.HC cs.LG cs.MA
\\
  Climate data science faces persistent barriers stemming from the fragmented
nature of data sources, heterogeneous formats, and the steep technical
expertise required to identify, acquire, and process datasets. These challenges
limit participation, slow discovery, and reduce the reproducibility of
scientific workflows. In this paper, we present a proof of concept for
addressing these barriers through the integration of a curated knowledge graph
(KG) with AI agents designed for cloud-native scientific workflows. The KG
provides a unifying layer that organizes datasets, tools, and workflows, while
AI agents -- powered by generative AI services -- enable natural language
interaction, automated data access, and streamlined analysis. Together, these
components drastically lower the technical threshold for engaging in climate
data science, enabling non-specialist users to identify and analyze relevant
datasets. By leveraging existing cloud-ready API data portals, we demonstrate
that "a knowledge graph is all you need" to unlock scalable and agentic
workflows for scientific inquiry. The open-source design of our system further
supports community contributions, ensuring that the KG and associated tools can
evolve as a shared commons. Our results illustrate a pathway toward
democratizing access to climate data and establishing a reproducible,
extensible framework for human--AI collaboration in scientific research.
\\ ( https://arxiv.org/abs/2509.21553 ,  1877kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21567
Date: Thu, 25 Sep 2025 20:50:29 GMT   (312kb)

Title: EEG-Based Consumer Behaviour Prediction: An Exploration from Classical
  Machine Learning to Graph Neural Networks
Authors: Mohammad Parsa Afshar, Aryan Azimi
Categories: cs.AI cs.LG
\\
  Prediction of consumer behavior is one of the important purposes in
marketing, cognitive neuroscience, and human-computer interaction. The
electroencephalography (EEG) data can help analyze the decision process by
providing detailed information about the brain's neural activity. In this
research, a comparative approach is utilized for predicting consumer behavior
by EEG data. In the first step, the features of the EEG data from the NeuMa
dataset were extracted and cleaned. For the Graph Neural Network (GNN) models,
the brain connectivity features were created. Different machine learning
models, such as classical models and Graph Neural Networks, are used and
compared. The GNN models with different architectures are implemented to have a
comprehensive comparison; furthermore, a wide range of classical models, such
as ensemble models, are applied, which can be very helpful to show the
difference and performance of each model on the dataset. Although the results
did not show a significant difference overall, the GNN models generally
performed better in some basic criteria where classical models were not
satisfactory. This study not only shows that combining EEG signal analysis and
machine learning models can provide an approach to deeper understanding of
consumer behavior, but also provides a comprehensive comparison between the
machine learning models that have been widely used in previous studies in the
EEG-based neuromarketing such as Support Vector Machine (SVM), and the models
which are not used or rarely used in the field, like Graph Neural Networks.
\\ ( https://arxiv.org/abs/2509.21567 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21593
Date: Thu, 25 Sep 2025 21:03:57 GMT   (7670kb)

Title: GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large
  Language Models
Authors: Peng Luo, Xiayin Lou, Yu Zheng, Zhuo Zheng, Stefano Ermon
Categories: cs.AI physics.soc-ph
\\
  Geospatial modeling provides critical solutions for pressing global
challenges such as sustainability and climate change. Existing large language
model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at
evolving generic code but lack the domain knowledge and multi-step reasoning
required for complex geospatial problems. We introduce GeoEvolve, a multi-agent
LLM framework that couples evolutionary search with geospatial domain knowledge
to automatically design and refine geospatial algorithms. GeoEvolve operates in
two nested loops: an inner loop leverages a code evolver to generate and mutate
candidate solutions, while an outer agentic controller evaluates global elites
and queries a GeoKnowRAG module -- a structured geospatial knowledge base that
injects theoretical priors from geography. This knowledge-guided evolution
steers the search toward theoretically meaningful and computationally efficient
algorithms. We evaluate GeoEvolve on two fundamental and classical tasks:
spatial interpolation (kriging) and spatial uncertainty quantification
(geospatial conformal prediction). Across these benchmarks, GeoEvolve
automatically improves and discovers new algorithms, incorporating geospatial
theory on top of classical models. It reduces spatial interpolation error
(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%.
Ablation studies confirm that domain-guided retrieval is essential for stable,
high-quality evolution. These results demonstrate that GeoEvolve provides a
scalable path toward automated, knowledge-driven geospatial modeling, opening
new opportunities for trustworthy and efficient AI-for-Science discovery.
\\ ( https://arxiv.org/abs/2509.21593 ,  7670kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21600
Date: Thu, 25 Sep 2025 21:13:39 GMT   (2409kb)

Title: Automated and Interpretable Survival Analysis from Multimodal Data
Authors: Mafalda Malafaia, Peter A.N. Bosman, Coen Rasch, Tanja Alderliesten
Categories: cs.AI cs.LG
Comments: 4 figures; 4 tables; 24 pages
\\
  Accurate and interpretable survival analysis remains a core challenge in
oncology. With growing multimodal data and the clinical need for transparent
models to support validation and trust, this challenge increases in complexity.
We propose an interpretable multimodal AI framework to automate survival
analysis by integrating clinical variables and computed tomography imaging. Our
MultiFIX-based framework uses deep learning to infer survival-relevant features
that are further explained: imaging features are interpreted via Grad-CAM,
while clinical variables are modeled as symbolic expressions through genetic
programming. Risk estimation employs a transparent Cox regression, enabling
stratification into groups with distinct survival outcomes. Using the
open-source RADCURE dataset for head and neck cancer, MultiFIX achieves a
C-index of 0.838 (prediction) and 0.826 (stratification), outperforming the
clinical and academic baseline approaches and aligning with known prognostic
markers. These results highlight the promise of interpretable multimodal AI for
precision oncology with MultiFIX.
\\ ( https://arxiv.org/abs/2509.21600 ,  2409kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21633
Date: Thu, 25 Sep 2025 21:48:48 GMT   (775kb)

Title: Semantic F1 Scores: Fair Evaluation Under Fuzzy Class Boundaries
Authors: Georgios Chochlakis, Jackson Trager, Vedant Jhaveri, Nikhil
  Ravichandran, Alexandros Potamianos, Shrikanth Narayanan
Categories: cs.AI
Comments: 33 pages, 1 table, 29 figures, 4 algorithms
\\
  We propose Semantic F1 Scores, novel evaluation metrics for subjective or
fuzzy multi-label classification that quantify semantic relatedness between
predicted and gold labels. Unlike the conventional F1 metrics that treat
semantically related predictions as complete failures, Semantic F1 incorporates
a label similarity matrix to compute soft precision-like and recall-like
scores, from which the Semantic F1 scores are derived. Unlike existing
similarity-based metrics, our novel two-step precision-recall formulation
enables the comparison of label sets of arbitrary sizes without discarding
labels or forcing matches between dissimilar labels. By granting partial credit
for semantically related but nonidentical labels, Semantic F1 better reflects
the realities of domains marked by human disagreement or fuzzy category
boundaries. In this way, it provides fairer evaluations: it recognizes that
categories overlap, that annotators disagree, and that downstream decisions
based on similar predictions lead to similar outcomes. Through theoretical
justification and extensive empirical validation on synthetic and real data, we
show that Semantic F1 demonstrates greater interpretability and ecological
validity. Because it requires only a domain-appropriate similarity matrix,
which is robust to misspecification, and not a rigid ontology, it is applicable
across tasks and modalities.
\\ ( https://arxiv.org/abs/2509.21633 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21651
Date: Thu, 25 Sep 2025 22:09:17 GMT   (4056kb)

Title: Can AI Perceive Physical Danger and Intervene?
Authors: Abhishek Jindal, Dmitry Kalashnikov, Oscar Chang, Divya Garikapati,
  Anirudha Majumdar, Pierre Sermanet, Vikas Sindhwani
Categories: cs.AI
Report-no: Report-no: GDM-01-01
\\
  When AI interacts with the physical world -- as a robot or an assistive agent
-- new safety challenges emerge beyond those of purely ``digital AI". In such
interactions, the potential for physical harm is direct and immediate. How well
do state-of-the-art foundation models understand common-sense facts about
physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of
coffee should not be handed to a child? In this paper, our contributions are
three-fold: first, we develop a highly scalable approach to continuous physical
safety benchmarking of Embodied AI systems, grounded in real-world injury
narratives and operational safety constraints. To probe multi-modal safety
understanding, we turn these narratives and constraints into photorealistic
images and videos capturing transitions from safe to unsafe states, using
advanced generative models. Secondly, we comprehensively analyze the ability of
major foundation models to perceive risks, reason about safety, and trigger
interventions; this yields multi-faceted insights into their deployment
readiness for safety-critical agentic applications. Finally, we develop a
post-training paradigm to teach models to explicitly reason about
embodiment-specific safety constraints provided through system instructions.
The resulting models generate thinking traces that make safety reasoning
interpretable and transparent, achieving state of the art performance in
constraint satisfaction evaluations. The benchmark will be released at
https://asimov-benchmark.github.io/v2
\\ ( https://arxiv.org/abs/2509.21651 ,  4056kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21718
Date: Fri, 26 Sep 2025 00:28:50 GMT   (173kb)

Title: Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided
  Online Preference Optimization
Authors: Shehzeen Hussain, Paarth Neekhara, Xuesong Yang, Edresson Casanova,
  Subhankar Ghosh, Roy Fejgin, Ryan Langman, Mikyas Desta, Leili Tavabi, Jason
  Li
Categories: cs.AI cs.LG eess.AS
Comments: Submitted to ICASSP 2026
\\
  Developing high-quality text-to-speech (TTS) systems for low-resource
languages is challenging due to the scarcity of paired text and speech data. In
contrast, automatic speech recognition (ASR) models for such languages are
often more accessible, owing to large-scale multilingual pre-training efforts.
We propose a framework based on Group Relative Policy Optimization (GRPO) to
adapt an autoregressive, multilingual TTS model to new languages. Our method
first establishes a language-agnostic foundation for TTS synthesis by training
a multilingual baseline with International Phonetic Alphabet (IPA) tokens.
Next, we fine-tune this model on limited paired data of the new languages to
capture the target language's prosodic features. Finally, we apply GRPO to
optimize the model using only unpaired text and speaker prompts, guided by a
multi-objective reward from pretrained ASR, speaker verification, and audio
quality estimation models. Experiments demonstrate that this pipeline produces
intelligible and speaker-consistent speech in low-resource languages,
substantially outperforming fine-tuning alone. Furthermore, our GRPO-based
framework also improves TTS performance in high-resource languages, surpassing
offline alignment methods such as Direct Preference Optimization (DPO) yielding
superior intelligibility, speaker similarity, and audio quality.
\\ ( https://arxiv.org/abs/2509.21718 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21743
Date: Fri, 26 Sep 2025 01:17:35 GMT   (743kb)

Title: Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts
Authors: Ammar Ahmed, Azal Ahmad Khan, Ayaan Ahmad, Sheng Di, Zirui Liu, Ali
  Anwar
Categories: cs.AI cs.LG
\\
  Large reasoning models improve accuracy by producing long reasoning traces,
but this inflates latency and cost, motivating inference-time efficiency. We
propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable
``thought" steps to guide new problems. RoT organizes steps into a thought
graph with sequential and semantic edges to enable fast retrieval and flexible
recombination. At inference, RoT retrieves query-relevant nodes and applies
reward-guided traversal to assemble a problem-specific template that guides
generation. This dynamic template reuse reduces redundant exploration and,
therefore, reduces output tokens while preserving accuracy. We evaluate RoT on
reasoning benchmarks with multiple models, measuring accuracy, token usage,
latency, and memory overhead. Findings show small prompt growth but substantial
efficiency gains, with RoT reducing output tokens by up to 40%, inference
latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a
scalable paradigm for efficient LRM reasoning via dynamic template construction
through retrieval.
\\ ( https://arxiv.org/abs/2509.21743 ,  743kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21765
Date: Fri, 26 Sep 2025 02:03:48 GMT   (1058kb)

Title: Lifelong Learning with Behavior Consolidation for Vehicle Routing
Authors: Jiyuan Pei, Yi Mei, Jialin Liu, Mengjie Zhang, Xin Yao
Categories: cs.AI cs.LG
\\
  Recent neural solvers have demonstrated promising performance in learning to
solve routing problems. However, existing studies are primarily based on
one-off training on one or a set of predefined problem distributions and
scales, i.e., tasks. When a new task arises, they typically rely on either
zero-shot generalization, which may be poor due to the discrepancies between
the new task and the training task(s), or fine-tuning the pretrained solver on
the new task, which possibly leads to catastrophic forgetting of knowledge
acquired from previous tasks. This paper explores a novel lifelong learning
paradigm for neural VRP solvers, where multiple tasks with diverse
distributions and scales arise sequentially over time. Solvers are required to
effectively and efficiently learn to solve new tasks while maintaining their
performance on previously learned tasks. Consequently, a novel framework called
Lifelong Learning Router with Behavior Consolidation (LLR-BC) is proposed.
LLR-BC consolidates prior knowledge effectively by aligning behaviors of the
solver trained on a new task with the buffered ones in a decision-seeking way.
To encourage more focus on crucial experiences, LLR-BC assigns greater
consolidated weights to decisions with lower confidence. Extensive experiments
on capacitated vehicle routing problems and traveling salesman problems
demonstrate LLR-BC's effectiveness in training high-performance neural solvers
in a lifelong learning setting, addressing the catastrophic forgetting issue,
maintaining their plasticity, and improving zero-shot generalization ability.
\\ ( https://arxiv.org/abs/2509.21765 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21766
Date: Fri, 26 Sep 2025 02:04:00 GMT   (4027kb)

Title: UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon
  Scenarios
Authors: Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin,
  Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu,
  Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, and Li
  Shen
Categories: cs.AI cs.CL
\\
  Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}
\\ ( https://arxiv.org/abs/2509.21766 ,  4027kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21782
Date: Fri, 26 Sep 2025 02:38:14 GMT   (9431kb)

Title: Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and
  Safety
Authors: Junliang Liu and Jingyu Xiao and Wenxin Tang and Wenxuan Wang and
  Zhixian Wang and Minrui Zhang and Shuanghe Yu
Categories: cs.AI
\\
  Multimodal large language models (MLLMs) are increasingly positioned as AI
collaborators for building complex web-related applications like GUI agents and
front-end code generation. However, existing benchmarks largely emphasize
visual perception or UI code generation, showing insufficient evaluation on the
reasoning, robustness and safety capability required for end-to-end web
applications. To bridge the gap, we introduce a comprehensive web understanding
benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and
Safety across eight tasks, such as position relationship reasoning, color
robustness, and safety critical detection, etc. The benchmark is constructed
from 729 websites and contains 3799 question answer pairs that probe multi-step
inference over page structure, text, widgets, and safety-critical interactions.
To ensure reliable measurement, we adopt standardized prompts, deterministic
evaluation scripts, and multi-stage quality control combining automatic checks
with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The
results reveal significant gaps, models still struggle with compositional and
cross-element reasoning over realistic layouts, show limited robustness when
facing perturbations in user interfaces and content such as layout
rearrangements or visual style shifts, and are rather conservative in
recognizing and avoiding safety critical or irreversible actions. Our code is
available at https://github.com/jinliang-byte/webssrbench.
\\ ( https://arxiv.org/abs/2509.21782 ,  9431kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21799
Date: Fri, 26 Sep 2025 02:56:19 GMT   (9006kb)

Title: D-Artemis: A Deliberative Cognitive Framework for Mobile GUI
  Multi-Agents
Authors: Hongze Mi, Yibo Feng, Wenjie Lu, Yuqi Wang, Jinyuan Li, Song Cao, He
  Cui, Tengfei Tian, Xuelin Zhang, Haotian Luo, Di Sun, Naiqiang Tan, Gang Pan
Categories: cs.AI
\\
  Graphical User Interface (GUI) agents aim to automate a wide spectrum of
human tasks by emulating user interaction. Despite rapid advancements, current
approaches are hindered by several critical challenges: data bottleneck in
end-to-end training, high cost of delayed error detection, and risk of
contradictory guidance. Inspired by the human cognitive loop of Thinking,
Alignment, and Reflection, we present D-Artemis -- a novel deliberative
framework in this paper. D-Artemis leverages a fine-grained, app-specific tip
retrieval mechanism to inform its decision-making process. It also employs a
proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)
Check module and Action Correction Agent (ACA) work in concert to mitigate the
risk of execution failures. A post-execution Status Reflection Agent (SRA)
completes the cognitive loop, enabling strategic learning from experience.
Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal
large language models (MLLMs) for GUI tasks without the need for training on
complex trajectory datasets, demonstrating strong generalization. D-Artemis
establishes new state-of-the-art (SOTA) results across both major benchmarks,
achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.
Extensive ablation studies further demonstrate the significant contribution of
each component to the framework.
\\ ( https://arxiv.org/abs/2509.21799 ,  9006kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21823
Date: Fri, 26 Sep 2025 03:29:36 GMT   (11615kb)

Title: ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor
  Collaboration
Authors: Gaole Dai, Shiqi Jiang, Ting Cao, Yuqing Yang, Yuanchun Li, Rui Tan,
  Mo Li, Lili Qiu
Categories: cs.AI
Comments: 10 pages, 7 figures
\\
  Reward is critical to the evaluation and training of large language models
(LLMs). However, existing rule-based or model-based reward methods struggle to
generalize to GUI agents, where access to ground-truth trajectories or
application databases is often unavailable, and static trajectory-based
LLM-as-a-Judge approaches suffer from limited accuracy. To address these
challenges, we propose ProRe, a proactive reward system that leverages a
general-purpose reasoner and domain-specific evaluator agents (actors). The
reasoner schedules targeted state probing tasks, which the evaluator agents
then execute by actively interacting with the environment to collect additional
observations. This enables the reasoner to assign more accurate and verifiable
rewards to GUI agents. Empirical results on over 3K trajectories demonstrate
that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%,
respectively. Furthermore, integrating ProRe with state-of-the-art policy
agents yields a success rate improvement of up to 22.4%.
\\ ( https://arxiv.org/abs/2509.21823 ,  11615kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21825
Date: Fri, 26 Sep 2025 03:38:12 GMT   (955kb)

Title: DS-STAR: Data Science Agent via Iterative Planning and Verification
Authors: Jaehyun Nam, Jinsung Yoon, Jiefeng Chen, Jinwoo Shin, Tomas Pfister
Categories: cs.AI
\\
  Data science, which transforms raw data into actionable insights, is critical
for data-driven decision-making. However, these tasks are often complex,
involving steps for exploring multiple data sources and synthesizing findings
to deliver insightful answers. While large language models (LLMs) show
significant promise in automating this process, they often struggle with
heterogeneous data formats and generate sub-optimal analysis plans, as
verifying plan sufficiency is inherently difficult without ground-truth labels
for such open-ended tasks. To overcome these limitations, we introduce DS-STAR,
a novel data science agent. Specifically, DS-STAR makes three key
contributions: (1) a data file analysis module that automatically explores and
extracts context from diverse data formats, including unstructured types; (2) a
verification step where an LLM-based judge evaluates the sufficiency of the
analysis plan at each stage; and (3) a sequential planning mechanism that
starts with a simple, executable plan and iteratively refines it based on the
DS-STAR's feedback until its sufficiency is verified. This iterative refinement
allows DS-STAR to reliably navigate complex analyses involving diverse data
sources. Our experiments show that DS-STAR achieves state-of-the-art
performance across three challenging benchmarks: DABStep, KramaBench, and
DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks
that require processing multiple data files with heterogeneous formats.
\\ ( https://arxiv.org/abs/2509.21825 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21836
Date: Fri, 26 Sep 2025 03:50:55 GMT   (22kb)

Title: Axiomatic Choice and the Decision-Evaluation Paradox
Authors: Ben Abramowitz, Nicholas Mattei
Categories: cs.AI cs.MA
\\
  We introduce a framework for modeling decisions with axioms that are
statements about decisions, e.g., ethical constraints. Using our framework we
define a taxonomy of decision axioms based on their structural properties and
demonstrate a tension between the use of axioms to make decisions and the use
of axioms to evaluate decisions which we call the Decision-Evaluation Paradox.
We argue that the Decision-Evaluation Paradox arises with realistic axiom
structures, and the paradox illuminates why one must be exceptionally careful
when training models on decision data or applying axioms to make and evaluate
decisions.
\\ ( https://arxiv.org/abs/2509.21836 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21842
Date: Fri, 26 Sep 2025 04:03:52 GMT   (5576kb)

Title: DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for
  Autonomous Travel Planning Agents
Authors: Yansong Ning, Rui Liu, Jun Wang, Kai Chen, Wei Li, Jun Fang, Kan
  Zheng, Naiqiang Tan, Hao Liu
Categories: cs.AI
Comments: Under review
\\
  Travel planning (TP) agent has recently worked as an emerging building block
to interact with external tools and resources for travel itinerary generation,
ensuring enjoyable user experience. Despite its benefits, existing studies rely
on hand craft prompt and fixed agent workflow, hindering more flexible and
autonomous TP agent. This paper proposes DeepTravel, an end to end agentic
reinforcement learning framework for building autonomous travel planning agent,
capable of autonomously planning, executing tools, and reflecting on tool
responses to explore, verify, and refine intermediate actions in multi step
reasoning. To achieve this, we first construct a robust sandbox environment by
caching transportation, accommodation and POI data, facilitating TP agent
training without being constrained by real world APIs limitations (e.g.,
inconsistent outputs). Moreover, we develop a hierarchical reward modeling
system, where a trajectory level verifier first checks spatiotemporal
feasibility and filters unsatisfied travel itinerary, and then the turn level
verifier further validate itinerary detail consistency with tool responses,
enabling efficient and precise reward service. Finally, we propose the reply
augmented reinforcement learning method that enables TP agent to periodically
replay from a failures experience buffer, emerging notable agentic capacity. We
deploy trained TP agent on DiDi Enterprise Solutions App and conduct
comprehensive online and offline evaluations, demonstrating that DeepTravel
enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing
frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.
\\ ( https://arxiv.org/abs/2509.21842 ,  5576kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21862
Date: Fri, 26 Sep 2025 04:38:59 GMT   (2018kb)

Title: Reimagining Agent-based Modeling with Large Language Model Agents via
  Shachi
Authors: So Kuroki, Yingtao Tian, Kou Misaki, Takashi Ikegami, Takuya Akiba,
  Yujin Tang
Categories: cs.AI cs.MA cs.SI econ.GN q-fin.EC
\\
  The study of emergent behaviors in large language model (LLM)-driven
multi-agent systems is a critical research challenge, yet progress is limited
by a lack of principled methodologies for controlled experimentation. To
address this, we introduce Shachi, a formal methodology and modular framework
that decomposes an agent's policy into core cognitive components: Configuration
for intrinsic traits, Memory for contextual persistence, and Tools for expanded
capabilities, all orchestrated by an LLM reasoning engine. This principled
architecture moves beyond brittle, ad-hoc agent designs and enables the
systematic analysis of how specific architectural choices influence collective
behavior. We validate our methodology on a comprehensive 10-task benchmark and
demonstrate its power through novel scientific inquiries. Critically, we
establish the external validity of our approach by modeling a real-world U.S.
tariff shock, showing that agent behaviors align with observed market reactions
only when their cognitive architecture is appropriately configured with memory
and tools. Our work provides a rigorous, open-source foundation for building
and evaluating LLM agents, aimed at fostering more cumulative and
scientifically grounded research.
\\ ( https://arxiv.org/abs/2509.21862 ,  2018kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21886
Date: Fri, 26 Sep 2025 05:22:32 GMT   (851kb)

Title: TRACE: Learning to Compute on Graphs
Authors: Ziyang Zheng, Jiaying Zhu, Jingyi Zhou, Qiang Xu
Categories: cs.AI
\\
  Learning to compute, the ability to model the functional behavior of a
computational graph, is a fundamental challenge for graph representation
learning. Yet, the dominant paradigm is architecturally mismatched for this
task. This flawed assumption, central to mainstream message passing neural
networks (MPNNs) and their conventional Transformer-based counterparts,
prevents models from capturing the position-aware, hierarchical nature of
computation. To resolve this, we introduce \textbf{TRACE}, a new paradigm built
on an architecturally sound backbone and a principled learning objective.
First, TRACE employs a Hierarchical Transformer that mirrors the step-by-step
flow of computation, providing a faithful architectural backbone that replaces
the flawed permutation-invariant aggregation. Second, we introduce
\textbf{function shift learning}, a novel objective that decouples the learning
problem. Instead of predicting the complex global function directly, our model
is trained to predict only the \textit{function shift}, the discrepancy between
the true global function and a simple local approximation that assumes input
independence. We validate this paradigm on electronic circuits, one of the most
complex and economically critical classes of computational graphs. Across a
comprehensive suite of benchmarks, TRACE substantially outperforms all prior
architectures. These results demonstrate that our architecturally-aligned
backbone and decoupled learning objective form a more robust paradigm for the
fundamental challenge of learning to compute on graphs.
\\ ( https://arxiv.org/abs/2509.21886 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21896
Date: Fri, 26 Sep 2025 05:30:43 GMT   (629kb)

Title: GenesisGeo: Technical Report
Authors: Minfeng Zhu, Zi Wang, Sizhe Ji, Zhengtong Du, Junming Ke, Xiao Deng,
  Zanlang Yin, Xiuqi Huang, Heyu Wang, Wei Chen
Categories: cs.AI
\\
  We present GenesisGeo, an automated theorem prover in Euclidean geometry. We
have open-sourced a large-scale geometry dataset of 21.8 million geometric
problems, over 3 million of which contain auxiliary constructions. Specially,
we significantly accelerate the symbolic deduction engine DDARN by 120x through
theorem matching, combined with a C++ implementation of its core components.
Furthermore, we build our neuro-symbolic prover, GenesisGeo, upon
Qwen3-0.6B-Base, which solves 24 of 30 problems (IMO silver medal level) in the
IMO-AG-30 benchmark using a single model, and achieves 26 problems (IMO gold
medal level) with a dual-model ensemble.
\\ ( https://arxiv.org/abs/2509.21896 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21902
Date: Fri, 26 Sep 2025 05:35:51 GMT   (468kb)

Title: DyRo-MCTS: A Robust Monte Carlo Tree Search Approach to Dynamic Job Shop
  Scheduling
Authors: Ruiqi Chen, Yi Mei, Fangfang Zhang, Mengjie Zhang
Categories: cs.AI
\\
  Dynamic job shop scheduling, a fundamental combinatorial optimisation problem
in various industrial sectors, poses substantial challenges for effective
scheduling due to frequent disruptions caused by the arrival of new jobs.
State-of-the-art methods employ machine learning to learn scheduling policies
offline, enabling rapid responses to dynamic events. However, these offline
policies are often imperfect, necessitating the use of planning techniques such
as Monte Carlo Tree Search (MCTS) to improve performance at online decision
time. The unpredictability of new job arrivals complicates online planning, as
decisions based on incomplete problem information are vulnerable to
disturbances. To address this issue, we propose the Dynamic Robust MCTS
(DyRo-MCTS) approach, which integrates action robustness estimation into MCTS.
DyRo-MCTS guides the production environment toward states that not only yield
good scheduling outcomes but are also easily adaptable to future job arrivals.
Extensive experiments show that DyRo-MCTS significantly improves the
performance of offline-learned policies with negligible additional online
planning time. Moreover, DyRo-MCTS consistently outperforms vanilla MCTS across
various scheduling scenarios. Further analysis reveals that its ability to make
robust scheduling decisions leads to long-term, sustainable performance gains
under disturbances.
\\ ( https://arxiv.org/abs/2509.21902 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21943
Date: Fri, 26 Sep 2025 06:25:02 GMT   (1264kb)

Title: Outlier Detection in Plantar Pressure: Human-Centered Comparison of
  Statistical Parametric Mapping and Explainable Machine Learning
Authors: Carlo Dindorf, Jonas Dully, Steven Simon, Dennis Perchthaler, Stephan
  Becker, Hannah Ehmann, Kjell Heitmann, Bernd Stetter, Christian Diers,
  Michael Fr\"ohlich
Categories: cs.AI cs.LG
\\
  Plantar pressure mapping is essential in clinical diagnostics and sports
science, yet large heterogeneous datasets often contain outliers from technical
errors or procedural inconsistencies. Statistical Parametric Mapping (SPM)
provides interpretable analyses but is sensitive to alignment and its capacity
for robust outlier detection remains unclear. This study compares an SPM
approach with an explainable machine learning (ML) approach to establish
transparent quality-control pipelines for plantar pressure datasets. Data from
multiple centers were annotated by expert consensus and enriched with synthetic
anomalies resulting in 798 valid samples and 2000 outliers. We evaluated (i) a
non-parametric, registration-dependent SPM approach and (ii) a convolutional
neural network (CNN), explained using SHapley Additive exPlanations (SHAP).
Performance was assessed via nested cross-validation; explanation quality via a
semantic differential survey with domain experts. The ML model reached high
accuracy and outperformed SPM, which misclassified clinically meaningful
variations and missed true outliers. Experts perceived both SPM and SHAP
explanations as clear, useful, and trustworthy, though SPM was assessed less
complex. These findings highlight the complementary potential of SPM and
explainable ML as approaches for automated outlier detection in plantar
pressure data, and underscore the importance of explainability in translating
complex model outputs into interpretable insights that can effectively inform
decision-making.
\\ ( https://arxiv.org/abs/2509.21943 ,  1264kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21981
Date: Fri, 26 Sep 2025 07:03:52 GMT   (2029kb)

Title: CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief
  World for Optimizing Embodied Multi-Agent Collaboration
Authors: Zhimin Wang, Shaokang He, Duo Wu, Jinghe Wang, Linjia Kang, Jing Yu,
  Zhi Wang
Categories: cs.AI cs.MA
\\
  Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.
\\ ( https://arxiv.org/abs/2509.21981 ,  2029kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21982
Date: Fri, 26 Sep 2025 07:05:01 GMT   (1838kb)

Title: RISK: A Framework for GUI Agents in E-commerce Risk Management
Authors: Renqi Chen, Zeyin Tao, Jianming Guo, Jingzhe Zhu, Yiheng Peng,
  Qingqing Sun, Tianyi Zhang, Shuai Chen
Categories: cs.AI cs.CL
\\
  E-commerce risk management requires aggregating diverse, deeply embedded web
data through multi-step, stateful interactions, which traditional scraping
methods and most existing Graphical User Interface (GUI) agents cannot handle.
These agents are typically limited to single-step tasks and lack the ability to
manage dynamic, interactive content critical for effective risk assessment. To
address this challenge, we introduce RISK, a novel framework designed to build
and deploy GUI agents for this domain. RISK integrates three components: (1)
RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction
trajectories, collected through a high-fidelity browser framework and a
meticulous data curation process; (2) RISK-Bench, a benchmark with 802
single-step and 320 multi-step trajectories across three difficulty levels for
standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning
framework considering four aspects: (i) Output Format: Updated format reward to
enhance output syntactic correctness and task comprehension, (ii) Single-step
Level: Stepwise accuracy reward to provide granular feedback during early
training stages, (iii) Multi-step Level: Process reweight to emphasize critical
later steps in interaction sequences, and (iv) Task Level: Level reweight to
focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms
existing baselines, achieving a 6.8% improvement in offline single-step and an
8.8% improvement in offline multi-step. Moreover, it attains a top task success
rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific
solution for automating complex web interactions, advancing the state of the
art in e-commerce risk management.
\\ ( https://arxiv.org/abs/2509.21982 ,  1838kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21993
Date: Fri, 26 Sep 2025 07:19:39 GMT   (1465kb)

Title: Bilinear relational structure fixes reversal curse and enables
  consistent model editing
Authors: Dong-Kyum Kim, Minsung Kim, Jea Kwon, Nakyeong Yang, Meeyoung Cha
Categories: cs.AI cs.LG
Comments: 9 pages
\\
  The reversal curse -- a language model's (LM) inability to infer an unseen
fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a
fundamental limitation. We show that this is not an inherent failure but an
artifact of how models encode knowledge. By training LMs from scratch on a
synthetic dataset of relational knowledge graphs, we demonstrate that bilinear
relational structure emerges in their hidden representations. This structure
substantially alleviates the reversal curse, enabling LMs to infer unseen
reverse facts. Crucially, we also find that this bilinear structure plays a key
role in consistent model editing. When a fact is updated in a LM with this
structure, the edit correctly propagates to its reverse and other logically
dependent facts. In contrast, models lacking this representation not only
suffer from the reversal curse but also fail to generalize edits, further
introducing logical inconsistencies. Our results establish that training on a
relational knowledge dataset induces the emergence of bilinear internal
representations, which in turn enable LMs to behave in a logically consistent
manner after editing. This implies that the success of model editing depends
critically not just on editing algorithms but on the underlying
representational geometry of the knowledge being modified.
\\ ( https://arxiv.org/abs/2509.21993 ,  1465kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21998
Date: Fri, 26 Sep 2025 07:24:37 GMT   (17927kb)

Title: GSM-Agent: Understanding Agentic Reasoning Using Controllable
  Environments
Authors: Hanlin Zhu, Tianyu Guo, Song Mei, Stuart Russell, Nikhil Ghosh,
  Alberto Bietti, Jiantao Jiao
Categories: cs.AI cs.LG
Comments: 35 pages, 8 figures
\\
  As LLMs are increasingly deployed as agents, agentic reasoning - the ability
to combine tool use, especially search, and reasoning - becomes a critical
skill. However, it is hard to disentangle agentic reasoning when evaluated in
complex environments and tasks. Current agent benchmarks often mix agentic
reasoning with challenging math reasoning, expert-level knowledge, and other
advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,
where an LLM agent is required to solve grade-school-level reasoning problems,
but is only presented with the question in the prompt without the premises that
contain the necessary information to solve the task, and needs to proactively
collect that information using tools. Although the original tasks are
grade-school math problems, we observe that even frontier models like GPT-5
only achieve 67% accuracy. To understand and analyze the agentic reasoning
patterns, we propose the concept of agentic reasoning graph: cluster the
environment's document embeddings into nodes, and map each tool call to its
nearest node to build a reasoning path. Surprisingly, we identify that the
ability to revisit a previously visited node, widely taken as a crucial pattern
in static reasoning, is often missing for agentic reasoning for many models.
Based on the insight, we propose a tool-augmented test-time scaling method to
improve LLM's agentic reasoning performance by adding tools to encourage models
to revisit. We expect our benchmark and the agentic reasoning framework to aid
future studies of understanding and pushing the boundaries of agentic
reasoning.
\\ ( https://arxiv.org/abs/2509.21998 ,  17927kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22034
Date: Fri, 26 Sep 2025 08:12:13 GMT   (343kb)

Title: The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs
  through Model Merging
Authors: Xiaochong Lan, Yu Zheng, Shiteng Cao, Yong Li
Categories: cs.AI cs.CL
\\
  The growing demand for large language models (LLMs) with tunable reasoning
capabilities in many real-world applications highlights a critical need for
methods that can efficiently produce a spectrum of models balancing reasoning
depth and computational cost. Model merging has emerged as a promising,
training-free technique to address this challenge by arithmetically combining
the weights of a general-purpose model with a specialized reasoning model.
While various merging techniques exist, their potential to create a spectrum of
models with fine-grained control over reasoning abilities remains largely
unexplored. This work presents a large-scale empirical study evaluating a range
of model merging techniques across multiple reasoning benchmarks. We
systematically vary merging strengths to construct accuracy-efficiency curves,
providing the first comprehensive view of the tunable performance landscape.
Our findings reveal that model merging offers an effective and controllable
method for calibrating the trade-off between reasoning accuracy and token
efficiency, even when parent models have highly divergent weight spaces.
Crucially, we identify instances of Pareto Improvement, where a merged model
achieves both higher accuracy and lower token consumption than one of its
parents. Our study provides the first comprehensive analysis of this tunable
space, offering practical guidelines for creating LLMs with specific reasoning
profiles to meet diverse application demands.
\\ ( https://arxiv.org/abs/2509.22034 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22044
Date: Fri, 26 Sep 2025 08:27:03 GMT   (849kb)

Title: A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning
Authors: Ziqi Wang, Boye Niu, Zhongli Li, Linghui Meng, Jing Liu, Zhi Zheng,
  Tong Xu, Hua Wu, Haifeng Wang, Enhong Chen
Categories: cs.AI cs.CL
Comments: 15 pages, 3 figures
\\
  Recent Large Reasoning Models have achieved significant improvements in
complex task-solving capabilities by allocating more computation at the
inference stage with a "thinking longer" paradigm. Even as the foundational
reasoning capabilities of models advance rapidly, the persistent gap between a
model's performance in a single attempt and its latent potential, often
revealed only across multiple solution paths, starkly highlights the disparity
between its realized and inherent capabilities. To address this, we present
A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge
the gap between a model's potential and its actual performance. In this
framework, an "explorer" model first generates potential solutions in parallel
through repeated sampling. Subsequently,a "synthesizer" model integrates these
references for a more refined, second stage of reasoning. This two-stage
process allows computation to be scaled orthogonally to existing sequential
methods. Our work makes two key innovations: First, we present A2R as a
plug-and-play parallel reasoning framework that explicitly enhances a model's
capabilities on complex questions. For example, using our framework, the
Qwen3-8B-distill model achieves a 75% performance improvement compared to its
self-consistency baseline. Second, through a systematic analysis of the
explorer and synthesizer roles, we identify an effective asymmetric scaling
paradigm. This insight leads to A2R-Efficient, a "small-to-big" variant that
combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration
surpasses the average performance of a monolithic Qwen3-32B model at a nearly
30% lower cost. Collectively, these results show that A2R is not only a
performance-boosting framework but also an efficient and practical solution for
real-world applications.
\\ ( https://arxiv.org/abs/2509.22044 ,  849kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22085
Date: Fri, 26 Sep 2025 09:06:03 GMT   (26866kb)

Title: Generalizing Multi-Objective Search via Objective-Aggregation Functions
Authors: Hadar Peer, Eyal Weiss, Ron Alterovitz, Oren Salzman
Categories: cs.AI
\\
  Multi-objective search (MOS) has become essential in robotics, as real-world
robotic systems need to simultaneously balance multiple, often conflicting
objectives. Recent works explore complex interactions between objectives,
leading to problem formulations that do not allow the usage of out-of-the-box
state-of-the-art MOS algorithms. In this paper, we suggest a generalized
problem formulation that optimizes solution objectives via aggregation
functions of hidden (search) objectives. We show that our formulation supports
the application of standard MOS algorithms, necessitating only to properly
extend several core operations to reflect the specific aggregation functions
employed. We demonstrate our approach in several diverse robotics planning
problems, spanning motion-planning for navigation, manipulation and planning fr
medical systems under obstacle uncertainty as well as inspection planning, and
route planning with different road types. We solve the problems using
state-of-the-art MOS algorithms after properly extending their core operations,
and provide empirical evidence that they outperform by orders of magnitude the
vanilla versions of the algorithms applied to the same problems but without
objective aggregation.
\\ ( https://arxiv.org/abs/2509.22085 ,  26866kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22092
Date: Fri, 26 Sep 2025 09:12:21 GMT   (541kb)

Title: Ground-Truthing AI Energy Consumption: Validating CodeCarbon Against
  External Measurements
Authors: Raphael Fischer
Categories: cs.AI
\\
  Although machine learning (ML) and artificial intelligence (AI) present
fascinating opportunities for innovation, their rapid development is also
significantly impacting our environment. In response to growing
resource-awareness in the field, quantification tools such as the ML Emissions
Calculator and CodeCarbon were developed to estimate the energy consumption and
carbon emissions of running AI models. They are easy to incorporate into AI
projects, however also make pragmatic assumptions and neglect important
factors, raising the question of estimation accuracy. This study systematically
evaluates the reliability of static and dynamic energy estimation approaches
through comparisons with ground-truth measurements across hundreds of AI
experiments. Based on the proposed validation framework, investigative insights
into AI energy demand and estimation inaccuracies are provided. While generally
following the patterns of AI energy consumption, the established estimation
approaches are shown to consistently make errors of up to 40%. By providing
empirical evidence on energy estimation quality and errors, this study
establishes transparency and validates widely used tools for sustainable AI
development. It moreover formulates guidelines for improving the
state-of-the-art and offers code for extending the validation to other domains
and tools, thus making important contributions to resource-aware ML and AI
sustainability research.
\\ ( https://arxiv.org/abs/2509.22092 ,  541kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22137
Date: Fri, 26 Sep 2025 09:56:44 GMT   (9264kb)

Title: Log2Plan: An Adaptive GUI Automation Framework Integrated with Task
  Mining Approach
Authors: Seoyoung Lee, Seonbin Yoon, Seongbeen Lee, Hyesoo Kim, Joo Yong Sim
Categories: cs.AI cs.HC cs.MA cs.RO
MSC-class: 68N19, 68T09
ACM-class: H.5.2; D.2.2
\\
  GUI task automation streamlines repetitive tasks, but existing LLM or
VLM-based planner-executor agents suffer from brittle generalization, high
latency, and limited long-horizon coherence. Their reliance on single-shot
reasoning or static plans makes them fragile under UI changes or complex tasks.
Log2Plan addresses these limitations by combining a structured two-level
planning framework with a task mining approach over user behavior logs,
enabling robust and adaptable GUI automation. Log2Plan constructs high-level
plans by mapping user commands to a structured task dictionary, enabling
consistent and generalizable automation. To support personalization and reuse,
it employs a task mining approach from user behavior logs that identifies
user-specific patterns. These high-level plans are then grounded into low-level
action sequences by interpreting real-time GUI context, ensuring robust
execution across varying interfaces. We evaluated Log2Plan on 200 real-world
tasks, demonstrating significant improvements in task success rate and
execution time. Notably, it maintains over 60.0% success rate even on
long-horizon task sequences, highlighting its robustness in complex, multi-step
workflows.
\\ ( https://arxiv.org/abs/2509.22137 ,  9264kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22242
Date: Fri, 26 Sep 2025 11:56:58 GMT   (134kb)

Title: Clinical Uncertainty Impacts Machine Learning Evaluations
Authors: Simone Lionetti, Fabian Gr\"oger, Philippe Gottfrois, Alvaro
  Gonzalez-Jimenez, Ludovic Amruthalingam, Alexander A. Navarini, Marc Pouly
Categories: cs.AI cs.CV cs.LG
\\
  Clinical dataset labels are rarely certain as annotators disagree and
confidence is not uniform across cases. Typical aggregation procedures, such as
majority voting, obscure this variability. In simple experiments on medical
imaging benchmarks, accounting for the confidence in binary labels
significantly impacts model rankings. We therefore argue that machine-learning
evaluations should explicitly account for annotation uncertainty using
probabilistic metrics that directly operate on distributions. These metrics can
be applied independently of the annotations' generating process, whether
modeled by simple counting, subjective confidence ratings, or probabilistic
response models. They are also computationally lightweight, as closed-form
expressions have linear-time implementations once examples are sorted by model
score. We thus urge the community to release raw annotations for datasets and
to adopt uncertainty-aware evaluation so that performance estimates may better
reflect clinical data.
\\ ( https://arxiv.org/abs/2509.22242 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22255
Date: Fri, 26 Sep 2025 12:19:22 GMT   (1141kb)

Title: Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase
  Heuristics for 2D Bin-Packing
Authors: Syed Mahbubul Huq, Daniel Brito, Daniel Sikar, Rajesh Mojumder
Categories: cs.AI
Comments: 1 table, 6 figures. 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025) Accepted for the Workshop: Evaluating the Evolving LLM
  Lifecycle Benchmarks, Emergent Abilities, and Scaling
\\
  This paper presents an evaluation framework for assessing Large Language
Models' (LLMs) capabilities in combinatorial optimization, specifically
addressing the 2D bin-packing problem. We introduce a systematic methodology
that combines LLMs with evolutionary algorithms to generate and refine
heuristic solutions iteratively. Through comprehensive experiments comparing
LLM generated heuristics against traditional approaches (Finite First-Fit and
Hybrid First-Fit), we demonstrate that LLMs can produce more efficient
solutions while requiring fewer computational resources. Our evaluation reveals
that GPT-4o achieves optimal solutions within two iterations, reducing average
bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78
to 0.83. This work contributes to understanding LLM evaluation in specialized
domains and establishes benchmarks for assessing LLM performance in
combinatorial optimization tasks.
\\ ( https://arxiv.org/abs/2509.22255 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22261
Date: Fri, 26 Sep 2025 12:26:16 GMT   (9876kb)

Title: InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with
  Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning
Authors: Guanghao Zhu, Zhitian Hou, Zeyu Liu, Zhijie Sang, Congkai Xie, Hongxia
  Yang
Categories: cs.AI cs.CL
\\
  Multimodal large language models (MLLMs) have shown remarkable potential in
various domains, yet their application in the medical field is hindered by
several challenges. General-purpose MLLMs often lack the specialized knowledge
required for medical tasks, leading to uncertain or hallucinatory responses.
Knowledge distillation from advanced models struggles to capture
domain-specific expertise in radiology and pharmacology. Additionally, the
computational cost of continual pretraining with large-scale medical data poses
significant efficiency challenges. To address these issues, we propose
InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs
designed to deliver state-of-the-art performance in medical applications. We
combined high-quality general-purpose and medical multimodal data and proposed
a novel five-dimensional quality assessment framework to curate high-quality
multimodal medical datasets. We employ low-to-high image resolution and
multimodal sequence packing to enhance training efficiency, enabling the
integration of extensive medical data. Furthermore, a three-stage supervised
fine-tuning process ensures effective knowledge extraction for complex medical
tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B
outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B
and MedGemma-27B-IT, demonstrating superior performance in medical visual
question answering and diagnostic tasks. By addressing key challenges in data
quality, training efficiency, and domain-specific knowledge extraction, our
work paves the way for more reliable and effective AI-driven solutions in
healthcare. InfiMed-Foundation-4B model is available at
\href{https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B}{InfiMed-Foundation-4B}.
\\ ( https://arxiv.org/abs/2509.22261 ,  9876kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22284
Date: Fri, 26 Sep 2025 12:46:30 GMT   (1616kb)

Title: Structured Sparse Transition Matrices to Enable State Tracking in
  State-Space Models
Authors: Aleksandar Terzi\'c, Nicolas Menet, Michael Hersche, Thomas Hofmann,
  Abbas Rahimi
Categories: cs.AI cs.LG
Comments: 10 pages, NeurIPS 2025 Spotlight
\\
  Modern state-space models (SSMs) often utilize transition matrices which
enable efficient computation but pose restrictions on the model's expressivity,
as measured in terms of the ability to emulate finite-state automata (FSA).
While unstructured transition matrices are optimal in terms of expressivity,
they come at a prohibitively high compute and memory cost even for moderate
state sizes. We propose a structured sparse parametrization of transition
matrices in SSMs that enables FSA state tracking with optimal state size and
depth, while keeping the computational cost of the recurrence comparable to
that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix
as the product of a column one-hot matrix ($P$) and a complex-valued diagonal
matrix ($D$). Consequently, the computational cost of parallel scans scales
linearly with the state size. Theoretically, the model is BIBO-stable and can
emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout
of size $N \times N$, significantly improving on all current structured SSM
guarantees. Experimentally, the model significantly outperforms a wide
collection of modern SSM variants on various FSA state tracking tasks. On
multiclass time-series classification, the performance is comparable to that of
neural controlled differential equations, a paradigm explicitly built for
time-series analysis. Finally, we integrate PD-SSM into a hybrid
Transformer-SSM architecture and demonstrate that the model can effectively
track the states of a complex FSA in which transitions are encoded as a set of
variable-length English sentences. The code is available at
https://github.com/IBM/expressive-sparse-state-space-model
\\ ( https://arxiv.org/abs/2509.22284 ,  1616kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22297
Date: Fri, 26 Sep 2025 12:59:41 GMT   (23kb)

Title: Large Language Models as Nondeterministic Causal Models
Authors: Sander Beckers
Categories: cs.AI
Comments: Preprint: under review
\\
  Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first
time, a method for generating counterfactuals of probabilistic Large Language
Models. Such counterfactuals tell us what would - or might - have been the
output of an LLM if some factual prompt ${\bf x}$ had been ${\bf x}^*$ instead.
The ability to generate such counterfactuals is an important necessary step
towards explaining, evaluating, and comparing, the behavior of LLMs. I argue,
however, that the existing method rests on an ambiguous interpretation of LLMs:
it does not interpret LLMs literally, for the method involves the assumption
that one can change the implementation of an LLM's sampling process without
changing the LLM itself, nor does it interpret LLMs as intended, for the method
involves explicitly representing a nondeterministic LLM as a deterministic
causal model. I here present a much simpler method for generating
counterfactuals that is based on an LLM's intended interpretation by
representing it as a nondeterministic causal model instead. The advantage of my
simpler method is that it is directly applicable to any black-box LLM without
modification, as it is agnostic to any implementation details. The advantage of
the existing method, on the other hand, is that it directly implements the
generation of a specific type of counterfactuals that is useful for certain
purposes, but not for others. I clarify how both methods relate by offering a
theoretical foundation for reasoning about counterfactuals in LLMs based on
their intended semantics, thereby laying the groundwork for novel
application-specific methods for generating counterfactuals.
\\ ( https://arxiv.org/abs/2509.22297 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22315
Date: Fri, 26 Sep 2025 13:16:36 GMT   (11060kb)

Title: PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning
Authors: Hieu Tran, Zonghai Yao, Nguyen Luong Tran, Zhichao Yang, Feiyun
  Ouyang, Shuo Han, Razieh Rahimi, Hong Yu
Categories: cs.AI cs.CL
Comments: 8 pages
\\
  Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.
\\ ( https://arxiv.org/abs/2509.22315 ,  11060kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22391
Date: Fri, 26 Sep 2025 14:18:50 GMT   (2767kb)

Title: Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for
  Epistemic Competence in Information-Seeking Agents
Authors: Jiaqi Shao, Yuxiang Lin, Munish Prasad Lohani, Yufeng Miao, Bing Luo
Categories: cs.AI
\\
  Recent work has explored training Large Language Model (LLM) search agents
with reinforcement learning (RL) for open-domain question answering (QA).
However, most evaluations focus solely on final answer accuracy, overlooking
how these agents reason with and act on external evidence. We introduce
SeekBench, the first benchmark for evaluating the \textit{epistemic competence}
of LLM search agents through step-level analysis of their response traces.
SeekBench comprises 190 expert-annotated traces with over 1,800 response steps
generated by LLM search agents, each enriched with evidence annotations for
granular analysis of whether agents (1) generate reasoning steps grounded in
observed evidence, (2) adaptively reformulate searches to recover from
low-quality results, and (3) have proper calibration to correctly assess
whether the current evidence is sufficient for providing an answer.
\\ ( https://arxiv.org/abs/2509.22391 ,  2767kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22407
Date: Fri, 26 Sep 2025 14:34:44 GMT   (3654kb)

Title: EMMA: Generalizing Real-World Robot Manipulation via Generative Visual
  Transfer
Authors: Zhehao Dong, Xiaofeng Wang, Zheng Zhu, Yirui Wang, Yang Wang, Yukun
  Zhou, Boyuan Wang, Chaojun Ni, Runqi Ouyang, Wenkang Qin, Xinze Chen, Yun Ye,
  Guan Huang
Categories: cs.AI cs.RO
\\
  Vision-language-action (VLA) models increasingly rely on diverse training
data to achieve robust generalization. However, collecting large-scale
real-world robot manipulation data across varied object appearances and
environmental conditions remains prohibitively time-consuming and expensive. To
overcome this bottleneck, we propose Embodied Manipulation Media Adaptation
(EMMA), a VLA policy enhancement framework that integrates a generative data
engine with an effective training pipeline. We introduce DreamTransfer, a
diffusion Transformer-based framework for generating multi-view consistent,
geometrically grounded embodied manipulation videos. DreamTransfer enables
text-controlled visual editing of robot videos, transforming foreground,
background, and lighting conditions without compromising 3D structure or
geometrical plausibility. Furthermore, we explore hybrid training with real and
generated data, and introduce AdaMix, a hard-sample-aware training strategy
that dynamically reweights training batches to focus optimization on
perceptually or kinematically challenging samples. Extensive experiments show
that videos generated by DreamTransfer significantly outperform prior video
generation methods in multi-view consistency, geometric fidelity, and
text-conditioning accuracy. Crucially, VLAs trained with generated data enable
robots to generalize to unseen object categories and novel visual domains using
only demonstrations from a single appearance. In real-world robotic
manipulation tasks with zero-shot visual domains, our approach achieves over a
200% relative performance gain compared to training on real data alone, and
further improves by 13% with AdaMix, demonstrating its effectiveness in
boosting policy generalization.
\\ ( https://arxiv.org/abs/2509.22407 ,  3654kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22447
Date: Fri, 26 Sep 2025 15:03:24 GMT   (3772kb)

Title: Guiding Evolution of Artificial Life Using Vision-Language Models
Authors: Nikhil Baid, Hannah Erlebach, Paul Hellegouarch, Frederico Wieser
Categories: cs.AI cs.NE
Comments: 9 pages, 6 figures. Accepted for publication in the Proceedings of
  the Artificial Life Conference 2025 (MIT Press)
\\
  Foundation models (FMs) have recently opened up new frontiers in the field of
artificial life (ALife) by providing powerful tools to automate search through
ALife simulations. Previous work aligns ALife simulations with natural language
target prompts using vision-language models (VLMs). We build on Automated
Search for Artificial Life (ASAL) by introducing ASAL++, a method for
open-ended-like search guided by multimodal FMs. We use a second FM to propose
new evolutionary targets based on a simulation's visual history. This induces
an evolutionary trajectory with increasingly complex targets.
  We explore two strategies: (1) evolving a simulation to match a single new
prompt at each iteration (Evolved Supervised Targets: EST) and (2) evolving a
simulation to match the entire sequence of generated prompts (Evolved Temporal
Targets: ETT). We test our method empirically in the Lenia substrate using
Gemma-3 to propose evolutionary targets, and show that EST promotes greater
visual novelty, while ETT fosters more coherent and interpretable evolutionary
sequences.
  Our results suggest that ASAL++ points towards new directions for FM-driven
ALife discovery with open-ended characteristics.
\\ ( https://arxiv.org/abs/2509.22447 ,  3772kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22460
Date: Fri, 26 Sep 2025 15:12:04 GMT   (1957kb)

Title: GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning
  with Auxiliary Line Construction and Affine Transformation
Authors: Shichao Weng, Zhiqiang Wang, Yuhua Zhou, Rui Lu, Ting Liu, Zhiyang
  Teng, Xiaozhang Liu, Hanmeng Liu
Categories: cs.AI
\\
  Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large
Language Models (MLLMs), requiring not only the joint interpretation of text
and diagrams but also iterative visuospatial reasoning. While existing
approaches process diagrams as static images, they lack the capacity for
dynamic manipulation - a core aspect of human geometric reasoning involving
auxiliary line construction and affine transformations. We present GeoSketch, a
neural-symbolic framework that recasts geometric reasoning as an interactive
perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module
that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning
module that applies geometric theorems to decide the next deductive step, and
(3) a Sketch Action module that executes operations such as drawing auxiliary
lines or applying transformations, thereby updating the diagram in a closed
loop. To train this agent, we develop a two-stage pipeline: supervised
fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement
learning with dense, symbolic rewards to enhance robustness and strategic
exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a
high-quality set of 390 geometry problems requiring auxiliary construction or
affine transformations. Experiments on strong MLLM baselines demonstrate that
GeoSketch significantly improves stepwise reasoning accuracy and
problem-solving success over static perception methods. By unifying
hierarchical decision-making, executable visual actions, and symbolic
verification, GeoSketch advances multimodal reasoning from static
interpretation to dynamic, verifiable interaction, establishing a new
foundation for solving complex visuospatial problems.
\\ ( https://arxiv.org/abs/2509.22460 ,  1957kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22502
Date: Fri, 26 Sep 2025 15:44:09 GMT   (5798kb)

Title: InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios
Authors: Chenglin Yu, Yang Yu, Songmiao Wang, Yucheng Wang, Yifan Yang, Jinjia
  Li, Ming Li, Hongxia Yang
Categories: cs.AI cs.HC
Comments: 9 pages of main content and 32 pages of others, 2 figures, under
  review as a conference paper at ICLR 2026
\\
  Large Language Model (LLM) agents have demonstrated remarkable capabilities
in organizing and executing complex tasks, and many such agents are now widely
used in various application scenarios. However, developing these agents
requires carefully designed workflows, carefully crafted prompts, and iterative
tuning, which requires LLM techniques and domain-specific expertise. These
hand-crafted limitations hinder the scalability and cost-effectiveness of LLM
agents across a wide range of industries. To address these challenges, we
propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that
can be applied to \textbf{infi}nite scenarios, which introduces several key
innovations: a generalized "agent-as-a-tool" mechanism that automatically
decomposes complex agents into hierarchical multi-agent systems; a dual-audit
mechanism that ensures the quality and stability of task completion; an agent
routing function that enables efficient task-agent matching; and an agent
self-evolution mechanism that autonomously restructures the agent DAG based on
new tasks, poor performance, or optimization opportunities. Furthermore,
InfiAgent's atomic task design supports agent parallelism, significantly
improving execution efficiency. This framework evolves into a versatile
pyramid-like multi-agent system capable of solving a wide range of problems.
Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\%
higher performance compared to ADAS (similar auto-generated agent framework),
while a case study of the AI research assistant InfiHelper shows that it
generates scientific papers that have received recognition from human reviewers
at top-tier IEEE conferences.
\\ ( https://arxiv.org/abs/2509.22502 ,  5798kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22504
Date: Fri, 26 Sep 2025 15:46:14 GMT   (12233kb)

Title: Estimating the Empowerment of Language Model Agents
Authors: Jinyeop Song, Jeff Gore, Max Kleiman-Weiner
Categories: cs.AI cs.LG
Comments: 10 pages, 8 figures. Submitted to ICLR 2026
\\
  As language model (LM) agents become more capable and gain broader access to
real-world tools, there is a growing need for scalable evaluation frameworks of
agentic capability. However, conventional benchmark-centric evaluations are
costly to design and require human designers to come up with valid tasks that
translate into insights about general model capabilities. In this work, we
propose information-theoretic evaluation based on empowerment, the mutual
information between an agent's actions and future states, as an open-ended
method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of
Language Model Agents), an algorithm for approximating effective empowerment
from multi-turn text interactions. We validate EELMA on both language games and
scaled-up realistic web-browsing scenarios. We find that empowerment strongly
correlates with average task performance, characterize the impact of
environmental complexity and agentic factors such as chain-of-thought, model
scale, and memory length on estimated empowerment, and that high empowerment
states and actions are often pivotal moments for general capabilities.
Together, these results demonstrate empowerment as an appealing general-purpose
metric for evaluating and monitoring LM agents in complex, open-ended settings.
\\ ( https://arxiv.org/abs/2509.22504 ,  12233kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22516
Date: Fri, 26 Sep 2025 16:00:36 GMT   (1871kb)

Title: TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent
  and Explainable Digital Assessments
Authors: Rakesh Thakur, Shivaansh Kaushik, Gauri Chopra, Harsh Rohilla
Categories: cs.AI cs.LG
\\
  This paper introduces TrueGradeAI, an AI-driven digital examination framework
designed to overcome the shortcomings of traditional paper-based assessments,
including excessive paper usage, logistical complexity, grading delays, and
evaluator bias. The system preserves natural handwriting by capturing stylus
input on secure tablets and applying transformer-based optical character
recognition for transcription. Evaluation is conducted through a
retrieval-augmented pipeline that integrates faculty solutions, cache layers,
and external references, enabling a large language model to assign scores with
explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems
that primarily digitize responses, TrueGradeAI advances the field by
incorporating explainable automation, bias mitigation, and auditable grading
trails. By uniting handwriting preservation with scalable and transparent
evaluation, the framework reduces environmental costs, accelerates feedback
cycles, and progressively builds a reusable knowledge base, while actively
working to mitigate grading bias and ensure fairness in assessment.
\\ ( https://arxiv.org/abs/2509.22516 ,  1871kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22518
Date: Fri, 26 Sep 2025 16:02:27 GMT   (648kb)

Title: REMA: A Unified Reasoning Manifold Framework for Interpreting Large
  Language Model
Authors: Bo Li, Guanzhi Deng, Ronghao Chen, Junrong Yue, Shuo Zhang, Qinghua
  Zhao, Linqi Song, Lijie Wen
Categories: cs.AI cs.LG
\\
  Understanding how Large Language Models (LLMs) perform complex reasoning and
their failure mechanisms is a challenge in interpretability research. To
provide a measurable geometric analysis perspective, we define the concept of
the Reasoning Manifold, a latent low-dimensional geometric structure formed by
the internal representations corresponding to all correctly reasoned
generations. This structure can be conceptualized as the embodiment of the
effective thinking paths that the model has learned to successfully solve a
given task. Based on this concept, we build REMA, a framework that explains the
origins of failures by quantitatively comparing the spatial relationships of
internal model representations corresponding to both erroneous and correct
reasoning samples. Specifically, REMA first quantifies the geometric deviation
of each erroneous representation by calculating its k-nearest neighbors
distance to the approximated manifold formed by correct representations,
thereby providing a unified failure signal. It then localizes the divergence
points where these deviations first become significant by tracking this
deviation metric across the model's layers and comparing it against a baseline
of internal fluctuations from correct representations, thus identifying where
the reasoning chain begins to go off-track. Our extensive experiments on
diverse language and multimodal models and tasks demonstrate the
low-dimensional nature of the reasoning manifold and the high separability
between erroneous and correct reasoning representations. The results also
validate the effectiveness of the REMA framework in analyzing the origins of
reasoning failures. This research connects abstract reasoning failures to
measurable geometric deviations in representations, providing new avenues for
in-depth understanding and diagnosis of the internal computational processes of
black-box models.
\\ ( https://arxiv.org/abs/2509.22518 ,  648kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22537
Date: Fri, 26 Sep 2025 16:17:29 GMT   (1111kb)

Title: The Emergence of Altruism in Large-Language-Model Agents Society
Authors: Haoyang Li, Xiao Jia, Zhanzhan Zhao
Categories: cs.AI
\\
  Leveraging Large Language Models (LLMs) for social simulation is a frontier
in computational social science. Understanding the social logics these agents
embody is critical to this attempt. However, existing research has primarily
focused on cooperation in small-scale, task-oriented games, overlooking how
altruism, which means sacrificing self-interest for collective benefit, emerges
in large-scale agent societies. To address this gap, we introduce a
Schelling-variant urban migration model that creates a social dilemma,
compelling over 200 LLM agents to navigate an explicit conflict between
egoistic (personal utility) and altruistic (system utility) goals. Our central
finding is a fundamental difference in the social tendencies of LLMs. We
identify two distinct archetypes: "Adaptive Egoists", which default to
prioritizing self-interest but whose altruistic behaviors significantly
increase under the influence of a social norm-setting message board; and
"Altruistic Optimizers", which exhibit an inherent altruistic logic,
consistently prioritizing collective benefit even at a direct cost to
themselves. Furthermore, to qualitatively analyze the cognitive underpinnings
of these decisions, we introduce a method inspired by Grounded Theory to
systematically code agent reasoning. In summary, this research provides the
first evidence of intrinsic heterogeneity in the egoistic and altruistic
tendencies of different LLMs. We propose that for social simulation, model
selection is not merely a matter of choosing reasoning capability, but of
choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a
more suitable choice for simulating complex human societies, "Altruistic
Optimizers" are better suited for modeling idealized pro-social actors or
scenarios where collective welfare is the primary consideration.
\\ ( https://arxiv.org/abs/2509.22537 ,  1111kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22558
Date: Fri, 26 Sep 2025 16:39:10 GMT   (1603kb)

Title: StepORLM: A Self-Evolving Framework With Generative Process Supervision
  For Operations Research Language Models
Authors: Chenyu Zhou, Tianyi Xu, Jianghao Lin and Dongdong Ge
Categories: cs.AI
\\
  Large Language Models (LLMs) have shown promising capabilities for solving
Operations Research (OR) problems. While reinforcement learning serves as a
powerful paradigm for LLM training on OR problems, existing works generally
face two key limitations. First, outcome reward suffers from the credit
assignment problem, where correct final answers can reinforce flawed reasoning.
Second, conventional discriminative process supervision is myopic, failing to
evaluate the interdependent steps of OR modeling holistically. To this end, we
introduce StepORLM, a novel self-evolving framework with generative process
supervision. At its core, StepORLM features a co-evolutionary loop where a
policy model and a generative process reward model (GenPRM) iteratively improve
on each other. This loop is driven by a dual-feedback mechanism: definitive,
outcome-based verification from an external solver, and nuanced, holistic
process evaluation from the GenPRM. The combined signal is used to align the
policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously
refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new
state-of-the-art across six benchmarks, significantly outperforming vastly
larger generalist models, agentic methods, and specialized baselines. Moreover,
the co-evolved GenPRM is able to act as a powerful and universally applicable
process verifier, substantially boosting the inference scaling performance of
both our own model and other existing LLMs.
\\ ( https://arxiv.org/abs/2509.22558 ,  1603kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22570
Date: Fri, 26 Sep 2025 16:46:12 GMT   (4733kb)

Title: UniMIC: Token-Based Multimodal Interactive Coding for Human-AI
  Collaboration
Authors: Qi Mao, Tinghan Yang, Jiahao Li, Bin Li, Libiao Jin, Yan Lu
Categories: cs.AI
\\
  The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI
agents is transforming human-AI collaboration into bidirectional, multimodal
interaction. However, existing codecs remain optimized for unimodal, one-way
communication, resulting in repeated degradation under conventional
compress-transmit-reconstruct pipelines. To address this limitation, we propose
UniMIC, a Unified token-based Multimodal Interactive Coding framework that
bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or
plain text, UniMIC employs compact tokenized representations as the
communication medium, enabling efficient low-bitrate transmission while
maintaining compatibility with LMMs. To further enhance compression,
lightweight Transformer-based entropy models with scenario-specific
designs-generic, masked, and text-conditioned-effectively minimize inter-token
redundancy. Extensive experiments on text-to-image generation, text-guided
inpainting, outpainting, and visual question answering show that UniMIC
achieves substantial bitrate savings and remains robust even at ultra-low
bitrates (<0.05bpp), without compromising downstream task performance. These
results establish UniMIC as a practical and forward-looking paradigm for
next-generation multimodal interactive communication.
\\ ( https://arxiv.org/abs/2509.22570 ,  4733kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22572
Date: Fri, 26 Sep 2025 16:49:10 GMT   (410kb)

Title: Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs
  at Test Time
Authors: Yixuan Han, Fan Ma, Ruijie Quan, Yi Yang
Categories: cs.AI cs.CL cs.LG
\\
  Test-Time Scaling (TTS) enhances the reasoning ability of large language
models (LLMs) by allocating additional computation during inference. However,
existing approaches primarily rely on output-level sampling while overlooking
the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we
observe that varying the number of activated experts yields complementary
solution sets with stable accuracy, revealing a new and underexplored source of
diversity. Motivated by this observation, we propose Dynamic Experts Search
(DES), a TTS strategy that elevates expert activation into a controllable
dimension of the search space. DES integrates two key components: (1) Dynamic
MoE, which enables direct control of expert counts during inference to generate
diverse reasoning trajectories without additional cost; and (2) Expert
Configuration Inheritance, which preserves consistent expert counts within a
reasoning path while varying them across runs, thereby balancing stability and
diversity throughout the search. Extensive experiments across MoE
architectures, verifiers and reasoning benchmarks (i.e., math, code and
knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing
accuracy and stability without additional cost. These results highlight DES as
a practical and scalable form of architecture-aware TTS, illustrating how
structural flexibility in modern LLMs can advance reasoning.
\\ ( https://arxiv.org/abs/2509.22572 ,  410kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22613
Date: Fri, 26 Sep 2025 17:39:48 GMT   (13035kb)

Title: Benefits and Pitfalls of Reinforcement Learning for Language Model
  Planning: A Theoretical Perspective
Authors: Siwei Wang, Yifei Shen, Haoran Sun, Shi Feng, Shang-Hua Teng, Li Dong,
  Yaru Hao, Wei Chen
Categories: cs.AI cs.LG stat.ML
\\
  Recent reinforcement learning (RL) methods have substantially enhanced the
planning capabilities of Large Language Models (LLMs), yet the theoretical
basis for their effectiveness remains elusive. In this work, we investigate
RL's benefits and limitations through a tractable graph-based abstraction,
focusing on policy gradient (PG) and Q-learning methods. Our theoretical
analyses reveal that supervised fine-tuning (SFT) may introduce
co-occurrence-based spurious solutions, whereas RL achieves correct planning
primarily through exploration, underscoring exploration's role in enabling
better generalization. However, we also show that PG suffers from diversity
collapse, where output diversity decreases during training and persists even
after perfect accuracy is attained. By contrast, Q-learning provides two key
advantages: off-policy learning and diversity preservation at convergence. We
further demonstrate that careful reward design is necessary to prevent reward
hacking in Q-learning. Finally, applying our framework to the real-world
planning benchmark Blocksworld, we confirm that these behaviors manifest in
practice.
\\ ( https://arxiv.org/abs/2509.22613 ,  13035kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21357
Date: Sat, 20 Sep 2025 06:48:22 GMT   (1525kb)

Title: A Novel Differential Feature Learning for Effective Hallucination
  Detection and Classification
Authors: Wenkai Wang, Vincent Lee, Yizhen Zheng
Categories: cs.CL cs.AI
Comments: 10 pages, 7 figures, 13 tables
\\
  Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.
\\ ( https://arxiv.org/abs/2509.21357 ,  1525kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21359
Date: Sun, 21 Sep 2025 07:19:09 GMT   (1026kb)

Title: Influence Guided Context Selection for Effective Retrieval-Augmented
  Generation
Authors: Jiale Deng, Yanyan Shen, Ziyuan Pei, Youmin Chen, Linpeng Huang
Categories: cs.CL cs.AI
\\
  Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.
\\ ( https://arxiv.org/abs/2509.21359 ,  1026kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21361
Date: Sun, 21 Sep 2025 14:38:17 GMT   (1200kb)

Title: Context Is What You Need: The Maximum Effective Context Window for Real
  World Limits of LLMs
Authors: Norman Paulsen
Categories: cs.CL cs.AI
Comments: 20 pages, 4 charts
ACM-class: I.2.7
\\
  Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.
\\ ( https://arxiv.org/abs/2509.21361 ,  1200kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21404
Date: Wed, 24 Sep 2025 15:51:42 GMT   (10kb)

Title: How Large Language Models Need Symbolism
Authors: Xiaotie Deng, Hanyu Li
Categories: cs.CL cs.AI
Journal-ref: National Science Review, Volume 12, Issue 10, October 2025,
  nwaf339
DOI: 10.1093/nsr/nwaf339
\\
  We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.
\\ ( https://arxiv.org/abs/2509.21404 ,  10kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21443
Date: Thu, 25 Sep 2025 19:14:17 GMT   (2573kb)

Title: One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in
  Computational Moral Reasoning
Authors: Sualeha Farid, Jayden Lin, Zean Chen, Shivani Kumar, David Jurgens
Categories: cs.CL cs.AI
Comments: 22 pages, 11 figures, 6 tables
\\
  Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.
\\ ( https://arxiv.org/abs/2509.21443 ,  2573kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21450
Date: Thu, 25 Sep 2025 19:22:04 GMT   (129kb)

Title: LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and
  Challenges with GPT-5
Authors: Gaurav Kumar Gupta, Nirajan Acharya, Pranal Pande
Categories: cs.CL
\\
  Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.
\\ ( https://arxiv.org/abs/2509.21450 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21456
Date: Thu, 25 Sep 2025 19:26:00 GMT   (790kb)

Title: Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on
  Gender Stereotypes
Authors: Guangliang Liu, Bocheng Chen, Xitong Zhang, Kristen Marie Johnson
Categories: cs.CL
\\
  Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.
\\ ( https://arxiv.org/abs/2509.21456 ,  790kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21459
Date: Thu, 25 Sep 2025 19:27:35 GMT   (124kb)

Title: A State-of-the-Art SQL Reasoning Model using RLVR
Authors: Alnur Ali, Ashutosh Baheti, Jonathan Chang, Ta-Chung Chi, Brandon Cui,
  Andrew Drozdov, Jonathan Frankle, Abhay Gupta, Pallavi Koppol, Sean Kulinski,
  Jonathan Li, Dipendra Misra, Krista Opsahl-Ong, Jose Javier Gonzalez Ortiz,
  Matei Zaharia, and Yue Zhang
Categories: cs.CL cs.AI cs.DB cs.LG
\\
  Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.
\\ ( https://arxiv.org/abs/2509.21459 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21482
Date: Thu, 25 Sep 2025 19:44:24 GMT   (7069kb)

Title: Learning to Reason with Mixture of Tokens
Authors: Adit Jain, Brendan Rappazzo
Categories: cs.CL cs.AI cs.LG
Comments: 30 page
\\
  Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.
\\ ( https://arxiv.org/abs/2509.21482 ,  7069kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21487
Date: Thu, 25 Sep 2025 19:47:19 GMT   (689kb)

Title: Dual-Head Reasoning Distillation: Improving Classifier Accuracy with
  Train-Time-Only Reasoning
Authors: Jillian Xu, Dylan Zhou, Vinay Shukla, Yang Yang, Junrui Ruan, Shuhuai
  Lin, Wenfei Zou, Yinxiao Liu, Karthik Lakshmanan
Categories: cs.CL cs.AI
Comments: Accepted by the Workshop on Efficient Reasoning, Neurips 2025, 5
  pages
\\
  Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.
\\ ( https://arxiv.org/abs/2509.21487 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21499
Date: Thu, 25 Sep 2025 19:57:36 GMT   (663kb)

Title: On Code-Induced Reasoning in LLMs
Authors: Abdul Waheed, Zhen Wu, Carolyn Ros\'e, Daphne Ippolito
Categories: cs.CL cs.PL
\\
  Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.
\\ ( https://arxiv.org/abs/2509.21499 ,  663kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21535
Date: Thu, 25 Sep 2025 20:22:09 GMT   (823kb)

Title: Agribot: agriculture-specific question answer system
Authors: Naman Jain, Pranjali Jain, Pratik Kayal, Jayakrishna Sahit, Soham
  Pachpande, Jayesh Choudhari
Categories: cs.CL cs.AI cs.LG
\\
  India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.
\\ ( https://arxiv.org/abs/2509.21535 ,  823kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21554
Date: Thu, 25 Sep 2025 20:38:51 GMT   (18kb)

Title: Domain-Aware Speaker Diarization On African-Accented English
Authors: Chibuzor Okocha, Kelechi Ezema, Christan Grant
Categories: cs.CL cs.AI cs.LG
Comments: 5 pages
\\
  This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.
\\ ( https://arxiv.org/abs/2509.21554 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21557
Date: Thu, 25 Sep 2025 20:39:26 GMT   (165kb)

Title: Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM
  Attribution
Authors: Yash Saxena, Raviteja Bommireddy, Ankur Padia, Manas Gaur
Categories: cs.CL
Comments: Accepted at NeurIPS 2025 LLM Evaluation Workshop
\\
  Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/
\\ ( https://arxiv.org/abs/2509.21557 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21562
Date: Thu, 25 Sep 2025 20:42:30 GMT   (453kb)

Title: Comparative Personalization for Multi-document Summarization
Authors: Haoyuan Li, Snigdha Chaturvedi
Categories: cs.CL
\\
  Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.
\\ ( https://arxiv.org/abs/2509.21562 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21576
Date: Thu, 25 Sep 2025 20:55:11 GMT   (1525kb)

Title: Vision Language Models Cannot Plan, but Can They Formalize?
Authors: Muyu He, Yuxi Zheng, Yuchen Liu, Zijian An, Bill Cai, Jiani Huang,
  Lifeng Zhou, Feng Liu, Ziyang Li, Li Zhang
Categories: cs.CL
\\
  The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.
\\ ( https://arxiv.org/abs/2509.21576 ,  1525kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21577
Date: Thu, 25 Sep 2025 20:55:36 GMT   (329kb)

Title: "Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM
  Translations
Authors: Madison Van Doren, Cory Holland
Categories: cs.CL
\\
  This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.
\\ ( https://arxiv.org/abs/2509.21577 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21613
Date: Thu, 25 Sep 2025 21:29:08 GMT   (90kb)

Title: Multi-Objective Reinforcement Learning for Large Language Model
  Optimization: Visionary Perspective
Authors: Lingxiao Kong, Cong Yang, Oya Deniz Beyan and Zeyd Boukhers
Categories: cs.CL cs.AI cs.LG cs.MA
Comments: 3 pages, 1 figure, accepted by ECAI MODeM 2025
\\
  Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.
\\ ( https://arxiv.org/abs/2509.21613 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21623
Date: Thu, 25 Sep 2025 21:42:27 GMT   (577kb)

Title: OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's
  Rule
Authors: Yuxuan Zhu, David H. Yang, Mohammad Mohammadi Amiri, Keerthiram
  Murugesan, Tejaswini Pedapati, Pin-Yu Chen
Categories: cs.CL cs.AI cs.LG
\\
  The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.
\\ ( https://arxiv.org/abs/2509.21623 ,  577kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21631
Date: Thu, 25 Sep 2025 21:47:39 GMT   (325kb)

Title: Towards Transparent AI: A Survey on Explainable Language Models
Authors: Avash Palikhe, Zichong Wang, Zhipeng Yin, Rui Guo, Qiang Duan, Jie
  Yang, Wenbin Zhang
Categories: cs.CL
\\
  Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.
\\ ( https://arxiv.org/abs/2509.21631 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21679
Date: Thu, 25 Sep 2025 22:55:05 GMT   (243kb)

Title: ReviewScore: Misinformed Peer Review Detection with Large Language
  Models
Authors: Hyun Ryu, Doohyuk Jang, Hyemin S. Lee, Joonhyun Jeong, Gyeongman Kim,
  Donghyeon Cho, Gyouk Chu, Minyeong Hwang, Hyeongwon Jang, Changhun Kim,
  Haechan Kim, Jina Kim, Joowon Kim, Yoonjeon Kim, Kwanhyung Lee, Chanjae Park,
  Heecheol Yun, Gregor Betz, Eunho Yang
Categories: cs.CL
\\
  Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.
\\ ( https://arxiv.org/abs/2509.21679 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21698
Date: Thu, 25 Sep 2025 23:48:33 GMT   (784kb)

Title: GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic
  Discovery in Financial Disclosures
Authors: Ying Li and Tiejun Ma
Categories: cs.CL
Comments: 39th Conference on Neural Information Processing Systems (NeurIPS
  2025) Workshop: NeurIPS 2025 Workshop on Generative AI in Finance
\\
  Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.
\\ ( https://arxiv.org/abs/2509.21698 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21710
Date: Fri, 26 Sep 2025 00:13:10 GMT   (2826kb)

Title: Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on
  Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval
Authors: Xiaojun Wu, Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang,
  Yuanliang Sun, Hui Xiong, Jia Li, Jian Guo
Categories: cs.CL
Comments: 28 pages, 17 figures
\\
  Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.
\\ ( https://arxiv.org/abs/2509.21710 ,  2826kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21730
Date: Fri, 26 Sep 2025 00:57:27 GMT   (3676kb)

Title: ProPerSim: Developing Proactive and Personalized AI Assistants through
  User-Assistant Simulation
Authors: Jiho Kim, Junseong Choi, Woosog Chay, Daeun Kyung, Yeonsu Kwon, Yohan
  Jo, Edward Choi
Categories: cs.CL
\\
  As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.
\\ ( https://arxiv.org/abs/2509.21730 ,  3676kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21732
Date: Fri, 26 Sep 2025 00:58:01 GMT   (103kb)

Title: How Accurate Are LLMs at Multi-Question Answering on Conversational
  Transcripts?
Authors: Xiliang Zhu, Shi Zong, David Rossouw
Categories: cs.CL
Comments: Accepted by EMNLP 2025 Industry Track
\\
  Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.
\\ ( https://arxiv.org/abs/2509.21732 ,  103kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21740
Date: Fri, 26 Sep 2025 01:13:37 GMT   (107kb)

Title: Self-Speculative Biased Decoding for Faster Live Translation
Authors: Linxiao Zeng, Haoyun Deng, Kangyuan Shu, Shizhen Wang
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.
\\ ( https://arxiv.org/abs/2509.21740 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21749
Date: Fri, 26 Sep 2025 01:27:59 GMT   (1254kb)

Title: Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning
  in Large Audio-Language Models
Authors: Zhen Xiong, Yujun Cai, Zhecheng Li, Junsong Yuan, Yiwei Wang
Categories: cs.CL cs.SD
\\
  Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.
\\ ( https://arxiv.org/abs/2509.21749 ,  1254kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21777
Date: Fri, 26 Sep 2025 02:27:04 GMT   (355kb)

Title: SynerGen: Contextualized Generative Recommender for Unified Search and
  Recommendation
Authors: Vianne R. Gao, Chen Xue, Marc Versage, Xie Zhou, Zhongruo Wang, Chao
  Li, Yeon Seonwoo, Nan Chen, Zhen Ge, Gourab Kundu, Weiqi Zhang, Tian Wang,
  Qingjun Cui, Trishul Chilimbi
Categories: cs.CL
Comments: Generative Recommender, Recommendation System, Information Retrieval
\\
  The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.
\\ ( https://arxiv.org/abs/2509.21777 ,  355kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21791
Date: Fri, 26 Sep 2025 02:47:11 GMT   (2186kb)

Title: Navigating the Impact of Structured Output Format on Large Language
  Models through the Compass of Causal Inference
Authors: Han Yuan and Yue Zhao and Li Zhang and Wuqiong Luo and Zheng Ma
Categories: cs.CL cs.LG
\\
  Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.
\\ ( https://arxiv.org/abs/2509.21791 ,  2186kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21798
Date: Fri, 26 Sep 2025 02:56:06 GMT   (796kb)

Title: Evaluating and Improving Cultural Awareness of Reward Models for LLM
  Alignment
Authors: Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang
Categories: cs.CL cs.AI
Comments: Under review on ICLR 2026;Work in progress;
\\
  Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.
\\ ( https://arxiv.org/abs/2509.21798 ,  796kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21801
Date: Fri, 26 Sep 2025 02:57:36 GMT   (2077kb)

Title: Redefining Machine Simultaneous Interpretation: From Incremental
  Translation to Human-Like Strategies
Authors: Qianen Zhang and Satoshi Nakamura
Categories: cs.CL
\\
  Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.
\\ ( https://arxiv.org/abs/2509.21801 ,  2077kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21805
Date: Fri, 26 Sep 2025 03:04:23 GMT   (270kb)

Title: Towards Minimal Causal Representations for Human Multimodal Language
  Understanding
Authors: Menghua Jiang, Yuncheng Jiang, Haifeng Hu, Sijie Mai
Categories: cs.CL
\\
  Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.
\\ ( https://arxiv.org/abs/2509.21805 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21820
Date: Fri, 26 Sep 2025 03:26:28 GMT   (6147kb)

Title: Can LLMs Solve and Generate Linguistic Olympiad Puzzles?
Authors: Neh Majmudar and Elena Filatova
Categories: cs.CL
Comments: To be published in the Proceedings of Main Conference of the 2025
  Conference on Empirical Methods in Natural Language Processing (EMNLP 2025)
\\
  In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.
\\ ( https://arxiv.org/abs/2509.21820 ,  6147kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21826
Date: Fri, 26 Sep 2025 03:38:27 GMT   (1098kb)

Title: ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language
  Models
Authors: Zihan Lin, Xiaohan Wang, Jie Cao, Jiajun Chai, Guojun Yin, Wei Lin,
  Ran He
Categories: cs.CL
\\
  Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.
\\ ( https://arxiv.org/abs/2509.21826 ,  1098kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21837
Date: Fri, 26 Sep 2025 03:51:28 GMT   (2680kb)

Title: Semantic Agreement Enables Efficient Open-Ended LLM Cascades
Authors: Duncan Soiffer, Steven Kolawole, and Virginia Smith
Categories: cs.CL
Comments: EMNLP 2025 Industry Track
\\
  Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.
\\ ( https://arxiv.org/abs/2509.21837 ,  2680kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21849
Date: Fri, 26 Sep 2025 04:20:37 GMT   (133kb)

Title: Following the TRACE: A Structured Path to Empathetic Response Generation
  with Multi-Agent Models
Authors: Ziqi Liu, Ziyang Zhou, Yilin Li, Haiyang Zhang, Yangbin Chen
Categories: cs.CL cs.MA
\\
  Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.
\\ ( https://arxiv.org/abs/2509.21849 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21856
Date: Fri, 26 Sep 2025 04:32:29 GMT   (1610kb)

Title: KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question
  Answering in Multi-Turn Dialogues
Authors: Junhao Chen, Yu Huang, Siyuan Li, Rui Yao, Hanqian Li, Hanyu Zhang,
  Jungang Li, Jian Chen, Bowen Wang, Xuming Hu
Categories: cs.CL
\\
  Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.
\\ ( https://arxiv.org/abs/2509.21856 ,  1610kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21870
Date: Fri, 26 Sep 2025 04:54:02 GMT   (424kb)

Title: Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations
Authors: Guanzhi Deng, Mingyang Liu, Dapeng Wu, Yinqiao Li, Linqi Song
Categories: cs.CL cs.AI
Comments: This manuscript has been submitted to IEEE Journal of Selected Topics
  in Signal Processing (JSTSP) for review. Until the moment I submitted the
  manuscript to arXiv, we haven't received any review comments from JSTSP
\\
  Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.
\\ ( https://arxiv.org/abs/2509.21870 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21875
Date: Fri, 26 Sep 2025 04:57:46 GMT   (780kb)

Title: LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge
  Signals
Authors: Min-Hsuan Yeh, Yixuan Li, Tanwi Mallick
Categories: cs.CL
\\
  Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.
\\ ( https://arxiv.org/abs/2509.21875 ,  780kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21880
Date: Fri, 26 Sep 2025 05:03:54 GMT   (282kb)

Title: No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM
  Reinforcement Learning via Entropy-Guided Advantage Shaping
Authors: Thanh-Long V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang
Categories: cs.CL cs.AI cs.LG
\\
  Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.
\\ ( https://arxiv.org/abs/2509.21880 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21889
Date: Fri, 26 Sep 2025 05:26:52 GMT   (1936kb)

Title: QoNext: Towards Next-generation QoE for Foundation Models
Authors: Yijin Guo, Ye Shen, Farong Wen, Junying Wang, Zicheng Zhang, Qi Jia,
  Guangtao Zhai
Categories: cs.CL
\\
  Existing evaluations of foundation models, including recent human-centric
approaches, fail to capture what truly matters: user's experience during
interaction. Current methods treat evaluation as a matter of output correctness
alone, overlooking that user satisfaction emerges from the interplay between
response quality and interaction, which limits their ability to account for the
mechanisms underlying user experience. To address this gap, we introduce
QoNext, the first framework that adapts Quality of Experience (QoE) principles
from networking and multimedia to the assessment of foundation models. QoNext
identifies experiential factors that shape user experience and incorporates
them into controlled experiments, where human ratings are collected under
varied configurations. From these studies we construct a QoE-oriented database
and train predictive models that estimate perceived user experience from
measurable system parameters. Our results demonstrate that QoNext not only
enables proactive and fine-grained evaluation but also provides actionable
guidance for productized services of optimizing foundation models in practice.
\\ ( https://arxiv.org/abs/2509.21889 ,  1936kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21892
Date: Fri, 26 Sep 2025 05:29:19 GMT   (2083kb)

Title: Elastic MoE: Unlocking the Inference-Time Scalability of
  Mixture-of-Experts
Authors: Naibin Gu, Zhenyu Zhang, Yuchen Feng, Yilong Chen, Peng Fu, Zheng Lin,
  Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang
Categories: cs.CL cs.AI cs.LG
\\
  Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.
\\ ( https://arxiv.org/abs/2509.21892 ,  2083kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21907
Date: Fri, 26 Sep 2025 05:44:04 GMT   (385kb)

Title: A Large-Scale Dataset and Citation Intent Classification in Turkish with
  LLMs
Authors: Kemal Sami Karaca, Bahaeddin Eravc{\i}
Categories: cs.CL cs.AI
Comments: Submitted to IEEE UBMK 2025 International Conference on Computer
  Science and Engineering
\\
  Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.
\\ ( https://arxiv.org/abs/2509.21907 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21910
Date: Fri, 26 Sep 2025 05:45:14 GMT   (239kb)

Title: AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language
  Models via Structured Component Recognition
Authors: Yun Wang, Zhaojun Ding, Xuansheng Wu, Siyue Sun, Ninghao Liu, Xiaoming
  Zhai
Categories: cs.CL cs.AI
Comments: 9 pages, 2 figures
\\
  Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.
\\ ( https://arxiv.org/abs/2509.21910 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21932
Date: Fri, 26 Sep 2025 06:18:10 GMT   (176kb)

Title: SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech
  Translation
Authors: Haotian Tan, Hiroki Ouchi, Sakriani Sakti
Categories: cs.CL
Comments: \c{opyright} 2026 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\
  How to make human-interpreter-like read/write decisions for simultaneous
speech translation (SimulST) systems? Current state-of-the-art systems
formulate SimulST as a multi-turn dialogue task, requiring specialized
interleaved training data and relying on computationally expensive large
language model (LLM) inference for decision-making. In this paper, we propose
SimulSense, a novel framework for SimulST that mimics human interpreters by
continuously reading input speech and triggering write decisions to produce
translation when a new sense unit is perceived. Experiments against two
state-of-the-art baseline systems demonstrate that our proposed method achieves
a superior quality-latency tradeoff and substantially improved real-time
efficiency, where its decision-making is up to 9.6x faster than the baselines.
\\ ( https://arxiv.org/abs/2509.21932 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21933
Date: Fri, 26 Sep 2025 06:18:15 GMT   (4163kb)

Title: Why Chain of Thought Fails in Clinical Text Understanding
Authors: Jiageng Wu, Kevin Xie, Bowen Gu, Nils Kr\"uger, Kueiyu Joshua Lin, Jie
  Yang
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.
\\ ( https://arxiv.org/abs/2509.21933 ,  4163kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21946
Date: Fri, 26 Sep 2025 06:26:21 GMT   (1136kb)

Title: Debiasing Large Language Models in Thai Political Stance Detection via
  Counterfactual Calibration
Authors: Kasidit Sermsri, Teerapong Panboonyuen
Categories: cs.CL cs.AI
Comments: 9 pages
\\
  Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.
\\ ( https://arxiv.org/abs/2509.21946 ,  1136kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21978
Date: Fri, 26 Sep 2025 07:02:05 GMT   (752kb)

Title: MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic
  Dialogue for Enhanced LLM Ideation
Authors: Xinping Lei, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao
Categories: cs.CL
Comments: EMNLP2025 Findings
\\
  Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.
\\ ( https://arxiv.org/abs/2509.21978 ,  752kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21999
Date: Fri, 26 Sep 2025 07:26:16 GMT   (717kb)

Title: Black-Box Hallucination Detection via Consistency Under the Uncertain
  Expression
Authors: Seongho Joo, Kyungmin Min, Jahyun Koo, Kyomin Jung
Categories: cs.CL cs.AI
\\
  Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.
\\ ( https://arxiv.org/abs/2509.21999 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22009
Date: Fri, 26 Sep 2025 07:45:56 GMT   (871kb)

Title: GraphSearch: An Agentic Deep Searching Workflow for Graph
  Retrieval-Augmented Generation
Authors: Cehao Yang, Xiaojun Wu, Xueyuan Lin, Chengjin Xu, Xuhui Jiang,
  Yuanliang Sun, Jia Li, Hui Xiong, Jian Guo
Categories: cs.CL
\\
  Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.
\\ ( https://arxiv.org/abs/2509.22009 ,  871kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22030
Date: Fri, 26 Sep 2025 08:07:02 GMT   (8303kb)

Title: From Outliers to Topics in Language Models: Anticipating Trends in News
  Corpora
Authors: Evangelia Zve, Benjamin Icard, Alice Breton, Lila Sainero, Gauvain
  Bourgne, and Jean-Gabriel Ganascia
Categories: cs.CL
Comments: presented at ICNLSP 2025; to appear in the ACL Anthology; received
  the Best Full Paper Award
\\
  This paper examines how outliers, often dismissed as noise in topic modeling,
can act as weak signals of emerging topics in dynamic news corpora. Using
vector embeddings from state-of-the-art language models and a cumulative
clustering approach, we track their evolution over time in French and English
news datasets focused on corporate social responsibility and climate change.
The results reveal a consistent pattern: outliers tend to evolve into coherent
topics over time across both models and languages.
\\ ( https://arxiv.org/abs/2509.22030 ,  8303kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22041
Date: Fri, 26 Sep 2025 08:22:59 GMT   (8219kb)

Title: Taxonomy of Comprehensive Safety for Clinical Agents
Authors: Jean Seo, Hyunkyung Lee, Gibaeg Kim, Wooseok Han, Jaehyo Yoo,
  Seungseop Lim, Kihun Shin, Eunho Yang
Categories: cs.CL
Comments: EMNLP 2025 Industry
\\
  Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.
\\ ( https://arxiv.org/abs/2509.22041 ,  8219kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22054
Date: Fri, 26 Sep 2025 08:36:38 GMT   (929kb)

Title: Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from
  Fuzziness to Clarity
Authors: Ping Chen, Xiang Liu, Zhaoxiang Liu, Zezhou Chen, Xingpeng Zhang, Huan
  Hu, Zipeng Wang, Kai Wang, Shuming Shi, Shiguo Lian
Categories: cs.CL cs.AI
Comments: Accepet by EMNLP 2025 Findings (11 pages, 1 figures)
\\
  With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.
\\ ( https://arxiv.org/abs/2509.22054 ,  929kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22055
Date: Fri, 26 Sep 2025 08:36:45 GMT   (1281kb)

Title: RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated
  Text in Social Media
Authors: Yudong Li, Yufei Sun, Yuhan Yao, Peiru Yang, Wanyue Li, Jiajun Zou,
  Yongfeng Huang, Linlin Shen
Categories: cs.CL
\\
  The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.
\\ ( https://arxiv.org/abs/2509.22055 ,  1281kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22064
Date: Fri, 26 Sep 2025 08:49:03 GMT   (7932kb)

Title: The QCET Taxonomy of Standard Quality Criterion Names and Definitions
  for the Evaluation of NLP Systems
Authors: Anya Belz, Simon Mille, Craig Thomson
Categories: cs.CL cs.AI
Comments: 39 pages, 7 figures
ACM-class: I.2.m
\\
  Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.
\\ ( https://arxiv.org/abs/2509.22064 ,  7932kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22072
Date: Fri, 26 Sep 2025 08:53:13 GMT   (526kb)

Title: Fine-tuning Done Right in Model Editing
Authors: Wanli Yang, Fei Sun, Rui Tang, Hongyu Zang, Du Su, Qi Cao, Jingang
  Wang, Huawei Shen, Xueqi Cheng
Categories: cs.CL
\\
  Fine-tuning, a foundational method for adapting large language models, has
long been considered ineffective for model editing. Here, we challenge this
belief, arguing that the reported failure arises not from the inherent
limitation of fine-tuning itself, but from adapting it to the sequential nature
of the editing task, a single-pass depth-first pipeline that optimizes each
sample to convergence before moving on. While intuitive, this depth-first
pipeline coupled with sample-wise updating over-optimizes each edit and induces
interference across edits. Our controlled experiments reveal that simply
restoring fine-tuning to the standard breadth-first (i.e., epoch-based)
pipeline with mini-batch optimization substantially improves its effectiveness
for model editing. Moreover, fine-tuning in editing also suffers from
suboptimal tuning parameter locations inherited from prior methods. Through
systematic analysis of tuning locations, we derive LocFT-BF, a simple and
effective localized editing method built on the restored fine-tuning framework.
Extensive experiments across diverse LLMs and datasets demonstrate that
LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our
knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x
beyond prior practice, without sacrificing general capabilities. By clarifying
a long-standing misconception and introducing a principled localized tuning
strategy, we advance fine-tuning from an underestimated baseline to a leading
method for model editing, establishing a solid foundation for future research.
\\ ( https://arxiv.org/abs/2509.22072 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22075
Date: Fri, 26 Sep 2025 08:55:09 GMT   (3874kb)

Title: COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary
  Learning
Authors: Dmitriy Shopkhoev, Denis Makhov, Magauiya Zhussip, Ammar Ali,
  Stamatios Lefkimmiatis
Categories: cs.CL
\\
  Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.
\\ ( https://arxiv.org/abs/2509.22075 ,  3874kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22086
Date: Fri, 26 Sep 2025 09:09:08 GMT   (9423kb)

Title: Multilingual Dialogue Generation and Localization with Dialogue Act
  Scripting
Authors: Justin Vasselli, Eunike Andriani Kardinata, Yusuke Sakai, Taro
  Watanabe
Categories: cs.CL
Comments: 16 pages, 10 tables, 2 figures, Accepted at EMNLP Main 2025
\\
  Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.
\\ ( https://arxiv.org/abs/2509.22086 ,  9423kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22099
Date: Fri, 26 Sep 2025 09:21:17 GMT   (211kb)

Title: S2J: Bridging the Gap Between Solving and Judging Ability in Generative
  Reward Models
Authors: Shaoning Sun, Jiachen Yu, Zongqi Wang, Xuewei Yang, Tianle Gu, Yujiu
  Yang
Categories: cs.CL
\\
  With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.
\\ ( https://arxiv.org/abs/2509.22099 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22101
Date: Fri, 26 Sep 2025 09:23:35 GMT   (1525kb)

Title: Think Right, Not More: Test-Time Scaling for Numerical Claim
  Verification
Authors: Primakov Chungkham, V Venktesh, Vinay Setty, Avishek Anand
Categories: cs.CL
Comments: Accepted to EMNLP 2025, 19 pages
\\
  Fact-checking real-world claims, particularly numerical claims, is inherently
complex that require multistep reasoning and numerical reasoning for verifying
diverse aspects of the claim. Although large language models (LLMs) including
reasoning models have made tremendous advances, they still fall short on
fact-checking real-world claims that require a combination of compositional and
numerical reasoning. They are unable to understand nuance of numerical aspects,
and are also susceptible to the reasoning drift issue, where the model is
unable to contextualize diverse information resulting in misinterpretation and
backtracking of reasoning process. In this work, we systematically explore
scaling test-time compute (TTS) for LLMs on the task of fact-checking complex
numerical claims, which entails eliciting multiple reasoning paths from an LLM.
We train a verifier model (VERIFIERFC) to navigate this space of possible
reasoning paths and select one that could lead to the correct verdict. We
observe that TTS helps mitigate the reasoning drift issue, leading to
significant performance gains for fact-checking numerical claims. To improve
compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS
selectively based on the perceived complexity of the claim. This approach
achieves 1.8x higher efficiency than standard TTS, while delivering a notable
18.8% performance improvement over single-shot claim verification methods. Our
code and data can be found at https://github.com/VenkteshV/VerifierFC
\\ ( https://arxiv.org/abs/2509.22101 ,  1525kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22119
Date: Fri, 26 Sep 2025 09:42:20 GMT   (640kb)

Title: Universal Legal Article Prediction via Tight Collaboration between
  Supervised Classification Model and LLM
Authors: Xiao Chi, Wenlin Zhong, Yiquan Wu, Wei Wang, Kun Kuang, Fei Wu,
  Minghui Xiong
Categories: cs.CL cs.AI
Comments: 10 pages, 6 figures, Accepted to ICAIL 2025 (International Conference
  on Artificial Intelligence and Law)
DOI: 10.1145/3769126.3769221
\\
  Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.
\\ ( https://arxiv.org/abs/2509.22119 ,  640kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22123
Date: Fri, 26 Sep 2025 09:46:13 GMT   (9186kb)

Title: Multilingual Vision-Language Models, A Survey
Authors: Andrei-Alexandru Manea, Jind\v{r}ich Libovick\'y
Categories: cs.CL
\\
  This survey examines multilingual vision-language models that process text
and images across languages. We review 31 models and 21 benchmarks, spanning
encoder-only and generative architectures, and identify a key tension between
language neutrality (consistent cross-lingual representations) and cultural
awareness (adaptation to cultural contexts). Current training methods favor
neutrality through contrastive learning, while cultural awareness depends on
diverse data. Two-thirds of evaluation benchmarks use translation-based
approaches prioritizing semantic consistency, though recent work incorporates
culturally grounded content. We find discrepancies in cross-lingual
capabilities and gaps between training objectives and evaluation goals.
\\ ( https://arxiv.org/abs/2509.22123 ,  9186kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22125
Date: Fri, 26 Sep 2025 09:47:35 GMT   (673kb)

Title: FoodSEM: Large Language Model Specialized in Food Named-Entity Linking
Authors: Ana Gjorgjevikj and Matej Martinc and Gjorgjina Cenikj and Sa\v{s}o
  D\v{z}eroski and Barbara Korou\v{s}i\'c Seljak and Tome Eftimov
Categories: cs.CL cs.IR
Comments: To appear in the Proceedings of the 28th International Conference on
  Discovery Science (DS 2025)
\\
  This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.
\\ ( https://arxiv.org/abs/2509.22125 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22131
Date: Fri, 26 Sep 2025 09:53:41 GMT   (712kb)

Title: R-Capsule: Compressing High-Level Plans for Efficient Large Language
  Model Reasoning
Authors: Hongyu Shan, Mingyang Song, Chang Dai, Di Liang, Han Chen
Categories: cs.CL cs.AI
\\
  Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0
\\ ( https://arxiv.org/abs/2509.22131 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22134
Date: Fri, 26 Sep 2025 09:55:35 GMT   (179kb)

Title: Bridging Draft Policy Misalignment: Group Tree Optimization for
  Speculative Decoding
Authors: Shijing Hu, Jingyang Li, Zhihui Lu and Pan Zhou
Categories: cs.CL cs.AI
\\
  Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.
\\ ( https://arxiv.org/abs/2509.22134 ,  179kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22141
Date: Fri, 26 Sep 2025 10:01:24 GMT   (412kb)

Title: NFDI4DS Shared Tasks for Scholarly Document Processing
Authors: Raia Abu Ahmad, Rana Abdulla, Tilahun Abedissa Taffa, Soeren Auer,
  Hamed Babaei Giglou, Ekaterina Borisova, Zongxiong Chen, Stefan Dietze,
  Jennifer DSouza, Mayra Elwes, Genet-Asefa Gesese, Shufan Jiang, Ekaterina
  Kutafina, Philipp Mayr, Georg Rehm, Sameer Sadruddin, Sonja Schimmler, Daniel
  Schneider, Kanishka Silva, Sharmila Upadhyaya, Ricardo Usbeck
Categories: cs.CL
Comments: Accepted at the RDI4DS 2025 Workshop
\\
  Shared tasks are powerful tools for advancing research through
community-based standardised evaluation. As such, they play a key role in
promoting findable, accessible, interoperable, and reusable (FAIR), as well as
transparent and reproducible research practices. This paper presents an updated
overview of twelve shared tasks developed and hosted under the German National
Research Data Infrastructure for Data Science and Artificial Intelligence
(NFDI4DS) consortium, covering a diverse set of challenges in scholarly
document processing. Hosted at leading venues, the tasks foster methodological
innovations and contribute open-access datasets, models, and tools for the
broader research community, which are integrated into the consortium's research
data infrastructure.
\\ ( https://arxiv.org/abs/2509.22141 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22144
Date: Fri, 26 Sep 2025 10:02:48 GMT   (628kb)

Title: From Long to Lean: Performance-aware and Adaptive Chain-of-Thought
  Compression via Multi-round Refinement
Authors: Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Zike Yuan, Yang Xiang,
  Buzhou Tang
Categories: cs.CL cs.AI
Comments: 17 pages, 8 figures
\\
  Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.
\\ ( https://arxiv.org/abs/2509.22144 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22147
Date: Fri, 26 Sep 2025 10:05:22 GMT   (2327kb)

Title: Mixture of Detectors: A Compact View of Machine-Generated Text Detection
Authors: Sai Teja Lekkala, Yadagiri Annepaka, Arun Kumar Challa, Samatha Reddy
  Machireddy, Partha Pakray, Chukhu Chunka
Categories: cs.CL
Comments: 20 pages, 3 figures
\\
  Large Language Models (LLMs) are gearing up to surpass human creativity. The
veracity of the statement needs careful consideration. In recent developments,
critical questions arise regarding the authenticity of human work and the
preservation of their creativity and innovative abilities. This paper
investigates such issues. This paper addresses machine-generated text detection
across several scenarios, including document-level binary and multiclass
classification or generator attribution, sentence-level segmentation to
differentiate between human-AI collaborative text, and adversarial attacks
aimed at reducing the detectability of machine-generated text. We introduce a
new work called BMAS English: an English language dataset for binary
classification of human and machine text, for multiclass classification, which
not only identifies machine-generated text but can also try to determine its
generator, and Adversarial attack addressing where it is a common act for the
mitigation of detection, and Sentence-level segmentation, for predicting the
boundaries between human and machine-generated text. We believe that this paper
will address previous work in Machine-Generated Text Detection (MGTD) in a more
meaningful way.
\\ ( https://arxiv.org/abs/2509.22147 ,  2327kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22158
Date: Fri, 26 Sep 2025 10:16:28 GMT   (81kb)

Title: Context Parametrization with Compositional Adapters
Authors: Josip Juki\'c, Martin Tutek, Jan \v{S}najder
Categories: cs.CL
\\
  Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.
\\ ( https://arxiv.org/abs/2509.22158 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22193
Date: Fri, 26 Sep 2025 10:53:52 GMT   (1421kb)

Title: When Does Reasoning Matter? A Controlled Study of Reasoning's
  Contribution to Model Performance
Authors: Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Kevin El-Haddad,
  C\'eline Hudelot, Pierre Colombo
Categories: cs.CL
\\
  Large Language Models (LLMs) with reasoning capabilities have achieved
state-of-the-art performance on a wide range of tasks. Despite its empirical
success, the tasks and model scales at which reasoning becomes effective, as
well as its training and inference costs, remain underexplored. In this work,
we rely on a synthetic data distillation framework to conduct a large-scale
supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models
of varying sizes, on a wide range of math-centric and general-purpose tasks,
evaluating both multiple-choice and open-ended formats. Our analysis reveals
that reasoning consistently improves model performance, often matching or
surpassing significantly larger IFT systems. Notably, while IFT remains
Pareto-optimal in training and inference costs, reasoning models become
increasingly valuable as model size scales, overcoming IFT performance limits
on reasoning-intensive and open-ended tasks.
\\ ( https://arxiv.org/abs/2509.22193 ,  1421kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22206
Date: Fri, 26 Sep 2025 11:21:22 GMT   (399kb)

Title: The Outputs of Large Language Models are Meaningless
Authors: Anandi Hattiangadi and Anders J. Schoubye
Categories: cs.CL cs.AI
Comments: 24 pages, 2 figures, forthcoming in Herman Cappelen and Rachel
  Sterken, eds. Communicating with AI: Philosophical Perspectives. Oxford:
  Oxford University Press
\\
  In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.
\\ ( https://arxiv.org/abs/2509.22206 ,  399kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22211
Date: Fri, 26 Sep 2025 11:27:22 GMT   (1352kb)

Title: Question-Driven Analysis and Synthesis: Building Interpretable Thematic
  Trees with LLMs for Text Clustering and Controllable Generation
Authors: Tiago Fernandes Tavares
Categories: cs.CL cs.AI
\\
  Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.
\\ ( https://arxiv.org/abs/2509.22211 ,  1352kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22220
Date: Fri, 26 Sep 2025 11:32:51 GMT   (480kb)

Title: StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient
  SpeechLLMs
Authors: Yuhan Song, Linhao Zhang, Chuhan Wu, Aiwei Liu, Wei Jia, Houfeng Wang,
  Xiao Zhou
Categories: cs.CL
\\
  Prevalent semantic speech tokenizers, designed to capture linguistic content,
are surprisingly fragile. We find they are not robust to meaning-irrelevant
acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech
is perfectly intelligible, their output token sequences can change drastically,
increasing the learning burden for downstream LLMs. This instability stems from
two flaws: a brittle single-path quantization architecture and a distant
training signal indifferent to intermediate token stability. To address this,
we introduce StableToken, a tokenizer that achieves stability through a
consensus-driven mechanism. Its multi-branch architecture processes audio in
parallel, and these representations are merged via a powerful bit-wise voting
mechanism to form a single, stable token sequence. StableToken sets a new
state-of-the-art in token stability, drastically reducing Unit Edit Distance
(UED) under diverse noise conditions. This foundational stability translates
directly to downstream benefits, significantly improving the robustness of
SpeechLLMs on a variety of tasks.
\\ ( https://arxiv.org/abs/2509.22220 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22224
Date: Fri, 26 Sep 2025 11:38:03 GMT   (257kb)

Title: Thinking in Many Modes: How Composite Reasoning Elevates Large Language
  Model Performance with Limited Data
Authors: Zishan Ahmad, Saisubramaniam Gopalakrishnan
Categories: cs.CL cs.AI
Comments: 7 pages, 3 figures
\\
  Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.
\\ ( https://arxiv.org/abs/2509.22224 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22230
Date: Fri, 26 Sep 2025 11:40:32 GMT   (748kb)

Title: In Their Own Words: Reasoning Traces Tailored for Small Models Make Them
  Better Reasoners
Authors: Jaehoon Kim, Kwangwook Seo, and Dongha Lee
Categories: cs.CL
\\
  Transferring reasoning capabilities from larger language models to smaller
ones through supervised fine-tuning often fails counterintuitively, with
performance degrading despite access to high-quality teacher demonstrations. We
identify that this failure stems from distributional misalignment: reasoning
traces from larger models contain tokens that are low probability under the
student's distribution, exceeding the internal representation capacity of
smaller architectures and creating learning barriers rather than helpful
guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for
generating student-friendly reasoning traces in which the teacher model
proposes candidate tokens but the student model determines acceptance based on
its own probability distributions, filtering low probability tokens. When
applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data
degrades average performance across major reasoning benchmarks by 20.5\%, while
the same model trained on RSD-generated reasoning traces achieves meaningful
improvements of 4.9\%. Our analysis reveals that low probability tokens
constitute the critical bottleneck in reasoning ability transfer. However,
cross-model experiments demonstrate that RSD traces are model-specific rather
than universally applicable, indicating that distributional alignment must be
tailored for each student architecture's unique internal representation.
\\ ( https://arxiv.org/abs/2509.22230 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22237
Date: Fri, 26 Sep 2025 11:47:50 GMT   (710kb)

Title: FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe
  Coding
Authors: Haorui Chen, Chengze Li, Jia Li
Categories: cs.CL cs.AI cs.SE
\\
  The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.
\\ ( https://arxiv.org/abs/2509.22237 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22243
Date: Fri, 26 Sep 2025 11:57:42 GMT   (679kb)

Title: FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction
Authors: Yuan Ge, Saihan Chen, Jingqi Xiao, Xiaoqian Liu, Tong Xiao, Yan Xiang,
  Zhengtao Yu, Jingbo Zhu
Categories: cs.CL
\\
  Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling real-time spoken dialogue systems.
However, benchmarking and modeling these models remains a fundamental
challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human
spoken interaction that explicitly incorporates model interruption in emergency
scenarios. FLEXI systematically evaluates the latency, quality, and
conversational effectiveness of real-time dialogue through six diverse
human-LLM interaction scenarios, revealing significant gaps between open source
and commercial models in emergency awareness, turn terminating, and interaction
latency. Finally, we suggest that next token-pair prediction offers a promising
path toward achieving truly seamless and human-like full-duplex interaction.
\\ ( https://arxiv.org/abs/2509.22243 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22250
Date: Fri, 26 Sep 2025 12:11:29 GMT   (287kb)

Title: Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of
  Compliance
Authors: Wenbin Hu and Huihao Jing and Haochen Shi and Haoran Li and Yangqiu
  Song
Categories: cs.CL cs.AI
\\
  The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.
\\ ( https://arxiv.org/abs/2509.22250 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22251
Date: Fri, 26 Sep 2025 12:14:01 GMT   (3765kb)

Title: Beyond Textual Context: Structural Graph Encoding with Adaptive Space
  Alignment to alleviate the hallucination of LLMs
Authors: Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong
Categories: cs.CL cs.AI
Comments: 11 pages, 5 figures
\\
  Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.
\\ ( https://arxiv.org/abs/2509.22251 ,  3765kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22291
Date: Fri, 26 Sep 2025 12:53:20 GMT   (1024kb)

Title: Bridging Fairness and Explainability: Can Input-Based Explanations
  Promote Fairness in Hate Speech Detection?
Authors: Yifan Wang, Mayank Jobanputra, Ji-Ung Lee, Soyoung Oh, Isabel Valera,
  Vera Demberg
Categories: cs.CL cs.AI
\\
  Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.
\\ ( https://arxiv.org/abs/2509.22291 ,  1024kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22338
Date: Fri, 26 Sep 2025 13:30:50 GMT   (41kb)

Title: Advancing Natural Language Formalization to First Order Logic with
  Fine-tuned LLMs
Authors: Felix Vossel, Till Mossakowski and Bj\"orn Gehrke
Categories: cs.CL cs.AI
Comments: 15 pages, 7 tables, accepted at the International Joint Conference on
  Learning & Reasoning (IJCLR 2025)
MSC-class: 03B10
ACM-class: I.2.7; I.2.3
\\
  Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.
\\ ( https://arxiv.org/abs/2509.22338 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22343
Date: Fri, 26 Sep 2025 13:39:09 GMT   (4932kb)

Title: Transformers Can Learn Connectivity in Some Graphs but Not Others
Authors: Amit Roy, Abulhair Saparov
Categories: cs.CL cs.AI cs.LG cs.LO
Comments: Under Review
\\
  Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.
\\ ( https://arxiv.org/abs/2509.22343 ,  4932kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22345
Date: Fri, 26 Sep 2025 13:42:32 GMT   (94kb)

Title: The InviTE Corpus: Annotating Invectives in Tudor English Texts for
  Computational Modeling
Authors: Sophie Spliethoff, Sanne Hoeken, Silke Schwandt, Sina Zarrie{\ss},
  \"Ozge Ala\c{c}am
Categories: cs.CL
\\
  In this paper, we aim at the application of Natural Language Processing (NLP)
techniques to historical research endeavors, particularly addressing the study
of religious invectives in the context of the Protestant Reformation in Tudor
England. We outline a workflow spanning from raw data, through pre-processing
and data selection, to an iterative annotation process. As a result, we
introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English
(EModE) sentences, which are enriched with expert annotations regarding
invective language throughout 16th-century England. Subsequently, we assess and
compare the performance of fine-tuned BERT-based models and zero-shot prompted
instruction-tuned large language models (LLMs), which highlights the
superiority of models pre-trained on historical data and fine-tuned to
invective detection.
\\ ( https://arxiv.org/abs/2509.22345 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22354
Date: Fri, 26 Sep 2025 13:50:49 GMT   (40kb)

Title: Conversational Implicatures: Modelling Relevance Theory
  Probabilistically
Authors: Christoph Unger, Hendrik Buschmeier
Categories: cs.CL
\\
  Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.
\\ ( https://arxiv.org/abs/2509.22354 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22360
Date: Fri, 26 Sep 2025 13:56:16 GMT   (4108kb)

Title: CHRONOBERG: Capturing Language Evolution and Temporal Awareness in
  Foundation Models
Authors: Niharika Hegde, Subarnaduti Paul, Lars Joel-Frey, Manuel Brack,
  Kristian Kersting, Martin Mundt, Patrick Schramowski
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).
\\ ( https://arxiv.org/abs/2509.22360 ,  4108kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22366
Date: Fri, 26 Sep 2025 14:00:20 GMT   (1065kb)

Title: Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance
  Logs using Large Language Models
Authors: Max Malyi, Jonathan Shek, Andre Biscaya
Categories: cs.CL
\\
  A wealth of operational intelligence is locked within the unstructured
free-text of wind turbine maintenance logs, a resource largely inaccessible to
traditional quantitative reliability analysis. While machine learning has been
applied to this data, existing approaches typically stop at classification,
categorising text into predefined labels. This paper addresses the gap in
leveraging modern large language models (LLMs) for more complex reasoning
tasks. We introduce an exploratory framework that uses LLMs to move beyond
classification and perform deep semantic analysis. We apply this framework to a
large industrial dataset to execute four analytical workflows: failure mode
identification, causal chain inference, comparative site analysis, and data
quality auditing. The results demonstrate that LLMs can function as powerful
"reliability co-pilots," moving beyond labelling to synthesise textual
information and generate actionable, expert-level hypotheses. This work
contributes a novel and reproducible methodology for using LLMs as a reasoning
tool, offering a new pathway to enhance operational intelligence in the wind
energy sector by unlocking insights previously obscured in unstructured data.
\\ ( https://arxiv.org/abs/2509.22366 ,  1065kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22367
Date: Fri, 26 Sep 2025 14:00:51 GMT   (4408kb)

Title: What Is The Political Content in LLMs' Pre- and Post-Training Data?
Authors: Tanise Ceron, Dmitry Nikolaev, Dominik Stammbach, Debora Nozza
Categories: cs.CL cs.AI cs.CY
Comments: 9 pages, under review
\\
  Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.
\\ ( https://arxiv.org/abs/2509.22367 ,  4408kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22437
Date: Fri, 26 Sep 2025 14:55:04 GMT   (4258kb)

Title: Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding
Authors: Ziheng Chi, Yifan Hou, Chenxi Pang, Shaobo Cui, Mubashara Akhtar,
  Mrinmaya Sachan
Categories: cs.CL cs.AI
Comments: Our code (https://github.com/CHIzhP/Chimera) and data
  (https://huggingface.co/datasets/CHIzhP/Chimera) are publicly available
\\
  Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.
\\ ( https://arxiv.org/abs/2509.22437 ,  4258kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22449
Date: Fri, 26 Sep 2025 15:04:32 GMT   (161kb)

Title: Detecting (Un)answerability in Large Language Models with Linear
  Directions
Authors: Maor Juliet Lavi, Tova Milo, Mor Geva
Categories: cs.CL
\\
  Large language models (LLMs) often respond confidently to questions even when
they lack the necessary information, leading to hallucinated answers. In this
work, we study the problem of (un)answerability detection, focusing on
extractive question answering (QA) where the model should determine if a
passage contains sufficient information to answer a given question. We propose
a simple approach for identifying a direction in the model's activation space
that captures unanswerability and uses it for classification. This direction is
selected by applying activation additions during inference and measuring their
impact on the model's abstention behavior. We show that projecting hidden
activations onto this direction yields a reliable score for (un)answerability
classification. Experiments on two open-weight LLMs and four extractive QA
benchmarks show that our method effectively detects unanswerable questions and
generalizes better across datasets than existing prompt-based and
classifier-based approaches. Moreover, the obtained directions extend beyond
extractive QA to unanswerability that stems from factors, such as lack of
scientific consensus and subjectivity. Last, causal interventions show that
adding or ablating the directions effectively controls the abstention behavior
of the model.
\\ ( https://arxiv.org/abs/2509.22449 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22472
Date: Fri, 26 Sep 2025 15:19:12 GMT   (737kb)

Title: Evaluating the Limits of Large Language Models in Multilingual Legal
  Reasoning
Authors: Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve
  G\"urel
Categories: cs.CL cs.AI cs.LG
Comments: 39 pages, 36 figures. Code and evaluation pipeline available at
  https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs
\\
  In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.
\\ ( https://arxiv.org/abs/2509.22472 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22479
Date: Fri, 26 Sep 2025 15:25:59 GMT   (4235kb)

Title: NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between
  Lexical Systems and Language Use
Authors: Yuqing Zhang, Ecesu \"Urker, Tessa Verhoef, Gemma Boleda, Arianna
  Bisazza
Categories: cs.CL
Comments: Findings of EMNLP 2025
\\
  Lexical semantic change has primarily been investigated with observational
and experimental methods; however, observational methods (corpus analysis,
distributional semantic modeling) cannot get at causal mechanisms, and
experimental paradigms with humans are hard to apply to semantic change due to
the extended diachronic processes involved. This work introduces NeLLCom-Lex, a
neural-agent framework designed to simulate semantic change by first grounding
agents in a real lexical system (e.g. English) and then systematically
manipulating their communicative needs. Using a well-established color naming
task, we simulate the evolution of a lexical system within a single generation,
and study which factors lead agents to: (i) develop human-like naming behavior
and lexicons, and (ii) change their behavior and lexicons according to their
communicative needs. Our experiments with different supervised and
reinforcement learning pipelines show that neural agents trained to 'speak' an
existing language can reproduce human-like patterns in color naming to a
remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the
mechanisms of semantic change.
\\ ( https://arxiv.org/abs/2509.22479 ,  4235kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22480
Date: Fri, 26 Sep 2025 15:27:50 GMT   (2343kb)

Title: Exploring Solution Divergence and Its Effect on Large Language Model
  Problem Solving
Authors: Hang Li, Kaiqi Yang, Yucheng Chu, Hui Liu, Jiliang Tang
Categories: cs.CL cs.AI
Comments: 17 pages, 11 figures
\\
  Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.
\\ ( https://arxiv.org/abs/2509.22480 ,  2343kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22490
Date: Fri, 26 Sep 2025 15:35:38 GMT   (40kb)

Title: JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited
  Resources for Slavic Languages: MT and QA
Authors: Hossain Shaikh Saadi, Minh Duc Bui, Mario Sanz-Guerrero, Katharina von
  der Wense
Categories: cs.CL
Comments: WMT 25 Shared Task LLMs with Limited Resources for Slavic Languages:
  MT and QA
\\
  This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.
\\ ( https://arxiv.org/abs/2509.22490 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22506
Date: Fri, 26 Sep 2025 15:48:10 GMT   (1210kb)

Title: Representing LLMs in Prompt Semantic Task Space
Authors: Idan Kashani, Avi Mendelson and Yaniv Nemcovsky
Categories: cs.CL cs.LG
Comments: Accepted to Findings of the Association for Computational
  Linguistics: EMNLP 2025
MSC-class: 68T07, 68T50, 65F20
ACM-class: I.2.7; I.2.6; H.3.3
\\
  Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.
\\ ( https://arxiv.org/abs/2509.22506 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22510
Date: Fri, 26 Sep 2025 15:52:21 GMT   (9990kb)

Title: We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before
  They Go Wrong
Authors: Gautam Siddharth Kashyap, Mark Dras, Usman Naseem
Categories: cs.CL
\\
  Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.
\\ ( https://arxiv.org/abs/2509.22510 ,  9990kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22536
Date: Fri, 26 Sep 2025 16:16:49 GMT   (544kb)

Title: InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced
  Language Models
Authors: Wenjun Wang, Shuo Cai, Congkai Xie, Mingfa Feng, Yiming Zhang, Zhen
  Li, Kejing Yang, Ming Li, Jiannong Cao, Yuan Xie, Hongxia Yang
Categories: cs.CL cs.AI
\\
  The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.
\\ ( https://arxiv.org/abs/2509.22536 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22546
Date: Fri, 26 Sep 2025 16:27:29 GMT   (3294kb)

Title: Think Socially via Cognitive Reasoning
Authors: Jinfeng Zhou, Zheyu Chen, Shuai Wang, Quanyu Dai, Zhenhua Dong,
  Hongning Wang, Minlie Huang
Categories: cs.CL
Comments: Repository: https://github.com/thu-coai/CogFlow
\\
  LLMs trained for logical reasoning excel at step-by-step deduction to reach
verifiable answers. However, this paradigm is ill-suited for navigating social
situations, which induce an interpretive process of analyzing ambiguous cues
that rarely yield a definitive outcome. To bridge this gap, we introduce
Cognitive Reasoning, a paradigm modeled on human social cognition. It
formulates the interpretive process into a structured cognitive flow of
interconnected cognitive units (e.g., observation or attribution), which
combine adaptively to enable effective social thinking and responses. We then
propose CogFlow, a complete framework that instills this capability in LLMs.
CogFlow first curates a dataset of cognitive flows by simulating the
associative and progressive nature of human thought via tree-structured
planning. After instilling the basic cognitive reasoning capability via
supervised fine-tuning, CogFlow adopts reinforcement learning to enable the
model to improve itself via trial and error, guided by a multi-objective reward
that optimizes both cognitive flow and response quality. Extensive experiments
show that CogFlow effectively enhances the social cognitive capabilities of
LLMs, and even humans, leading to more effective social decision-making.
\\ ( https://arxiv.org/abs/2509.22546 ,  3294kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22565
Date: Fri, 26 Sep 2025 16:42:43 GMT   (626kb)

Title: Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages:
  Error Taxonomy Construction and Large-Scale Evaluation
Authors: Wenyuan Chen, Fateme Nateghi Haredasht, Kameron C. Black, Francois
  Grolleau, Emily Alsentzer, Jonathan H. Chen, and Stephen P. Ma
Categories: cs.CL cs.AI cs.IR
\\
  Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.
\\ ( https://arxiv.org/abs/2509.22565 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22582
Date: Fri, 26 Sep 2025 17:03:24 GMT   (204kb)

Title: Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs
Authors: Yehonatan Pesiakhovsky and Zorik Gekhman and Yosi Mass and Liat
  Ein-Dor and Roi Reichart
Categories: cs.CL
\\
  Context-grounded hallucinations are cases where model outputs contain
information not verifiable against the source text. We study the applicability
of LLMs for localizing such hallucinations, as a more practical alternative to
existing complex evaluation pipelines. In the absence of established benchmarks
for meta-evaluation of hallucinations localization, we construct one tailored
to LLMs, involving a challenging human annotation of over 1,000 examples. We
complement the benchmark with an LLM-based evaluation protocol, verifying its
quality in a human evaluation. Since existing representations of hallucinations
limit the types of errors that can be expressed, we propose a new
representation based on free-form textual descriptions, capturing the full
range of possible errors. We conduct a comprehensive study, evaluating four
large-scale LLMs, which highlights the benchmark's difficulty, as the best
model achieves an F1 score of only 0.67. Through careful analysis, we offer
insights into optimal prompting strategies for the task and identify the main
factors that make it challenging for LLMs: (1) a tendency to incorrectly flag
missing details as inconsistent, despite being instructed to check only facts
in the output; and (2) difficulty with outputs containing factually correct
information absent from the source - and thus not verifiable - due to alignment
with the model's parametric knowledge.
\\ ( https://arxiv.org/abs/2509.22582 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22589
Date: Fri, 26 Sep 2025 17:06:07 GMT   (9678kb)

Title: ArabJobs: A Multinational Corpus of Arabic Job Ads
Authors: Mo El-Haj
Categories: cs.CL
\\
  ArabJobs is a publicly available corpus of Arabic job advertisements
collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates.
Comprising over 8,500 postings and more than 550,000 words, the dataset
captures linguistic, regional, and socio-economic variation in the Arab labour
market. We present analyses of gender representation and occupational
structure, and highlight dialectal variation across ads, which offers
opportunities for future research. We also demonstrate applications such as
salary estimation and job category normalisation using large language models,
alongside benchmark tasks for gender bias detection and profession
classification. The findings show the utility of ArabJobs for fairness-aware
Arabic NLP and labour market research. The dataset is publicly available on
GitHub: https://github.com/drelhaj/ArabJobs.
\\ ( https://arxiv.org/abs/2509.22589 ,  9678kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22598
Date: Fri, 26 Sep 2025 17:17:15 GMT   (331kb)

Title: From Formal Language Theory to Statistical Learning: Finite
  Observability of Subregular Languages
Authors: Katsuhiko Hayashi and Hidetaka Kamigaito
Categories: cs.CL cs.FL cs.LG
Comments: 12 pages, 5 figures
\\
  We prove that all standard subregular language classes are linearly separable
when represented by their deciding predicates. This establishes finite
observability and guarantees learnability with simple linear models. Synthetic
experiments confirm perfect separability under noise-free conditions, while
real-data experiments on English morphology show that learned features align
with well-known linguistic constraints. These results demonstrate that the
subregular hierarchy provides a rigorous and interpretable foundation for
modeling natural language structure. Our code used in real-data experiments is
available at https://github.com/UTokyo-HayashiLab/subregular.
\\ ( https://arxiv.org/abs/2509.22598 ,  331kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22603
Date: Fri, 26 Sep 2025 17:23:55 GMT   (560kb)

Title: Capturing Opinion Shifts in Deliberative Discourse through
  Frequency-based Quantum deep learning methods
Authors: Rakesh Thakur, Harsh Chaturvedi, Ruqayya Shah, Janvi Chauhan, Ayush
  Sharma
Categories: cs.CL
Comments: 9 pages, 2 figures, 1 table
\\
  Deliberation plays a crucial role in shaping outcomes by weighing diverse
perspectives before reaching decisions. With recent advancements in Natural
Language Processing, it has become possible to computationally model
deliberation by analyzing opinion shifts and predicting potential outcomes
under varying scenarios. In this study, we present a comparative analysis of
multiple NLP techniques to evaluate how effectively models interpret
deliberative discourse and produce meaningful insights. Opinions from
individuals of varied backgrounds were collected to construct a self-sourced
dataset that reflects diverse viewpoints. Deliberation was simulated using
product presentations enriched with striking facts, which often prompted
measurable shifts in audience opinions. We have given comparative analysis
between two models namely Frequency-Based Discourse Modulation and
Quantum-Deliberation Framework which outperform the existing state of art
models. The findings highlight practical applications in public policy-making,
debate evaluation, decision-support frameworks, and large-scale social media
opinion mining.
\\ ( https://arxiv.org/abs/2509.22603 ,  560kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22612
Date: Fri, 26 Sep 2025 17:37:55 GMT   (73kb)

Title: From tests to effect sizes: Quantifying uncertainty and statistical
  variability in multilingual and multitask NLP evaluation benchmarks
Authors: Jonne S\"alev\"a, Duygu Ataman and Constantine Lignos
Categories: cs.CL
Comments: Paper currently under review at ACL Rolling Review
\\
  In this paper, we introduce a set of resampling-based methods for quantifying
uncertainty and statistical precision of evaluation metrics in multilingual
and/or multitask NLP benchmarks. We show how experimental variation in
performance scores arises from both model- and data-related sources, and that
accounting for both of them is necessary to avoid substantially underestimating
the overall variability over hypothetical replications. Using multilingual
question answering, machine translation, and named entity recognition as
example tasks, we also demonstrate how resampling methods are useful for
computing sampling distributions for various quantities used in leaderboards
such as the average/median, pairwise differences between models, and rankings.
\\ ( https://arxiv.org/abs/2509.22612 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22630
Date: Fri, 26 Sep 2025 17:55:22 GMT   (398kb)

Title: StateX: Enhancing RNN Recall via Post-training State Expansion
Authors: Xingyu Shen, Yingfa Chen, Zhen Leng Thai, Xu Han, Zhiyuan Liu and
  Maosong Sun
Categories: cs.CL cs.AI cs.LG
\\
  While Transformer-based models have demonstrated remarkable language modeling
performance, their high complexities result in high costs when processing long
contexts. In contrast, recurrent neural networks (RNNs) such as linear
attention and state space models have gained popularity due to their constant
per-token complexities. However, these recurrent models struggle with tasks
that require accurate recall of contextual information from long contexts,
because all contextual information is compressed into a constant-size recurrent
state. Previous works have shown that recall ability is positively correlated
with the recurrent state size, yet directly training RNNs with larger recurrent
states results in high training costs. In this paper, we introduce StateX, a
training pipeline for efficiently expanding the states of pre-trained RNNs
through post-training. For two popular classes of RNNs, linear attention and
state space models, we design post-training architectural modifications to
scale up the state size with no or negligible increase in model parameters.
Experiments on models up to 1.3B parameters demonstrate that StateX efficiently
enhances the recall and in-context learning ability of RNNs without incurring
high post-training costs or compromising other capabilities.
\\ ( https://arxiv.org/abs/2509.22630 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22637
Date: Fri, 26 Sep 2025 17:58:10 GMT   (562kb)

Title: Variational Reasoning for Language Models
Authors: Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan
  Li, Liang Wang, Tianyu Pang
Categories: cs.CL cs.AI cs.LG
\\
  We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.
\\ ( https://arxiv.org/abs/2509.22637 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22638
Date: Fri, 26 Sep 2025 17:58:27 GMT   (1701kb)

Title: Language Models Can Learn from Verbal Feedback Without Scalar Rewards
Authors: Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen,
  Wei Lu, Tianyu Pang
Categories: cs.CL cs.AI cs.LG
\\
  LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.
\\ ( https://arxiv.org/abs/2509.22638 ,  1701kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22641
Date: Fri, 26 Sep 2025 17:59:05 GMT   (3328kb)

Title: Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual
  Creativity
Authors: Arkadiy Saakyan, Najoung Kim, Smaranda Muresan, Tuhin Chakrabarty
Categories: cs.CL cs.AI cs.HC
Comments: 26 pages, 10 figures, under review
\\
  N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.
\\ ( https://arxiv.org/abs/2509.22641 ,  3328kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22644
Date: Fri, 26 Sep 2025 17:59:51 GMT   (5623kb)

Title: WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level
  Feedback and Step-Level Reinforcement Learning
Authors: Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting
  Pan, Mingjie Zhan, Hongsheng Li
Categories: cs.CL cs.AI
\\
  Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.
\\ ( https://arxiv.org/abs/2509.22644 ,  5623kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22651
Date: Fri, 26 Sep 2025 17:59:59 GMT   (9766kb)

Title: VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,
  Speaking, and Viewing
Authors: Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li
Categories: cs.CL cs.AI cs.CV cs.HC cs.SD
\\
  The growing capabilities of large language models and multimodal systems have
spurred interest in voice-first AI assistants, yet existing benchmarks are
inadequate for evaluating the full range of these systems' capabilities. We
introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI
assistants across listening, speaking, and viewing. VoiceAssistant-Eval
comprises 10,497 curated examples spanning 13 task categories. These tasks
include natural sounds, music, and spoken dialogue for listening; multi-turn
dialogue, role-play imitation, and various scenarios for speaking; and highly
heterogeneous images for viewing. To demonstrate its utility, we evaluate 21
open-source models and GPT-4o-Audio, measuring the quality of the response
content and speech, as well as their consistency. The results reveal three key
findings: (1) proprietary models do not universally outperform open-source
models; (2) most models excel at speaking tasks but lag in audio understanding;
and (3) well-designed smaller models can rival much larger ones. Notably, the
mid-sized Step-Audio-2-mini (7B) achieves more than double the listening
accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal
(audio plus visual) input and role-play voice imitation tasks are difficult for
current models, and significant gaps persist in robustness and safety
alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous
framework for evaluating and guiding the development of next-generation AI
assistants. Code and data will be released at
https://mathllm.github.io/VoiceAssistantEval/ .
\\ ( https://arxiv.org/abs/2509.22651 ,  9766kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21351
Date: Fri, 19 Sep 2025 10:53:45 GMT   (86kb)

Title: Random Direct Preference Optimization for Radiography Report Generation
Authors: Valentin Samokhin, Boris Shirokikh, Mikhail Goncharov, Dmitriy
  Umerenkov, Maksim Bobrin, Ivan Oseledets, Dmitry Dylov, Mikhail Belyaev
Categories: cs.CV cs.AI cs.CL
\\
  Radiography Report Generation (RRG) has gained significant attention in
medical image analysis as a promising tool for alleviating the growing workload
of radiologists. However, despite numerous advancements, existing methods have
yet to achieve the quality required for deployment in real-world clinical
settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated
remarkable progress in the general domain by adopting training strategies
originally designed for Large Language Models (LLMs), such as alignment
techniques. In this paper, we introduce a model-agnostic framework to enhance
RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages
random contrastive sampling to construct training pairs, eliminating the need
for reward models or human preference annotations. Experiments on supplementing
three state-of-the-art models with our Random DPO show that our method improves
clinical performance metrics by up to 5%, without requiring any additional
training data.
\\ ( https://arxiv.org/abs/2509.21351 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21352
Date: Fri, 19 Sep 2025 13:08:50 GMT   (529kb)

Title: Improving Autism Detection with Multimodal Behavioral Analysis
Authors: William Saakyan, Matthias Norden, Lola Eversmann, Simon Kirsch, Muyu
  Lin, Simon Guendelman, Isabel Dziobek, and Hanna Drimalla
Categories: cs.CV cs.LG
\\
  Due to the complex and resource-intensive nature of diagnosing Autism
Spectrum Condition (ASC), several computer-aided diagnostic support methods
have been proposed to detect autism by analyzing behavioral cues in patient
video data. While these models show promising results on some datasets, they
struggle with poor gaze feature performance and lack of real-world
generalizability. To tackle these challenges, we analyze a standardized video
dataset comprising 168 participants with ASC (46% female) and 157 non-autistic
participants (46% female), making it, to our knowledge, the largest and most
balanced dataset available. We conduct a multimodal analysis of facial
expressions, voice prosody, head motion, heart rate variability (HRV), and gaze
behavior. To address the limitations of prior gaze models, we introduce novel
statistical descriptors that quantify variability in eye gaze angles, improving
gaze-based classification accuracy from 64% to 69% and aligning computational
findings with clinical research on gaze aversion in ASC. Using late fusion, we
achieve a classification accuracy of 74%, demonstrating the effectiveness of
integrating behavioral markers across multiple modalities. Our findings
highlight the potential for scalable, video-based screening tools to support
autism assessment.
\\ ( https://arxiv.org/abs/2509.21352 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21354
Date: Sat, 20 Sep 2025 02:04:24 GMT   (345kb)

Title: KV-Efficient VLA: A Method of Speed up Vision Language Model with
  RNN-Gated Chunked KV Cache
Authors: Wanshun Xu, Long Zhuang
Categories: cs.CV cs.AI
\\
  Vision-Language-Action (VLA) models promise unified robotic perception and
control, yet their scalability is constrained by the quadratic cost of
attention and the unbounded growth of key-value (KV) memory during long-horizon
inference. While recent methods improve generalization through scaling backbone
architectures, they often neglect the inference inefficiencies critical to
real-time deployment. In this work, we present KV-Efficient VLA, a
model-agnostic memory compression framework that addresses these limitations by
introducing a lightweight, training-friendly mechanism to selectively retain
high-utility context. Our method partitions the KV cache into fixed size chunks
and employs a recurrent gating module to summarize and filter historical
context according to learned utility scores. This design preserves recent
fine-grained detail while aggressively pruning stale, low-relevance memory, all
while maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x
inference speedup and 36% KV memory reduction, with minimal impact on task
success. Our method integrates seamlessly into existing autoregressive and
hybrid VLA stacks, enabling scalable inference without modifying training
pipelines or downstream control logic.
\\ ( https://arxiv.org/abs/2509.21354 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21356
Date: Sat, 20 Sep 2025 04:33:44 GMT   (3526kb)

Title: Phrase-grounded Fact-checking for Automatically Generated Chest X-ray
  Reports
Authors: Razi Mahmood, Diego Machado-Reyes, Joy Wu, Parisa Kaviani, Ken C.L.
  Wong, Niharika D'Souza, Mannudeep Kalra, Ge Wang, Pingkun Yan, Tanveer
  Syeda-Mahmood
Categories: cs.CV cs.AI
Comments: In proceedings MICCAI 2025
\\
  With the emergence of large-scale vision language models (VLM), it is now
possible to produce realistic-looking radiology reports for chest X-ray images.
However, their clinical translation has been hampered by the factual errors and
hallucinations in the produced descriptions during inference. In this paper, we
present a novel phrase-grounded fact-checking model (FC model) that detects
errors in findings and their indicated locations in automatically generated
chest radiology reports.
  Specifically, we simulate the errors in reports through a large synthetic
dataset derived by perturbing findings and their locations in ground truth
reports to form real and fake findings-location pairs with images. A new
multi-label cross-modal contrastive regression network is then trained on this
dataset. We present results demonstrating the robustness of our method in terms
of accuracy of finding veracity prediction and localization on multiple X-ray
datasets. We also show its effectiveness for error detection in reports of SOTA
report generators on multiple datasets achieving a concordance correlation
coefficient of 0.997 with ground truth-based verification, thus pointing to its
utility during clinical inference in radiology workflows.
\\ ( https://arxiv.org/abs/2509.21356 ,  3526kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21358
Date: Sun, 21 Sep 2025 05:46:35 GMT   (649kb)

Title: MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for
  Contextually Aware Fundoscopic Image Classification
Authors: Jason Jordan, Mohammadreza Akbari Lor, Peter Koulen, Mei-Ling Shyu,
  Shu-Ching Chen
Categories: cs.CV cs.AI
Comments: Word count: 5157, Table count: 2, Figure count: 5
\\
  This study aimed to enhance disease classification accuracy from retinal
fundus images by integrating fine-grained image features and global textual
context using a novel multimodal deep learning architecture. Existing
multimodal large language models (MLLMs) often struggle to capture low-level
spatial details critical for diagnosing retinal diseases such as glaucoma,
diabetic retinopathy, and retinitis pigmentosa. This model development and
validation study was conducted on 1,305 fundus image-text pairs compiled from
three public datasets (FIVES, HRF, and StoneRounds), covering acquired and
inherited retinal diseases, and evaluated using classification accuracy and
F1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers
into cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are
patch-wise projected and fused using scaled cross-attention and FiLM-based
U-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease
classification task. MDF-MLLM, with both U-Net and MLLM components fully
fine-tuned during training, achieved a significantly higher accuracy of 94%,
representing a 56% improvement. Recall and F1-scores improved by as much as 67%
and 35% over baseline, respectively. Ablation studies confirmed that the
multi-depth fusion approach contributed to substantial gains in spatial
reasoning and classification, particularly for inherited diseases with rich
clinical text. MDF-MLLM presents a generalizable, interpretable, and modular
framework for fundus image classification, outperforming traditional MLLM
baselines through multi-scale feature fusion. The architecture holds promise
for real-world deployment in clinical decision support systems. Future work
will explore synchronized training techniques, a larger pool of diseases for
more generalizability, and extending the model for segmentation tasks.
\\ ( https://arxiv.org/abs/2509.21358 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21360
Date: Sun, 21 Sep 2025 11:22:32 GMT   (5042kb)

Title: Multimodal Prompt Decoupling Attack on the Safety Filters in
  Text-to-Image Models
Authors: Xingkai Peng, Jun Jiang, Meng Tong, Shuai Li, Weiming Zhang, Nenghai
  Yu, Kejiang Chen
Categories: cs.CV cs.AI
\\
  Text-to-image (T2I) models have been widely applied in generating
high-fidelity images across various domains. However, these models may also be
abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks.
Existing jailbreak methods primarily manipulate the textual prompt, leaving
potential vulnerabilities in image-based inputs largely unexplored. Moreover,
text-based methods face challenges in bypassing the model's safety filters. In
response to these limitations, we propose the Multimodal Prompt Decoupling
Attack (MPDA), which utilizes image modality to separate the harmful semantic
components of the original unsafe prompt. MPDA follows three core steps:
firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe
prompts and harmful prompts. The former are seemingly harmless sub-prompts that
can bypass filters, while the latter are sub-prompts with unsafe semantics that
trigger filters. Subsequently, the LLM rewrites the harmful prompts into
natural adversarial prompts to bypass safety filters, which guide the T2I model
to modify the base image into an NSFW output. Finally, to ensure semantic
consistency between the generated NSFW images and the original unsafe prompts,
the visual language model generates image captions, providing a new pathway to
guide the LLM in iterative rewriting and refining the generated content.
\\ ( https://arxiv.org/abs/2509.21360 ,  5042kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21363
Date: Sun, 21 Sep 2025 22:30:32 GMT   (136kb)

Title: A Mutual Learning Method for Salient Object Detection with intertwined
  Multi-Supervision--Revised
Authors: Runmin Wu, Mengyang Feng, Wenlong Guan, Dong Wang, Huchuan Lu, Errui
  Ding
Categories: cs.CV cs.AI
Comments: 11 pages
Journal-ref: CVPR.2019.00834
\\
  Though deep learning techniques have made great progress in salient object
detection recently, the predicted saliency maps still suffer from incomplete
predictions due to the internal complexity of objects and inaccurate boundaries
caused by strides in convolution and pooling operations. To alleviate these
issues, we propose to train saliency detection networks by exploiting the
supervision from not only salient object detection, but also foreground contour
detection and edge detection. First, we leverage salient object detection and
foreground contour detection tasks in an intertwined manner to generate
saliency maps with uniform highlight. Second, the foreground contour and edge
detection tasks guide each other simultaneously, thereby leading to precise
foreground contour prediction and reducing the local noises for edge
prediction. In addition, we develop a novel mutual learning module (MLM) which
serves as the building block of our method. Each MLM consists of multiple
network branches trained in a mutual learning manner, which improves the
performance by a large margin. Extensive experiments on seven challenging
datasets demonstrate that the proposed method has delivered state-of-the-art
results in both salient object detection and edge detection.
\\ ( https://arxiv.org/abs/2509.21363 ,  136kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21365
Date: Mon, 22 Sep 2025 08:51:19 GMT   (680kb)

Title: MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint
  Representation
Authors: Zhicheng Du, Qingyang Shi, Jiasheng Lu, Yingshan Liang, Xinyu Zhang,
  Yiran Wang, Peiwu Qin
Categories: cs.CV cs.AI
\\
  The multimodal relevance metric is usually borrowed from the embedding
ability of pretrained contrastive learning models for bimodal data, which is
used to evaluate the correlation between cross-modal data (e.g., CLIP).
However, the commonly used evaluation metrics are only suitable for the
associated analysis between two modalities, which greatly limits the evaluation
of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation
metric for the relevance of multiple modalities (N modalities, N>=3) via
multimodal joint representation for the first time. The ability of multimodal
joint representation to integrate multiple modalities into the same latent
space can accurately represent different modalities at one scale, providing
support for fair relevance scoring. Extensive experiments have shown that
MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by
13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves
as a more reliable metric for evaluating similarity on large-scale multimodal
datasets and multimodal model performance evaluation.
\\ ( https://arxiv.org/abs/2509.21365 ,  680kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21368
Date: Mon, 22 Sep 2025 14:43:20 GMT   (2905kb)

Title: Safety Assessment of Scaffolding on Construction Site using AI
Authors: Sameer Prabhu, Amit Patwardhan, Ramin Karim
Categories: cs.CV cs.AI
\\
  In the construction industry, safety assessment is vital to ensure both the
reliability of assets and the safety of workers. Scaffolding, a key structural
support asset requires regular inspection to detect and identify alterations
from the design rules that may compromise the integrity and stability. At
present, inspections are primarily visual and are conducted by site manager or
accredited personnel to identify deviations. However, visual inspection is
time-intensive and can be susceptible to human errors, which can lead to unsafe
conditions. This paper explores the use of Artificial Intelligence (AI) and
digitization to enhance the accuracy of scaffolding inspection and contribute
to the safety improvement. A cloud-based AI platform is developed to process
and analyse the point cloud data of scaffolding structure. The proposed system
detects structural modifications through comparison and evaluation of certified
reference data with the recent point cloud data. This approach may enable
automated monitoring of scaffolding, reducing the time and effort required for
manual inspections while enhancing the safety on a construction site.
\\ ( https://arxiv.org/abs/2509.21368 ,  2905kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21375
Date: Tue, 23 Sep 2025 07:22:31 GMT   (8081kb)

Title: Automated Prompt Generation for Creative and Counterfactual
  Text-to-image Synthesis
Authors: Aleksa Jelaca, Ying Jiao, Chang Tian, Marie-Francine Moens
Categories: cs.CV cs.AI
Comments: text-to-image generation, automatic prompt, DPO, Counterfactual
\\
  Text-to-image generation has advanced rapidly with large-scale multimodal
training, yet fine-grained controllability remains a critical challenge.
Counterfactual controllability, defined as the capacity to deliberately
generate images that contradict common-sense patterns, remains a major
challenge but plays a crucial role in enabling creativity and exploratory
applications. In this work, we address this gap with a focus on counterfactual
size (e.g., generating a tiny walrus beside a giant button) and propose an
automatic prompt engineering framework that adapts base prompts into revised
prompts for counterfactual images. The framework comprises three components: an
image evaluator that guides dataset construction by identifying successful
image generations, a supervised prompt rewriter that produces revised prompts,
and a DPO-trained ranker that selects the optimal revised prompt. We construct
the first counterfactual size text-image dataset and enhance the image
evaluator by extending Grounded SAM with refinements, achieving a 114 percent
improvement over its backbone. Experiments demonstrate that our method
outperforms state-of-the-art baselines and ChatGPT-4o, establishing a
foundation for future research on counterfactual controllability.
\\ ( https://arxiv.org/abs/2509.21375 ,  8081kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21376
Date: Tue, 23 Sep 2025 07:32:40 GMT   (1987kb)

Title: In silico Deep Learning Protocols for Label-Free Super-Resolution
  Microscopy: A Comparative Study of Network Architectures and SNR Dependence
Authors: Shiraz S Kaderuppan, Jonathan Mar, Andrew Irvine, Anurag Sharma,
  Muhammad Ramadan Saifuddin, Wai Leong Eugene Wong, Wai Lok Woo
Categories: cs.CV cs.AI
Comments: 20 pages, 10 figures
\\
  The field of optical microscopy spans across numerous industries and research
domains, ranging from education to healthcare, quality inspection and analysis.
Nonetheless, a key limitation often cited by optical microscopists refers to
the limit of its lateral resolution (typically defined as ~200nm), with
potential circumventions involving either costly external modules (e.g.
confocal scan heads, etc) and/or specialized techniques [e.g. super-resolution
(SR) fluorescent microscopy]. Addressing these challenges in a normal
(non-specialist) context thus remains an aspect outside the scope of most
microscope users & facilities. This study thus seeks to evaluate an alternative
& economical approach to achieving SR optical microscopy, involving
non-fluorescent phase-modulated microscopical modalities such as Zernike phase
contrast (PCM) and differential interference contrast (DIC) microscopy. Two in
silico deep neural network (DNN) architectures which we developed previously
(termed O-Net and Theta-Net) are assessed on their abilities to resolve a
custom-fabricated test target containing nanoscale features calibrated via
atomic force microscopy (AFM). The results of our study demonstrate that
although both O-Net and Theta-Net seemingly performed well when super-resolving
these images, they were complementary (rather than competing) approaches to be
considered for image SR, particularly under different image signal-to-noise
ratios (SNRs). High image SNRs favoured the application of O-Net models, while
low SNRs inclined preferentially towards Theta-Net models. These findings
demonstrate the importance of model architectures (in conjunction with the
source image SNR) on model performance and the SR quality of the generated
images where DNN models are utilized for non-fluorescent optical nanoscopy,
even where the same training dataset & number of epochs are being used.
\\ ( https://arxiv.org/abs/2509.21376 ,  1987kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21377
Date: Tue, 23 Sep 2025 09:31:00 GMT   (973kb)

Title: Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation
Authors: Yinfeng Yu, Hailong Zhang, Meiling Zhu
Categories: cs.CV cs.AI
Comments: Main paper (8 pages). Accepted for publication by ECAI( European
  Conference on Artificial Intelligence) 2025
\\
  Audiovisual embodied navigation enables robots to locate audio sources by
dynamically integrating visual observations from onboard sensors with the
auditory signals emitted by the target. The core challenge lies in effectively
leveraging multimodal cues to guide navigation. While prior works have explored
basic fusion of visual and audio data, they often overlook deeper perceptual
context. To address this, we propose the Dynamic Multi-Target Fusion for
Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target
architecture coupled with a refined Transformer mechanism to filter and
selectively fuse cross-modal information. Extensive experiments on the Replica
and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art
performance, outperforming existing methods in success rate (SR), path
efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits
strong scalability and generalizability, paving the way for advanced multimodal
fusion strategies in robotic navigation. The code and videos are available at
  https://github.com/zzzmmm-svg/DMTF.
\\ ( https://arxiv.org/abs/2509.21377 ,  973kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21379
Date: Tue, 23 Sep 2025 11:29:30 GMT   (19354kb)

Title: SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders
Authors: Enrico Cassano, Riccardo Renzulli, Marco Nurisso, Mirko Zaffaroni,
  Alan Perotti, Marco Grangetto
Categories: cs.CV cs.AI
\\
  Effective concept unlearning in text-to-image diffusion models requires
precise localization of concept representations within the model's latent
space. While sparse autoencoders successfully reduce neuron polysemanticity
(i.e., multiple concepts per neuron) compared to the original network,
individual concept representations can still be distributed across multiple
latent features, requiring extensive search procedures for concept unlearning.
We introduce SAEmnesia, a supervised sparse autoencoder training method that
promotes one-to-one concept-neuron mappings through systematic concept
labeling, mitigating feature splitting and promoting feature centralization.
Our approach learns specialized neurons with significantly stronger concept
associations compared to unsupervised baselines. The only computational
overhead introduced by SAEmnesia is limited to cross-entropy computation during
training. At inference time, this interpretable representation reduces
hyperparameter search by 96.67% with respect to current approaches. On the
UnlearnCanvas benchmark, SAEmnesia achieves a 9.22% improvement over the
state-of-the-art. In sequential unlearning tasks, we demonstrate superior
scalability with a 28.4% improvement in unlearning accuracy for 9-object
removal.
\\ ( https://arxiv.org/abs/2509.21379 ,  19354kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21380
Date: Tue, 23 Sep 2025 12:45:53 GMT   (1005kb)

Title: Coreset selection based on Intra-class diversity
Authors: Imran Ashraf, Mukhtar Ullah, Muhammad Faisal Nadeem, Muhammad Nouman
  Noor
Categories: cs.CV cs.LG
\\
  Deep Learning models have transformed various domains, including the
healthcare sector, particularly biomedical image classification by learning
intricate features and enabling accurate diagnostics pertaining to complex
diseases. Recent studies have adopted two different approaches to train DL
models: training from scratch and transfer learning. Both approaches demand
substantial computational time and resources due to the involvement of massive
datasets in model training. These computational demands are further increased
due to the design-space exploration required for selecting optimal
hyperparameters, which typically necessitates several training rounds. With the
growing sizes of datasets, exploring solutions to this problem has recently
gained the research community's attention. A plausible solution is to select a
subset of the dataset for training and hyperparameter search. This subset,
referred to as the corset, must be a representative set of the original
dataset. A straightforward approach to selecting the coreset could be employing
random sampling, albeit at the cost of compromising the representativeness of
the original dataset. A critical limitation of random sampling is the bias
towards the dominant classes in an imbalanced dataset. Even if the dataset has
inter-class balance, this random sampling will not capture intra-class
diversity. This study addresses this issue by introducing an intelligent,
lightweight mechanism for coreset selection. Specifically, it proposes a method
to extract intra-class diversity, forming per-class clusters that are utilized
for the final sampling. We demonstrate the efficacy of the proposed methodology
by conducting extensive classification experiments on a well-known biomedical
imaging dataset. Results demonstrate that the proposed scheme outperforms the
random sampling approach on several performance metrics for uniform conditions.
\\ ( https://arxiv.org/abs/2509.21380 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21383
Date: Tue, 23 Sep 2025 15:57:55 GMT   (3349kb)

Title: The LongiMam model for improved breast cancer risk prediction using
  longitudinal mammograms
Authors: Manel Rakez, Thomas Louis, Julien Guillaumin, Foucauld Chamming's,
  Pierre Fillard, Brice Amadeo, Virginie Rondeau
Categories: cs.CV cs.LG
\\
  Risk-adapted breast cancer screening requires robust models that leverage
longitudinal imaging data. Most current deep learning models use single or
limited prior mammograms and lack adaptation for real-world settings marked by
imbalanced outcome distribution and heterogeneous follow-up. We developed
LongiMam, an end-to-end deep learning model that integrates both current and up
to four prior mammograms. LongiMam combines a convolutional and a recurrent
neural network to capture spatial and temporal patterns predictive of breast
cancer. The model was trained and evaluated using a large, population-based
screening dataset with disproportionate case-to-control ratio typical of
clinical screening. Across several scenarios that varied in the number and
composition of prior exams, LongiMam consistently improved prediction when
prior mammograms were included. The addition of prior and current visits
outperformed single-visit models, while priors alone performed less well,
highlighting the importance of combining historical and recent information.
Subgroup analyses confirmed the model's efficacy across key risk groups,
including women with dense breasts and those aged 55 years or older. Moreover,
the model performed best in women with observed changes in mammographic density
over time. These findings demonstrate that longitudinal modeling enhances
breast cancer prediction and support the use of repeated mammograms to refine
risk stratification in screening programs. LongiMam is publicly available as
open-source software.
\\ ( https://arxiv.org/abs/2509.21383 ,  3349kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21384
Date: Tue, 23 Sep 2025 18:18:03 GMT   (4265kb)

Title: Assessing the Alignment of Popular CNNs to the Brain for Valence
  Appraisal
Authors: Laurent Mertens, Elahe' Yargholi, Laura Van Hove, Hans Op de Beeck,
  Jan Van den Stock, Joost Vennekens
Categories: cs.CV
Comments: 12 pages, 4 figures
\\
  Convolutional Neural Networks (CNNs) are a popular type of computer model
that have proven their worth in many computer vision tasks. Moreover, they form
an interesting study object for the field of psychology, with shown
correspondences between the workings of CNNs and the human brain. However,
these correspondences have so far mostly been studied in the context of general
visual perception. In contrast, this paper explores to what extent this
correspondence also holds for a more complex brain process, namely social
cognition. To this end, we assess the alignment between popular CNN
architectures and both human behavioral and fMRI data for image valence
appraisal through a correlation analysis. We show that for this task CNNs
struggle to go beyond simple visual processing, and do not seem to reflect
higher-order brain processing. Furthermore, we present Object2Brain, a novel
framework that combines GradCAM and object detection at the CNN-filter level
with the aforementioned correlation analysis to study the influence of
different object classes on the CNN-to-human correlations. Despite similar
correlation trends, different CNN architectures are shown to display different
object class sensitivities.
\\ ( https://arxiv.org/abs/2509.21384 ,  4265kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21385
Date: Tue, 23 Sep 2025 18:32:46 GMT   (2081kb)

Title: Debugging Concept Bottleneck Models through Removal and Retraining
Authors: Eric Enouen, Sainyam Galhotra
Categories: cs.CV cs.LG
\\
  Concept Bottleneck Models (CBMs) use a set of human-interpretable concepts to
predict the final task label, enabling domain experts to not only validate the
CBM's predictions, but also intervene on incorrect concepts at test time.
However, these interventions fail to address systemic misalignment between the
CBM and the expert's reasoning, such as when the model learns shortcuts from
biased data. To address this, we present a general interpretable debugging
framework for CBMs that follows a two-step process of Removal and Retraining.
In the Removal step, experts use concept explanations to identify and remove
any undesired concepts. In the Retraining step, we introduce CBDebug, a novel
method that leverages the interpretability of CBMs as a bridge for converting
concept-level user feedback into sample-level auxiliary labels. These labels
are then used to apply supervised bias mitigation and targeted augmentation,
reducing the model's reliance on undesired concepts. We evaluate our framework
with both real and automated expert feedback, and find that CBDebug
significantly outperforms prior retraining methods across multiple CBM
architectures (PIP-Net, Post-hoc CBM) and benchmarks with known spurious
correlations.
\\ ( https://arxiv.org/abs/2509.21385 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21386
Date: Tue, 23 Sep 2025 19:42:43 GMT   (3894kb)

Title: ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar
  Data
Authors: Anja Sheppard, Tyler Smithline, Andrew Scheffer, David Smith, Advaith
  V. Sethuraman, Ryan Bird, Sabrina Lin, Katherine A. Skinner
Categories: cs.CV cs.RO eess.IV
Comments: Accepted to OCEANS 2025 Great Lakes
\\
  In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that
detects shipwrecks from multibeam sonar data. Shipwrecks are an important
historical marker of maritime history, and can be discovered through manual
inspection of bathymetric data. However, this is a time-consuming process and
often requires expert analysis. Our proposed tool allows users to automatically
preprocess bathymetry data, perform deep learning inference, threshold model
outputs, and produce either pixel-wise segmentation masks or bounding boxes of
predicted shipwrecks. The backbone of this open-source tool is a deep learning
model, which is trained on a variety of shipwreck data from the Great Lakes and
the coasts of Ireland. Additionally, we employ synthetic data generation in
order to increase the size and diversity of our dataset. We demonstrate
superior segmentation performance with our open-source tool and training
pipeline as compared to a deep learning-based ArcGIS toolkit and a more
classical inverse sinkhole detection method. The open-source tool can be found
at https://github.com/umfieldrobotics/ShipwreckFinderQGISPlugin.
\\ ( https://arxiv.org/abs/2509.21386 ,  3894kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21387
Date: Tue, 23 Sep 2025 20:10:23 GMT   (3630kb)

Title: Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of
  Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence
Authors: Sanish Suwal, Dipkamal Bhusal, Michael Clifford, Nidhi Rastogi
Categories: cs.CV cs.AI cs.LG
Comments: 4 pages
\\
  Prior works have shown that neural networks can be heavily pruned while
preserving performance, but the impact of pruning on model interpretability
remains unclear. In this work, we investigate how magnitude-based pruning
followed by fine-tuning affects both low-level saliency maps and high-level
concept representations. Using a ResNet-18 trained on ImageNette, we compare
post-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG)
across pruning levels, evaluating sparsity and faithfulness. We further apply
CRAFT-based concept extraction to track changes in semantic coherence of
learned concepts. Our results show that light-to-moderate pruning improves
saliency-map focus and faithfulness while retaining distinct, semantically
meaningful concepts. In contrast, aggressive pruning merges heterogeneous
features, reducing saliency map sparsity and concept coherence despite
maintaining accuracy. These findings suggest that while pruning can shape
internal representations toward more human-aligned attention patterns,
excessive pruning undermines interpretability.
\\ ( https://arxiv.org/abs/2509.21387 ,  3630kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21388
Date: Tue, 23 Sep 2025 20:24:07 GMT   (14497kb)

Title: TUN3D: Towards Real-World Scene Understanding from Unposed Images
Authors: Anton Konushin, Nikita Drozdov, Bulat Gabdullin, Alexey Zakharov, Anna
  Vorontsova, Danila Rukhovich, Maksim Kolodiazhnyi
Categories: cs.CV eess.IV
\\
  Layout estimation and 3D object detection are two fundamental tasks in indoor
scene understanding. When combined, they enable the creation of a compact yet
semantically rich spatial representation of a scene. Existing approaches
typically rely on point cloud input, which poses a major limitation since most
consumer cameras lack depth sensors and visual-only data remains far more
common. We address this issue with TUN3D, the first method that tackles joint
layout estimation and 3D object detection in real scans, given multi-view
images as input, and does not require ground-truth camera poses or depth
supervision. Our approach builds on a lightweight sparse-convolutional backbone
and employs two dedicated heads: one for 3D object detection and one for layout
estimation, leveraging a novel and effective parametric wall representation.
Extensive experiments show that TUN3D achieves state-of-the-art performance
across three challenging scene understanding benchmarks: (i) using ground-truth
point clouds, (ii) using posed images, and (iii) using unposed images. While
performing on par with specialized 3D object detection methods, TUN3D
significantly advances layout estimation, setting a new benchmark in holistic
indoor scene understanding. Code is available at
https://github.com/col14m/tun3d .
\\ ( https://arxiv.org/abs/2509.21388 ,  14497kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21394
Date: Wed, 24 Sep 2025 07:46:38 GMT   (7876kb)

Title: Large AI Model-Enabled Generative Semantic Communications for Image
  Transmission
Authors: Qiyu Ma, Wanli Ni and Zhijin Qin
Categories: cs.CV cs.AI cs.IT math.IT
Comments: Accepted to the IEEE GLOBECOM 2025
\\
  The rapid development of generative artificial intelligence (AI) has
introduced significant opportunities for enhancing the efficiency and accuracy
of image transmission within semantic communication systems. Despite these
advancements, existing methodologies often neglect the difference in importance
of different regions of the image, potentially compromising the reconstruction
quality of visually critical content. To address this issue, we introduce an
innovative generative semantic communication system that refines semantic
granularity by segmenting images into key and non-key regions. Key regions,
which contain essential visual information, are processed using an image
oriented semantic encoder, while non-key regions are efficiently compressed
through an image-to-text modeling approach. Additionally, to mitigate the
substantial storage and computational demands posed by large AI models, the
proposed system employs a lightweight deployment strategy incorporating model
quantization and low-rank adaptation fine-tuning techniques, significantly
boosting resource utilization without sacrificing performance. Simulation
results demonstrate that the proposed system outperforms traditional methods in
terms of both semantic fidelity and visual quality, thereby affirming its
effectiveness for image transmission tasks.
\\ ( https://arxiv.org/abs/2509.21394 ,  7876kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21396
Date: Wed, 24 Sep 2025 11:49:08 GMT   (1490kb)

Title: mmHSense: Multi-Modal and Distributed mmWave ISAC Datasets for Human
  Sensing
Authors: Nabeel Nisar Bhat, Maksim Karnaukh, Stein Vandenbroeke, Wouter
  Lemoine, Jakob Struye, Jesus Omar Lacruz, Siddhartha Kumar, Mohammad Hossein
  Moghaddam, Joerg Widmer, Rafael Berkvens, Jeroen Famaey
Categories: cs.CV cs.LG
\\
  This article presents mmHSense, a set of open labeled mmWave datasets to
support human sensing research within Integrated Sensing and Communication
(ISAC) systems. The datasets can be used to explore mmWave ISAC for various end
applications such as gesture recognition, person identification, pose
estimation, and localization. Moreover, the datasets can be used to develop and
advance signal processing and deep learning research on mmWave ISAC. This
article describes the testbed, experimental settings, and signal features for
each dataset. Furthermore, the utility of the datasets is demonstrated through
validation on a specific downstream task. In addition, we demonstrate the use
of parameter-efficient fine-tuning to adapt ISAC models to different tasks,
significantly reducing computational complexity while maintaining performance
on prior tasks.
\\ ( https://arxiv.org/abs/2509.21396 ,  1490kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21398
Date: Wed, 24 Sep 2025 12:14:12 GMT   (548kb)

Title: Skeleton Sparsification and Densification Scale-Spaces
Authors: Julia Gierke and Pascal Peter
Categories: cs.CV eess.IV
\\
  The Hamilton-Jacobi skeleton, also known as the medial axis, is a powerful
shape descriptor that represents binary objects in terms of the centres of
maximal inscribed discs. Despite its broad applicability, the medial axis
suffers from sensitivity to noise: minor boundary variations can lead to
disproportionately large and undesirable expansions of the skeleton. Classical
pruning methods mitigate this shortcoming by systematically removing extraneous
skeletal branches. This sequential simplification of skeletons resembles the
principle of sparsification scale-spaces that embed images into a family of
reconstructions from increasingly sparse pixel representations.
  We combine both worlds by introducing skeletonisation scale-spaces: They
leverage sparsification of the medial axis to achieve hierarchical
simplification of shapes. Unlike conventional pruning, our framework inherently
satisfies key scale-space properties such as hierarchical architecture,
controllable simplification, and equivariance to geometric transformations. We
provide a rigorous theoretical foundation in both continuous and discrete
formulations and extend the concept further with densification. This allows
inverse progression from coarse to fine scales and can even reach beyond the
original skeleton to produce overcomplete shape representations with relevancy
for practical applications.
  Through proof-of-concept experiments, we demonstrate the effectiveness of our
framework for practical tasks including robust skeletonisation, shape
compression, and stiffness enhancement for additive manufacturing.
\\ ( https://arxiv.org/abs/2509.21398 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21399
Date: Wed, 24 Sep 2025 12:19:51 GMT   (2845kb)

Title: Downscaling climate projections to 1 km with single-image super
  resolution
Authors: Petr Ko\v{s}\v{t}\'al, Pavel Kord\'ik, Ond\v{r}ej Podsztavek
Categories: cs.CV cs.LG
\\
  High-resolution climate projections are essential for local decision-making.
However, available climate projections have low spatial resolution (e.g. 12.5
km), which limits their usability. We address this limitation by leveraging
single-image super-resolution models to statistically downscale climate
projections to 1-km resolution. Since high-resolution climate projections are
unavailable for training, we train models on a high-resolution observational
gridded data set and apply them to low-resolution climate projections. We
propose a climate indicator-based assessment using observed climate indices
computed at weather station locations to evaluate the downscaled climate
projections without ground-truth high-resolution climate projections.
Experiments on daily mean temperature demonstrate that single-image
super-resolution models can downscale climate projections without increasing
the error of climate indicators compared to low-resolution climate projections.
\\ ( https://arxiv.org/abs/2509.21399 ,  2845kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21401
Date: Wed, 24 Sep 2025 14:17:31 GMT   (732kb)

Title: JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image
  Perturbation
Authors: Md Jueal Mia, M. Hadi Amini
Categories: cs.CV
\\
  Vision-Language Models (VLMs) have remarkable abilities in generating
multimodal reasoning tasks. However, potential misuse or safety alignment
concerns of VLMs have increased significantly due to different categories of
attack vectors. Among various attack vectors, recent studies have demonstrated
that image-based perturbations are particularly effective in generating harmful
outputs. In the literature, many existing techniques have been proposed to
jailbreak VLMs, leading to unstable performance and visible perturbations. In
this study, we propose Jailbreaking with Loss-guided Image Perturbation
(JaiLIP), a jailbreaking attack in the image space that minimizes a joint
objective combining the mean squared error (MSE) loss between clean and
adversarial image with the models harmful-output loss. We evaluate our proposed
method on VLMs using standard toxicity metrics from Perspective API and
Detoxify. Experimental results demonstrate that our method generates highly
effective and imperceptible adversarial images, outperforming existing methods
in producing toxicity. Moreover, we have evaluated our method in the
transportation domain to demonstrate the attacks practicality beyond toxic text
generation in specific domain. Our findings emphasize the practical challenges
of image-based jailbreak attacks and the need for efficient defense mechanisms
for VLMs.
\\ ( https://arxiv.org/abs/2509.21401 ,  732kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21419
Date: Thu, 25 Sep 2025 07:54:03 GMT   (1938kb)

Title: Overview of ExpertLifeCLEF 2018: how far automated identification
  systems are from the best experts?
Authors: Herve Goeau, Pierre Bonnet, Alexis Joly
Categories: cs.CV
Comments: 11 pages, 2 figures, CLEF 2018 Conference and Labs of the Evaluation
  Forum, September 10 to 14, 2018, Avignon, France
\\
  Automated identification of plants and animals has improved considerably in
the last few years, in particular thanks to the recent advances in deep
learning. The next big question is how far such automated systems are from the
human expertise. Indeed, even the best experts are sometimes confused and/or
disagree between each others when validating visual or audio observations of
living organism. A picture actually contains only a partial information that is
usually not sufficient to determine the right species with certainty.
Quantifying this uncertainty and comparing it to the performance of automated
systems is of high interest for both computer scientists and expert
naturalists. The LifeCLEF 2018 ExpertCLEF challenge presented in this paper was
designed to allow this comparison between human experts and automated systems.
In total, 19 deep-learning systems implemented by 4 different research teams
were evaluated with regard to 9 expert botanists of the French flora. The main
outcome of this work is that the performance of state-of-the-art deep learning
models is now close to the most advanced human expertise. This paper presents
more precisely the resources and assessments of the challenge, summarizes the
approaches and systems employed by the participating research groups, and
provides an analysis of the main outcomes.
\\ ( https://arxiv.org/abs/2509.21419 ,  1938kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21420
Date: Thu, 25 Sep 2025 07:58:36 GMT   (35053kb)

Title: QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models
Authors: Jian Liu, Chunshi Wang, Song Guo, Haohan Weng, Zhen Zhou, Zhiqi Li,
  Jiaao Yu, Yiling Zhu, Jing Xu, Biwen Lei, Zhuo Chen, Chunchao Guo
Categories: cs.CV
\\
  The generation of quadrilateral-dominant meshes is a cornerstone of
professional 3D content creation. However, existing generative models generate
quad meshes by first generating triangle meshes and then merging triangles into
quadrilaterals with some specific rules, which typically produces quad meshes
with poor topology. In this paper, we introduce QuadGPT, the first
autoregressive framework for generating quadrilateral meshes in an end-to-end
manner. QuadGPT formulates this as a sequence prediction paradigm,
distinguished by two key innovations: a unified tokenization method to handle
mixed topologies of triangles and quadrilaterals, and a specialized
Reinforcement Learning fine-tuning method tDPO for better generation quality.
Extensive experiments demonstrate that QuadGPT significantly surpasses previous
triangle-to-quad conversion pipelines in both geometric accuracy and
topological quality. Our work establishes a new benchmark for native quad-mesh
generation and showcases the power of combining large-scale autoregressive
models with topology-aware RL refinement for creating structured 3D assets.
\\ ( https://arxiv.org/abs/2509.21420 ,  35053kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21433
Date: Thu, 25 Sep 2025 15:16:17 GMT   (3168kb)

Title: DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level
  Orthogonal LoRA Adaptation
Authors: Jiaqi Liu, Lan Zhang, Xiaoyong Yuan
Categories: cs.CV cs.AI cs.LG
\\
  Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted
styles and protected visual concepts, raising legal and ethical concerns.
Concept erasure has emerged as a safeguard, aiming to selectively suppress such
concepts through fine-tuning. However, existing methods do not scale to
practical settings where providers must erase multiple and possibly conflicting
concepts. The core bottleneck is their reliance on static erasure: a single
checkpoint is fine-tuned to remove all target concepts, regardless of the
actual erasure needs at inference. This rigid design mismatches real-world
usage, where requests vary per generation, leading to degraded erasure success
and reduced fidelity for non-target content. We propose DyME, an on-demand
erasure framework that trains lightweight, concept-specific LoRA adapters and
dynamically composes only those needed at inference. This modular design
enables flexible multi-concept erasure, but naive composition causes
interference among adapters, especially when many or semantically related
concepts are suppressed. To overcome this, we introduce bi-level orthogonality
constraints at both the feature and parameter levels, disentangling
representation shifts and enforcing orthogonal adapter subspaces. We further
develop ErasureBench-H, a new hierarchical benchmark with
brand-series-character structure, enabling principled evaluation across
semantic granularities and erasure set sizes. Experiments on ErasureBench-H and
standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME
consistently outperforms state-of-the-art baselines, achieving higher
multi-concept erasure fidelity with minimal collateral degradation.
\\ ( https://arxiv.org/abs/2509.21433 ,  3168kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21451
Date: Thu, 25 Sep 2025 19:22:57 GMT   (3023kb)

Title: VideoJudge: Bootstrapping Enables Scalable Supervision of
  MLLM-as-a-Judge for Video Understanding
Authors: Abdul Waheed, Zhen Wu, Dareen Alharthi, Seungone Kim, Bhiksha Raj
Categories: cs.CV cs.CL
Comments: Work in progress
\\
  Precisely evaluating video understanding models remains challenging: commonly
used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of
human judgment, while obtaining such judgments through manual evaluation is
costly. Recent work has explored using large language models (LLMs) or
multimodal LLMs (MLLMs) as evaluators, but their extension to video
understanding remains relatively unexplored. In this work, we introduce
VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from
video understanding models (\textit{i.e.}, text responses conditioned on
videos). To train VideoJudge, our recipe builds on the interplay between a
generator and an evaluator: the generator is prompted to produce responses
conditioned on a target rating, and responses not matching the evaluator's
rating are discarded. Across three out of four meta-evaluation benchmarks,
VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B
and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than
MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve
performance, indicating that providing video inputs is crucial for evaluation
of video understanding tasks.
\\ ( https://arxiv.org/abs/2509.21451 ,  3023kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21464
Date: Thu, 25 Sep 2025 19:30:27 GMT   (7830kb)

Title: Residual Vector Quantization For Communication-Efficient Multi-Agent
  Perception
Authors: Dereje Shenkut, B.V.K Vijaya Kumar
Categories: cs.CV cs.RO
Comments: 5 pages
\\
  Multi-agent collaborative perception (CP) improves scene understanding by
sharing information across connected agents such as autonomous vehicles,
unmanned aerial vehicles, and robots. Communication bandwidth, however,
constrains scalability. We present ReVQom, a learned feature codec that
preserves spatial identity while compressing intermediate features. ReVQom is
an end-to-end method that compresses feature dimensions via a simple bottleneck
network followed by multi-stage residual vector quantization (RVQ). This allows
only per-pixel code indices to be transmitted, reducing payloads from 8192 bits
per pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent
with minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves
273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x),
ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables
ultra-low-bandwidth operation with graceful degradation. ReVQom allows
efficient and accurate multi-agent collaborative perception with a step toward
practical V2X deployment.
\\ ( https://arxiv.org/abs/2509.21464 ,  7830kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21466
Date: Thu, 25 Sep 2025 19:30:51 GMT   (1383kb)

Title: Gender Stereotypes in Professional Roles Among Saudis: An Analytical
  Study of AI-Generated Images Using Language Models
Authors: Khaloud S. AlKhalifah, Malak Mashaabi and Hend Al-Khalifa
Categories: cs.CV cs.AI cs.CL
\\
  This study investigates the extent to which contemporary Text-to-Image
artificial intelligence (AI) models perpetuate gender stereotypes and cultural
inaccuracies when generating depictions of professionals in Saudi Arabia. We
analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse
Saudi professions using neutral prompts. Two trained Saudi annotators evaluated
each image on five dimensions: perceived gender, clothing and appearance,
background and setting, activities and interactions, and age. A third senior
researcher adjudicated whenever the two primary raters disagreed, yielding
10,100 individual judgements. The results reveal a strong gender imbalance,
with ImageFX outputs being 85\% male, Grok 86.6\% male, and DALL-E V3 96\%
male, indicating that DALL-E V3 exhibited the strongest overall gender
stereotyping. This imbalance was most evident in leadership and technical
roles. Moreover, cultural inaccuracies in clothing, settings, and depicted
activities were frequently observed across all three models.
Counter-stereotypical images often arise from cultural misinterpretations
rather than genuinely progressive portrayals. We conclude that current models
mirror societal biases embedded in their training data, generated by humans,
offering only a limited reflection of the Saudi labour market's gender dynamics
and cultural nuances. These findings underscore the urgent need for more
diverse training data, fairer algorithms, and culturally sensitive evaluation
frameworks to ensure equitable and authentic visual outputs.
\\ ( https://arxiv.org/abs/2509.21466 ,  1383kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21486
Date: Thu, 25 Sep 2025 19:46:34 GMT   (2618kb)

Title: Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large
  Language Models for Short Video Content Moderation
Authors: Zixuan Wang, Yu Sun, Hongwei Wang, Baoyu Jing, Xiang Shen, Xin Dong,
  Zhuolin Hao, Hongyu Xiong, Yang Song
Categories: cs.CV
\\
  Short video platforms are evolving rapidly, making the identification of
inappropriate content increasingly critical. Existing approaches typically
train separate and small classification models for each type of issue, which
requires extensive human-labeled data and lacks cross-issue generalization. We
propose a reasoning-enhanced multimodal large language model (MLLM) pretraining
paradigm for unified inappropriate content detection. To address the
distribution gap between short video content and the original pretraining data
of MLLMs, as well as the complex issue definitions, we introduce three targeted
pretraining tasks: (1) \textit{Caption}, to enhance the MLLM's perception of
video details; (2) \textit{Visual Question Answering (VQA)}, to deepen the
MLLM's understanding of issue definitions and annotation guidelines; (3)
\textit{Chain-of-Thought (CoT)}, to enhance the MLLM's reasoning capability.
Experimental results show that our pretraining approach significantly improves
the MLLM's performance in both zero-shot and supervised fine-tuning (SFT)
settings. In addition, our pretrained model demonstrates strong generalization
capabilities to emergent, previously unseen issues.
\\ ( https://arxiv.org/abs/2509.21486 ,  2618kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21552
Date: Thu, 25 Sep 2025 20:38:01 GMT   (13751kb)

Title: Learning GUI Grounding with Spatial Reasoning from Visual Feedback
Authors: Yu Zhao, Wei-Ning Chen, Huseyin Atahan Inan, Samuel Kessler, Lu Wang,
  Lukas Wutschitz, Fangkai Yang, Chaoyun Zhang, Pasquale Minervini, Saravan
  Rajmohan, Robert Sim
Categories: cs.CV cs.CL
\\
  Graphical User Interface (GUI) grounding is commonly framed as a coordinate
prediction task -- given a natural language instruction, generate on-screen
coordinates for actions such as clicks and keystrokes. However, recent Vision
Language Models (VLMs) often fail to predict accurate numeric coordinates when
processing high-resolution GUI images with complex layouts. To address this
issue, we reframe GUI grounding as an \emph{interactive search task}, where the
VLM generates actions to move a cursor in the GUI to locate UI elements. At
each step, the model determines the target object, evaluates the spatial
relations between the cursor and the target, and moves the cursor closer to the
target conditioned on the movement history. In this interactive process, the
rendered cursor provides visual feedback to help the model align its
predictions with the corresponding on-screen locations. We train our GUI
grounding model, GUI-Cursor, using multi-step online reinforcement learning
with a dense trajectory-based reward function. Our experimental results show
that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy
and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\% \rightarrow
93.9\%$) and ScreenSpot-Pro ($26.8\% \rightarrow 56.5\%$). Moreover, we observe
that GUI-Cursor learns to solve the problem within two steps for 95\% of
instances and can adaptively conduct more steps on more difficult examples.
\\ ( https://arxiv.org/abs/2509.21552 ,  13751kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21559
Date: Thu, 25 Sep 2025 20:39:45 GMT   (1058kb)

Title: X-CoT: Explainable Text-to-Video Retrieval via LLM-based
  Chain-of-Thought Reasoning
Authors: Prasanna Reddy Pulakurthi, Jiamian Wang, Majid Rabbani, Sohail Dianat,
  Raghuveer Rao, Zhiqiang Tao
Categories: cs.CV
Comments: 12 pages, 7 figures. Accepted at EMNLP 2025 (Main Conference)
\\
  Prevalent text-to-video retrieval systems mainly adopt embedding models for
feature extraction and compute cosine similarities for ranking. However, this
design presents two limitations. Low-quality text-video data pairs could
compromise the retrieval, yet are hard to identify and examine. Cosine
similarity alone provides no explanation for the ranking results, limiting the
interpretability. We ask that can we interpret the ranking results, so as to
assess the retrieval models and examine the text-video data? This work proposes
X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of
the embedding model-based similarity ranking. We first expand the existing
benchmarks with additional video annotations to support semantic understanding
and reduce data bias. We also devise a retrieval CoT consisting of pairwise
comparison steps, yielding detailed reasoning and complete ranking. X-CoT
empirically improves the retrieval performance and produces detailed
rationales. It also facilitates the model behavior and data quality analysis.
Code and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.
\\ ( https://arxiv.org/abs/2509.21559 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21561
Date: Thu, 25 Sep 2025 20:40:52 GMT   (2771kb)

Title: Unsupervised Defect Detection for Surgical Instruments
Authors: Joseph Huang, Yichi Zhang, Jingxi Yu, Wei Chen, Seunghyun Hwang, Qiang
  Qiu, Amy R. Reibman, Edward J. Delp, Fengqing Zhu
Categories: cs.CV
\\
  Ensuring the safety of surgical instruments requires reliable detection of
visual defects. However, manual inspection is prone to error, and existing
automated defect detection methods, typically trained on natural/industrial
images, fail to transfer effectively to the surgical domain. We demonstrate
that simply applying or fine-tuning these approaches leads to issues: false
positive detections arising from textured backgrounds, poor sensitivity to
small, subtle defects, and inadequate capture of instrument-specific features
due to domain shift. To address these challenges, we propose a versatile method
that adapts unsupervised defect detection methods specifically for surgical
instruments. By integrating background masking, a patch-based analysis
strategy, and efficient domain adaptation, our method overcomes these
limitations, enabling the reliable detection of fine-grained defects in
surgical instrument imagery.
\\ ( https://arxiv.org/abs/2509.21561 ,  2771kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21565
Date: Thu, 25 Sep 2025 20:46:48 GMT   (14921kb)

Title: No Alignment Needed for Generation: Learning Linearly Separable
  Representations in Diffusion Models
Authors: Junno Yun, Ya\c{s}ar Utku Al\c{c}alar, Mehmet Ak\c{c}akaya
Categories: cs.CV cs.AI cs.LG
\\
  Efficient training strategies for large-scale diffusion models have recently
emphasized the importance of improving discriminative feature representations
in these models. A central line of work in this direction is representation
alignment with features obtained from powerful external encoders, which
improves the representation quality as assessed through linear probing.
Alignment-based approaches show promise but depend on large pretrained
encoders, which are computationally expensive to obtain. In this work, we
propose an alternative regularization for training, based on promoting the
Linear SEParability (LSEP) of intermediate layer representations. LSEP
eliminates the need for an auxiliary encoder and representation alignment,
while incorporating linear probing directly into the network's learning
dynamics rather than treating it as a simple post-hoc evaluation tool. Our
results demonstrate substantial improvements in both training efficiency and
generation quality on flow-based transformer architectures such as SiTs,
achieving an FID of 1.46 on $256 \times 256$ ImageNet dataset.
\\ ( https://arxiv.org/abs/2509.21565 ,  14921kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21573
Date: Thu, 25 Sep 2025 20:53:06 GMT   (321kb)

Title: Enhancing Contrastive Learning for Geolocalization by Discovering Hard
  Negatives on Semivariograms
Authors: Boyi Chen, Zhangyu Wang, Fabian Deuser, Johann Maximilian Zollner,
  Martin Werner
Categories: cs.CV cs.AI
\\
  Accurate and robust image-based geo-localization at a global scale is
challenging due to diverse environments, visually ambiguous scenes, and the
lack of distinctive landmarks in many regions. While contrastive learning
methods show promising performance by aligning features between street-view
images and corresponding locations, they neglect the underlying spatial
dependency in the geographic space. As a result, they fail to address the issue
of false negatives -- image pairs that are both visually and geographically
similar but labeled as negatives, and struggle to effectively distinguish hard
negatives, which are visually similar but geographically distant. To address
this issue, we propose a novel spatially regularized contrastive learning
strategy that integrates a semivariogram, which is a geostatistical tool for
modeling how spatial correlation changes with distance. We fit the
semivariogram by relating the distance of images in feature space to their
geographical distance, capturing the expected visual content in a spatial
correlation. With the fitted semivariogram, we define the expected visual
dissimilarity at a given spatial distance as reference to identify hard
negatives and false negatives. We integrate this strategy into GeoCLIP and
evaluate it on the OSV5M dataset, demonstrating that explicitly modeling
spatial priors improves image-based geo-localization performance, particularly
at finer granularity.
\\ ( https://arxiv.org/abs/2509.21573 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21574
Date: Thu, 25 Sep 2025 20:53:27 GMT   (5189kb)

Title: X-Streamer: Unified Human World Modeling with Audiovisual Interaction
Authors: You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, Guoxian Song, Xiaochen
  Zhao, Chao Liang, Jianwen Jiang, Hongyi Xu, Linjie Luo
Categories: cs.CV
Comments: Project Page at https://byteaigc.github.io/X-Streamer
\\
  We introduce X-Streamer, an end-to-end multimodal human world modeling
framework for building digital human agents capable of infinite interactions
across text, speech, and video within a single unified architecture. Starting
from a single portrait, X-Streamer enables real-time, open-ended video calls
driven by streaming multimodal inputs. At its core is a Thinker-Actor
dual-transformer architecture that unifies multimodal understanding and
generation, turning a static portrait into persistent and intelligent
audiovisual interactions. The Thinker module perceives and reasons over
streaming user inputs, while its hidden states are translated by the Actor into
synchronized multimodal streams in real time. Concretely, the Thinker leverages
a pretrained large language-speech model, while the Actor employs a chunk-wise
autoregressive diffusion model that cross-attends to the Thinker's hidden
states to produce time-aligned multimodal responses with interleaved discrete
text and audio tokens and continuous video latents. To ensure long-horizon
stability, we design inter- and intra-chunk attentions with time-aligned
multimodal positional embeddings for fine-grained cross-modality alignment and
context retention, further reinforced by chunk-wise diffusion forcing and
global identity referencing. X-Streamer runs in real time on two A100 GPUs,
sustaining hours-long consistent video chat experiences from arbitrary
portraits and paving the way toward unified world modeling of interactive
digital humans.
\\ ( https://arxiv.org/abs/2509.21574 ,  5189kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21592
Date: Thu, 25 Sep 2025 21:03:56 GMT   (6590kb)

Title: What Happens Next? Anticipating Future Motion by Generating Point
  Trajectories
Authors: Gabrijel Boduljak, Laurynas Karazija, Iro Laina, Christian Rupprecht,
  Andrea Vedaldi
Categories: cs.CV cs.AI cs.LG
\\
  We consider the problem of forecasting motion from a single image, i.e.,
predicting how objects in the world are likely to move, without the ability to
observe other parameters such as the object velocities or the forces applied to
them. We formulate this task as conditional generation of dense trajectory
grids with a model that closely follows the architecture of modern video
generators but outputs motion trajectories instead of pixels. This approach
captures scene-wide dynamics and uncertainty, yielding more accurate and
diverse predictions than prior regressors and generators. We extensively
evaluate our method on simulated data, demonstrate its effectiveness on
downstream applications such as robotics, and show promising accuracy on
real-world intuitive physics datasets. Although recent state-of-the-art video
generators are often regarded as world models, we show that they struggle with
forecasting motion from a single image, even in simple physical scenarios such
as falling blocks or mechanical object interactions, despite fine-tuning on
such data. We show that this limitation arises from the overhead of generating
pixels rather than directly modeling motion.
\\ ( https://arxiv.org/abs/2509.21592 ,  6590kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21595
Date: Thu, 25 Sep 2025 21:05:07 GMT   (4193kb)

Title: Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature
  Representations for Video Action Analysis
Authors: Sai Varun Kodathala and Rakesh Vunnam
Categories: cs.CV cs.AI
\\
  This study presents a comprehensive comparative analysis of two prominent
self-supervised learning architectures for video action recognition: DINOv3,
which processes frames independently through spatial feature extraction, and
V-JEPA2, which employs joint temporal modeling across video sequences. We
evaluate both approaches on the UCF Sports dataset, examining feature quality
through multiple dimensions including classification accuracy, clustering
performance, intra-class consistency, and inter-class discrimination. Our
analysis reveals fundamental architectural trade-offs: DINOv3 achieves superior
clustering performance (Silhouette score: 0.31 vs 0.21) and demonstrates
exceptional discrimination capability (6.16x separation ratio) particularly for
pose-identifiable actions, while V-JEPA2 exhibits consistent reliability across
all action types with significantly lower performance variance (0.094 vs
0.288). Through action-specific evaluation, we identify that DINOv3's spatial
processing architecture excels at static pose recognition but shows degraded
performance on motion-dependent actions, whereas V-JEPA2's temporal modeling
provides balanced representation quality across diverse action categories.
These findings contribute to the understanding of architectural design choices
in video analysis systems and provide empirical guidance for selecting
appropriate feature extraction methods based on task requirements and
reliability constraints.
\\ ( https://arxiv.org/abs/2509.21595 ,  4193kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21609
Date: Thu, 25 Sep 2025 21:21:00 GMT   (12244kb)

Title: VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster
  Assessment
Authors: Md. Mahfuzur Rahman, Kishor Datta Gupta, Marufa Kamal, Fahad Rahman,
  Sunzida Siddique, Ahmed Rafi Hasan, Mohd Ariful Haque, Roy George
Categories: cs.CV cs.LG
Comments: 29 pages, 40 figures, 3 algorithms
\\
  Immediate damage assessment is essential after natural catastrophes; yet,
conventional hand evaluation techniques are sluggish and perilous. Although
satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives
of impacted regions, current computer vision methodologies generally yield just
classification labels or segmentation masks, so constraining their capacity to
deliver a thorough situational comprehension. We introduce the Vision Language
Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive,
contextually-informed explanations of disaster imagery. VLCE employs a
dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone
pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision
Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.
Both systems utilize external semantic knowledge from ConceptNet and WordNet to
expand vocabulary coverage and improve description accuracy. We assess VLCE in
comparison to leading vision-language models (LLaVA and QwenVL) utilizing
CLIPScore for semantic alignment and InfoMetIC for caption informativeness.
Experimental findings indicate that VLCE markedly surpasses baseline models,
attaining a maximum of 95.33% on InfoMetIC while preserving competitive
semantic alignment. Our dual-architecture system demonstrates significant
potential for improving disaster damage assessment by automating the production
of actionable, information-dense descriptions from satellite and drone photos.
\\ ( https://arxiv.org/abs/2509.21609 ,  12244kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21628
Date: Thu, 25 Sep 2025 21:46:09 GMT   (1525kb)

Title: A Data-driven Typology of Vision Models from Integrated Representational
  Metrics
Authors: Jialin Wu, Shreya Saha, Yiqing Bo, Meenakshi Khosla
Categories: cs.CV cs.AI
\\
  Large vision models differ widely in architecture and training paradigm, yet
we lack principled methods to determine which aspects of their representations
are shared across families and which reflect distinctive computational
strategies. We leverage a suite of representational similarity metrics, each
capturing a different facet-geometry, unit tuning, or linear decodability-and
assess family separability using multiple complementary measures. Metrics
preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family
discrimination, whereas flexible mappings such as Linear Predictivity show
weaker separation. These findings indicate that geometry and tuning carry
family-specific signatures, while linearly decodable information is more
broadly shared. To integrate these complementary facets, we adapt Similarity
Network Fusion (SNF), a method inspired by multi-omics integration. SNF
achieves substantially sharper family separation than any individual metric and
produces robust composite signatures. Clustering of the fused similarity matrix
recovers both expected and surprising patterns: supervised ResNets and ViTs
form distinct clusters, yet all self-supervised models group together across
architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with
masked autoencoders, suggesting convergence between architectural modernization
and reconstruction-based training. This biology-inspired framework provides a
principled typology of vision models, showing that emergent computational
strategies-shaped jointly by architecture and training objective-define
representational structure beyond surface design categories.
\\ ( https://arxiv.org/abs/2509.21628 ,  1525kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21657
Date: Thu, 25 Sep 2025 22:24:23 GMT   (6471kb)

Title: FantasyWorld: Geometry-Consistent World Modeling via Unified Video and
  3D Prediction
Authors: Yixiang Dai, Fan Jiang, Chiyu Wang, Mu Xu, Yonggang Qi
Categories: cs.CV
\\
  High-quality 3D world models are pivotal for embodied intelligence and
Artificial General Intelligence (AGI), underpinning applications such as AR/VR
content creation and robotic navigation. Despite the established strong
imaginative priors, current video foundation models lack explicit 3D grounding
capabilities, thus being limited in both spatial consistency and their utility
for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a
geometry-enhanced framework that augments frozen video foundation models with a
trainable geometric branch, enabling joint modeling of video latents and an
implicit 3D field in a single forward pass. Our approach introduces
cross-branch supervision, where geometry cues guide video generation and video
priors regularize 3D prediction, thus yielding consistent and generalizable
3D-aware video representations. Notably, the resulting latents from the
geometric branch can potentially serve as versatile representations for
downstream 3D tasks such as novel view synthesis and navigation, without
requiring per-scene optimization or fine-tuning. Extensive experiments show
that FantasyWorld effectively bridges video imagination and 3D perception,
outperforming recent geometry-consistent baselines in multi-view coherence and
style consistency. Ablation studies further confirm that these gains stem from
the unified backbone and cross-branch information exchange.
\\ ( https://arxiv.org/abs/2509.21657 ,  6471kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21670
Date: Thu, 25 Sep 2025 22:38:36 GMT   (12502kb)

Title: MORPH: Shape-agnostic PDE Foundation Models
Authors: Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C.
  Love, Ayan Biswas, Diane Oyen, Earl Lawrence
Categories: cs.CV cs.AI cs.LG physics.comp-ph
\\
  We introduce MORPH, a shape-agnostic, autoregressive foundation model for
partial differential equations (PDEs). MORPH is built on a convolutional vision
transformer backbone that seamlessly handles heterogeneous spatiotemporal
datasets of varying data dimensionality (1D--3D) at different resolutions,
multiple fields with mixed scalar and vector components. The architecture
combines (i) component-wise convolution, which jointly processes scalar and
vector channels to capture local interactions, (ii) inter-field
cross-attention, which models and selectively propagates information between
different physical fields, (iii) axial attentions, which factorizes full
spatiotemporal self-attention along individual spatial and temporal axes to
reduce computational burden while retaining expressivity. We pretrain multiple
model variants on a diverse collection of heterogeneous PDE datasets and
evaluate transfer to a range of downstream prediction tasks. Using both
full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH
outperforms models trained from scratch in both zero-shot and full-shot
generalization. Across extensive evaluations, MORPH matches or surpasses strong
baselines and recent state-of-the-art models. Collectively, these capabilities
present a flexible and powerful backbone for learning from heterogeneous and
multimodal nature of scientific observations, charting a path toward scalable
and data-efficient scientific machine learning.
\\ ( https://arxiv.org/abs/2509.21670 ,  12502kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21696
Date: Thu, 25 Sep 2025 23:48:06 GMT   (829kb)

Title: MS-YOLO: Infrared Object Detection for Edge Deployment via MobileNetV4
  and SlideLoss
Authors: Jiali Zhang, Thomas S. White, Haoliang Zhang, Wenqing Hu, Donald C.
  Wunsch II, Jian Liu
Categories: cs.CV
Comments: Accepted by the International Joint Conference on Neural Networks
  (IJCNN) 2025. Keywords: Infrared Object Detection, MobileNetV4, SlideLoss,
  YOLO Model
\\
  Infrared imaging has emerged as a robust solution for urban object detection
under low-light and adverse weather conditions, offering significant advantages
over traditional visible-light cameras. However, challenges such as class
imbalance, thermal noise, and computational constraints can significantly
hinder model performance in practical settings. To address these issues, we
evaluate multiple YOLO variants on the FLIR ADAS V2 dataset, ultimately
selecting YOLOv8 as our baseline due to its balanced accuracy and efficiency.
Building on this foundation, we present \texttt{MS-YOLO} (\textbf{M}obileNetv4
and \textbf{S}lideLoss based on YOLO), which replaces YOLOv8's CSPDarknet
backbone with the more efficient MobileNetV4, reducing computational overhead
by \textbf{1.5%} while sustaining high accuracy. In addition, we introduce
\emph{SlideLoss}, a novel loss function that dynamically emphasizes
under-represented and occluded samples, boosting precision without sacrificing
recall. Experiments on the FLIR ADAS V2 benchmark show that \texttt{MS-YOLO}
attains competitive mAP and superior precision while operating at only
\textbf{6.7 GFLOPs}. These results demonstrate that \texttt{MS-YOLO}
effectively addresses the dual challenge of maintaining high detection quality
while minimizing computational costs, making it well-suited for real-time edge
deployment in urban environments.
\\ ( https://arxiv.org/abs/2509.21696 ,  829kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21715
Date: Fri, 26 Sep 2025 00:25:30 GMT   (13631kb)

Title: Motion-Aware Transformer for Multi-Object Tracking
Authors: Xu Yang and Gady Agam
Categories: cs.CV
\\
  Multi-object tracking (MOT) in videos remains challenging due to complex
object motions and crowded scenes. Recent DETR-based frameworks offer
end-to-end solutions but typically process detection and tracking queries
jointly within a single Transformer Decoder layer, leading to conflicts and
degraded association accuracy. We introduce the Motion-Aware Transformer
(MATR), which explicitly predicts object movements across frames to update
track queries in advance. By reducing query collisions, MATR enables more
consistent training and improves both detection and association. Extensive
experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers
significant gains across standard metrics. On DanceTrack, MATR improves HOTA by
more than 9 points over MOTR without additional data and reaches a new
state-of-the-art score of 71.3 with supplementary data. MATR also achieves
state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6
mHOTA) without relying on external datasets. These results demonstrate that
explicitly modeling motion within end-to-end Transformers offers a simple yet
highly effective approach to advancing multi-object tracking.
\\ ( https://arxiv.org/abs/2509.21715 ,  13631kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21719
Date: Fri, 26 Sep 2025 00:29:36 GMT   (2341kb)

Title: DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video
  Deraining
Authors: Shuning Sun, Jialang Lu, Xiang Chen, Jichao Wang, Dianjie Lu, Guijuan
  Zhang, Guangwei Gao, and Zhuoran Zheng
Categories: cs.CV
\\
  Videos captured in the wild often suffer from rain streaks, blur, and noise.
In addition, even slight changes in camera pose can amplify cross-frame
mismatches and temporal artifacts. Existing methods rely on optical flow or
heuristic alignment, which are computationally expensive and less robust. To
address these challenges, Lie groups provide a principled way to represent
continuous geometric transformations, making them well-suited for enforcing
spatial and temporal consistency in video modeling. Building on this insight,
we propose DeLiVR, an efficient video deraining method that injects
spatiotemporal Lie-group differential biases directly into attention scores of
the network. Specifically, the method introduces two complementary components.
First, a rotation-bounded Lie relative bias predicts the in-plane angle of each
frame using a compact prediction module, where normalized coordinates are
rotated and compared with base coordinates to achieve geometry-consistent
alignment before feature aggregation. Second, a differential group displacement
computes angular differences between adjacent frames to estimate a velocity.
This bias computation combines temporal decay and attention masks to focus on
inter-frame relationships while precisely matching the direction of rain
streaks. Extensive experimental results demonstrate the effectiveness of our
method on publicly available benchmarks.
\\ ( https://arxiv.org/abs/2509.21719 ,  2341kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21722
Date: Fri, 26 Sep 2025 00:46:17 GMT   (845kb)

Title: On the Status of Foundation Models for SAR Imagery
Authors: Nathan Inkawhich
Categories: cs.CV eess.IV
\\
  In this work we investigate the viability of foundational AI/ML models for
Synthetic Aperture Radar (SAR) object recognition tasks. We are inspired by the
tremendous progress being made in the wider community, particularly in the
natural image domain where frontier labs are training huge models on web-scale
datasets with unprecedented computing budgets. It has become clear that these
models, often trained with Self-Supervised Learning (SSL), will transform how
we develop AI/ML solutions for object recognition tasks - they can be adapted
downstream with very limited labeled data, they are more robust to many forms
of distribution shift, and their features are highly transferable
out-of-the-box. For these reasons and more, we are motivated to apply this
technology to the SAR domain. In our experiments we first run tests with
today's most powerful visual foundational models, including DINOv2, DINOv3 and
PE-Core and observe their shortcomings at extracting semantically-interesting
discriminative SAR target features when used off-the-shelf. We then show that
Self-Supervised finetuning of publicly available SSL models with SAR data is a
viable path forward by training several AFRL-DINOv2s and setting a new
state-of-the-art for SAR foundation models, significantly outperforming today's
best SAR-domain model SARATR-X. Our experiments further analyze the performance
trade-off of using different backbones with different downstream
task-adaptation recipes, and we monitor each model's ability to overcome
challenges within the downstream environments (e.g., extended operating
conditions and low amounts of labeled data). We hope this work will inform and
inspire future SAR foundation model builders, because despite our positive
results, we still have a long way to go.
\\ ( https://arxiv.org/abs/2509.21722 ,  845kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21733
Date: Fri, 26 Sep 2025 01:02:00 GMT   (140kb)

Title: UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile
  Environments
Authors: Jiannan Xiang, Yun Zhu, Lei Shu, Maria Wang, Lijun Yu, Gabriel Barcik,
  James Lyon, Srinivas Sunkara, Jindong Chen
Categories: cs.CV cs.AI cs.CL cs.HC cs.LG
\\
  Developing and testing user interfaces (UIs) and training AI agents to
interact with them are challenging due to the dynamic and diverse nature of
real-world mobile environments. Existing methods often rely on cumbersome
physical devices or limited static analysis of screenshots, which hinders
scalable testing and the development of intelligent UI agents. We introduce
UISim, a novel image-based UI simulator that offers a dynamic and interactive
platform for exploring mobile phone environments purely from screen images. Our
system employs a two-stage method: given an initial phone screen image and a
user action, it first predicts the abstract layout of the next UI state, then
synthesizes a new, visually consistent image based on this predicted layout.
This approach enables the realistic simulation of UI transitions. UISim
provides immediate practical benefits for UI testing, rapid prototyping, and
synthetic data generation. Furthermore, its interactive capabilities pave the
way for advanced applications, such as UI navigation task planning for AI
agents. Our experimental results show that UISim outperforms end-to-end UI
generation baselines in generating realistic and coherent subsequent UI states,
highlighting its fidelity and potential to streamline UI development and
enhance AI agent training.
\\ ( https://arxiv.org/abs/2509.21733 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21738
Date: Fri, 26 Sep 2025 01:07:05 GMT   (1531kb)

Title: LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal
  Vessel Segmentation
Authors: Mehwish Mehmood, Ivor Spence, Muhammad Fahim
Categories: cs.CV cs.AI
\\
  Lightweight retinal vessel segmentation is important for the early diagnosis
of vision-threatening and systemic diseases, especially in a real-world
clinical environment with limited computational resources. Although
segmentation methods based on deep learning are improving, existing models are
still facing challenges of small vessel segmentation and high computational
costs. To address these challenges, we proposed a new vascular segmentation
network, LFA-Net, which incorporates a newly designed attention module,
LiteFusion-Attention. This attention module incorporates residual learning
connections, Vision Mamba-inspired dynamics, and modulation-based attention,
enabling the model to capture local and global context efficiently and in a
lightweight manner. LFA-Net offers high performance with 0.11 million
parameters, 0.42 MB memory size, and 4.46 GFLOPs, which make it ideal for
resource-constrained environments. We validated our proposed model on DRIVE,
STARE, and CHASE_DB with outstanding performance in terms of dice scores of
83.28, 87.44, and 84.50% and Jaccard indices of 72.85, 79.31, and 74.70%,
respectively. The code of LFA-Net is available online
https://github.com/Mehwish4593/LFA-Net.
\\ ( https://arxiv.org/abs/2509.21738 ,  1531kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21747
Date: Fri, 26 Sep 2025 01:25:39 GMT   (754kb)

Title: Incorporating Scene Context and Semantic Labels for Enhanced Group-level
  Emotion Recognition
Authors: Qing Zhu, Wangdong Guo, Qirong Mao, Xiaohua Huang, Xiuyan Shao,
  Wenming Zheng
Categories: cs.CV
Comments: 10 pages, 5figures, submitted to IEEE Transactions on Human-Machine
  Systems
\\
  Group-level emotion recognition (GER) aims to identify holistic emotions
within a scene involving multiple individuals. Current existed methods
underestimate the importance of visual scene contextual information in modeling
individual relationships. Furthermore, they overlook the crucial role of
semantic information from emotional labels for complete understanding of
emotions. To address this limitation, we propose a novel framework that
incorporates visual scene context and label-guided semantic information to
improve GER performance. It involves the visual context encoding module that
leverages multi-scale scene information to diversely encode individual
relationships. Complementarily, the emotion semantic encoding module utilizes
group-level emotion labels to prompt a large language model to generate nuanced
emotion lexicons. These lexicons, in conjunction with the emotion labels, are
then subsequently refined into comprehensive semantic representations through
the utilization of a structured emotion tree. Finally, similarity-aware
interaction is proposed to align and integrate visual and semantic information,
thereby generating enhanced group-level emotion representations and
subsequently improving the performance of GER. Experiments on three widely
adopted GER datasets demonstrate that our proposed method achieves competitive
performance compared to state-of-the-art methods.
\\ ( https://arxiv.org/abs/2509.21747 ,  754kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21750
Date: Fri, 26 Sep 2025 01:29:12 GMT   (192kb)

Title: KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via
  Conditional Random Fields
Authors: Yu Li, Da Chang, Xi Xiao
Categories: cs.CV
\\
  While the Segment Anything Model (SAM) has achieved remarkable success in
image segmentation, its direct application to medical imaging remains hindered
by fundamental challenges, including ambiguous boundaries, insufficient
modeling of anatomical relationships, and the absence of uncertainty
quantification. To address these limitations, we introduce KG-SAM, a
knowledge-guided framework that synergistically integrates anatomical priors
with boundary refinement and uncertainty estimation. Specifically, KG-SAM
incorporates (i) a medical knowledge graph to encode fine-grained anatomical
relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce
anatomically consistent predictions, and (iii) an uncertainty-aware fusion
module to enhance reliability in high-stakes clinical scenarios. Extensive
experiments across multi-center medical datasets demonstrate the effectiveness
of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate
segmentation and delivers substantial gains in abdominal segmentation, reaching
78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and
generalizable framework for advancing medical image segmentation.
\\ ( https://arxiv.org/abs/2509.21750 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21760
Date: Fri, 26 Sep 2025 01:43:40 GMT   (8152kb)

Title: UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models
Authors: Lan Chen, Yuchao Gu, Qi Mao
Categories: cs.CV
\\
  Large language models, trained on extensive corpora, successfully unify
diverse linguistic tasks within a single generative framework. Inspired by
this, recent works like Large Vision Model (LVM) extend this paradigm to vision
by organizing tasks into sequential visual sentences, where visual prompts
serve as the context to guide outputs. However, such modeling requires
task-specific pre-training across modalities and sources, which is costly and
limits scalability to unseen tasks. Given that pre-trained video generation
models inherently capture temporal sequence dependencies, we explore a more
unified and scalable alternative: can a pre-trained video generation model
adapt to diverse image and video tasks? To answer this, we propose UniVid, a
framework that fine-tunes a video diffusion transformer to handle various
vision tasks without task-specific modifications. Tasks are represented as
visual sentences, where the context sequence defines both the task and the
expected output modality. We evaluate the generalization of UniVid from two
perspectives: (1) cross-modal inference with contexts composed of both images
and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks
from natural to annotated data, without multi-source pre-training. Despite
being trained solely on natural video data, UniVid generalizes well in both
settings. Notably, understanding and generation tasks can easily switch by
simply reversing the visual sentence order in this paradigm. These findings
highlight the potential of pre-trained video generation models to serve as a
scalable and unified foundation for vision modeling. Our code will be released
at https://github.com/CUC-MIPG/UniVid.
\\ ( https://arxiv.org/abs/2509.21760 ,  8152kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21764
Date: Fri, 26 Sep 2025 01:59:29 GMT   (4507kb)

Title: CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones
Authors: Wenyi Gong, Mieszko Lis
Categories: cs.CV cs.LG
\\
  Many modern ViT backbones adopt spatial architectural designs, such as window
attention, decomposed relative positional embeddings in SAM, and RoPE in
DINOv3. Such architectures impose new challenges on token reduction, as the
vast majority of existing methods fail to preserve the spatial structure these
architectures depend on. In this paper, we introduce a simple yet effective
token merging method that maintains spatial integrity, enabling seamless
compatibility with spatial architectures. We reconcile two seemingly
conflicting requirements: (i)exploiting the uneven information distribution
across the spatial layout while (ii)preserving the spatial structure
post-merging. Our approach employs (i)a 2D reduction strategy to enforce
structured token layouts, (ii)a spatial-aware merging algorithm that maintains
relative token positions, and (iii)a novel max-magnitude-per-dimension token
representation that preserves salient features. Our method demonstrates strong
performance both off-the-shelf and with fine-tuning, achieving state-of-the-art
results on spatial and non-spatial architectures across various vision tasks.
Specifically, we achieve 1.25x speedup on SAM-H with only 0.7% mIOU drop
evaluated on COCO off-the-shelf, and 1.15x speedup on DeiT-B with no top-1
accuracy drop on ImageNet within just one epoch of fine-tuning.
\\ ( https://arxiv.org/abs/2509.21764 ,  4507kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21774
Date: Fri, 26 Sep 2025 02:22:12 GMT   (780kb)

Title: Training-Free Multimodal Deepfake Detection via Graph Reasoning
Authors: Yuxin Liu, Fei Wang, Kun Li, Yiqi Nie, Junjie Chen, Yanyan Wei,
  Zhangling Duan, Zhaohong Jia
Categories: cs.CV cs.CY
\\
  Multimodal deepfake detection (MDD) aims to uncover manipulations across
visual, textual, and auditory modalities, thereby reinforcing the reliability
of modern information systems. Although large vision-language models (LVLMs)
exhibit strong multimodal reasoning, their effectiveness in MDD is limited by
challenges in capturing subtle forgery cues, resolving cross-modal
inconsistencies, and performing task-aligned retrieval. To this end, we propose
Guided Adaptive Scorer and Propagation In-Context Learning (GASP-ICL), a
training-free framework for MDD. GASP-ICL employs a pipeline to preserve
semantic relevance while injecting task-aware knowledge into LVLMs. We leverage
an MDD-adapted feature extractor to retrieve aligned image-text pairs and build
a candidate set. We further design the Graph-Structured Taylor Adaptive Scorer
(GSTAS) to capture cross-sample relations and propagate query-aligned signals,
producing discriminative exemplars. This enables precise selection of
semantically aligned, task-relevant demonstrations, enhancing LVLMs for robust
MDD. Experiments on four forgery types show that GASP-ICL surpasses strong
baselines, delivering gains without LVLM fine-tuning.
\\ ( https://arxiv.org/abs/2509.21774 ,  780kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21783
Date: Fri, 26 Sep 2025 02:39:40 GMT   (536kb)

Title: Prompt-guided Representation Disentanglement for Action Recognition
Authors: Tianci Wu, Guangming Zhu, Jiang Lu, Siyuan Wang, Ning Wang, Nuoye
  Xiong, Zhang Liang
Categories: cs.CV
\\
  Action recognition is a fundamental task in video understanding. Existing
methods typically extract unified features to process all actions in one video,
which makes it challenging to model the interactions between different objects
in multi-action scenarios. To alleviate this issue, we explore disentangling
any specified actions from complex scenes as an effective solution. In this
paper, we propose Prompt-guided Disentangled Representation for Action
Recognition (ProDA), a novel framework that disentangles any specified actions
from a multi-action scene. ProDA leverages Spatio-temporal Scene Graphs (SSGs)
and introduces Dynamic Prompt Module (DPM) to guide a Graph Parsing Neural
Network (GPNN) in generating action-specific representations. Furthermore, we
design a video-adapted GPNN that aggregates information using dynamic weights.
Experiments in video action recognition demonstrate the effectiveness of our
approach when compared with the state-of-the-art methods. Our code can be found
in https://github.com/iamsnaping/ProDA.git
\\ ( https://arxiv.org/abs/2509.21783 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21787
Date: Fri, 26 Sep 2025 02:42:59 GMT   (2540kb)

Title: DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate
  Speech in Images
Authors: Dwip Dalal, Gautam Vashishtha, Anku Ranui, Aishwarya Reganti, Parth
  Patwa, Mohd Sarique, Chandan Gupta, Keshav Nath, Viswanatha Reddy, Vinija
  Jain, Aman Chadha, Amitava Das, Amit Sheth, and Asif Ekbal
Categories: cs.CV cs.CL
Comments: Defactify 3 workshop at AAAI 2024
\\
  The rise in harmful online content not only distorts public discourse but
also poses significant challenges to maintaining a healthy digital environment.
In response to this, we introduce a multimodal dataset uniquely crafted for
identifying hate in digital content. Central to our methodology is the
innovative application of watermarked, stability-enhanced, stable diffusion
techniques combined with the Digital Attention Analysis Module (DAAM). This
combination is instrumental in pinpointing the hateful elements within images,
thereby generating detailed hate attention maps, which are used to blur these
regions from the image, thereby removing the hateful sections of the image. We
release this data set as a part of the dehate shared task. This paper also
describes the details of the shared task. Furthermore, we present DeHater, a
vision-language model designed for multimodal dehatification tasks. Our
approach sets a new standard in AI-driven image hate detection given textual
prompts, contributing to the development of more ethical AI applications in
social media.
\\ ( https://arxiv.org/abs/2509.21787 ,  2540kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21788
Date: Fri, 26 Sep 2025 02:43:22 GMT   (304kb)

Title: MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning
Authors: Lihao Zheng, Jiawei Chen, Xintian Shen, Hao Ma, Tao Wei
Categories: cs.CV
\\
  Multi-image reasoning and grounding require understanding complex cross-image
relationships at both object levels and image levels. Current Large Visual
Language Models (LVLMs) face two critical challenges: the lack of cross-image
reasoning capabilities and insufficient cross-image reference reward modeling.
To address these issues, we propose a unified framework - Multi-Image Reasoning
and Grounding with Reinforcement Learning (MIRG-RL). Specifically, our
two-stage training paradigm combines supervised fine-tuning with annotated
trajectories and image-aware reinforcement learning optimization, progressively
developing multi-image reasoning capabilities. Furthermore, we innovatively
propose a method for constructing the trajectory data, which integrates
object-level and image-level annotation information, and use this method to
generate a lightweight reasoning-enhanced dataset. To effectively resolve
cross-image ambiguities, we design an image-aware RL policy with dual reward
functions for objects and images. Experiments demonstrate that MIRG-RL achieves
state-of-the-art (SOTA) performance in multi-image grounding benchmarks,
attaining 64.82% on cross-image reasoning tasks - exceeding the previous best
method by 1%. The code and dataset have been released at
https://github.com/ZEUS2035/MIRG-RL.
\\ ( https://arxiv.org/abs/2509.21788 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21790
Date: Fri, 26 Sep 2025 02:47:05 GMT   (2142kb)

Title: LongScape: Advancing Long-Horizon Embodied World Models with
  Context-Aware MoE
Authors: Yu Shang, Lei Jin, Yiding Ma, Xin Zhang, Chen Gao, Wei Wu, Yong Li
Categories: cs.CV
Comments: 13 pages, 8 figures
\\
  Video-based world models hold significant potential for generating
high-quality embodied manipulation data. However, current video generation
methods struggle to achieve stable long-horizon generation: classical
diffusion-based approaches often suffer from temporal inconsistency and visual
drift over multiple rollouts, while autoregressive methods tend to compromise
on visual detail. To solve this, we introduce LongScape, a hybrid framework
that adaptively combines intra-chunk diffusion denoising with inter-chunk
autoregressive causal generation. Our core innovation is an action-guided,
variable-length chunking mechanism that partitions video based on the semantic
context of robotic actions. This ensures each chunk represents a complete,
coherent action, enabling the model to flexibly generate diverse dynamics. We
further introduce a Context-aware Mixture-of-Experts (CMoE) framework that
adaptively activates specialized experts for each chunk during generation,
guaranteeing high visual quality and seamless chunk transitions. Extensive
experimental results demonstrate that our method achieves stable and consistent
long-horizon generation over extended rollouts. Our code is available at:
https://github.com/tsinghua-fib-lab/Longscape.
\\ ( https://arxiv.org/abs/2509.21790 ,  2142kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21797
Date: Fri, 26 Sep 2025 02:54:36 GMT   (1050kb)

Title: MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel
  Feature Modulation
Authors: Yu Shang, Yangcheng Yu, Xin Zhang, Xin Jin, Haisheng Su, Wei Wu, Yong
  Li
Categories: cs.CV
Comments: 11 pages, 4 figures
\\
  Embodied action planning is a core challenge in robotics, requiring models to
generate precise actions from visual observations and language instructions.
While video generation world models are promising, their reliance on
pixel-level reconstruction often introduces visual redundancies that hinder
action decoding and generalization. Latent world models offer a compact,
motion-aware representation, but overlook the fine-grained details critical for
precise manipulation. To overcome these limitations, we propose MoWM, a
mixture-of-world-model framework that fuses representations from hybrid world
models for embodied action planning. Our approach uses motion-aware
representations from a latent model as a high-level prior, which guides the
extraction of fine-grained visual features from the pixel space model. This
design allows MoWM to highlight the informative visual details needed for
action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that
our method achieves state-of-the-art task success rates and superior
generalization. We also provide a comprehensive analysis of the strengths of
each feature space, offering valuable insights for future research in embodied
planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.
\\ ( https://arxiv.org/abs/2509.21797 ,  1050kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21839
Date: Fri, 26 Sep 2025 03:53:31 GMT   (32705kb)

Title: DiTraj: training-free trajectory control for video diffusion transformer
Authors: Cheng Lei, Jiayu Zhang, Yue Ma, Xinyu Wang, Long Chen, Liang Tang,
  Yiqiang Yan, Fei Su, Zhicheng Zhao
Categories: cs.CV cs.AI
\\
  Diffusion Transformers (DiT)-based video generation models with 3D full
attention exhibit strong generative capabilities. Trajectory control represents
a user-friendly task in the field of controllable video generation. However,
existing methods either require substantial training resources or are
specifically designed for U-Net, do not take advantage of the superior
performance of DiT. To address these issues, we propose DiTraj, a simple but
effective training-free framework for trajectory control in text-to-video
generation, tailored for DiT. Specifically, first, to inject the object's
trajectory, we propose foreground-background separation guidance: we use the
Large Language Model (LLM) to convert user-provided prompts into foreground and
background prompts, which respectively guide the generation of foreground and
background regions in the video. Then, we analyze 3D full attention and explore
the tight correlation between inter-token attention scores and position
embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled
3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding,
STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening
cross-frame attention among them and thus enhancing trajectory control.
Additionally, we achieve 3D-aware trajectory control by regulating the density
of position embedding. Extensive experiments demonstrate that our method
outperforms previous methods in both video quality and trajectory
controllability.
\\ ( https://arxiv.org/abs/2509.21839 ,  32705kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21845
Date: Fri, 26 Sep 2025 04:11:10 GMT   (535kb)

Title: A Comprehensive Evaluation of Transformer-Based Question Answering
  Models and RAG-Enhanced Design
Authors: Zichen Zhang, Kunlong Zhang, Hongwei Ruan, Yiming Luo
Categories: cs.CV
\\
  Transformer-based models have advanced the field of question answering, but
multi-hop reasoning, where answers require combining evidence across multiple
passages, remains difficult. This paper presents a comprehensive evaluation of
retrieval strategies for multi-hop question answering within a
retrieval-augmented generation framework. We compare cosine similarity, maximal
marginal relevance, and a hybrid method that integrates dense embeddings with
lexical overlap and re-ranking. To further improve retrieval, we adapt the
EfficientRAG pipeline for query optimization, introducing token labeling and
iterative refinement while maintaining efficiency. Experiments on the HotpotQA
dataset show that the hybrid approach substantially outperforms baseline
methods, achieving a relative improvement of 50 percent in exact match and 47
percent in F1 score compared to cosine similarity. Error analysis reveals that
hybrid retrieval improves entity recall and evidence complementarity, while
remaining limited in handling distractors and temporal reasoning. Overall, the
results suggest that hybrid retrieval-augmented generation provides a practical
zero-shot solution for multi-hop question answering, balancing accuracy,
efficiency, and interpretability.
\\ ( https://arxiv.org/abs/2509.21845 ,  535kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21853
Date: Fri, 26 Sep 2025 04:29:22 GMT   (8050kb)

Title: Dynamic Novel View Synthesis in High Dynamic Range
Authors: Kaixuan Zhang, Zhipeng Xiong, Minxian Li, Mingwu Ren, Jiankang Deng
  and Xiatian Zhu
Categories: cs.CV
\\
  High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D
model from Low Dynamic Range (LDR) training images captured under conventional
imaging conditions. Current methods primarily focus on static scenes,
implicitly assuming all scene elements remain stationary and non-living.
However, real-world scenarios frequently feature dynamic elements, such as
moving objects, varying lighting conditions, and other temporal events, thereby
presenting a significantly more challenging scenario. To address this gap, we
propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR
DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of
jointly modeling temporal radiance variations alongside sophisticated 3D
translation between LDR and HDR. To tackle this complex, intertwined challenge,
we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an
innovative dynamic tone-mapping module that explicitly connects HDR and LDR
domains, maintaining temporal radiance coherence by dynamically adapting
tone-mapping functions according to the evolving radiance distributions across
the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance
consistency and spatially accurate color translation, enabling photorealistic
HDR renderings from arbitrary viewpoints and time instances. Extensive
experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art
methods in both quantitative performance and visual fidelity. Source code will
be released.
\\ ( https://arxiv.org/abs/2509.21853 ,  8050kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21859
Date: Fri, 26 Sep 2025 04:34:34 GMT   (1694kb)

Title: SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware
  Neural Image Representations and Explicit 3D Meshes
Authors: Minje Kim, Tae-Kyun Kim
Categories: cs.CV
Comments: 10 pages, 6 figures
\\
  Reconstructing detailed hand avatars plays a crucial role in various
applications. While prior works have focused on capturing high-fidelity hand
geometry, they heavily rely on high-resolution multi-view image inputs and
struggle to generalize on low-resolution images. Multi-view image
super-resolution methods have been proposed to enforce 3D view consistency.
These methods, however, are limited to static objects/scenes with fixed
resolutions and are not applicable to articulated deformable hands. In this
paper, we propose SRHand (Super-Resolution Hand), the method for reconstructing
detailed 3D geometry as well as textured images of hands from low-resolution
images. SRHand leverages the advantages of implicit image representation with
explicit hand meshes. Specifically, we introduce a geometric-aware implicit
image function (GIIF) that learns detailed hand prior by upsampling the coarse
input images. By jointly optimizing the implicit image function and explicit 3D
hand shapes, our method preserves multi-view and pose consistency among
upsampled hand images, and achieves fine-detailed 3D reconstruction (wrinkles,
nails). In experiments using the InterHand2.6M and Goliath datasets, our method
significantly outperforms state-of-the-art image upsampling methods adapted to
hand datasets, and 3D hand reconstruction methods, quantitatively and
qualitatively. Project page: https://yunminjin2.github.io/projects/srhand
\\ ( https://arxiv.org/abs/2509.21859 ,  1694kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21864
Date: Fri, 26 Sep 2025 04:40:13 GMT   (570kb)

Title: Deepfakes: we need to re-think the concept of "real" images
Authors: Janis Keuper and Margret Keuper
Categories: cs.CV
\\
  The wide availability and low usability barrier of modern image generation
models has triggered the reasonable fear of criminal misconduct and negative
social implications. The machine learning community has been engaging this
problem with an extensive series of publications proposing algorithmic
solutions for the detection of "fake", e.g. entirely generated or partially
manipulated images. While there is undoubtedly some progress towards technical
solutions of the problem, we argue that current and prior work is focusing too
much on generative algorithms and "fake" data-samples, neglecting a clear
definition and data collection of "real" images. The fundamental question "what
is a real image?" might appear to be quite philosophical, but our analysis
shows that the development and evaluation of basically all current
"fake"-detection methods is relying on only a few, quite old low-resolution
datasets of "real" images like ImageNet. However, the technology for the
acquisition of "real" images, aka taking photos, has drastically evolved over
the last decade: Today, over 90% of all photographs are produced by smartphones
which typically use algorithms to compute an image from multiple inputs (over
time) from multiple sensors. Based on the fact that these image formation
algorithms are typically neural network architectures which are closely related
to "fake"-image generators, we state the position that today, we need to
re-think the concept of "real" images. The purpose of this position paper is to
raise the awareness of the current shortcomings in this active field of
research and to trigger an open discussion whether the detection of "fake"
images is a sound objective at all. At the very least, we need a clear
technical definition of "real" images and new benchmark datasets.
\\ ( https://arxiv.org/abs/2509.21864 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21871
Date: Fri, 26 Sep 2025 04:55:00 GMT   (6120kb)

Title: Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with
  Relative-Absolute Policy Optimization
Authors: Boyang Liu, Yifan Hu, Senjie Jin, Shihan Dou, Gonglei Shi, Jie Shao,
  Tao Gui, Xuanjing Huang
Categories: cs.CV cs.AI
\\
  Multimodal large language models (MLLMs) are well suited to image aesthetic
assessment, as they can capture high-level aesthetic features leveraging their
cross-modal understanding capacity. However, the scarcity of multimodal
aesthetic reasoning data and the inherently subjective nature of aesthetic
judgment make it difficult for MLLMs to generate accurate aesthetic judgments
with interpretable rationales. To this end, we propose Aes-R1, a comprehensive
aesthetic reasoning framework with reinforcement learning (RL). Concretely,
Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality
chain-of-thought aesthetic reasoning data used for cold-start. After teaching
the model to generate structured explanations prior to scoring, we then employ
the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that
jointly optimizes absolute score regression and relative ranking order,
improving both per-image accuracy and cross-image preference judgments. Aes-R1
enables MLLMs to generate grounded explanations alongside faithful scores,
thereby enhancing aesthetic scoring and reasoning in a unified framework.
Extensive experiments demonstrate that Aes-R1 improves the backbone's average
PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar
size. More ablation studies validate Aes-R1's robust generalization under
limited supervision and in out-of-distribution scenarios.
\\ ( https://arxiv.org/abs/2509.21871 ,  6120kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21887
Date: Fri, 26 Sep 2025 05:23:31 GMT   (7847kb)

Title: StableDub: Taming Diffusion Prior for Generalized and Efficient Visual
  Dubbing
Authors: Liyang Chen, Tianze Zhou, Xu He, Boshi Tang, Zhiyong Wu, Yang Huang,
  Yang Wu, Zhongqian Sun, Wei Yang, Helen Meng
Categories: cs.CV cs.MM
\\
  The visual dubbing task aims to generate mouth movements synchronized with
the driving audio, which has seen significant progress in recent years.
However, two critical deficiencies hinder their wide application: (1)
Audio-only driving paradigms inadequately capture speaker-specific lip habits,
which fail to generate lip movements similar to the target avatar; (2)
Conventional blind-inpainting approaches frequently produce visual artifacts
when handling obstructions (e.g., microphones, hands), limiting practical
deployment. In this paper, we propose StableDub, a novel and concise framework
integrating lip-habit-aware modeling with occlusion-robust synthesis.
Specifically, building upon the Stable-Diffusion backbone, we develop a
lip-habit-modulated mechanism that jointly models phonemic audio-visual
synchronization and speaker-specific orofacial dynamics. To achieve plausible
lip geometries and object appearances under occlusion, we introduce the
occlusion-aware training strategy by explicitly exposing the occlusion objects
to the inpainting process. By incorporating the proposed designs, the model
eliminates the necessity for cost-intensive priors in previous methods, thereby
exhibiting superior training efficiency on the computationally intensive
diffusion-based backbone. To further optimize training efficiency from the
perspective of model architecture, we introduce a hybrid Mamba-Transformer
architecture, which demonstrates the enhanced applicability in low-resource
research scenarios. Extensive experimental results demonstrate that StableDub
achieves superior performance in lip habit resemblance and occlusion
robustness. Our method also surpasses other methods in audio-lip sync, video
quality, and resolution consistency. We expand the applicability of visual
dubbing methods from comprehensive aspects, and demo videos can be found at
https://stabledub.github.io.
\\ ( https://arxiv.org/abs/2509.21887 ,  7847kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21888
Date: Fri, 26 Sep 2025 05:23:45 GMT   (7509kb)

Title: Drag4D: Align Your Motion with Text-Driven 3D Scene Generation
Authors: Minjun Kang, Inkyu Shin, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon
Categories: cs.CV
Comments: version 1
\\
  We introduce Drag4D, an interactive framework that integrates object motion
control within text-driven 3D scene generation. This framework enables users to
define 3D trajectories for the 3D objects generated from a single image,
seamlessly integrating them into a high-quality 3D background. Our Drag4D
pipeline consists of three stages. First, we enhance text-to-3D background
generation by applying 2D Gaussian Splatting with panoramic images and
inpainted novel views, resulting in dense and visually complete 3D
reconstructions. In the second stage, given a reference image of the target
object, we introduce a 3D copy-and-paste approach: the target instance is
extracted in a full 3D mesh using an off-the-shelf image-to-3D model and
seamlessly composited into the generated 3D scene. The object mesh is then
positioned within the 3D scene via our physics-aware object position learning,
ensuring precise spatial alignment. Lastly, the spatially aligned object is
temporally animated along a user-defined 3D trajectory. To mitigate motion
hallucination and ensure view-consistent temporal alignment, we develop a
part-augmented, motion-conditioned video diffusion model that processes
multiview image pairs together with their projected 2D trajectories. We
demonstrate the effectiveness of our unified architecture through evaluations
at each stage and in the final results, showcasing the harmonized alignment of
user-controlled object motion within a high-quality 3D background.
\\ ( https://arxiv.org/abs/2509.21888 ,  7509kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21893
Date: Fri, 26 Sep 2025 05:30:06 GMT   (1796kb)

Title: Syncphony: Synchronized Audio-to-Video Generation with Diffusion
  Transformers
Authors: Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh
Categories: cs.CV
Comments: Project page: https://jibin86.github.io/syncphony_project_page
\\
  Text-to-video and image-to-video generation have made rapid progress in
visual quality, but they remain limited in controlling the precise timing of
motion. In contrast, audio provides temporal cues aligned with video motion,
making it a promising condition for temporally controlled video generation.
However, existing audio-to-video (A2V) models struggle with fine-grained
synchronization due to indirect conditioning mechanisms or limited temporal
modeling capacity. We present Syncphony, which generates 380x640 resolution,
24fps videos synchronized with diverse audio inputs. Our approach builds upon a
pre-trained video backbone and incorporates two key components to improve
synchronization: (1) Motion-aware Loss, which emphasizes learning at
high-motion regions; (2) Audio Sync Guidance, which guides the full model using
a visually aligned off-sync model without audio layers to better exploit audio
cues at inference while maintaining visual quality. To evaluate
synchronization, we propose CycleSync, a video-to-audio-based metric that
measures the amount of motion cues in the generated video to reconstruct the
original audio. Experiments on AVSync15 and The Greatest Hits datasets
demonstrate that Syncphony outperforms existing methods in both synchronization
accuracy and visual quality. Project page is available at:
https://jibin86.github.io/syncphony_project_page
\\ ( https://arxiv.org/abs/2509.21893 ,  1796kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21894
Date: Fri, 26 Sep 2025 05:30:11 GMT   (2420kb)

Title: LG-CD: Enhancing Language-Guided Change Detection through SAM2
  Adaptation
Authors: Yixiao Liu (1), Yizhou Yang (1), Jinwen Li (2), Jun Tao (1), Ruoyu Li
  (1), Xiangkun Wang (1), Min Zhu (1), Junlong Cheng (1) ((1) College of
  Computer Science, Sichuan University, China, (2) School of Computer Science
  and Technology, Xinjiang University, China)
Categories: cs.CV
Comments: *Corresponding authors: Min Zhu (min.zhu@scu.edu.cn) and Junlong
  Cheng (jlcheng@scu.edu.cn)
\\
  Remote Sensing Change Detection (RSCD) typically identifies changes in land
cover or surface conditions by analyzing multi-temporal images. Currently, most
deep learning-based methods primarily focus on learning unimodal visual
information, while neglecting the rich semantic information provided by
multimodal data such as text. To address this limitation, we propose a novel
Language-Guided Change Detection model (LG-CD). This model leverages natural
language prompts to direct the network's attention to regions of interest,
significantly improving the accuracy and robustness of change detection.
Specifically, LG-CD utilizes a visual foundational model (SAM2) as a feature
extractor to capture multi-scale pyramid features from high-resolution to
low-resolution across bi-temporal remote sensing images. Subsequently,
multi-layer adapters are employed to fine-tune the model for downstream tasks,
ensuring its effectiveness in remote sensing change detection. Additionally, we
design a Text Fusion Attention Module (TFAM) to align visual and textual
information, enabling the model to focus on target change regions using text
prompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented,
which deeply integrates visual and semantic information through a
cross-attention mechanism to produce highly accurate change detection masks.
Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstrate
that LG-CD consistently outperforms state-of-the-art change detection methods.
Furthermore, our approach provides new insights into achieving generalized
change detection by leveraging multimodal information.
\\ ( https://arxiv.org/abs/2509.21894 ,  2420kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21905
Date: Fri, 26 Sep 2025 05:39:03 GMT   (47554kb)

Title: TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image
  Manipulation
Authors: Qihang Wang, Yaxiong Wang, Lechao Cheng, Zhun Zhong
Categories: cs.CV
\\
  This paper explores image editing under the joint control of text and drag
interactions. While recent advances in text-driven and drag-driven editing have
achieved remarkable progress, they suffer from complementary limitations:
text-driven methods excel in texture manipulation but lack precise spatial
control, whereas drag-driven approaches primarily modify shape and structure
without fine-grained texture guidance. To address these limitations, we propose
a unified diffusion-based framework for joint drag-text image editing,
integrating the strengths of both paradigms. Our framework introduces two key
innovations: (1) Point-Cloud Deterministic Drag, which enhances latent-space
layout control through 3D feature mapping, and (2) Drag-Text Guided Denoising,
dynamically balancing the influence of drag and text conditions during
denoising. Notably, our model supports flexible editing modes - operating with
text-only, drag-only, or combined conditions - while maintaining strong
performance in each setting. Extensive quantitative and qualitative experiments
demonstrate that our method not only achieves high-fidelity joint editing but
also matches or surpasses the performance of specialized text-only or drag-only
approaches, establishing a versatile and generalizable solution for
controllable image manipulation. Code will be made publicly available to
reproduce all results presented in this work.
\\ ( https://arxiv.org/abs/2509.21905 ,  47554kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21916
Date: Fri, 26 Sep 2025 05:55:41 GMT   (3534kb)

Title: Enhancing Vehicle Detection under Adverse Weather Conditions with
  Contrastive Learning
Authors: Boying Li, Chang Liu, Petter Ky\"osti, Mattias \"Ohman, Devashish
  Singha Roy, Sofia Plazzi, Hamam Mokayed and Olle Hagner
Categories: cs.CV
\\
  Aside from common challenges in remote sensing like small, sparse targets and
computation cost limitations, detecting vehicles from UAV images in the Nordic
regions faces strong visibility challenges and domain shifts caused by diverse
levels of snow coverage. Although annotated data are expensive, unannotated
data is cheaper to obtain by simply flying the drones. In this work, we
proposed a sideload-CL-adaptation framework that enables the use of unannotated
data to improve vehicle detection using lightweight models. Specifically, we
propose to train a CNN-based representation extractor through contrastive
learning on the unannotated data in the pretraining stage, and then sideload it
to a frozen YOLO11n backbone in the fine-tuning stage. To find a robust
sideload-CL-adaptation, we conducted extensive experiments to compare various
fusion methods and granularity. Our proposed sideload-CL-adaptation model
improves the detection performance by 3.8% to 9.5% in terms of mAP50 on the NVD
dataset.
\\ ( https://arxiv.org/abs/2509.21916 ,  3534kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21917
Date: Fri, 26 Sep 2025 05:57:04 GMT   (10666kb)

Title: Taming Flow-based I2V Models for Creative Video Editing
Authors: Xianghao Kong, Hansheng Chen, Yuwei Guo, Lvmin Zhang, Gordon
  Wetzstein, Maneesh Agrawala, Anyi Rao
Categories: cs.CV cs.MM
\\
  Although image editing techniques have advanced significantly, video editing,
which aims to manipulate videos according to user intent, remains an emerging
challenge. Most existing image-conditioned video editing methods either require
inversion with model-specific design or need extensive optimization, limiting
their capability of leveraging up-to-date image-to-video (I2V) models to
transfer the editing capability of image editing models to the video domain. To
this end, we propose IF-V2V, an Inversion-Free method that can adapt
off-the-shelf flow-matching-based I2V models for video editing without
significant computational overhead. To circumvent inversion, we devise Vector
Field Rectification with Sample Deviation to incorporate information from the
source video into the denoising process by introducing a deviation term into
the denoising vector field. To further ensure consistency with the source video
in a model-agnostic way, we introduce Structure-and-Motion-Preserving
Initialization to generate motion-aware temporally correlated noise with
structural information embedded. We also present a Deviation Caching mechanism
to minimize the additional computational cost for denoising vector
rectification without significantly impacting editing quality. Evaluations
demonstrate that our method achieves superior editing quality and consistency
over existing approaches, offering a lightweight plug-and-play solution to
realize visual creativity.
\\ ( https://arxiv.org/abs/2509.21917 ,  10666kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21918
Date: Fri, 26 Sep 2025 05:59:12 GMT   (1509kb)

Title: Multi-View Crowd Counting With Self-Supervised Learning
Authors: Hong Mo, Xiong Zhang, Tengfei Shi, Zhongbo Wu
Categories: cs.CV
\\
  Multi-view counting (MVC) methods have attracted significant research
attention and stimulated remarkable progress in recent years. Despite their
success, most MVC methods have focused on improving performance by following
the fully supervised learning (FSL) paradigm, which often requires large
amounts of annotated data. In this work, we propose SSLCounter, a novel
self-supervised learning (SSL) framework for MVC that leverages neural
volumetric rendering to alleviate the reliance on large-scale annotated
datasets. SSLCounter learns an implicit representation w.r.t. the scene,
enabling the reconstruction of continuous geometry shape and the complex,
view-dependent appearance of their 2D projections via differential neural
rendering. Owing to its inherent flexibility, the key idea of our method can be
seamlessly integrated into exsiting frameworks. Notably, extensive experiments
demonstrate that SSLCounter not only demonstrates state-of-the-art performances
but also delivers competitive performance with only using 70% proportion of
training data, showcasing its superior data efficiency across multiple MVC
benchmarks.
\\ ( https://arxiv.org/abs/2509.21918 ,  1509kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21922
Date: Fri, 26 Sep 2025 06:06:19 GMT   (16881kb)

Title: Spatial Reasoning in Foundation Models: Benchmarking Object-Centric
  Spatial Understanding
Authors: Vahid Mirjalili, Ramin Giahi, Sriram Kollipara, Akshay Kekuda, Kehui
  Yao, Kai Zhao, Jianpeng Xu, Kaushiki Nag, Sinduja Subramaniam, Topojoy
  Biswas, Evren Korpeoglu, Kannan Achan
Categories: cs.CV
Comments: 4 pages, NeurIPS Workshop SpaVLE
\\
  Spatial understanding is a critical capability for vision foundation models.
While recent advances in large vision models or vision-language models (VLMs)
have expanded recognition capabilities, most benchmarks emphasize localization
accuracy rather than whether models capture how objects are arranged and
related within a scene. This gap is consequential; effective scene
understanding requires not only identifying objects, but reasoning about their
relative positions, groupings, and depth. In this paper, we present a
systematic benchmark for object-centric spatial reasoning in foundation models.
Using a controlled synthetic dataset, we evaluate state-of-the-art vision
models (e.g., GroundingDINO, Florence-2, OWLv2) and large VLMs (e.g., InternVL,
LLaVA, GPT-4o) across three tasks: spatial localization, spatial reasoning, and
downstream retrieval tasks. We find a stable trade-off: detectors such as
GroundingDINO and OWLv2 deliver precise boxes with limited relational
reasoning, while VLMs like SmolVLM and GPT-4o provide coarse layout cues and
fluent captions but struggle with fine-grained spatial context. Our study
highlights the gap between localization and true spatial understanding, and
pointing toward the need for spatially-aware foundation models in the
community.
\\ ( https://arxiv.org/abs/2509.21922 ,  16881kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21926
Date: Fri, 26 Sep 2025 06:13:40 GMT   (4761kb)

Title: PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context
  Learning
Authors: Jiahao Zhang, Bowen Wang, Hong Liu, Yuta Nakashima, Hajime Nagahara
Categories: cs.CV
Comments: 21 pages, 12 figures
\\
  Visual In-Context Learning (VICL) uses input-output image pairs, referred to
as in-context pairs (or examples), as prompts alongside query images to guide
models in performing diverse vision tasks. However, VICL often suffers from
over-reliance on a single in-context pair, which can lead to biased and
unstable predictions. We introduce PAtch-based $k$-Nearest neighbor visual
In-Context Learning (PANICL), a general training-free framework that mitigates
this issue by leveraging multiple in-context pairs. PANICL smooths assignment
scores across pairs, reducing bias without requiring additional training.
Extensive experiments on a variety of tasks, including foreground segmentation,
single object detection, colorization, multi-object segmentation, and keypoint
detection, demonstrate consistent improvements over strong baselines. Moreover,
PANICL exhibits strong robustness to domain shifts, including dataset-level
shift (e.g., from COCO to Pascal) and label-space shift (e.g., FSS-1000), and
generalizes well to other VICL models such as SegGPT, Painter, and LVM,
highlighting its versatility and broad applicability.
\\ ( https://arxiv.org/abs/2509.21926 ,  4761kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21927
Date: Fri, 26 Sep 2025 06:14:41 GMT   (11056kb)

Title: SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB
  Reference
Authors: Jiahui Wang, Haiyue Zhu, Haoren Guo, Abdullah Al Mamun, Cheng Xiang,
  Tong Heng Lee
Categories: cs.CV
Comments: Accepted as a poster in NeurIPS 2025
\\
  Recent 6D pose estimation methods demonstrate notable performance but still
face some practical limitations. For instance, many of them rely heavily on
sensor depth, which may fail with challenging surface conditions, such as
transparent or highly reflective materials. In the meantime, RGB-based
solutions provide less robust matching performance in low-light and
texture-less scenes due to the lack of geometry information. Motivated by
these, we propose SingRef6D, a lightweight pipeline requiring only a single RGB
image as a reference, eliminating the need for costly depth sensors, multi-view
image acquisition, or training view synthesis models and neural fields. This
enables SingRef6D to remain robust and capable even under resource-limited
settings where depth or dense templates are unavailable. Our framework
incorporates two key innovations. First, we propose a token-scaler-based
fine-tuning mechanism with a novel optimization loss on top of Depth-Anything
v2 to enhance its ability to predict accurate depth, even for challenging
surfaces. Our results show a 14.41% improvement (in $\delta_{1.05}$) on REAL275
depth prediction compared to Depth-Anything v2 (with fine-tuned head). Second,
benefiting from depth availability, we introduce a depth-aware matching process
that effectively integrates spatial relationships within LoFTR, enabling our
system to handle matching for challenging materials and lighting conditions.
Evaluations of pose estimation on the REAL275, ClearPose, and Toyota-Light
datasets show that our approach surpasses state-of-the-art methods, achieving a
6.1% improvement in average recall.
\\ ( https://arxiv.org/abs/2509.21927 ,  11056kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21930
Date: Fri, 26 Sep 2025 06:15:31 GMT   (2499kb)

Title: DynaNav: Dynamic Feature and Layer Selection for Efficient Visual
  Navigation
Authors: Jiahui Wang, Changhao Chen
Categories: cs.CV cs.RO
Comments: Accepted as a poster in NeurIPS 2025
\\
  Visual navigation is essential for robotics and embodied AI. However,
existing foundation models, particularly those with transformer decoders,
suffer from high computational overhead and lack interpretability, limiting
their deployment in resource-tight scenarios. To address this, we propose
DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer
selection based on scene complexity. It employs a trainable hard feature
selector for sparse operations, enhancing efficiency and interpretability.
Additionally, we integrate feature selection into an early-exit mechanism, with
Bayesian Optimization determining optimal exit thresholds to reduce
computational cost. Extensive experiments in real-world-based datasets and
simulated environments demonstrate the effectiveness of DynaNav. Compared to
ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time,
and 32.8% lower memory usage, while improving navigation performance across
four public datasets.
\\ ( https://arxiv.org/abs/2509.21930 ,  2499kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21938
Date: Fri, 26 Sep 2025 06:22:15 GMT   (21336kb)

Title: SemanticControl: A Training-Free Approach for Handling Loosely Aligned
  Visual Conditions in ControlNet
Authors: Woosung Joung, Daewon Chae, Jinkyu Kim
Categories: cs.CV cs.AI
Comments: BMVC 2025
\\
  ControlNet has enabled detailed spatial control in text-to-image diffusion
models by incorporating additional visual conditions such as depth or edge
maps. However, its effectiveness heavily depends on the availability of visual
conditions that are precisely aligned with the generation goal specified by
text prompt-a requirement that often fails in practice, especially for uncommon
or imaginative scenes. For example, generating an image of a cat cooking in a
specific pose may be infeasible due to the lack of suitable visual conditions.
In contrast, structurally similar cues can often be found in more common
settings-for instance, poses of humans cooking are widely available and can
serve as rough visual guides. Unfortunately, existing ControlNet models
struggle to use such loosely aligned visual conditions, often resulting in low
text fidelity or visual artifacts. To address this limitation, we propose
SemanticControl, a training-free method for effectively leveraging misaligned
but semantically relevant visual conditions. Our approach adaptively suppresses
the influence of the visual condition where it conflicts with the prompt, while
strengthening guidance from the text. The key idea is to first run an auxiliary
denoising process using a surrogate prompt aligned with the visual condition
(e.g., "a human playing guitar" for a human pose condition) to extract
informative attention masks, and then utilize these masks during the denoising
of the actual target prompt (e.g., cat playing guitar). Experimental results
demonstrate that our method improves performance under loosely aligned
conditions across various conditions, including depth maps, edge maps, and
human skeletons, outperforming existing baselines. Our code is available at
https://mung3477.github.io/semantic-control.
\\ ( https://arxiv.org/abs/2509.21938 ,  21336kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21950
Date: Fri, 26 Sep 2025 06:30:39 GMT   (9539kb)

Title: Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary,
  Multifaceted, and Scalable Approach
Authors: Daiqing Wu, Dongbao Yang, Sicheng Zhao, Can Ma, Yu Zhou
Categories: cs.CV
\\
  Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional
performance across diverse tasks, continually surpassing previous expectations
regarding their capabilities. Nevertheless, their proficiency in perceiving
emotions from images remains debated, with studies yielding divergent results
in zero-shot scenarios. We argue that this inconsistency stems partly from
constraints in existing evaluation methods, including the oversight of
plausible responses, limited emotional taxonomies, neglect of contextual
factors, and labor-intensive annotations. To facilitate customized visual
emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task
that overcomes these constraints. Complementing this task, we devise an
automated pipeline that efficiently constructs emotion-centric statements with
minimal human effort. Through systematically evaluating prevailing MLLMs, our
study showcases their stronger performance in emotion interpretation and
context-based emotion judgment, while revealing relative limitations in
comprehending perception subjectivity. When compared to humans, even
top-performing MLLMs like GPT4o demonstrate remarkable performance gaps,
underscoring key areas for future improvement. By developing a fundamental
evaluation framework and conducting a comprehensive MLLM assessment, we hope
this work contributes to advancing emotional intelligence in MLLMs. Project
page: https://github.com/wdqqdw/MVEI.
\\ ( https://arxiv.org/abs/2509.21950 ,  9539kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21953
Date: Fri, 26 Sep 2025 06:41:43 GMT   (25242kb)

Title: MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially
  Disentangled Attention and Identity-Aware Reinforcement Learning
Authors: Tao Wu, Yibo Jiang, Yehao Lu, Zhizhong Wang, Zeyi Huang, Zequn Qin, Xi
  Li
Categories: cs.CV
Comments: Project Page: https://wutao-cs.github.io/MultiCrafter/
\\
  Multi-subject image generation aims to synthesize user-provided subjects in a
single image while preserving subject fidelity, ensuring prompt consistency,
and aligning with human aesthetic preferences. However, existing methods,
particularly those built on the In-Context-Learning paradigm, are limited by
their reliance on simple reconstruction-based objectives, leading to both
severe attribute leakage that compromises subject fidelity and failing to align
with nuanced human preferences. To address this, we propose MultiCrafter, a
framework that ensures high-fidelity, preference-aligned generation. First, we
find that the root cause of attribute leakage is a significant entanglement of
attention between different subjects during the generation process. Therefore,
we introduce explicit positional supervision to explicitly separate attention
regions for each subject, effectively mitigating attribute leakage. To enable
the model to accurately plan the attention region of different subjects in
diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the
model's capacity, allowing different experts to focus on different scenarios.
Finally, we design a novel online reinforcement learning framework to align the
model with human preferences, featuring a scoring mechanism to accurately
assess multi-subject fidelity and a more stable training strategy tailored for
the MoE architecture. Experiments validate that our framework significantly
improves subject fidelity while aligning with human preferences better.
\\ ( https://arxiv.org/abs/2509.21953 ,  25242kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21965
Date: Fri, 26 Sep 2025 06:52:35 GMT   (11018kb)

Title: PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native
  3D Data
Authors: Zhe Zhu and Le Wan and Rui Xu and Yiheng Zhang and Honghua Chen and
  Zhiyang Dou and Cheng Lin and Yuan Liu and Mingqiang Wei
Categories: cs.CV
\\
  Segmenting 3D objects into parts is a long-standing challenge in computer
vision. To overcome taxonomy constraints and generalize to unseen 3D objects,
recent works turn to open-world part segmentation. These approaches typically
transfer supervision from 2D foundation models, such as SAM, by lifting
multi-view masks into 3D. However, this indirect paradigm fails to capture
intrinsic geometry, leading to surface-only understanding, uncontrolled
decomposition, and limited generalization. We present PartSAM, the first
promptable part segmentation model trained natively on large-scale 3D data.
Following the design philosophy of SAM, PartSAM employs an encoder-decoder
architecture in which a triplane-based dual-branch encoder produces spatially
structured tokens for scalable part-aware representation learning. To enable
large-scale supervision, we further introduce a model-in-the-loop annotation
pipeline that curates over five million 3D shape-part pairs from online assets,
providing diverse and fine-grained labels. This combination of scalable
architecture and diverse 3D data yields emergent open-world capabilities: with
a single prompt, PartSAM achieves highly accurate part identification, and in a
Segment-Every-Part mode, it automatically decomposes shapes into both surface
and internal structures. Extensive experiments show that PartSAM outperforms
state-of-the-art methods by large margins across multiple benchmarks, marking a
decisive step toward foundation models for 3D part understanding. Our code and
model will be released soon.
\\ ( https://arxiv.org/abs/2509.21965 ,  11018kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21967
Date: Fri, 26 Sep 2025 06:54:37 GMT   (1948kb)

Title: No-Reference Image Contrast Assessment with Customized EfficientNet-B0
Authors: Javad Hassannataj Joloudari, Bita Mesbahzadeh, Omid Zare, Emrah
  Arslan, Roohallah Alizadehsani, Hossein Moosaei
Categories: cs.CV cs.AI
Comments: 32 pages, 9 tables, 6 figures
\\
  Image contrast was a fundamental factor in visual perception and played a
vital role in overall image quality. However, most no reference image quality
assessment NR IQA models struggled to accurately evaluate contrast distortions
under diverse real world conditions. In this study, we proposed a deep learning
based framework for blind contrast quality assessment by customizing and
fine-tuning three pre trained architectures, EfficientNet B0, ResNet18, and
MobileNetV2, for perceptual Mean Opinion Score, along with an additional model
built on a Siamese network, which indicated a limited ability to capture
perceptual contrast distortions. Each model is modified with a contrast-aware
regression head and trained end to end using targeted data augmentations on two
benchmark datasets, CID2013 and CCID2014, containing synthetic and authentic
contrast distortions. Performance is evaluated using Pearson Linear Correlation
Coefficient and Spearman Rank Order Correlation Coefficient, which assess the
alignment between predicted and human rated scores. Among these three models,
our customized EfficientNet B0 model achieved state-of-the-art performance with
PLCC = 0.9286 and SRCC = 0.9178 on CCID2014 and PLCC = 0.9581 and SRCC = 0.9369
on CID2013, surpassing traditional methods and outperforming other deep
baselines. These results highlighted the models robustness and effectiveness in
capturing perceptual contrast distortion. Overall, the proposed method
demonstrated that contrast aware adaptation of lightweight pre trained networks
can yield a high performing, scalable solution for no reference contrast
quality assessment suitable for real time and resource constrained
applications.
\\ ( https://arxiv.org/abs/2509.21967 ,  1948kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21976
Date: Fri, 26 Sep 2025 07:01:12 GMT   (3224kb)

Title: Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding
  with Reinforcement Fine-Tuning
Authors: Zilun Zhang, Zian Guan, Tiancheng Zhao, Haozhan Shen, Tianyu Li,
  Yuxiang Cai, Zhonggen Su, Zhaojun Liu, Jianwei Yin, Xiang Li
Categories: cs.CV cs.AI
\\
  Referring expression understanding in remote sensing poses unique challenges,
as it requires reasoning over complex object-context relationships. While
supervised fine-tuning (SFT) on multimodal large language models achieves
strong performance with massive labeled datasets, they struggle in data-scarce
scenarios, leading to poor generalization. To address this limitation, we
propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm
for few-shot geospatial referring. Geo-R1 enforces the model to first generate
explicit, interpretable reasoning chains that decompose referring expressions,
and then leverage these rationales to localize target objects. This "reason
first, then act" process enables the model to make more effective use of
limited annotations, enhances generalization, and provides interpretability. We
validate Geo-R1 on three carefully designed few-shot geospatial referring
benchmarks, where our model consistently and substantially outperforms SFT
baselines. It also demonstrates strong cross-dataset generalization,
highlighting its robustness. Code and data will be released at
http://geo-r1.github.io.
\\ ( https://arxiv.org/abs/2509.21976 ,  3224kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21979
Date: Fri, 26 Sep 2025 07:02:22 GMT   (3847kb)

Title: Benchmarking and Mitigate Psychological Sycophancy in Medical
  Vision-Language Models
Authors: Zikun Guo, Xinyue Xu, Pei Xiang, Shu Yang, Xin Han, Di Wang, Lijie Hu
Categories: cs.CV cs.AI
Comments: 19figures, 37pages
\\
  Vision language models(VLMs) are increasingly integrated into clinical
workflows, but they often exhibit sycophantic behavior prioritizing alignment
with user phrasing social cues or perceived authority over evidence based
reasoning. This study evaluate clinical sycophancy in medical visual question
answering through a novel clinically grounded benchmark. We propose a medical
sycophancy dataset construct from PathVQA, SLAKE, and VQA-RAD stratified by
different type organ system and modality. Using psychologically motivated
pressure templates including various sycophancy. In our adversarial experiments
on various VLMs, we found that these models are generally vulnerable,
exhibiting significant variations in the occurrence of adversarial responses,
with weak correlations to the model accuracy or size. Imitation and expert
provided corrections were found to be the most effective triggers, suggesting
that the models possess a bias mechanism independent of visual evidence. To
address this, we propose Visual Information Purification for Evidence based
Response (VIPER) a lightweight mitigation strategy that filters non evidentiary
content for example social pressures and then generates constrained evidence
first answers. This framework reduces sycophancy by an average amount
outperforming baselines while maintaining interpretability. Our benchmark
analysis and mitigation framework lay the groundwork for robust deployment of
medical VLMs in real world clinician interactions emphasizing the need for
evidence anchored defenses.
\\ ( https://arxiv.org/abs/2509.21979 ,  3847kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21980
Date: Fri, 26 Sep 2025 07:02:40 GMT   (3302kb)

Title: Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction
  Paradigm
Authors: Zeyu Wang, Baiyu Chen, Kun Yan, Hongjing Piao, Hao Xue, Flora D.
  Salim, Yuanchun Shi, Yuntao Wang
Categories: cs.CV
\\
  With the rise in popularity of smart glasses, users' attention has been
integrated into Vision-Language Models (VLMs) to streamline multi-modal
querying in daily scenarios. However, leveraging gaze data to model users'
attention may introduce ambiguity challenges: (1) users' verbal questions
become ambiguous by using pronouns or skipping context, (2) humans' gaze
patterns can be noisy and exhibit complex spatiotemporal relationships with
their spoken questions. Previous works only consider single image as visual
modality input, failing to capture the dynamic nature of the user's attention.
In this work, we introduce GLARIFY, a novel method to leverage spatiotemporal
gaze information to enhance the model's effectiveness in real-world
applications. Initially, we analyzed hundreds of querying samples with the gaze
modality to demonstrate the noisy nature of users' gaze patterns. We then
utilized GPT-4o to design an automatic data synthesis pipeline to generate the
GLARIFY-Ambi dataset, which includes a dedicated chain-of-thought (CoT) process
to handle noisy gaze patterns. Finally, we designed a heatmap module to
incorporate gaze information into cutting-edge VLMs while preserving their
pretrained knowledge. We evaluated GLARIFY using a hold-out test set.
Experiments demonstrate that GLARIFY significantly outperforms baselines. By
robustly aligning VLMs with human attention, GLARIFY paves the way for a usable
and intuitive interaction paradigm with a visual assistant.
\\ ( https://arxiv.org/abs/2509.21980 ,  3302kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21984
Date: Fri, 26 Sep 2025 07:07:03 GMT   (1970kb)

Title: From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs
Authors: Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Weili Guan, Jun Yu,
  Min Zhang
Categories: cs.CV cs.CL
\\
  Large Vision-Language Models (LVLMs) have achieved remarkable success across
a wide range of multimodal tasks, yet their robustness to spatial variations
remains insufficiently understood. In this work, we present a systematic study
of the spatial bias of LVLMs, focusing on how models respond when identical key
visual information is placed at different locations within an image. Through a
carefully designed probing dataset, we demonstrate that current LVLMs often
produce inconsistent outputs under such spatial shifts, revealing a fundamental
limitation in their spatial-semantic understanding. Further analysis shows that
this phenomenon originates not from the vision encoder, which reliably
perceives and interprets visual content across positions, but from the
unbalanced design of position embeddings in the language model component. In
particular, the widely adopted position embedding strategies, such as RoPE,
introduce imbalance during cross-modal interaction, leading image tokens at
different positions to exert unequal influence on semantic understanding. To
mitigate this issue, we introduce Balanced Position Assignment (BaPA), a simple
yet effective mechanism that assigns identical position embeddings to all image
tokens, promoting a more balanced integration of visual information. Extensive
experiments show that BaPA enhances the spatial robustness of LVLMs without
retraining and further boosts their performance across diverse multimodal
benchmarks when combined with lightweight fine-tuning. Further analysis of
information flow reveals that BaPA yields balanced attention, enabling more
holistic visual understanding.
\\ ( https://arxiv.org/abs/2509.21984 ,  1970kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21989
Date: Fri, 26 Sep 2025 07:11:55 GMT   (6468kb)

Title: Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in
  Subject-Driven Generation
Authors: Abdelrahman Eldesokey, Aleksandar Cvejic, Bernard Ghanem, Peter Wonka
Categories: cs.CV
Comments: NeurIPS 2025 (Spotlight). Project Page:
  https://abdo-eldesokey.github.io/mind-the-glitch/
\\
  We propose a novel approach for disentangling visual and semantic features
from the backbones of pre-trained diffusion models, enabling visual
correspondence in a manner analogous to the well-established semantic
correspondence. While diffusion model backbones are known to encode
semantically rich features, they must also contain visual features to support
their image synthesis capabilities. However, isolating these visual features is
challenging due to the absence of annotated datasets. To address this, we
introduce an automated pipeline that constructs image pairs with annotated
semantic and visual correspondences based on existing subject-driven image
generation datasets, and design a contrastive architecture to separate the two
feature types. Leveraging the disentangled representations, we propose a new
metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies
in subject-driven image generation. Empirical results show that our approach
outperforms global feature-based metrics such as CLIP, DINO, and
vision--language models in quantifying visual inconsistencies while also
enabling spatial localization of inconsistent regions. To our knowledge, this
is the first method that supports both quantification and localization of
inconsistencies in subject-driven generation, offering a valuable tool for
advancing this task. Project
Page:https://abdo-eldesokey.github.io/mind-the-glitch/
\\ ( https://arxiv.org/abs/2509.21989 ,  6468kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21990
Date: Fri, 26 Sep 2025 07:13:37 GMT   (229kb)

Title: WAVE: Learning Unified & Versatile Audio-Visual Embeddings with
  Multimodal LLM
Authors: Changli Tang, Qinfan Xiao, Ke Mei, Tianyi Wang, Fengyun Rao, Chao
  Zhang
Categories: cs.CV cs.SD
\\
  While embeddings from multimodal large language models (LLMs) excel as
general-purpose representations, their application to dynamic modalities like
audio and video remains underexplored. We introduce WAVE (\textbf{u}nified \&
\textbf{v}ersatile \textbf{a}udio-\textbf{v}isual \textbf{e}mbeddings), the
first LLM-based embedding that creates a unified representation space for text,
audio, and video modalities. WAVE employs a novel hierarchical feature fusion
strategy and a joint multi-modal, multi-task training approach to enable two
key capabilities: any-to-any cross-modal retrieval and the generation of
prompt-aware embeddings tailored to user instructions. Experimentally, WAVE
sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves
superior results in audio and video-to-audio retrieval. Its prompt-aware nature
also yields remarkable performance in multimodal question answering,
significantly outperforming existing embedding models. Ablation studies
validate our joint training strategy, demonstrating improved performance across
all modalities. With a newly introduced benchmark for versatile audio-visual
learning, WAVE opens up broad possibilities for cross-modal, any-to-any
applications. Our code, checkpoints, and data will be released.
\\ ( https://arxiv.org/abs/2509.21990 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21991
Date: Fri, 26 Sep 2025 07:15:19 GMT   (2902kb)

Title: ERGO: Efficient High-Resolution Visual Understanding for Vision-Language
  Models
Authors: Jewon Lee, Wooksu Shin, Seungmin Yang, Ki-Ung Song, DongUk Lim,
  Jaeyeon Kim, Tae-Ho Kim, Bo-Kyeong Kim
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Efficient processing of high-resolution images is crucial for real-world
vision-language applications. However, existing Large Vision-Language Models
(LVLMs) incur substantial computational overhead due to the large number of
vision tokens. With the advent of "thinking with images" models, reasoning now
extends beyond text to the visual domain. This capability motivates our
two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is
analyzed to identify task-relevant regions; then, only these regions are
cropped at full resolution and processed in a subsequent reasoning stage. This
approach reduces computational cost while preserving fine-grained visual
details where necessary. A major challenge lies in inferring which regions are
truly relevant to a given query. Recent related methods often fail in the first
stage after input-image downsampling, due to perception-driven reasoning, where
clear visual information is required for effective reasoning. To address this
issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs
reasoning-driven perception-leveraging multimodal context to determine where to
focus. Our model can account for perceptual uncertainty, expanding the cropped
region to cover visually ambiguous areas for answering questions. To this end,
we develop simple yet effective reward components in a reinforcement learning
framework for coarse-to-fine perception. Across multiple datasets, our approach
delivers higher accuracy than the original model and competitive methods, with
greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V*
benchmark by 4.7 points while using only 23% of the vision tokens, achieving a
3x inference speedup. The code and models can be found at:
https://github.com/nota-github/ERGO.
\\ ( https://arxiv.org/abs/2509.21991 ,  2902kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21992
Date: Fri, 26 Sep 2025 07:15:36 GMT   (26898kb)

Title: DualFocus: Depth from Focus with Spatio-Focal Dual Variational
  Constraints
Authors: Sungmin Woo and Sangyoun Lee
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\
  Depth-from-Focus (DFF) enables precise depth estimation by analyzing focus
cues across a stack of images captured at varying focal lengths. While recent
learning-based approaches have advanced this field, they often struggle in
complex scenes with fine textures or abrupt depth changes, where focus cues may
become ambiguous or misleading. We present DualFocus, a novel DFF framework
that leverages the focal stack's unique gradient patterns induced by focus
variation, jointly modeling focus changes over spatial and focal dimensions.
Our approach introduces a variational formulation with dual constraints
tailored to DFF: spatial constraints exploit gradient pattern changes across
focus levels to distinguish true depth edges from texture artifacts, while
focal constraints enforce unimodal, monotonic focus probabilities aligned with
physical focus behavior. These inductive biases improve robustness and accuracy
in challenging regions. Comprehensive experiments on four public datasets
demonstrate that DualFocus consistently outperforms state-of-the-art methods in
both depth accuracy and perceptual quality.
\\ ( https://arxiv.org/abs/2509.21992 ,  26898kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21994
Date: Fri, 26 Sep 2025 07:21:32 GMT   (5841kb)

Title: Rate-Distortion Optimized Communication for Collaborative Perception
Authors: Genjia Liu, Anning Hu, Yue Hu, Wenjun Zhang, Siheng Chen
Categories: cs.CV
\\
  Collaborative perception emphasizes enhancing environmental understanding by
enabling multiple agents to share visual information with limited bandwidth
resources. While prior work has explored the empirical trade-off between task
performance and communication volume, a significant gap remains in the
theoretical foundation. To fill this gap, we draw on information theory and
introduce a pragmatic rate-distortion theory for multi-agent collaboration,
specifically formulated to analyze performance-communication trade-off in
goal-oriented multi-agent systems. This theory concretizes two key conditions
for designing optimal communication strategies: supplying pragmatically
relevant information and transmitting redundancy-less messages. Guided by these
two conditions, we propose RDcomm, a communication-efficient collaborative
perception framework that introduces two key innovations: i) task entropy
discrete coding, which assigns features with task-relevant codeword-lengths to
maximize the efficiency in supplying pragmatic information; ii)
mutual-information-driven message selection, which utilizes mutual information
neural estimation to approach the optimal redundancy-less condition.
Experiments on 3D object detection and BEV segmentation demonstrate that RDcomm
achieves state-of-the-art accuracy on DAIR-V2X and OPV2V, while reducing
communication volume by up to 108 times. The code will be released.
\\ ( https://arxiv.org/abs/2509.21994 ,  5841kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21995
Date: Fri, 26 Sep 2025 07:23:23 GMT   (2671kb)

Title: FailureAtlas:Mapping the Failure Landscape of T2I Models via Active
  Exploration
Authors: Muxi Chen, Zhaohua Zhang, Chenchen Zhao, Mingyang Chen, Wenyu Jiang,
  Tianwen Jiang, Jianhuan Zhuo, Yu Tang, Qiuyong Xiao, Jihong Zhang, Qiang Xu
Categories: cs.CV
\\
  Static benchmarks have provided a valuable foundation for comparing
Text-to-Image (T2I) models. However, their passive design offers limited
diagnostic power, struggling to uncover the full landscape of systematic
failures or isolate their root causes. We argue for a complementary paradigm:
active exploration. We introduce FailureAtlas, the first framework designed to
autonomously explore and map the vast failure landscape of T2I models at scale.
FailureAtlas frames error discovery as a structured search for minimal,
failure-inducing concepts. While it is a computationally explosive problem, we
make it tractable with novel acceleration techniques. When applied to Stable
Diffusion models, our method uncovers hundreds of thousands of previously
unknown error slices (over 247,000 in SD1.5 alone) and provides the first
large-scale evidence linking these failures to data scarcity in the training
set. By providing a principled and scalable engine for deep model auditing,
FailureAtlas establishes a new, diagnostic-first methodology to guide the
development of more robust generative AI. The code is available at
https://github.com/cure-lab/FailureAtlas
\\ ( https://arxiv.org/abs/2509.21995 ,  2671kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21997
Date: Fri, 26 Sep 2025 07:24:28 GMT   (5182kb)

Title: Exposing Hallucinations To Suppress Them: VLMs Representation Editing
  With Generative Anchors
Authors: Youxu Shi and Suorong Yang and Dong Liu
Categories: cs.CV
\\
  Multimodal large language models (MLLMs) have achieved remarkable success
across diverse vision-language tasks, yet they remain highly susceptible to
hallucinations, producing content that is fluent but inconsistent with visual
evidence. Such hallucinations, spanning objects, attributes, and relations,
persist even in larger models, while existing mitigation approaches often
require additional finetuning, handcrafted priors, or trade-offs that
compromise informativeness and scalability. To address this limitation, we
propose a training-free, self-supervised method for hallucination mitigation.
Our approach introduces a novel hallucination amplification mechanism: a
caption is projected into the visual space via a text-to-image model to reveal
implicit hallucination signals, serving as a negative anchor, while the
original image provides a positive anchor. Leveraging these dual anchors, we
edit decoder hidden states by pulling representations toward faithful semantics
and pushing them away from hallucination directions. This correction requires
no human priors or additional training costs, ensuring both effectiveness and
efficiency. Extensive experiments across multiple benchmarks show that our
method significantly reduces hallucinations at the object, attribute, and
relation levels while largely preserving recall and caption richness, e.g.,
achieving a hallucination reduction by over 5% using LLaVA-v1.5-7B on CHAIR.
Furthermore, results on diverse architectures, including LLaVA-NEXT-7B,
Cambrian-8B, and InstructBLIP-7B, validate strong cross-architecture
generalization. More importantly, when applied to hallucination-free captions,
our method introduces almost no side effects, underscoring its robustness and
practical plug-and-play applicability. The implementation will be publicly
available.
\\ ( https://arxiv.org/abs/2509.21997 ,  5182kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22010
Date: Fri, 26 Sep 2025 07:46:30 GMT   (596kb)

Title: CoFFT: Chain of Foresight-Focus Thought for Visual Language Models
Authors: Xinyu Zhang, Yuxuan Dong, Lingling Zhang, Chengyou Jia, Zhuohang Dang,
  Basura Fernando, Jun Liu, Mike Zheng Shou
Categories: cs.CV
\\
  Despite significant advances in Vision Language Models (VLMs), they remain
constrained by the complexity and redundancy of visual input. When images
contain large amounts of irrelevant information, VLMs are susceptible to
interference, thus generating excessive task-irrelevant reasoning processes or
even hallucinations. This limitation stems from their inability to discover and
process the required regions during reasoning precisely. To address this
limitation, we present the Chain of Foresight-Focus Thought (CoFFT), a novel
training-free approach that enhances VLMs' visual reasoning by emulating human
visual cognition. Each Foresight-Focus Thought consists of three stages: (1)
Diverse Sample Generation: generates diverse reasoning samples to explore
potential reasoning paths, where each sample contains several reasoning steps;
(2) Dual Foresight Decoding: rigorously evaluates these samples based on both
visual focus and reasoning progression, adding the first step of optimal sample
to the reasoning process; (3) Visual Focus Adjustment: precisely adjust visual
focus toward regions most beneficial for future reasoning, before returning to
stage (1) to generate subsequent reasoning samples until reaching the final
answer. These stages function iteratively, creating an interdependent cycle
where reasoning guides visual focus and visual focus informs subsequent
reasoning. Empirical results across multiple benchmarks using Qwen2.5-VL,
InternVL-2.5, and Llava-Next demonstrate consistent performance improvements of
3.1-5.8\% with controllable increasing computational overhead.
\\ ( https://arxiv.org/abs/2509.22010 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22014
Date: Fri, 26 Sep 2025 07:49:49 GMT   (596kb)

Title: Lightweight Structured Multimodal Reasoning for Clinical Scene
  Understanding in Robotics
Authors: Saurav Jha and Stefan K. Ehrlich
Categories: cs.CV cs.AI cs.HC cs.RO
Comments: 11 pages, 3 figures
\\
  Healthcare robotics requires robust multimodal perception and reasoning to
ensure safety in dynamic clinical environments. Current Vision-Language Models
(VLMs) demonstrate strong general-purpose capabilities but remain limited in
temporal reasoning, uncertainty estimation, and structured outputs needed for
robotic planning. We present a lightweight agentic multimodal framework for
video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model
with a SmolAgent-based orchestration layer, it supports chain-of-thought
reasoning, speech-vision fusion, and dynamic tool invocation. The framework
generates structured scene graphs and leverages a hybrid retrieval module for
interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark
and a custom clinical dataset show competitive accuracy and improved robustness
compared to state-of-the-art VLMs, demonstrating its potential for applications
in robot-assisted surgery, patient monitoring, and decision support.
\\ ( https://arxiv.org/abs/2509.22014 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22019
Date: Fri, 26 Sep 2025 07:52:26 GMT   (4852kb)

Title: EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional
  Interactions with Multi-modal LLM Benchmarking
Authors: Yuki Sakai, Ryosuke Furuta, Juichun Yen and Yoichi Sato
Categories: cs.CV
Comments: Accepted to the I-HFM Workshop at ICCV 2025
\\
  Analyzing instructional interactions between an instructor and a learner who
are co-present in the same physical space is a critical problem for educational
support and skill transfer. Yet such face-to-face instructional scenes have not
been systematically studied in computer vision. We identify two key reasons: i)
the lack of suitable datasets and ii) limited analytical techniques. To address
this gap, we present a new egocentric video dataset of face-to-face instruction
and provide ground-truth annotations for two fundamental tasks that serve as a
first step toward a comprehensive understanding of instructional interactions:
procedural step segmentation and conversation-state classification. Using this
dataset, we benchmark multimodal large language models (MLLMs) against
conventional task-specific models. Since face-to-face instruction involves
multiple modalities (speech content and prosody, gaze and body motion, and
visual context), effective understanding requires methods that handle verbal
and nonverbal communication in an integrated manner. Accordingly, we evaluate
recently introduced MLLMs that jointly process images, audio, and text. This
evaluation quantifies the extent to which current machine learning models
understand face-to-face instructional scenes. In experiments, MLLMs outperform
specialized baselines even without task-specific fine-tuning, suggesting their
promise for holistic understanding of instructional interactions.
\\ ( https://arxiv.org/abs/2509.22019 ,  4852kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22063
Date: Fri, 26 Sep 2025 08:46:00 GMT   (9291kb)

Title: High-Quality Sound Separation Across Diverse Categories via
  Visually-Guided Generative Modeling
Authors: Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, Chenliang Xu
Categories: cs.CV cs.SD
Comments: Accepted to IJCV
\\
  We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that
solves the audio-visual sound source separation task through generative
learning. Existing methods typically frame sound separation as a mask-based
regression problem, achieving significant progress. However, they face
limitations in capturing the complex data distribution required for
high-quality separation of sounds from diverse categories. In contrast, DAVIS
circumvents these issues by leveraging potent generative modeling paradigms,
specifically Denoising Diffusion Probabilistic Models (DDPM) and the more
recent Flow Matching (FM), integrated within a specialized Separation U-Net
architecture. Our framework operates by synthesizing the desired separated
sound spectrograms directly from a noise distribution, conditioned concurrently
on the mixed audio input and associated visual information. The inherent nature
of its generative objective makes DAVIS particularly adept at producing
high-quality sound separations for diverse sound categories. We present
comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching
variants, against leading methods on the standard AVE and MUSIC datasets. The
results affirm that both variants surpass existing approaches in separation
quality, highlighting the efficacy of our generative framework for tackling the
audio-visual source separation task.
\\ ( https://arxiv.org/abs/2509.22063 ,  9291kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22070
Date: Fri, 26 Sep 2025 08:51:59 GMT   (4599kb)

Title: SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake
  Detection
Authors: Inzamamul Alam, Md Tanvir Islam, and Simon S. Woo
Categories: cs.CV
Comments: ACM MM Accepted
DOI: 10.1145/3746027.3755707
\\
  The increasing realism of content generated by GANs and diffusion models has
made deepfake detection significantly more challenging. Existing approaches
often focus solely on spatial or frequency-domain features, limiting their
generalization to unseen manipulations. We propose the Spectral
Cross-Attentional Network (SpecXNet), a dual-domain architecture for robust
deepfake detection. The core \textbf{Dual-Domain Feature Coupler (DDFC)}
decomposes features into a local spatial branch for capturing texture-level
anomalies and a global spectral branch that employs Fast Fourier Transform to
model periodic inconsistencies. This dual-domain formulation allows SpecXNet to
jointly exploit localized detail and global structural coherence, which are
critical for distinguishing authentic from manipulated images. We also
introduce the \textbf{Dual Fourier Attention (DFA)} module, which dynamically
fuses spatial and spectral features in a content-aware manner. Built atop a
modified XceptionNet backbone, we embed the DDFC and DFA modules within a
separable convolution block. Extensive experiments on multiple deepfake
benchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly
under cross-dataset and unseen manipulation scenarios, while maintaining
real-time feasibility. Our results highlight the effectiveness of unified
spatial-spectral learning for robust and generalizable deepfake detection. To
ensure reproducibility, we released the full code on
\href{https://github.com/inzamamulDU/SpecXNet}{\textcolor{blue}{\textbf{GitHub}}}.
\\ ( https://arxiv.org/abs/2509.22070 ,  4599kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22112
Date: Fri, 26 Sep 2025 09:35:12 GMT   (6636kb)

Title: Large Material Gaussian Model for Relightable 3D Generation
Authors: Jingrui Ye, Lingting Zhu, Runze Zhang, Zeyu Hu, Yingda Yin, Lanjiong
  Li, Lequan Yu, Qingmin Liao
Categories: cs.CV
\\
  The increasing demand for 3D assets across various industries necessitates
efficient and automated methods for 3D content creation. Leveraging 3D Gaussian
Splatting, recent large reconstruction models (LRMs) have demonstrated the
ability to efficiently achieve high-quality 3D rendering by integrating
multiview diffusion for generation and scalable transformers for
reconstruction. However, existing models fail to produce the material
properties of assets, which is crucial for realistic rendering in diverse
lighting environments. In this paper, we introduce the Large Material Gaussian
Model (MGM), a novel framework designed to generate high-quality 3D content
with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and
metallic properties, rather than merely producing RGB textures with
uncontrolled light baking. Specifically, we first fine-tune a new multiview
material diffusion model conditioned on input depth and normal maps. Utilizing
the generated multiview PBR images, we explore a Gaussian material
representation that not only aligns with 2D Gaussian Splatting but also models
each channel of the PBR materials. The reconstructed point clouds can then be
rendered to acquire PBR attributes, enabling dynamic relighting by applying
various ambient light maps. Extensive experiments demonstrate that the
materials produced by our method not only exhibit greater visual appeal
compared to baseline methods but also enhance material modeling, thereby
enabling practical downstream rendering applications.
\\ ( https://arxiv.org/abs/2509.22112 ,  6636kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22132
Date: Fri, 26 Sep 2025 09:53:55 GMT   (18791kb)

Title: Self-Supervised Point Cloud Completion based on Multi-View Augmentations
  of Single Partial Point Cloud
Authors: Jingjing Lu and Huilong Pi and Yunchuan Qin and Zhuo Tang and Ruihui
  Li
Categories: cs.CV
\\
  Point cloud completion aims to reconstruct complete shapes from partial
observations. Although current methods have achieved remarkable performance,
they still have some limitations: Supervised methods heavily rely on ground
truth, which limits their generalization to real-world datasets due to the
synthetic-to-real domain gap. Unsupervised methods require complete point
clouds to compose unpaired training data, and weakly-supervised methods need
multi-view observations of the object. Existing self-supervised methods
frequently produce unsatisfactory predictions due to the limited capabilities
of their self-supervised signals. To overcome these challenges, we propose a
novel self-supervised point cloud completion method. We design a set of novel
self-supervised signals based on multi-view augmentations of the single partial
point cloud. Additionally, to enhance the model's learning ability, we first
incorporate Mamba into self-supervised point cloud completion task, encouraging
the model to generate point clouds with better quality. Experiments on
synthetic and real-world datasets demonstrate that our method achieves
state-of-the-art results.
\\ ( https://arxiv.org/abs/2509.22132 ,  18791kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22139
Date: Fri, 26 Sep 2025 09:59:40 GMT   (4594kb)

Title: REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional
  Image Generation
Authors: Yicheng Jiang, Jin Yuan, Hua Yuan, Yao Zhang, Yong Rui
Categories: cs.CV cs.AI
Comments: 5 pages,17 figures
\\
  Conditional image generation models have achieved remarkable results by
leveraging text-based control to generate customized images. However, the high
resource demands of these models and the scarcity of well-annotated data have
hindered their deployment on edge devices, leading to enormous costs and
privacy concerns, especially when user data is sent to a third party. To
overcome these challenges, we propose Refine-Control, a semi-supervised
distillation framework. Specifically, we improve the performance of the student
model by introducing a tri-level knowledge fusion loss to transfer different
levels of knowledge. To enhance generalization and alleviate dataset scarcity,
we introduce a semi-supervised distillation method utilizing both labeled and
unlabeled data. Our experiments reveal that Refine-Control achieves significant
reductions in computational cost and latency, while maintaining high-fidelity
generation capabilities and controllability, as quantified by comparative
metrics.
\\ ( https://arxiv.org/abs/2509.22139 ,  4594kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22150
Date: Fri, 26 Sep 2025 10:09:58 GMT   (13497kb)

Title: Joint graph entropy knowledge distillation for point cloud
  classification and robustness against corruptions
Authors: Zhiqiang Tian, Weigang Li, Junwei Hu, Chunhua Deng
Categories: cs.CV cs.IR
\\
  Classification tasks in 3D point clouds often assume that class events
\replaced{are }{follow }independent and identically distributed (IID), although
this assumption destroys the correlation between classes. This \replaced{study
}{paper }proposes a classification strategy, \textbf{J}oint \textbf{G}raph
\textbf{E}ntropy \textbf{K}nowledge \textbf{D}istillation (JGEKD), suitable for
non-independent and identically distributed 3D point cloud data,
\replaced{which }{the strategy } achieves knowledge transfer of class
correlations through knowledge distillation by constructing a loss function
based on joint graph entropy. First\deleted{ly}, we employ joint graphs to
capture add{the }hidden relationships between classes\replaced{ and}{,}
implement knowledge distillation to train our model by calculating the entropy
of add{add }graph.\replaced{ Subsequently}{ Then}, to handle 3D point clouds
\deleted{that is }invariant to spatial transformations, we construct
\replaced{S}{s}iamese structures and develop two frameworks, self-knowledge
distillation and teacher-knowledge distillation, to facilitate information
transfer between different transformation forms of the same data. \replaced{In
addition}{ Additionally}, we use the above framework to achieve knowledge
transfer between point clouds and their corrupted forms, and increase the
robustness against corruption of model. Extensive experiments on ScanObject,
ModelNet40, ScanntV2\_cls and ModelNet-C demonstrate that the proposed strategy
can achieve competitive results.
\\ ( https://arxiv.org/abs/2509.22150 ,  13497kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22151
Date: Fri, 26 Sep 2025 10:10:25 GMT   (47081kb)

Title: MultiMat: Multimodal Program Synthesis for Procedural Materials using
  Large Multimodal Models
Authors: Jonas Belouadi, Tamy Boubekeur, Adrien Kaiser
Categories: cs.CV
Comments: Submitted to ICLR 2026
\\
  Material node graphs are programs that generate the 2D channels of procedural
materials, including geometry such as roughness and displacement maps, and
reflectance such as albedo and conductivity maps. They are essential in
computer graphics for representing the appearance of virtual 3D objects
parametrically and at arbitrary resolution. In particular, their directed
acyclic graph structures and intermediate states provide an intuitive
understanding and workflow for interactive appearance modeling. Creating such
graphs is a challenging task and typically requires professional training.
While recent neural program synthesis approaches attempt to simplify this
process, they solely represent graphs as textual programs, failing to capture
the inherently visual-spatial nature of node graphs that makes them accessible
to humans. To address this gap, we present MultiMat, a multimodal program
synthesis framework that leverages large multimodal models to process both
visual and textual graph representations for improved generation of procedural
material graphs. We train our models on a new dataset of production-quality
procedural materials and combine them with a constrained tree search inference
algorithm that ensures syntactic validity while efficiently navigating the
program space. Our experimental results show that our multimodal program
synthesis method is more efficient in both unconditional and conditional graph
synthesis with higher visual quality and fidelity than text-only baselines,
establishing new state-of-the-art performance.
\\ ( https://arxiv.org/abs/2509.22151 ,  47081kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22169
Date: Fri, 26 Sep 2025 10:30:49 GMT   (16814kb)

Title: DragGANSpace: Latent Space Exploration and Control for GANs
Authors: Kirsten Odendaal, Neela Kaushik, Spencer Halverson
Categories: cs.CV cs.LG
Comments: 6 pages with 7 figures and 3 tables
\\
  This work integrates StyleGAN, DragGAN and Principal Component Analysis (PCA)
to enhance the latent space efficiency and controllability of GAN-generated
images. Style-GAN provides a structured latent space, DragGAN enables intuitive
image manipulation, and PCA reduces dimensionality and facilitates cross-model
alignment for more streamlined and interpretable exploration of latent spaces.
We apply our techniques to the Animal Faces High Quality (AFHQ) dataset, and
find that our approach of integrating PCA-based dimensionality reduction with
the Drag-GAN framework for image manipulation retains performance while
improving optimization efficiency. Notably, introducing PCA into the latent W+
layers of DragGAN can consistently reduce the total optimization time while
maintaining good visual quality and even boosting the Structural Similarity
Index Measure (SSIM) of the optimized image, particularly in shallower latent
spaces (W+ layers = 3). We also demonstrate capability for aligning images
generated by two StyleGAN models trained on similar but distinct data domains
(AFHQ-Dog and AFHQ-Cat), and show that we can control the latent space of these
aligned images to manipulate the images in an intuitive and interpretable
manner. Our findings highlight the possibility for efficient and interpretable
latent space control for a wide range of image synthesis and editing
applications.
\\ ( https://arxiv.org/abs/2509.22169 ,  16814kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22186
Date: Fri, 26 Sep 2025 10:45:48 GMT   (39049kb)

Title: MinerU2.5: A Decoupled Vision-Language Model for Efficient
  High-Resolution Document Parsing
Authors: Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan
  Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, Zhenjiang Jin, Guang Liang,
  Rui Zhang, Wenzheng Zhang, Yuan Qu, Zhifei Ren, Yuefeng Sun, Yuanhong Zheng,
  Dongsheng Ma, Zirui Tang, Boyu Niu, Ziyang Miao, Hejun Dong, Siyi Qian,
  Junyuan Zhang, Jingzhou Chen, Fangdong Wang, Xiaomeng Zhao, Liqun Wei, Wei
  Li, Shasha Wang, Ruiliang Xu, Yuanyuan Cao, Lu Chen, Qianqian Wu, Huaiyu Gu,
  Lindong Lu, Keming Wang, Dechen Lin, Guanlin Shen, Xuanhe Zhou, Linfeng
  Zhang, Yuhang Zang, Xiaoyi Dong, Jiaqi Wang, Bo Zhang, Lei Bai, Pei Chu,
  Weijia Li, Jiang Wu, Lijun Wu, Zhenxiang Li, Guangyu Wang, Zhongying Tu, Chao
  Xu, Kai Chen, Yu Qiao, Bowen Zhou, Dahua Lin, Wentao Zhang, Conghui He
Categories: cs.CV cs.CL
Comments: Technical Report; GitHub Repo: https://github.com/opendatalab/MinerU;
  Hugging Face Model: https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B;
  Hugging Face Demo: https://huggingface.co/spaces/opendatalab/MinerU
\\
  We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language
model that achieves state-of-the-art recognition accuracy while maintaining
exceptional computational efficiency. Our approach employs a coarse-to-fine,
two-stage parsing strategy that decouples global layout analysis from local
content recognition. In the first stage, the model performs efficient layout
analysis on downsampled images to identify structural elements, circumventing
the computational overhead of processing high-resolution inputs. In the second
stage, guided by the global layout, it performs targeted content recognition on
native-resolution crops extracted from the original image, preserving
fine-grained details in dense text, complex formulas, and tables. To support
this strategy, we developed a comprehensive data engine that generates diverse,
large-scale training corpora for both pretraining and fine-tuning. Ultimately,
MinerU2.5 demonstrates strong document parsing ability, achieving
state-of-the-art performance on multiple benchmarks, surpassing both
general-purpose and domain-specific models across various recognition tasks,
while maintaining significantly lower computational overhead.
\\ ( https://arxiv.org/abs/2509.22186 ,  39049kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22221
Date: Fri, 26 Sep 2025 11:34:42 GMT   (3320kb)

Title: Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded
  GeoSpatial Chain-of-Thought for Vision-Language Models
Authors: Jiaqi Liu, Lang Sun, Ronghao Fu, Bo Yang
Categories: cs.CV
\\
  Vision-Language Models (VLMs) in remote sensing often fail at complex
analytical tasks, a limitation stemming from their end-to-end training paradigm
that bypasses crucial reasoning steps and leads to unverifiable outputs. To
address this limitation, we introduce the Perceptually-Grounded Geospatial
Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as
a verifiable, multi-step process. We instill this analytical process through a
two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale
dataset of structured Geo-CoT rationales. This strategy first employs
supervised fine-tuning (SFT) to instill the foundational cognitive
architecture, then leverages Group Reward Policy Optimization (GRPO) to refine
the model's reasoning policy towards factual correctness. The resulting model,
RSThinker, outputs both a final answer and its justifying, verifiable
analytical trace. This capability yields dominant performance, significantly
outperforming state-of-the-art models across a comprehensive range of tasks.
The public release of our Geo-CoT380k dataset and RSThinker model upon
publication serves as a concrete pathway from opaque perception towards
structured, verifiable reasoning for Earth Observation.
\\ ( https://arxiv.org/abs/2509.22221 ,  3320kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22225
Date: Fri, 26 Sep 2025 11:38:05 GMT   (11145kb)

Title: Polysemous Language Gaussian Splatting via Matching-based Mask Lifting
Authors: Jiayu Ding, Xinpeng Liu, Zhiyi Pan, Shiqiang Long, and Ge Li
Categories: cs.CV cs.AI
\\
  Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS)
scenes is a critical challenge. However, mainstream methods suffer from three
key flaws: (i) their reliance on costly per-scene retraining prevents
plug-and-play application; (ii) their restrictive monosemous design fails to
represent complex, multi-concept semantics; and (iii) their vulnerability to
cross-view semantic inconsistencies corrupts the final semantic representation.
To overcome these limitations, we introduce MUSplat, a training-free framework
that abandons feature optimization entirely. Leveraging a pre-trained 2D
segmentation model, our pipeline generates and lifts multi-granularity 2D masks
into 3D, where we estimate a foreground probability for each Gaussian point to
form initial object groups. We then optimize the ambiguous boundaries of these
initial groups using semantic entropy and geometric opacity. Subsequently, by
interpreting the object's appearance across its most representative viewpoints,
a Vision-Language Model (VLM) distills robust textual features that reconciles
visual inconsistencies, enabling open-vocabulary querying via semantic
matching. By eliminating the costly per-scene training process, MUSplat reduces
scene adaptation time from hours to mere minutes. On benchmark tasks for
open-vocabulary 3D object selection and semantic segmentation, MUSplat
outperforms established training-based frameworks while simultaneously
addressing their monosemous limitations.
\\ ( https://arxiv.org/abs/2509.22225 ,  11145kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22228
Date: Fri, 26 Sep 2025 11:38:57 GMT   (6537kb)

Title: UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual
  Understanding of City Scenes through Human Perspective
Authors: Jun He, Yi Lin, Zilong Huang, Jiacong Yin, Junyan Ye, Yuchuan Zhou,
  Weijia Li, Xiang Zhang
Categories: cs.CV
Comments: 13 pages, 6 figures
\\
  Urban development impacts over half of the global population, making
human-centered understanding of its structural and perceptual changes essential
for sustainable development. While Multimodal Large Language Models (MLLMs)
have shown remarkable capabilities across various domains, existing benchmarks
that explore their performance in urban environments remain limited, lacking
systematic exploration of temporal evolution and subjective perception of urban
environment that aligns with human perception. To address these limitations, we
propose UrbanFeel, a comprehensive benchmark designed to evaluate the
performance of MLLMs in urban development understanding and subjective
environmental perception. UrbanFeel comprises 14.3K carefully constructed
visual questions spanning three cognitively progressive dimensions: Static
Scene Perception, Temporal Change Understanding, and Subjective Environmental
Perception. We collect multi-temporal single-view and panoramic street-view
images from 11 representative cities worldwide, and generate high-quality
question-answer pairs through a hybrid pipeline of spatial clustering,
rule-based generation, model-assisted prompting, and manual annotation. Through
extensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5
Pro achieves the best overall performance, with its accuracy approaching human
expert levels and narrowing the average gap to just 1.5\%. Most models perform
well on tasks grounded in scene understanding. In particular, some models even
surpass human annotators in pixel-level change detection. However, performance
drops notably in tasks requiring temporal reasoning over urban development.
Additionally, in the subjective perception dimension, several models reach
human-level or even higher consistency in evaluating dimension such as
beautiful and safety.
\\ ( https://arxiv.org/abs/2509.22228 ,  6537kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22229
Date: Fri, 26 Sep 2025 11:39:50 GMT   (363kb)

Title: A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised
  Domain Adaptation
Authors: Jiaping Yu, Muli Yang, Jiapeng Ji, Jiexi Yan, Cheng Deng
Categories: cs.CV
\\
  Source-Free Unsupervised Domain Adaptation (SFUDA) addresses the realistic
challenge of adapting a source-trained model to a target domain without access
to the source data, driven by concerns over privacy and cost. Existing SFUDA
methods either exploit only the source model's predictions or fine-tune large
multimodal models, yet both neglect complementary insights and the latent
structure of target data. In this paper, we propose the Experts Cooperative
Learning (EXCL). EXCL contains the Dual Experts framework and
Retrieval-Augmentation-Interaction optimization pipeline. The Dual Experts
framework places a frozen source-domain model (augmented with Conv-Adapter) and
a pretrained vision-language model (with a trainable text prompt) on equal
footing to mine consensus knowledge from unlabeled target samples. To
effectively train these plug-in modules under purely unsupervised conditions,
we introduce Retrieval-Augmented-Interaction(RAIN), a three-stage pipeline that
(1) collaboratively retrieves pseudo-source and complex target samples, (2)
separately fine-tunes each expert on its respective sample set, and (3)
enforces learning object consistency via a shared learning result. Extensive
experiments on four benchmark datasets demonstrate that our approach matches
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2509.22229 ,  363kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22244
Date: Fri, 26 Sep 2025 11:59:30 GMT   (5981kb)

Title: FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image
  Editing
Authors: Junyi Wu, Zhiteng Li, Haotong Qin, Xiaohong Liu, Linghe Kong, Yulun
  Zhang and Xiaokang Yang
Categories: cs.CV
Comments: Our code will be made publicly available at
  https://github.com/JunyiWuCode/FlashEdit
\\
  Text-guided image editing with diffusion models has achieved remarkable
quality but suffers from prohibitive latency, hindering real-world
applications. We introduce FlashEdit, a novel framework designed to enable
high-fidelity, real-time image editing. Its efficiency stems from three key
innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses
costly iterative processes; (2) a Background Shield (BG-Shield) technique that
guarantees background preservation by selectively modifying features only
within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA)
mechanism that ensures precise, localized edits by suppressing semantic leakage
to the background. Extensive experiments demonstrate that FlashEdit maintains
superior background consistency and structural integrity, while performing
edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to
prior multi-step methods. Our code will be made publicly available at
https://github.com/JunyiWuCode/FlashEdit.
\\ ( https://arxiv.org/abs/2509.22244 ,  5981kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22258
Date: Fri, 26 Sep 2025 12:20:01 GMT   (7348kb)

Title: Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper
  Reasoning Benchmarks
Authors: Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Lijun Wang,
  Yuanyuan Peng, Huan Gao, Mingkun Xu, Shangyang Li
Categories: cs.CV cs.AI
Comments: 23 pages, 12 figures
\\
  Recent advances in vision-language models (VLMs) have achieved remarkable
performance on standard medical benchmarks, yet their true clinical reasoning
ability remains unclear. Existing datasets predominantly emphasize
classification accuracy, creating an evaluation illusion in which models appear
proficient while still failing at high-stakes diagnostic reasoning. We
introduce Neural-MedBench, a compact yet reasoning-intensive benchmark
specifically designed to probe the limits of multimodal clinical reasoning in
neurology. Neural-MedBench integrates multi-sequence MRI scans, structured
electronic health records, and clinical notes, and encompasses three core task
families: differential diagnosis, lesion recognition, and rationale generation.
To ensure reliable evaluation, we develop a hybrid scoring pipeline that
combines LLM-based graders, clinician validation, and semantic similarity
metrics. Through systematic evaluation of state-of-the-art VLMs, including
GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to
conventional datasets. Error analysis shows that reasoning failures, rather
than perceptual errors, dominate model shortcomings. Our findings highlight the
necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets
for statistical generalization, and depth-oriented, compact benchmarks such as
Neural-MedBench for reasoning fidelity. We release Neural-MedBench at
https://neuromedbench.github.io/ as an open and extensible diagnostic testbed,
which guides the expansion of future benchmarks and enables rigorous yet
cost-effective assessment of clinically trustworthy AI.
\\ ( https://arxiv.org/abs/2509.22258 ,  7348kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22262
Date: Fri, 26 Sep 2025 12:26:33 GMT   (25037kb)

Title: UniMapGen: A Generative Framework for Large-Scale Map Construction from
  Multi-modal Data
Authors: Yujian Yuan, Changjie Wu, Xinyuan Chang, Sijin Wang, Hang Zhang, Shiyi
  Liang, Shuang Zeng, Mu Xu
Categories: cs.CV
Comments: 17 pages, 10 figures
\\
  Large-scale map construction is foundational for critical applications such
as autonomous driving and navigation systems. Traditional large-scale map
construction approaches mainly rely on costly and inefficient special data
collection vehicles and labor-intensive annotation processes. While existing
satellite-based methods have demonstrated promising potential in enhancing the
efficiency and coverage of map construction, they exhibit two major
limitations: (1) inherent drawbacks of satellite data (e.g., occlusions,
outdatedness) and (2) inefficient vectorization from perception-based methods,
resulting in discontinuous and rough roads that require extensive
post-processing. This paper presents a novel generative framework, UniMapGen,
for large-scale map construction, offering three key innovations: (1)
representing lane lines as \textbf{discrete sequence} and establishing an
iterative strategy to generate more complete and smooth map vectors than
traditional perception-based methods. (2) proposing a flexible architecture
that supports \textbf{multi-modal} inputs, enabling dynamic selection among
BEV, PV, and text prompt, to overcome the drawbacks of satellite data. (3)
developing a \textbf{state update} strategy for global continuity and
consistency of the constructed large-scale map. UniMapGen achieves
state-of-the-art performance on the OpenSatMap dataset. Furthermore, UniMapGen
can infer occluded roads and predict roads missing from dataset annotations.
Our code will be released.
\\ ( https://arxiv.org/abs/2509.22262 ,  25037kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22276
Date: Fri, 26 Sep 2025 12:43:33 GMT   (16368kb)

Title: GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material
  Decomposition
Authors: Dinh Minh Nguyen, Malte Avenhaus, Thomas Lindemeier
Categories: cs.CV
Comments: 13 pages, 10 figures
\\
  We propose a unified solution for mesh reconstruction and material
decomposition from multi-view images based on 3D Gaussian Splatting, referred
to as GS-2M. Previous works handle these tasks separately and struggle to
reconstruct highly reflective surfaces, often relying on priors from external
models to enhance the decomposition results. Conversely, our method addresses
these two problems by jointly optimizing attributes relevant to the quality of
rendered depth and normals, maintaining geometric details while being resilient
to reflective surfaces. Although contemporary works effectively solve these
tasks together, they often employ sophisticated neural components to learn
scene properties, which hinders their performance at scale. To further
eliminate these neural components, we propose a novel roughness supervision
strategy based on multi-view photometric variation. When combined with a
carefully designed loss and optimization process, our unified framework
produces reconstruction results comparable to state-of-the-art methods,
delivering triangle meshes and their associated material components for
downstream tasks. We validate the effectiveness of our approach with widely
used datasets from previous works and qualitative comparisons with
state-of-the-art surface reconstruction methods.
\\ ( https://arxiv.org/abs/2509.22276 ,  16368kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22281
Date: Fri, 26 Sep 2025 12:46:00 GMT   (6688kb)

Title: MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial
  Reasoning
Authors: Jinkun Hao, Naifu Liang, Zhen Luo, Xudong Xu, Weipeng Zhong, Ran Yi,
  Yichen Jin, Zhaoyang Lyu, Feng Zheng, Lizhuang Ma, Jiangmiao Pang
Categories: cs.CV cs.RO
Comments: Accepted by NeurIPS 2025; Project page: https://mesatask.github.io/
\\
  The ability of robots to interpret human instructions and execute
manipulation tasks necessitates the availability of task-relevant tabletop
scenes for training. However, traditional methods for creating these scenes
rely on time-consuming manual layout design or purely randomized layouts, which
are limited in terms of plausibility or alignment with the tasks. In this
paper, we formulate a novel task, namely task-oriented tabletop scene
generation, which poses significant challenges due to the substantial gap
between high-level task instructions and the tabletop scenes. To support
research on such a challenging task, we introduce MesaTask-10K, a large-scale
dataset comprising approximately 10,700 synthetic tabletop scenes with manually
crafted layouts that ensure realistic layouts and intricate inter-object
relations. To bridge the gap between tasks and scenes, we propose a Spatial
Reasoning Chain that decomposes the generation process into object inference,
spatial interrelation reasoning, and scene graph construction for the final 3D
layout. We present MesaTask, an LLM-based framework that utilizes this
reasoning chain and is further enhanced with DPO algorithms to generate
physically plausible tabletop scenes that align well with given task
descriptions. Exhaustive experiments demonstrate the superior performance of
MesaTask compared to baselines in generating task-conforming tabletop scenes
with realistic layouts. Project page is at https://mesatask.github.io/
\\ ( https://arxiv.org/abs/2509.22281 ,  6688kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22283
Date: Fri, 26 Sep 2025 12:46:24 GMT   (709kb)

Title: Rule-Based Reinforcement Learning for Document Image Classification with
  Vision Language Models
Authors: Michael Jungo, Andreas Fischer
Categories: cs.CV
Comments: Code available at https://github.com/jungomi/vision-finetune
\\
  Rule-based reinforcement learning has been gaining popularity ever since
DeepSeek-R1 has demonstrated its success through simple verifiable rewards. In
the domain of document analysis, reinforcement learning is not as prevalent,
even though many downstream tasks may benefit from the emerging properties of
reinforcement learning, particularly the enhanced reason capabilities. We study
the effects of rule-based reinforcement learning with the task of Document
Image Classification which is one of the most commonly studied downstream tasks
in document analysis. We find that reinforcement learning tends to have better
generalisation capabilities to out-of-distritbution data, which we examine in
three different scenarios, namely out-of-distribution images, unseen classes
and different modalities. Our code is available at
https://github.com/jungomi/vision-finetune.
\\ ( https://arxiv.org/abs/2509.22283 ,  709kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22292
Date: Fri, 26 Sep 2025 12:54:23 GMT   (1482kb)

Title: Jailbreaking on Text-to-Video Models via Scene Splitting Strategy
Authors: Wonjun Lee, Haon Park, Doehyeon Lee, Bumsub Ham, Suhyun Kim
Categories: cs.CV cs.AI
\\
  Along with the rapid advancement of numerous Text-to-Video (T2V) models,
growing concerns have emerged regarding their safety risks. While recent
studies have explored vulnerabilities in models like LLMs, VLMs, and
Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely
unexplored, leaving a significant safety gap. To address this gap, we introduce
SceneSplit, a novel black-box jailbreak method that works by fragmenting a
harmful narrative into multiple scenes, each individually benign. This approach
manipulates the generative output space, the abstract set of all potential
video outputs for a given prompt, using the combination of scenes as a powerful
constraint to guide the final outcome. While each scene individually
corresponds to a wide and safe space where most outcomes are benign, their
sequential combination collectively restricts this space, narrowing it to an
unsafe region and significantly increasing the likelihood of generating a
harmful video. This core mechanism is further enhanced through iterative scene
manipulation, which bypasses the safety filter within this constrained unsafe
region. Additionally, a strategy library that reuses successful attack patterns
further improves the attack's overall effectiveness and robustness. To validate
our method, we evaluate SceneSplit across 11 safety categories on T2V models.
Our results show that it achieves a high average Attack Success Rate (ASR) of
77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly
outperforming the existing baseline. Through this work, we demonstrate that
current T2V safety mechanisms are vulnerable to attacks that exploit narrative
structure, providing new insights for understanding and improving the safety of
T2V models.
\\ ( https://arxiv.org/abs/2509.22292 ,  1482kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22300
Date: Fri, 26 Sep 2025 13:01:10 GMT   (41612kb)

Title: HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion
  Models
Authors: Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber
Categories: cs.CV cs.AI cs.LG
\\
  While diffusion models have made remarkable progress in image generation,
their outputs can still appear unrealistic and lack fine details, especially
when using fewer number of neural function evaluations (NFEs) or lower guidance
scales. To address this issue, we propose a novel momentum-based sampling
technique, termed history-guided sampling (HiGS), which enhances quality and
efficiency of diffusion sampling by integrating recent model predictions into
each inference step. Specifically, HiGS leverages the difference between the
current prediction and a weighted average of past predictions to steer the
sampling process toward more realistic outputs with better details and
structure. Our approach introduces practically no additional computation and
integrates seamlessly into existing diffusion frameworks, requiring neither
extra training nor fine-tuning. Extensive experiments show that HiGS
consistently improves image quality across diverse models and architectures and
under varying sampling budgets and guidance scales. Moreover, using a
pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for
unguided ImageNet generation at 256$\times$256 with only 30 sampling steps
(instead of the standard 250). We thus present HiGS as a plug-and-play
enhancement to standard diffusion sampling that enables faster generation with
higher fidelity.
\\ ( https://arxiv.org/abs/2509.22300 ,  41612kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22307
Date: Fri, 26 Sep 2025 13:12:43 GMT   (2117kb)

Title: Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical
  Segmentation
Authors: Jinpeng Lu, Linghan Cai, Yinda Chen, Guo Tang, Songhan Jiang, Haoyuan
  Shi, Zhiwei Xiong
Categories: cs.CV
\\
  Lightweight 3D medical image segmentation remains constrained by a
fundamental "efficiency / robustness conflict", particularly when processing
complex anatomical structures and heterogeneous modalities. In this paper, we
study how to redesign the framework based on the characteristics of
high-dimensional 3D images, and explore data synergy to overcome the fragile
representation of lightweight methods. Our approach, VeloxSeg, begins with a
deployable and extensible dual-stream CNN-Transformer architecture composed of
Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided
convolution (JLC). For each 3D image, we invoke a "glance-and-focus" principle,
where PWA rapidly retrieves multi-scale information, and JLC ensures robust
local feature extraction with minimal parameters, significantly enhancing the
model's ability to operate with low computational budget. Followed by an
extension of the dual-stream architecture that incorporates modal interaction
into the multi-scale image-retrieval process, VeloxSeg efficiently models
heterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer
(SDKT) via Gram matrices injects the texture prior extracted by a
self-supervised network into the segmentation network, yielding stronger
representations than baselines at no extra inference cost. Experimental results
on multimodal benchmarks show that VeloxSeg achieves a 26% Dice improvement,
alongside increasing GPU throughput by 11x and CPU by 48x. Codes are available
at https://github.com/JinPLu/VeloxSeg.
\\ ( https://arxiv.org/abs/2509.22307 ,  2117kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22318
Date: Fri, 26 Sep 2025 13:19:26 GMT   (43722kb)

Title: NIFTY: a Non-Local Image Flow Matching for Texture Synthesis
Authors: Pierrick Chatillon, Julien Rabin, David Tschumperl\'e
Categories: cs.CV cs.LG
\\
  This paper addresses the problem of exemplar-based texture synthesis. We
introduce NIFTY, a hybrid framework that combines recent insights on diffusion
models trained with convolutional neural networks, and classical patch-based
texture optimization techniques. NIFTY is a non-parametric flow-matching model
built on non-local patch matching, which avoids the need for neural network
training while alleviating common shortcomings of patch-based methods, such as
poor initialization or visual artifacts. Experimental results demonstrate the
effectiveness of the proposed approach compared to representative methods from
the literature. Code is available at https://github.com/PierrickCh/Nifty.git
\\ ( https://arxiv.org/abs/2509.22318 ,  43722kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22323
Date: Fri, 26 Sep 2025 13:20:52 GMT   (6218kb)

Title: RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion
  Transformer
Authors: Wangbo Zhao, Yizeng Han, Zhiwei Tang, Jiasheng Tang, Pengfei Zhou, Kai
  Wang, Bohan Zhuang, Zhangyang Wang, Fan Wang, Yang You
Categories: cs.CV
\\
  Diffusion Transformers (DiTs) excel at visual generation yet remain hampered
by slow sampling. Existing training-free accelerators - step reduction, feature
caching, and sparse attention - enhance inference speed but typically rely on a
uniform heuristic or a manually designed adaptive strategy for all images,
leaving quality on the table. Alternatively, dynamic neural networks offer
per-image adaptive acceleration, but their high fine-tuning costs limit broader
applicability. To address these limitations, we introduce RAPID3: Tri-Level
Reinforced Acceleration Policies for Diffusion Transformers, a framework that
delivers image-wise acceleration with zero updates to the base generator.
Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and
Sparse-Attention - observe the current denoising state and independently decide
their corresponding speed-up at each timestep. All policy parameters are
trained online via Group Relative Policy Optimization (GRPO) while the
generator remains frozen. Meanwhile, an adversarially learned discriminator
augments the reward signal, discouraging reward hacking by boosting returns
only when generated samples stay close to the original model's distribution.
Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,
RAPID3 achieves nearly 3x faster sampling with competitive generation quality.
\\ ( https://arxiv.org/abs/2509.22323 ,  6218kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22331
Date: Fri, 26 Sep 2025 13:25:43 GMT   (3014kb)

Title: Pedestrian Attribute Recognition via Hierarchical Cross-Modality
  HyperGraph Learning
Authors: Xiao Wang, Shujuan Wu, Xiaoxia Cheng, Changwei Bi, Jin Tang, Bin Luo
Categories: cs.CV cs.AI
Comments: The First Work that Exploits Multi-modal Knowledge Graph for
  Pedestrian Attribute Recognition
\\
  Current Pedestrian Attribute Recognition (PAR) algorithms typically focus on
mapping visual features to semantic labels or attempt to enhance learning by
fusing visual and attribute information. However, these methods fail to fully
exploit attribute knowledge and contextual information for more accurate
recognition. Although recent works have started to consider using attribute
text as additional input to enhance the association between visual and semantic
information, these methods are still in their infancy. To address the above
challenges, this paper proposes the construction of a multi-modal knowledge
graph, which is utilized to mine the relationships between local visual
features and text, as well as the relationships between attributes and
extensive visual context samples. Specifically, we propose an effective
multi-modal knowledge graph construction method that fully considers the
relationships among attributes and the relationships between attributes and
vision tokens. To effectively model these relationships, this paper introduces
a knowledge graph-guided cross-modal hypergraph learning framework to enhance
the standard pedestrian attribute recognition framework. Comprehensive
experiments on multiple PAR benchmark datasets have thoroughly demonstrated the
effectiveness of our proposed knowledge graph for the PAR task, establishing a
strong foundation for knowledge-guided pedestrian attribute recognition. The
source code of this paper will be released on
https://github.com/Event-AHU/OpenPAR
\\ ( https://arxiv.org/abs/2509.22331 ,  3014kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22339
Date: Fri, 26 Sep 2025 13:32:14 GMT   (1369kb)

Title: CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual
  Comprehension and Symbolic Reasoning in Engineering Design Process
Authors: Arman Akbari, Jian Gao, Yifei Zou, Mei Yang, Jinru Duan, Dmitrii
  Torbunov, Yanzhi Wang, Yihui Ren, Xuan Zhang
Categories: cs.CV
\\
  Engineering design operates through hierarchical abstraction from system
specifications to component implementations, requiring visual understanding
coupled with mathematical reasoning at each level. While Multi-modal Large
Language Models (MLLMs) excel at natural image tasks, their ability to extract
mathematical models from technical diagrams remains unexplored. We present
\textbf{CircuitSense}, a comprehensive benchmark evaluating circuit
understanding across this hierarchy through 8,006+ problems spanning
component-level schematics to system-level block diagrams. Our benchmark
uniquely examines the complete engineering workflow: Perception, Analysis, and
Design, with a particular emphasis on the critical but underexplored capability
of deriving symbolic equations from visual inputs. We introduce a hierarchical
synthetic generation pipeline consisting of a grid-based schematic generator
and a block diagram generator with auto-derived symbolic equation labels.
Comprehensive evaluation of six state-of-the-art MLLMs, including both
closed-source and open-source models, reveals fundamental limitations in
visual-to-mathematical reasoning. Closed-source models achieve over 85\%
accuracy on perception tasks involving component recognition and topology
identification, yet their performance on symbolic derivation and analytical
reasoning falls below 19\%, exposing a critical gap between visual parsing and
symbolic reasoning. Models with stronger symbolic reasoning capabilities
consistently achieve higher design task accuracy, confirming the fundamental
role of mathematical understanding in circuit synthesis and establishing
symbolic reasoning as the key metric for engineering competence.
\\ ( https://arxiv.org/abs/2509.22339 ,  1369kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22365
Date: Fri, 26 Sep 2025 13:59:02 GMT   (4160kb)

Title: HierLight-YOLO: A Hierarchical and Lightweight Object Detection Network
  for UAV Photography
Authors: Defan Chen, Yaohua Hu, Luchan Zhang
Categories: cs.CV
\\
  The real-time detection of small objects in complex scenes, such as the
unmanned aerial vehicle (UAV) photography captured by drones, has dual
challenges of detecting small targets (<32 pixels) and maintaining real-time
efficiency on resource-constrained platforms. While YOLO-series detectors have
achieved remarkable success in real-time large object detection, they suffer
from significantly higher false negative rates for drone-based detection where
small objects dominate, compared to large object scenarios. This paper proposes
HierLight-YOLO, a hierarchical feature fusion and lightweight model that
enhances the real-time detection of small objects, based on the YOLOv8
architecture. We propose the Hierarchical Extended Path Aggregation Network
(HEPAN), a multi-scale feature fusion method through hierarchical cross-level
connections, enhancing the small object detection accuracy. HierLight-YOLO
includes two innovative lightweight modules: Inverted Residual Depthwise
Convolution Block (IRDCB) and Lightweight Downsample (LDown) module, which
significantly reduce the model's parameters and computational complexity
without sacrificing detection capabilities. Small object detection head is
designed to further enhance spatial resolution and feature fusion to tackle the
tiny object (4 pixels) detection. Comparison experiments and ablation studies
on the VisDrone2019 benchmark demonstrate state-of-the-art performance of
HierLight-YOLO.
\\ ( https://arxiv.org/abs/2509.22365 ,  4160kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22377
Date: Fri, 26 Sep 2025 14:07:06 GMT   (177kb)

Title: Effectiveness of Large Multimodal Models in Detecting Disinformation:
  Experimental Results
Authors: Yasmina Kheddache and Marc Lalonde
Categories: cs.CV
Comments: 9 pages
\\
  The proliferation of disinformation, particularly in multimodal contexts
combining text and images, presents a significant challenge across digital
platforms. This study investigates the potential of large multimodal models
(LMMs) in detecting and mitigating false information. We propose to approach
multimodal disinformation detection by leveraging the advanced capabilities of
the GPT-4o model. Our contributions include: (1) the development of an
optimized prompt incorporating advanced prompt engineering techniques to ensure
precise and consistent evaluations; (2) the implementation of a structured
framework for multimodal analysis, including a preprocessing methodology for
images and text to comply with the model's token limitations; (3) the
definition of six specific evaluation criteria that enable a fine-grained
classification of content, complemented by a self-assessment mechanism based on
confidence levels; (4) a comprehensive performance analysis of the model across
multiple heterogeneous datasets Gossipcop, Politifact, Fakeddit, MMFakeBench,
and AMMEBA highlighting GPT-4o's strengths and limitations in disinformation
detection; (5) an investigation of prediction variability through repeated
testing, evaluating the stability and reliability of the model's
classifications; and (6) the introduction of confidence-level and
variability-based evaluation methods. These contributions provide a robust and
reproducible methodological framework for automated multimodal disinformation
analysis.
\\ ( https://arxiv.org/abs/2509.22377 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22383
Date: Fri, 26 Sep 2025 14:11:39 GMT   (3315kb)

Title: GPT-4 for Occlusion Order Recovery
Authors: Kaziwa Saleh, Zhyar Rzgar K Rostam, S\'andor Sz\'en\'asi, Zolt\'an
  V\'amossy
Categories: cs.CV
Comments: 6 pages, 4 figures
ACM-class: I.4.5
\\
  Occlusion remains a significant challenge for current vision models to
robustly interpret complex and dense real-world images and scenes. To address
this limitation and to enable accurate prediction of the occlusion order
relationship between objects, we propose leveraging the advanced capability of
a pre-trained GPT-4 model to deduce the order. By providing a specifically
designed prompt along with the input image, GPT-4 can analyze the image and
generate order predictions. The response can then be parsed to construct an
occlusion matrix which can be utilized in assisting with other occlusion
handling tasks and image understanding. We report the results of evaluating the
model on COCOA and InstaOrder datasets. The results show that by using semantic
context, visual patterns, and commonsense knowledge, the model can produce more
accurate order predictions. Unlike baseline methods, the model can reason about
occlusion relationships in a zero-shot fashion, which requires no annotated
training data and can easily be integrated into occlusion handling frameworks.
\\ ( https://arxiv.org/abs/2509.22383 ,  3315kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22392
Date: Fri, 26 Sep 2025 14:20:44 GMT   (775kb)

Title: Gradient-based multi-focus image fusion with focus-aware saliency
  enhancement
Authors: Haoyu Li and XiaoSong Li
Categories: cs.CV
Comments: iCIG 2025
\\
  Multi-focus image fusion (MFIF) aims to yield an all-focused image from
multiple partially focused inputs, which is crucial in applications cover
sur-veillance, microscopy, and computational photography. However, existing
methods struggle to preserve sharp focus-defocus boundaries, often resulting in
blurred transitions and focused details loss. To solve this problem, we propose
a MFIF method based on significant boundary enhancement, which generates
high-quality fused boundaries while effectively detecting focus in-formation.
Particularly, we propose a gradient-domain-based model that can obtain initial
fusion results with complete boundaries and effectively pre-serve the boundary
details. Additionally, we introduce Tenengrad gradient detection to extract
salient features from both the source images and the ini-tial fused image,
generating the corresponding saliency maps. For boundary refinement, we develop
a focus metric based on gradient and complementary information, integrating the
salient features with the complementary infor-mation across images to emphasize
focused regions and produce a high-quality initial decision result. Extensive
experiments on four public datasets demonstrate that our method consistently
outperforms 12 state-of-the-art methods in both subjective and objective
evaluations. We have realized codes in https://github.com/Lihyua/GICI
\\ ( https://arxiv.org/abs/2509.22392 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22393
Date: Fri, 26 Sep 2025 14:21:46 GMT   (7531kb)

Title: Text Adversarial Attacks with Dynamic Outputs
Authors: Wenqiang Wang, Siyuan Liang, Xiao Yan, Xiaochun Cao
Categories: cs.CV
\\
  Text adversarial attack methods are typically designed for static scenarios
with fixed numbers of output labels and a predefined label space, relying on
extensive querying of the victim model (query-based attacks) or the surrogate
model (transfer-based attacks). To address this gap, we introduce the Textual
Dynamic Outputs Attack (TDOA) method, which employs a clustering-based
surrogate model training approach to convert the dynamic-output scenario into a
static single-output scenario. To improve attack effectiveness, we propose the
farthest-label targeted attack strategy, which selects adversarial vectors that
deviate most from the model's coarse-grained labels, thereby maximizing
disruption. We extensively evaluate TDOA on four datasets and eight victim
models (e.g., ChatGPT-4o, ChatGPT-4.1), showing its effectiveness in crafting
adversarial examples and its strong potential to compromise large language
models with limited access. With a single query per text, TDOA achieves a
maximum attack success rate of 50.81\%. Additionally, we find that TDOA also
achieves state-of-the-art performance in conventional static output scenarios,
reaching a maximum ASR of 82.68\%. Meanwhile, by conceptualizing translation
tasks as classification problems with unbounded output spaces, we extend the
TDOA framework to generative settings, surpassing prior results by up to 0.64
RDBLEU and 0.62 RDchrF.
\\ ( https://arxiv.org/abs/2509.22393 ,  7531kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22399
Date: Fri, 26 Sep 2025 14:26:26 GMT   (110kb)

Title: Integrating Background Knowledge in Medical Semantic Segmentation with
  Logic Tensor Networks
Authors: Luca Bergamin, Giovanna Maria Dimitri, Fabio Aiolli
Categories: cs.CV cs.LG
Comments: Accepted at TAIM@IJCNN 2025
\\
  Semantic segmentation is a fundamental task in medical image analysis, aiding
medical decision-making by helping radiologists distinguish objects in an
image. Research in this field has been driven by deep learning applications,
which have the potential to scale these systems even in the presence of noise
and artifacts. However, these systems are not yet perfected. We argue that
performance can be improved by incorporating common medical knowledge into the
segmentation model's loss function. To this end, we introduce Logic Tensor
Networks (LTNs) to encode medical background knowledge using first-order logic
(FOL) rules. The encoded rules span from constraints on the shape of the
produced segmentation, to relationships between different segmented areas. We
apply LTNs in an end-to-end framework with a SwinUNETR for semantic
segmentation. We evaluate our method on the task of segmenting the hippocampus
in brain MRI scans. Our experiments show that LTNs improve the baseline
segmentation performance, especially when training data is scarce. Despite
being in its preliminary stages, we argue that neurosymbolic methods are
general enough to be adapted and applied to other medical semantic segmentation
tasks.
\\ ( https://arxiv.org/abs/2509.22399 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22400
Date: Fri, 26 Sep 2025 14:26:52 GMT   (45870kb)

Title: Closing the Safety Gap: Surgical Concept Erasure in Visual
  Autoregressive Models
Authors: Xinhao Zhong, Yimin Zhou, Zhiqi Zhang, Junhao Li, Yi Sun, Bin Chen,
  Shu-Tao Xia, Ke Xu
Categories: cs.CV
\\
  The rapid progress of visual autoregressive (VAR) models has brought new
opportunities for text-to-image generation, but also heightened safety
concerns. Existing concept erasure techniques, primarily designed for diffusion
models, fail to generalize to VARs due to their next-scale token prediction
paradigm. In this paper, we first propose a novel VAR Erasure framework VARE
that enables stable concept erasure in VAR models by leveraging auxiliary
visual tokens to reduce fine-tuning intensity. Building upon this, we introduce
S-VARE, a novel and effective concept erasure method designed for VAR, which
incorporates a filtered cross entropy loss to precisely identify and minimally
adjust unsafe visual tokens, along with a preservation loss to maintain
semantic fidelity, addressing the issues such as language drift and reduced
diversity introduce by na\"ive fine-tuning. Extensive experiments demonstrate
that our approach achieves surgical concept erasure while preserving generation
quality, thereby closing the safety gap in autoregressive text-to-image
generation by earlier methods.
\\ ( https://arxiv.org/abs/2509.22400 ,  45870kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22404
Date: Fri, 26 Sep 2025 14:32:03 GMT   (3933kb)

Title: RAU: Reference-based Anatomical Understanding with Vision Language
  Models
Authors: Yiwei Li, Yikang Liu, Jiaqi Guo, Lin Zhao, Zheyuan Zhang, Xiao Chen,
  Boris Mailhe, Ankush Mukherjee, Terrence Chen, Shanhui Sun
Categories: cs.CV cs.AI
\\
  Anatomical understanding through deep learning is critical for automatic
report generation, intra-operative navigation, and organ localization in
medical imaging; however, its progress is constrained by the scarcity of
expert-labeled data. A promising remedy is to leverage an annotated reference
image to guide the interpretation of an unlabeled target. Although recent
vision-language models (VLMs) exhibit non-trivial visual reasoning, their
reference-based understanding and fine-grained localization remain limited. We
introduce RAU, a framework for reference-based anatomical understanding with
VLMs. We first show that a VLM learns to identify anatomical regions through
relative spatial reasoning between reference and target images, trained on a
moderately sized dataset. We validate this capability through visual question
answering (VQA) and bounding box prediction. Next, we demonstrate that the
VLM-derived spatial cues can be seamlessly integrated with the fine-grained
segmentation capability of SAM2, enabling localization and pixel-level
segmentation of small anatomical regions, such as vessel segments. Across two
in-distribution and two out-of-distribution datasets, RAU consistently
outperforms a SAM2 fine-tuning baseline using the same memory setup, yielding
more accurate segmentations and more reliable localization. More importantly,
its strong generalization ability makes it scalable to out-of-distribution
datasets, a property crucial for medical image applications. To the best of our
knowledge, RAU is the first to explore the capability of VLMs for
reference-based identification, localization, and segmentation of anatomical
structures in medical images. Its promising performance highlights the
potential of VLM-driven approaches for anatomical understanding in automated
clinical workflows.
\\ ( https://arxiv.org/abs/2509.22404 ,  3933kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22412
Date: Fri, 26 Sep 2025 14:37:29 GMT   (4356kb)

Title: FreqDebias: Towards Generalizable Deepfake Detection via
  Consistency-Driven Frequency Debiasing
Authors: Hossein Kashiani, Niloufar Alipour Talemi, Fatemeh Afghah
Categories: cs.CV
Comments: Accepted to the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2025)
\\
  Deepfake detectors often struggle to generalize to novel forgery types due to
biases learned from limited training data. In this paper, we identify a new
type of model bias in the frequency domain, termed spectral bias, where
detectors overly rely on specific frequency bands, restricting their ability to
generalize across unseen forgeries. To address this, we propose FreqDebias, a
frequency debiasing framework that mitigates spectral bias through two
complementary strategies. First, we introduce a novel Forgery Mixup (Fo-Mixup)
augmentation, which dynamically diversifies frequency characteristics of
training samples. Second, we incorporate a dual consistency regularization
(CR), which enforces both local consistency using class activation maps (CAMs)
and global consistency through a von Mises-Fisher (vMF) distribution on a
hyperspherical embedding space. This dual CR mitigates over-reliance on certain
frequency components by promoting consistent representation learning under both
local and global supervision. Extensive experiments show that FreqDebias
significantly enhances cross-domain generalization and outperforms
state-of-the-art methods in both cross-domain and in-domain settings.
\\ ( https://arxiv.org/abs/2509.22412 ,  4356kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22414
Date: Fri, 26 Sep 2025 14:39:08 GMT   (36994kb)

Title: LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale
  Diffusion Transformer
Authors: Song Fei, Tian Ye, Lujia Wang and Lei Zhu
Categories: cs.CV
Comments: Project Page: https://w2genai-lab.github.io/LucidFlux
\\
  Universal image restoration (UIR) aims to recover images degraded by unknown
mixtures while preserving semantics -- conditions under which discriminative
restorers and UNet-based diffusion priors often oversmooth, hallucinate, or
drift. We present LucidFlux, a caption-free UIR framework that adapts a large
diffusion transformer (Flux.1) without image captions. LucidFlux introduces a
lightweight dual-branch conditioner that injects signals from the degraded
input and a lightly restored proxy to respectively anchor geometry and suppress
artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed
to route these cues across the backbone's hierarchy, in order to yield
coarse-to-fine and context-aware updates that protect the global structure
while recovering texture. After that, to avoid the latency and instability of
text prompts or MLLM captions, we enforce caption-free semantic alignment via
SigLIP features extracted from the proxy. A scalable curation pipeline further
filters large-scale data for structure-rich supervision. Across synthetic and
in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source
and commercial baselines, and ablation studies verify the necessity of each
component. LucidFlux shows that, for large DiTs, when, where, and what to
condition on -- rather than adding parameters or relying on text prompts -- is
the governing lever for robust and caption-free universal image restoration in
the wild.
\\ ( https://arxiv.org/abs/2509.22414 ,  36994kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22415
Date: Fri, 26 Sep 2025 14:39:13 GMT   (13625kb)

Title: Explaining multimodal LLMs via intra-modal token interactions
Authors: Jiawei Liang, Ruoyu Chen, Xianghao Jiao, Siyuan Liang, Shiming Liu,
  Qunli Zhang, Zheng Hu, Xiaochun Cao
Categories: cs.CV cs.AI
\\
  Multimodal Large Language Models (MLLMs) have achieved remarkable success
across diverse vision-language tasks, yet their internal decision-making
mechanisms remain insufficiently understood. Existing interpretability research
has primarily focused on cross-modal attribution, identifying which image
regions the model attends to during output generation. However, these
approaches often overlook intra-modal dependencies. In the visual modality,
attributing importance to isolated image patches ignores spatial context due to
limited receptive fields, resulting in fragmented and noisy explanations. In
the textual modality, reliance on preceding tokens introduces spurious
activations. Failing to effectively mitigate these interference compromises
attribution fidelity. To address these limitations, we propose enhancing
interpretability by leveraging intra-modal interaction. For the visual branch,
we introduce \textit{Multi-Scale Explanation Aggregation} (MSEA), which
aggregates attributions over multi-scale inputs to dynamically adjust receptive
fields, producing more holistic and spatially coherent visual explanations. For
the textual branch, we propose \textit{Activation Ranking Correlation} (ARC),
which measures the relevance of contextual tokens to the current token via
alignment of their top-$k$ prediction rankings. ARC leverages this relevance to
suppress spurious activations from irrelevant contexts while preserving
semantically coherent ones. Extensive experiments across state-of-the-art MLLMs
and benchmark datasets demonstrate that our approach consistently outperforms
existing interpretability methods, yielding more faithful and fine-grained
explanations of model behavior.
\\ ( https://arxiv.org/abs/2509.22415 ,  13625kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22444
Date: Fri, 26 Sep 2025 15:02:13 GMT   (2844kb)

Title: U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image
  Segmentation
Authors: Bohan Huang, Qianyun Bao, Haoyuan Ma
Categories: cs.CV
Comments: 5 pages
\\
  Medical image segmentation faces significant challenges in preserving
fine-grained details and precise boundaries due to complex anatomical
structures and pathological regions. These challenges primarily stem from two
key limitations of conventional U-Net architectures: (1) their simple skip
connections ignore the encoder-decoder semantic gap between various features,
and (2) they lack the capability for multi-scale feature extraction in deep
layers. To address these challenges, we propose the U-Net with Multi-scale
Adaptive KAN (U-MAN), a novel architecture that enhances the emerging
Kolmogorov-Arnold Network (KAN) with two specialized modules: Progressive
Attention-Guided Feature Fusion (PAGF) and the Multi-scale Adaptive KAN (MAN).
Our PAGF module replaces the simple skip connection, using attention to fuse
features from the encoder and decoder. The MAN module enables the network to
adaptively process features at multiple scales, improving its ability to
segment objects of various sizes. Experiments on three public datasets (BUSI,
GLAS, and CVC) show that U-MAN outperforms state-of-the-art methods,
particularly in defining accurate boundaries and preserving fine details.
\\ ( https://arxiv.org/abs/2509.22444 ,  2844kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22448
Date: Fri, 26 Sep 2025 15:03:55 GMT   (3928kb)

Title: $\gamma$-Quant: Towards Learnable Quantization for Low-bit Pattern
  Recognition
Authors: Mishal Fatima, Shashank Agnihotri, Marius Bock, Kanchana Vaishnavi
  Gandikota, Kristof Van Laerhoven, Michael Moeller, Margret Keuper
Categories: cs.CV
Comments: Accepted at DAGM GCPR 2025
\\
  Most pattern recognition models are developed on pre-proce\-ssed data. In
computer vision, for instance, RGB images processed through image signal
processing (ISP) pipelines designed to cater to human perception are the most
frequent input to image analysis networks. However, many modern vision tasks
operate without a human in the loop, raising the question of whether such
pre-processing is optimal for automated analysis. Similarly, human activity
recognition (HAR) on body-worn sensor data commonly takes normalized
floating-point data arising from a high-bit analog-to-digital converter (ADC)
as an input, despite such an approach being highly inefficient in terms of data
transmission, significantly affecting the battery life of wearable devices. In
this work, we target low-bandwidth and energy-constrained settings where
sensors are limited to low-bit-depth capture. We propose $\gamma$-Quant,
i.e.~the task-specific learning of a non-linear quantization for pattern
recognition. We exemplify our approach on raw-image object detection as well as
HAR of wearable data, and demonstrate that raw data with a learnable
quantization using as few as 4-bits can perform on par with the use of raw
12-bit data. All code to reproduce our experiments is publicly available via
https://github.com/Mishalfatima/Gamma-Quant
\\ ( https://arxiv.org/abs/2509.22448 ,  3928kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22450
Date: Fri, 26 Sep 2025 15:05:33 GMT   (18068kb)

Title: SSVIF: Self-Supervised Segmentation-Oriented Visible and Infrared Image
  Fusion
Authors: Zixian Zhao, Xingchen Zhang
Categories: cs.CV
\\
  Visible and infrared image fusion (VIF) has gained significant attention in
recent years due to its wide application in tasks such as scene segmentation
and object detection. VIF methods can be broadly classified into traditional
VIF methods and application-oriented VIF methods. Traditional methods focus
solely on improving the quality of fused images, while application-oriented VIF
methods additionally consider the performance of downstream tasks on fused
images by introducing task-specific loss terms during training. However,
compared to traditional methods, application-oriented VIF methods require
datasets labeled for downstream tasks (e.g., semantic segmentation or object
detection), making data acquisition labor-intensive and time-consuming. To
address this issue, we propose a self-supervised training framework for
segmentation-oriented VIF methods (SSVIF). Leveraging the consistency between
feature-level fusion-based segmentation and pixel-level fusion-based
segmentation, we introduce a novel self-supervised task-cross-segmentation
consistency-that enables the fusion model to learn high-level semantic features
without the supervision of segmentation labels. Additionally, we design a
two-stage training strategy and a dynamic weight adjustment method for
effective joint learning within our self-supervised framework. Extensive
experiments on public datasets demonstrate the effectiveness of our proposed
SSVIF. Remarkably, although trained only on unlabeled visible-infrared image
pairs, our SSVIF outperforms traditional VIF methods and rivals supervised
segmentation-oriented ones. Our code will be released upon acceptance.
\\ ( https://arxiv.org/abs/2509.22450 ,  18068kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22476
Date: Fri, 26 Sep 2025 15:23:17 GMT   (838kb)

Title: B\'ezier Meets Diffusion: Robust Generation Across Domains for Medical
  Image Segmentation
Authors: Chen Li, Meilong Xu, Xiaoling Hu, Weimin Lyu, Chao Chen
Categories: cs.CV
Comments: 17 pages, 7 figures
\\
  Training robust learning algorithms across different medical imaging
modalities is challenging due to the large domain gap. Unsupervised domain
adaptation (UDA) mitigates this problem by using annotated images from the
source domain and unlabeled images from the target domain to train the deep
models. Existing approaches often rely on GAN-based style transfer, but these
methods struggle to capture cross-domain mappings in regions with high
variability. In this paper, we propose a unified framework, B\'ezier Meets
Diffusion, for cross-domain image generation. First, we introduce a
B\'ezier-curve-based style transfer strategy that effectively reduces the
domain gap between source and target domains. The transferred source images
enable the training of a more robust segmentation model across domains.
Thereafter, using pseudo-labels generated by this segmentation model on the
target domain, we train a conditional diffusion model (CDM) to synthesize
high-quality, labeled target-domain images. To mitigate the impact of noisy
pseudo-labels, we further develop an uncertainty-guided score matching method
that improves the robustness of CDM training. Extensive experiments on public
datasets demonstrate that our approach generates realistic labeled images,
significantly augmenting the target domain and improving segmentation
performance.
\\ ( https://arxiv.org/abs/2509.22476 ,  838kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22481
Date: Fri, 26 Sep 2025 15:30:00 GMT   (5947kb)

Title: PSTTS: A Plug-and-Play Token Selector for Efficient Event-based
  Spatio-temporal Representation Learning
Authors: Xiangmo Zhao, Nan Yang, Yang Wang, Zhanwen Liu
Categories: cs.CV
\\
  Mainstream event-based spatio-temporal representation learning methods
typically process event streams by converting them into sequences of event
frames, achieving remarkable performance. However, they neglect the high
spatial sparsity and inter-frame motion redundancy inherent in event frame
sequences, leading to significant computational overhead. Existing token
sparsification methods for RGB videos rely on unreliable intermediate token
representations and neglect the influence of event noise, making them
ineffective for direct application to event data. In this paper, we propose
Progressive Spatio-Temporal Token Selection (PSTTS), a Plug-and-Play module for
event data without introducing any additional parameters. PSTTS exploits the
spatio-temporal distribution characteristics embedded in raw event data to
effectively identify and discard spatio-temporal redundant tokens, achieving an
optimal trade-off between accuracy and efficiency. Specifically, PSTTS consists
of two stages, Spatial Token Purification and Temporal Token Selection. Spatial
Token Purification discards noise and non-event regions by assessing the
spatio-temporal consistency of events within each event frame to prevent
interference with subsequent temporal redundancy evaluation. Temporal Token
Selection evaluates the motion pattern similarity between adjacent event
frames, precisely identifying and removing redundant temporal information. We
apply PSTTS to four representative backbones UniformerV2, VideoSwin, EVMamba,
and ExACT on the HARDVS, DailyDVS-200, and SeACT datasets. Experimental results
demonstrate that PSTTS achieves significant efficiency improvements.
Specifically, PSTTS reduces FLOPs by 29-43.6% and increases FPS by 21.6-41.3%
on the DailyDVS-200 dataset, while maintaining task accuracy. Our code will be
available.
\\ ( https://arxiv.org/abs/2509.22481 ,  5947kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22485
Date: Fri, 26 Sep 2025 15:33:18 GMT   (7434kb)

Title: Group Critical-token Policy Optimization for Autoregressive Image
  Generation
Authors: Guohui Zhang, Hu Yu, Xiaoxiao Ma, JingHao Zhang, Yaning Pan, Mingde
  Yao, Jie Xiao, Linjiang Huang, Feng Zhao
Categories: cs.CV
Comments: Code is available at https://github.com/zghhui/GCPO
\\
  Recent studies have extended Reinforcement Learning with Verifiable Rewards
(RLVR) to autoregressive (AR) visual generation and achieved promising
progress. However, existing methods typically apply uniform optimization across
all image tokens, while the varying contributions of different image tokens for
RLVR's training remain unexplored. In fact, the key obstacle lies in how to
identify more critical image tokens during AR generation and implement
effective token-wise optimization for them. To tackle this challenge, we
propose $\textbf{G}$roup $\textbf{C}$ritical-token $\textbf{P}$olicy
$\textbf{O}$ptimization ($\textbf{GCPO}$), which facilitates effective policy
optimization on critical tokens. We identify the critical tokens in RLVR-based
AR generation from three perspectives, specifically: $\textbf{(1)}$ Causal
dependency: early tokens fundamentally determine the later tokens and final
image effect due to unidirectional dependency; $\textbf{(2)}$ Entropy-induced
spatial structure: tokens with high entropy gradients correspond to image
structure and bridges distinct visual regions; $\textbf{(3)}$ RLVR-focused
token diversity: tokens with low visual similarity across a group of sampled
images contribute to richer token-level diversity. For these identified
critical tokens, we further introduce a dynamic token-wise advantage weight to
encourage exploration, based on confidence divergence between the policy model
and reference model. By leveraging 30\% of the image tokens, GCPO achieves
better performance than GRPO with full tokens. Extensive experiments on
multiple text-to-image benchmarks for both AR models and unified multimodal
models demonstrate the effectiveness of GCPO for AR visual generation.
\\ ( https://arxiv.org/abs/2509.22485 ,  7434kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22496
Date: Fri, 26 Sep 2025 15:38:42 GMT   (31024kb)

Title: Where MLLMs Attend and What They Rely On: Explaining Autoregressive
  Token Generation
Authors: Ruoyu Chen, Xiaoqing Guo, Kangwei Liu, Siyuan Liang, Shiming Liu,
  Qunli Zhang, Hua Zhang, Xiaochun Cao
Categories: cs.CV
\\
  Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in aligning visual inputs with natural language outputs. Yet, the
extent to which generated tokens depend on visual modalities remains poorly
understood, limiting interpretability and reliability. In this work, we present
EAGLE, a lightweight black-box framework for explaining autoregressive token
generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual
regions while quantifying the relative influence of language priors and
perceptual evidence. The framework introduces an objective function that
unifies sufficiency (insight score) and indispensability (necessity score),
optimized via greedy search over sparsified image regions for faithful and
efficient attribution. Beyond spatial attribution, EAGLE performs
modality-aware analysis that disentangles what tokens rely on, providing
fine-grained interpretability of model decisions. Extensive experiments across
open-source MLLMs show that EAGLE consistently outperforms existing methods in
faithfulness, localization, and hallucination diagnosis, while requiring
substantially less GPU memory. These results highlight its effectiveness and
practicality for advancing the interpretability of MLLMs. The code is available
at https://github.com/RuoyuChen10/EAGLE.
\\ ( https://arxiv.org/abs/2509.22496 ,  31024kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22524
Date: Fri, 26 Sep 2025 16:04:18 GMT   (14090kb)

Title: Color Names in Vision-Language Models
Authors: Alexandra Gomez-Villa, Pablo Hern\'andez-C\'amara, Muhammad Atif Butt,
  Valero Laparra, Jesus Malo, Javier Vazquez-Corral
Categories: cs.CV
\\
  Color serves as a fundamental dimension of human visual perception and a
primary means of communicating about objects and scenes. As vision-language
models (VLMs) become increasingly prevalent, understanding whether they name
colors like humans is crucial for effective human-AI interaction. We present
the first systematic evaluation of color naming capabilities across VLMs,
replicating classic color naming methodologies using 957 color samples across
five representative models. Our results show that while VLMs achieve high
accuracy on prototypical colors from classical studies, performance drops
significantly on expanded, non-prototypical color sets. We identify 21 common
color terms that consistently emerge across all models, revealing two distinct
approaches: constrained models using predominantly basic terms versus expansive
models employing systematic lightness modifiers. Cross-linguistic analysis
across nine languages demonstrates severe training imbalances favoring English
and Chinese, with hue serving as the primary driver of color naming decisions.
Finally, ablation studies reveal that language model architecture significantly
influences color naming independent of visual processing capabilities.
\\ ( https://arxiv.org/abs/2509.22524 ,  14090kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22527
Date: Fri, 26 Sep 2025 16:05:43 GMT   (38264kb)

Title: EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation
  Model
Authors: Andrii Litvynchuk, Ivan Livinsky, Anand Ravi, Nima Kalantari, Andrii
  Tsarov
Categories: cs.CV
Comments: 12 pages, 7 figures, 5 tables
\\
  Monocular depth estimation (MDE) plays a pivotal role in various computer
vision applications, such as robotics, augmented reality, and autonomous
driving. Despite recent advancements, existing methods often fail to meet key
requirements for 3D reconstruction and view synthesis, including geometric
consistency, fine details, robustness to real-world challenges like reflective
surfaces, and efficiency for edge devices. To address these challenges, we
introduce a novel MDE system, called EfficientDepth, which combines a
transformer architecture with a lightweight convolutional decoder, as well as a
bimodal density head that allows the network to estimate detailed depth maps.
We train our model on a combination of labeled synthetic and real images, as
well as pseudo-labeled real images, generated using a high-performing MDE
method. Furthermore, we employ a multi-stage optimization strategy to improve
training efficiency and produce models that emphasize geometric consistency and
fine detail. Finally, in addition to commonly used objectives, we introduce a
loss function based on LPIPS to encourage the network to produce detailed depth
maps. Experimental results demonstrate that EfficientDepth achieves performance
comparable to or better than existing state-of-the-art models, with
significantly reduced computational resources.
\\ ( https://arxiv.org/abs/2509.22527 ,  38264kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22542
Date: Fri, 26 Sep 2025 16:19:05 GMT   (3986kb)

Title: Category Discovery: An Open-World Perspective
Authors: Zhenqi He, Yuanpei Liu, Kai Han
Categories: cs.CV
\\
  Category discovery (CD) is an emerging open-world learning task, which aims
at automatically categorizing unlabelled data containing instances from unseen
classes, given some labelled data from seen classes. This task has attracted
significant attention over the years and leads to a rich body of literature
trying to address the problem from different perspectives. In this survey, we
provide a comprehensive review of the literature, and offer detailed analysis
and in-depth discussion on different methods. Firstly, we introduce a taxonomy
for the literature by considering two base settings, namely novel category
discovery (NCD) and generalized category discovery (GCD), and several derived
settings that are designed to address the extra challenges in different
real-world application scenarios, including continual category discovery,
skewed data distribution, federated category discovery, etc. Secondly, for each
setting, we offer a detailed analysis of the methods encompassing three
fundamental components, representation learning, label assignment, and
estimation of class number. Thirdly, we benchmark all the methods and distill
key insights showing that large-scale pretrained backbones, hierarchical and
auxiliary cues, and curriculum-style training are all beneficial for category
discovery, while challenges remain in the design of label assignment, the
estimation of class numbers, and scaling to complex multi-object
scenarios.Finally, we discuss the key insights from the literature so far and
point out promising future research directions. We compile a living survey of
the category discovery literature at
\href{https://github.com/Visual-AI/Category-Discovery}{https://github.com/Visual-AI/Category-Discovery}.
\\ ( https://arxiv.org/abs/2509.22542 ,  3986kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22544
Date: Fri, 26 Sep 2025 16:20:06 GMT   (23798kb)

Title: HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection
Authors: Mohammad Mahdi Hemmatyar, Mahdi Jafari, Mohammad Amin Yousefi,
  Mohammad Reza Nemati, Mobin Azadani, Hamid Reza Rastad, Amirmohammad Akbari
Categories: cs.CV
\\
  Video anomaly detection (VAD) is crucial for intelligent surveillance, but a
significant challenge lies in identifying complex anomalies, which are events
defined by intricate relationships and temporal dependencies among multiple
entities rather than by isolated actions. While self-supervised learning (SSL)
methods effectively model low-level spatiotemporal patterns, they often
struggle to grasp the semantic meaning of these interactions. Conversely, large
language models (LLMs) offer powerful contextual reasoning but are
computationally expensive for frame-by-frame analysis and lack fine-grained
spatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly
Detection, a hybrid SSL-LLM model that combines a multi-task SSL temporal
analyzer with LLM validator. The SSL module is built upon an nnFormer backbone
which is a transformer-based model for image segmentation. It is trained with
multiple proxy tasks, learns from video frames to identify those suspected of
anomaly. The selected frames are then forwarded to the LLM, which enriches the
analysis with semantic context by applying structured, rule-based reasoning to
validate the presence of anomalies. Experiments on the challenging ComplexVAD
dataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming
existing baselines by 12.5% while reducing LLM computation. We release our
interaction anomaly taxonomy, adaptive thresholding protocol, and code to
facilitate future research in complex VAD scenarios.
\\ ( https://arxiv.org/abs/2509.22544 ,  23798kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22548
Date: Fri, 26 Sep 2025 16:29:37 GMT   (13564kb)

Title: JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory
  for Vision-Language Navigation
Authors: Shuang Zeng and Dekang Qi and Xinyuan Chang and Feng Xiong and Shichao
  Xie and Xiaolong Wu and Shiyi Liang and Mu Xu and Xing Wei
Categories: cs.CV cs.RO
Comments: Project page: https://miv-xjtu.github.io/JanusVLN.github.io/
\\
  Vision-and-Language Navigation requires an embodied agent to navigate through
unseen environments, guided by natural language instructions and a continuous
video stream. Recent advances in VLN have been driven by the powerful semantic
understanding of Multimodal Large Language Models. However, these methods
typically rely on explicit semantic memory, such as building textual cognitive
maps or storing historical visual frames. This type of method suffers from
spatial information loss, computational redundancy, and memory bloat, which
impede efficient navigation. Inspired by the implicit scene representation in
human navigation, analogous to the left brain's semantic understanding and the
right brain's spatial cognition, we propose JanusVLN, a novel VLN framework
featuring a dual implicit neural memory that models spatial-geometric and
visual-semantic memory as separate, compact, and fixed-size neural
representations. This framework first extends the MLLM to incorporate 3D prior
knowledge from the spatial-geometric encoder, thereby enhancing the spatial
reasoning capabilities of models based solely on RGB input. Then, the
historical key-value caches from the spatial-geometric and visual-semantic
encoders are constructed into a dual implicit memory. By retaining only the KVs
of tokens in the initial and sliding window, redundant computation is avoided,
enabling efficient incremental updates. Extensive experiments demonstrate that
JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For
example, the success rate improves by 10.5-35.5 compared to methods using
multiple data types as input and by 3.6-10.8 compared to methods using more RGB
training data. This indicates that the proposed dual implicit neural memory, as
a novel paradigm, explores promising new directions for future VLN research.
Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.
\\ ( https://arxiv.org/abs/2509.22548 ,  13564kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22581
Date: Fri, 26 Sep 2025 17:01:21 GMT   (2453kb)

Title: SpikeMatch: Semi-Supervised Learning with Temporal Dynamics of Spiking
  Neural Networks
Authors: Jini Yang, Beomseok Oh, Seungryong Kim, Sunok Kim
Categories: cs.CV
\\
  Spiking neural networks (SNNs) have recently been attracting significant
attention for their biological plausibility and energy efficiency, but
semi-supervised learning (SSL) methods for SNN-based models remain
underexplored compared to those for artificial neural networks (ANNs). In this
paper, we introduce SpikeMatch, the first SSL framework for SNNs that leverages
the temporal dynamics through the leakage factor of SNNs for diverse
pseudo-labeling within a co-training framework. By utilizing agreement among
multiple predictions from a single SNN, SpikeMatch generates reliable
pseudo-labels from weakly-augmented unlabeled samples to train on
strongly-augmented ones, effectively mitigating confirmation bias by capturing
discriminative features with limited labels. Experiments show that SpikeMatch
outperforms existing SSL methods adapted to SNN backbones across various
standard benchmarks.
\\ ( https://arxiv.org/abs/2509.22581 ,  2453kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22615
Date: Fri, 26 Sep 2025 17:41:57 GMT   (4842kb)

Title: Vision-Language Alignment from Compressed Image Representations using 2D
  Gaussian Splatting
Authors: Yasmine Omri, Connor Ding, Tsachy Weissman, Thierry Tambe
Categories: cs.CV cs.AI cs.CL
\\
  Modern vision language pipelines are driven by RGB vision encoders trained on
massive image text corpora. While these pipelines have enabled impressive zero
shot capabilities and strong transfer across tasks, they still inherit two
structural inefficiencies from the pixel domain: (i) transmitting dense RGB
images from edge devices to the cloud is energy intensive and costly, and (ii)
patch based tokenization explodes sequence length, stressing attention budgets
and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative
visual substrate for alignment: a compact, spatially adaptive representation
that parameterizes images by a set of colored anisotropic Gaussians. We develop
a scalable 2DGS pipeline with structured initialization, luminance aware
pruning, and batched CUDA kernels, achieving over 90x faster fitting and about
97% GPU utilization compared to prior implementations. We further adapt
contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen
RGB-based transformer backbone with a lightweight splat aware input stem and a
perceiver resampler, training only about 7% of the total parameters. On large
DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K
performance while compressing inputs 3 to 20x relative to pixels. While
accuracy currently trails RGB encoders, our results establish 2DGS as a viable
multimodal substrate, pinpoint architectural bottlenecks, and open a path
toward representations that are both semantically powerful and transmission
efficient for edge cloud learning.
\\ ( https://arxiv.org/abs/2509.22615 ,  4842kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22622
Date: Fri, 26 Sep 2025 17:48:24 GMT   (22471kb)

Title: LongLive: Real-time Interactive Long Video Generation
Authors: Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao,
  Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, Yukang
  Chen
Categories: cs.CV
Comments: Code, model, and demos are available at
  https://github.com/NVlabs/LongLive
\\
  We present LongLive, a frame-level autoregressive (AR) framework for
real-time and interactive long video generation. Long video generation presents
challenges in both efficiency and quality. Diffusion and Diffusion-Forcing
models can produce high-quality videos but suffer from low efficiency due to
bidirectional attention. Causal attention AR models support KV caching for
faster inference, but often degrade in quality on long videos due to memory
challenges during long-video training. In addition, beyond static prompt-based
generation, interactive capabilities, such as streaming prompt inputs, are
critical for dynamic content creation, enabling users to guide narratives in
real time. This interactive requirement significantly increases complexity,
especially in ensuring visual consistency and semantic coherence during prompt
transitions. To address these challenges, LongLive adopts a causal, frame-level
AR design that integrates a KV-recache mechanism that refreshes cached states
with new prompts for smooth, adherent switches; streaming long tuning to enable
long video training and to align training and inference (train-long-test-long);
and short window attention paired with a frame-level attention sink, shorten as
frame sink, preserving long-range consistency while enabling faster generation.
With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model
to minute-long generation in just 32 GPU-days. At inference, LongLive sustains
20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both
short and long videos. LongLive supports up to 240-second videos on a single
H100 GPU. LongLive further supports INT8-quantized inference with only marginal
quality loss.
\\ ( https://arxiv.org/abs/2509.22622 ,  22471kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22624
Date: Fri, 26 Sep 2025 17:50:12 GMT   (4726kb)

Title: SPARK: Synergistic Policy And Reward Co-Evolving Framework
Authors: Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong,
  Haodong Duan, Dahua Lin, Jiaqi Wang
Categories: cs.CV cs.LG
Comments: Project:https://github.com/InternLM/Spark
\\
  Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)
increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL
with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback
(RLHF) for subjective tasks. However, RLHF incurs high costs and potential
reward-policy mismatch due to reliance on human preferences, while RLVR still
wastes supervision by discarding rollouts and correctness signals after each
update. To address these challenges, we introduce the Synergistic Policy And
Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable
method that builds on RLVR. Instead of discarding rollouts and correctness
data, SPARK recycles this valuable information to simultaneously train the
model itself as a generative reward model. This auxiliary training uses a mix
of objectives, such as pointwise reward score, pairwise comparison, and
evaluation conditioned on further-reflection responses, to teach the model to
evaluate and improve its own responses. Our process eliminates the need for a
separate reward model and costly human preference data. SPARK creates a
positive co-evolving feedback loop: improved reward accuracy yields better
policy gradients, which in turn produce higher-quality rollouts that further
refine the reward model. Our unified framework supports test-time scaling via
self-reflection without external reward models and their associated costs. We
show that SPARK achieves significant performance gains on multiple LLM and LVLM
models and multiple reasoning, reward models, and general benchmarks. For
example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,
12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the
baselines, demonstrating robustness and broad generalization.
\\ ( https://arxiv.org/abs/2509.22624 ,  4726kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22627
Date: Fri, 26 Sep 2025 17:51:28 GMT   (31127kb)

Title: CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach
Authors: Alexandre Lopes, Roberto Souza, Helio Pedrini
Categories: cs.CV
\\
  Depth Estimation plays a crucial role in recent applications in robotics,
autonomous vehicles, and augmented reality. These scenarios commonly operate
under constraints imposed by computational power. Stereo image pairs offer an
effective solution for depth estimation since it only needs to estimate the
disparity of pixels in image pairs to determine the depth in a known rectified
system. Due to the difficulty in acquiring reliable ground-truth depth data
across diverse scenarios, self-supervised techniques emerge as a solution,
particularly when large unlabeled datasets are available. We propose a novel
self-supervised convolutional approach that outperforms existing
state-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) while balancing computational cost. The proposed CCNeXt architecture
employs a modern CNN feature extractor with a novel windowed epipolar
cross-attention module in the encoder, complemented by a comprehensive redesign
of the depth estimation decoder. Our experiments demonstrate that CCNeXt
achieves competitive metrics on the KITTI Eigen Split test data while being
10.18$\times$ faster than the current best model and achieves state-of-the-art
results in all metrics in the KITTI Eigen Split Improved Ground Truth and
Driving Stereo datasets when compared to recently proposed techniques. To
ensure complete reproducibility, our project is accessible at
\href{https://github.com/alelopes/CCNext}{\texttt{https://github.com/alelopes/CCNext}}.
\\ ( https://arxiv.org/abs/2509.22627 ,  31127kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22628
Date: Fri, 26 Sep 2025 17:51:46 GMT   (2910kb)

Title: UML-CoT: Structured Reasoning and Planning with Unified Modeling
  Language for Robotic Room Cleaning
Authors: Hongyu Chen, Guangrun Wang
Categories: cs.CV
ACM-class: I.2.6; I.2.7; I.2.8; I.4.8; I.5.4
\\
  Chain-of-Thought (CoT) prompting improves reasoning in large language models
(LLMs), but its reliance on unstructured text limits interpretability and
executability in embodied tasks. Prior work has explored structured CoTs using
scene or logic graphs, yet these remain fundamentally limited: they model only
low-order relations, lack constructs like inheritance or behavioral
abstraction, and provide no standardized semantics for sequential or
conditional planning. We propose UML-CoT, a structured reasoning and planning
framework that leverages Unified Modeling Language (UML) to generate symbolic
CoTs and executable action plans. UML class diagrams capture compositional
object semantics, while activity diagrams model procedural control flow. Our
three-stage training pipeline combines supervised fine-tuning with Group
Relative Policy Optimization (GRPO), including reward learning from answer-only
data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered
room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in
interpretability, planning coherence, and execution success, highlighting UML
as a more expressive and actionable structured reasoning formalism.
\\ ( https://arxiv.org/abs/2509.22628 ,  2910kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22631
Date: Fri, 26 Sep 2025 17:55:26 GMT   (9844kb)

Title: LABELING COPILOT: A Deep Research Agent for Automated Data Curation in
  Computer Vision
Authors: Debargha Ganguly, Sumit Kumar, Ishwar Balappanawar, Weicong Chen,
  Shashank Kambhatla, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam
  Kumaraguru, Vipin Chaudhary
Categories: cs.CV cs.CL
\\
  Curating high-quality, domain-specific datasets is a major bottleneck for
deploying robust vision systems, requiring complex trade-offs between data
quality, diversity, and cost when researching vast, unlabeled data lakes. We
introduce Labeling Copilot, the first data curation deep research agent for
computer vision. A central orchestrator agent, powered by a large multimodal
language model, uses multi-step reasoning to execute specialized tools across
three core capabilities: (1) Calibrated Discovery sources relevant,
in-distribution data from large repositories; (2) Controllable Synthesis
generates novel data for rare scenarios with robust filtering; and (3)
Consensus Annotation produces accurate labels by orchestrating multiple
foundation models via a novel consensus mechanism incorporating non-maximum
suppression and voting. Our large-scale validation proves the effectiveness of
Labeling Copilot's components. The Consensus Annotation module excels at object
discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per
image-nearly double the 7.4 ground-truth objects-achieving a final annotation
mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class
imbalance to discover 903 new bounding box categories, expanding its capability
to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a
10-million sample scale, features an active learning strategy that is up to 40x
more computationally efficient than alternatives with equivalent sample
efficiency. These experiments validate that an agentic workflow with optimized,
scalable tools provides a robust foundation for curating industrial-scale
datasets.
\\ ( https://arxiv.org/abs/2509.22631 ,  9844kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22635
Date: Fri, 26 Sep 2025 17:57:32 GMT   (11595kb)

Title: Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance
Authors: Luc Boudier, Loris Manganelli, Eleftherios Tsonis, Nicolas Dufour,
  Vicky Kalogeiton
Categories: cs.CV cs.LG
Comments: BMVC 2025. Project page:
  https://www.lix.polytechnique.fr/vista/projects/2025_bmvc_dipsy/
\\
  Few-shot image classification remains challenging due to the limited
availability of labeled examples. Recent approaches have explored generating
synthetic training data using text-to-image diffusion models, but often require
extensive model fine-tuning or external information sources. We present a novel
training-free approach, called DIPSY, that leverages IP-Adapter for
image-to-image translation to generate highly discriminative synthetic images
using only the available few-shot examples. DIPSY introduces three key
innovations: (1) an extended classifier-free guidance scheme that enables
independent control over positive and negative image conditioning; (2) a class
similarity-based sampling strategy that identifies effective contrastive
examples; and (3) a simple yet effective pipeline that requires no model
fine-tuning or external captioning and filtering. Experiments across ten
benchmark datasets demonstrate that our approach achieves state-of-the-art or
comparable performance, while eliminating the need for generative model
adaptation or reliance on external tools for caption generation and image
filtering. Our results highlight the effectiveness of leveraging dual image
prompting with positive-negative guidance for generating class-discriminative
features, particularly for fine-grained classification tasks.
\\ ( https://arxiv.org/abs/2509.22635 ,  11595kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22636
Date: Fri, 26 Sep 2025 17:58:04 GMT   (3955kb)

Title: Scale-Wise VAR is Secretly Discrete Diffusion
Authors: Amandeep Kumar, Nithin Gopalakrishnan Nair, Vishal M. Patel
Categories: cs.CV cs.LG
Comments: Technical Reports
\\
  Autoregressive (AR) transformers have emerged as a powerful paradigm for
visual generation, largely due to their scalability, computational efficiency
and unified architecture with language and vision. Among them, next scale
prediction Visual Autoregressive Generation (VAR) has recently demonstrated
remarkable performance, even surpassing diffusion-based models. In this work,
we revisit VAR and uncover a theoretical insight: when equipped with a
Markovian attention mask, VAR is mathematically equivalent to a discrete
diffusion. We term this reinterpretation as Scalable Visual Refinement with
Discrete Diffusion (SRDD), establishing a principled bridge between AR
transformers and diffusion models. Leveraging this new perspective, we show how
one can directly import the advantages of diffusion such as iterative
refinement and reduce architectural inefficiencies into VAR, yielding faster
convergence, lower inference cost, and improved zero-shot reconstruction.
Across multiple datasets, we show that the diffusion based perspective of VAR
leads to consistent gains in efficiency and generation.
\\ ( https://arxiv.org/abs/2509.22636 ,  3955kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22645
Date: Fri, 26 Sep 2025 17:59:51 GMT   (2590kb)

Title: Hierarchical Representation Matching for CLIP-based Class-Incremental
  Learning
Authors: Zhen-Hao Wen, Yan Wang, Ji Feng, Han-Jia Ye, De-Chuan Zhan and Da-Wei
  Zhou
Categories: cs.CV cs.AI
\\
  Class-Incremental Learning (CIL) aims to endow models with the ability to
continuously adapt to evolving data streams. Recent advances in pre-trained
vision-language models (e.g., CLIP) provide a powerful foundation for this
task. However, existing approaches often rely on simplistic templates, such as
"a photo of a [CLASS]", which overlook the hierarchical nature of visual
concepts. For example, recognizing "cat" versus "car" depends on coarse-grained
cues, while distinguishing "cat" from "lion" requires fine-grained details.
Similarly, the current feature mapping in CLIP relies solely on the
representation from the last layer, neglecting the hierarchical information
contained in earlier layers. In this work, we introduce HiErarchical
Representation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages
LLMs to recursively generate discriminative textual descriptors, thereby
augmenting the semantic space with explicit hierarchical cues. These
descriptors are matched to different levels of the semantic hierarchy and
adaptively routed based on task-specific requirements, enabling precise
discrimination while alleviating catastrophic forgetting in incremental tasks.
Extensive experiments on multiple benchmarks demonstrate that our method
consistently achieves state-of-the-art performance.
\\ ( https://arxiv.org/abs/2509.22645 ,  2590kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22646
Date: Fri, 26 Sep 2025 17:59:54 GMT   (25131kb)

Title: Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal
  LLMs
Authors: Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang,
  Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah,
  Chung Un Lee, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch
Categories: cs.CV cs.AI cs.CL
Comments: Project Page: https://deeptracereward.github.io/
\\
  Can humans identify AI-generated (fake) videos and provide grounded reasons?
While video generation models have advanced rapidly, a critical dimension --
whether humans can detect deepfake traces within a generated video, i.e.,
spatiotemporal grounded visual artifacts that reveal a video as machine
generated -- has been largely overlooked. We introduce DeeptraceReward, the
first fine-grained, spatially- and temporally- aware benchmark that annotates
human-perceived fake traces for video generation reward. The dataset comprises
4.3K detailed annotations across 3.3K high-quality generated videos. Each
annotation provides a natural-language explanation, pinpoints a bounding-box
region containing the perceived trace, and marks precise onset and offset
timestamps. We consolidate these annotations into 9 major categories of
deepfake traces that lead humans to identify a video as AI-generated, and train
multimodal language models (LMs) as reward models to mimic human judgments and
localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by
34.7% on average across fake clue identification, grounding, and explanation.
Interestingly, we observe a consistent difficulty gradient: binary fake v.s.
real classification is substantially easier than fine-grained deepfake trace
detection; within the latter, performance degrades from natural language
explanations (easiest), to spatial grounding, to temporal labeling (hardest).
By foregrounding human-perceived deepfake traces, DeeptraceReward provides a
rigorous testbed and training signal for socially aware and trustworthy video
generation.
\\ ( https://arxiv.org/abs/2509.22646 ,  25131kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22647
Date: Fri, 26 Sep 2025 17:59:55 GMT   (4535kb)

Title: CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement
  Learning
Authors: Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong
  Huang, Jiaqi Wang, Feng Wu, Dahua Lin
Categories: cs.CV cs.AI cs.CL
Comments: Code is available at https://github.com/InternLM/CapRL
\\
  Image captioning is a fundamental task that bridges the visual and linguistic
domains, playing a critical role in pre-training Large Vision-Language Models
(LVLMs). Current state-of-the-art captioning models are typically trained with
Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable
data annotated by humans or proprietary models. This approach often leads to
models that memorize specific ground-truth answers, limiting their generality
and ability to generate diverse, creative descriptions. To overcome the
limitation of SFT, we propose applying the Reinforcement Learning with
Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.
A primary challenge, however, is designing an objective reward function for the
inherently subjective nature of what constitutes a "good" caption. We introduce
Captioning Reinforcement Learning (CapRL), a novel training framework that
redefines caption quality through its utility: a high-quality caption should
enable a non-visual language model to accurately answer questions about the
corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM
generates a caption, and the objective reward is derived from the accuracy of a
separate, vision-free LLM answering Multiple-Choice Questions based solely on
that caption. As the first study to apply RLVR to the subjective image
captioning task, we demonstrate that CapRL significantly enhances multiple
settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B
results in substantial gains across 12 benchmarks. Moreover, within the Prism
Framework for caption quality evaluation, CapRL achieves performance comparable
to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.
Code is available here: https://github.com/InternLM/CapRL.
\\ ( https://arxiv.org/abs/2509.22647 ,  4535kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22650
Date: Fri, 26 Sep 2025 17:59:57 GMT   (8033kb)

Title: RefAM: Attention Magnets for Zero-Shot Referral Segmentation
Authors: Anna Kukleva and Enis Simsar and Alessio Tonioni and Muhammad Ferjad
  Naeem and Federico Tombari and Jan Eric Lenssen and Bernt Schiele
Categories: cs.CV
Comments: Project Page: https://refam-diffusion.github.io/
\\
  Most existing approaches to referring segmentation achieve strong performance
only through fine-tuning or by composing multiple pre-trained models, often at
the cost of additional training and architectural modifications. Meanwhile,
large-scale generative diffusion models encode rich semantic information,
making them attractive as general-purpose feature extractors. In this work, we
introduce a new method that directly exploits features, attention scores, from
diffusion transformers for downstream tasks, requiring neither architectural
modifications nor additional training. To systematically evaluate these
features, we extend benchmarks with vision-language grounding tasks spanning
both images and videos. Our key insight is that stop words act as attention
magnets: they accumulate surplus attention and can be filtered to reduce noise.
Moreover, we identify global attention sinks (GAS) emerging in deeper layers
and show that they can be safely suppressed or redirected onto auxiliary
tokens, leading to sharper and more accurate grounding maps. We further propose
an attention redistribution strategy, where appended stop words partition
background activations into smaller clusters, yielding sharper and more
localized heatmaps. Building on these findings, we develop RefAM, a simple
training-free grounding framework that combines cross-attention maps, GAS
handling, and redistribution. Across zero-shot referring image and video
segmentation benchmarks, our approach consistently outperforms prior methods,
establishing a new state of the art without fine-tuning or additional
components.
\\ ( https://arxiv.org/abs/2509.22650 ,  8033kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21527
Date: Thu, 25 Sep 2025 20:13:49 GMT   (213kb)

Title: Redesigning GROMACS Halo Exchange: Improving Strong Scaling with
  GPU-initiated NVSHMEM
Authors: Mahesh Doijade, Andrey Alekseenko, Ania Brown, Alan Gray, Szil\'ard
  P\'all
Categories: cs.DC cs.PF physics.comp-ph
Comments: 17 pages, 8 figures, submitted to PAW-ATM Workshop, SC 2025
DOI: 10.1145/3731599.3767508
\\
  Improving time-to-solution in molecular dynamics simulations often requires
strong scaling due to fixed-sized problems. GROMACS is highly
latency-sensitive, with peak iteration rates in the sub-millisecond, making
scalability on heterogeneous supercomputers challenging. MPI's CPU-centric
nature introduces additional latencies on GPU-resident applications' critical
path, hindering GPU utilization and scalability. To address these limitations,
we present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain
decomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data
packing and communication, leveraging hardware latency-hiding for fine-grained
overlap. We employ kernel fusion across overlapped data forwarding
communication phases and utilize the asynchronous copy engine over NVLink to
optimize latency and bandwidth. Our GPU-resident formulation greatly increases
communication-computation overlap, improving GROMACS strong scaling performance
across NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x
multi-node over NVLink+InfiniBand. This demonstrates the profound benefits of
GPU-initiated communication for strong-scaling a broad range of
latency-sensitive applications.
\\ ( https://arxiv.org/abs/2509.21527 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21841
Date: Fri, 26 Sep 2025 04:02:21 GMT   (2561kb)

Title: Zeppelin: Balancing Variable-length Workloads in Data Parallel Large
  Model Training
Authors: Chang Chen, Tiancheng Chen, Jiangfei Duan, Qianchao Zhu, Zerui Wang,
  Qinghao Hu, Peng Sun, Xiuhong Li, Chao Yang, Torsten Hoefler
Categories: cs.DC
\\
  Training large language models (LLMs) with increasingly long and varying
sequence lengths introduces severe load imbalance challenges in large-scale
data-parallel training. Recent frameworks attempt to mitigate these issues
through data reorganization or hybrid parallel strategies. However, they often
overlook how computational and communication costs scale with sequence length,
resulting in suboptimal performance. We identify three critical challenges: (1)
varying computation-to-communication ratios across sequences of different
lengths in distributed attention, (2) mismatch between static NIC-GPU affinity
and dynamic parallel workloads, and (3) distinct optimal partitioning
strategies required for quadratic attention versus linear components. To
address these challenges, we present Zeppelin, a novel training system that
integrates three key techniques: (1) a hierarchical sequence partitioning
method for the attention module that reduces communication overhead and
balances computation, supported by an efficient attention engine that applies
divergent parallel strategies; (2) a routing layer that orchestrates inter-node
transfers to fully utilize NIC bandwidth; and (3) a remapping layer that
transforms sequence layouts between attention and linear modules, ensuring high
computational efficiency across both. Comprehensive evaluations across diverse
configurations show that Zeppelin delivers an average 2.80x speedup over
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2509.21841 ,  2561kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22068
Date: Fri, 26 Sep 2025 08:50:48 GMT   (169kb)

Title: Code once, Run Green: Automated Green Code Translation in Serverless
  Computing
Authors: Sebastian Werner and Mathis K\"ahler and Alireza Hakamian
Categories: cs.DC
Comments: Accepted at IC2E 2025
DOI: 10.1109/IC2E65552.2025.00026
\\
  The rapid digitization and the increasing use of emerging technologies such
as AI models have significantly contributed to the emissions of computing
infrastructure. Efforts to mitigate this impact typically focus on the
infrastructure level such as powering data centers with renewable energy, or
through the specific design of energy-efficient software. However, both
strategies rely on stakeholder intervention, making their adoption in legacy
and already-deployed systems unlikely. As a result, past architectural and
implementation decisions continue to incur additional energy usage - a
phenomenon we refer to as energy debt.
  Hence, in this paper, we investigate the potential of serverless computing
platforms to automatically reduce energy debt by leveraging the unique access
to function source code. Specifically, we explore whether large language models
(LLMs) can translate serverless functions into more energy-efficient
programming languages while preserving functional correctness. To this end, we
design and implement ReFaaS and integrate it into the Fission serverless
framework. We evaluate multiple LLMs on their ability to perform such code
translations and analyze their impact on energy consumption.
  Our preliminary results indicate that translated functions can reduce
invocation energy by up to 70%, achieving net energy savings after
approximately 3,000 to 5,000 invocations, depending on the LLM used.
Nonetheless, the approach faces several challenges: not all functions are
suitable for translation, and for some, the amortization threshold is
significantly higher or unreachable. Despite these limitations, we identify
four key research challenges whose resolution could unlock long-term, automated
mitigation of energy debt in serverless computing.
\\ ( https://arxiv.org/abs/2509.22068 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22117
Date: Fri, 26 Sep 2025 09:40:51 GMT   (176kb)

Title: The AI_INFN Platform: Artificial Intelligence Development in the Cloud
Authors: Lucio Anderlini, Giulio Bianchini, Diego Ciangottini, Stefano Dal Pra,
  Diego Michelotto, Rosa Petrini, Daniele Spiga
Categories: cs.DC cs.AI
Comments: To be published in SciPost Physics Proceedings for European AI for
  Fundamental Physics Conference (EuCAIFCon 2025)
\\
  Machine Learning (ML) is driving a revolution in the way scientists design,
develop, and deploy data-intensive software. However, the adoption of ML
presents new challenges for the computing infrastructure, particularly in terms
of provisioning and orchestrating access to hardware accelerators for
development, testing, and production. The INFN-funded project AI_INFN
(Artificial Intelligence at INFN) aims at fostering the adoption of ML
techniques within INFN use cases by providing support on multiple aspects,
including the provisioning of AI-tailored computing resources. It leverages
cloud-native solutions in the context of INFN Cloud, to share hardware
accelerators as effectively as possible, ensuring the diversity of the
Institute's research activities is not compromised. In this contribution, we
provide an update on the commissioning of a Kubernetes platform designed to
ease the development of GPU-powered data analysis workflows and their
scalability on heterogeneous distributed computing resources, also using the
offloading mechanism with Virtual Kubelet and InterLink API. This setup can
manage workflows across different resource providers, including sites of the
Worldwide LHC Computing Grid and supercomputers such as CINECA Leonardo,
providing a model for use cases requiring dedicated infrastructures for
different parts of the workload. Initial test results, emerging case studies,
and integration scenarios will be presented with functional tests and
benchmarks.
\\ ( https://arxiv.org/abs/2509.22117 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22233
Date: Fri, 26 Sep 2025 11:43:00 GMT   (116kb)

Title: Orientation does not help with 3-coloring a grid in online-LOCAL
Authors: Thomas Boudier, Filippo Casagrande, Avinandan Das, Massimo Equi,
  Henrik Lievonen, Augusto Modanese, and Ronja Stimpert
Categories: cs.DC cs.DS
Comments: 16 pages, 3 figures
\\
  The online-LOCAL and SLOCAL models are extensions of the LOCAL model where
nodes are processed in a sequential but potentially adversarial order. So far,
the only problem we know of where the global memory of the online-LOCAL model
has an advantage over SLOCAL is 3-coloring bipartite graphs. Recently, Chang et
al. [PODC 2024] showed that even in grids, 3-coloring requires $\Omega(\log n)$
locality in deterministic online-LOCAL. This result was subsequently extended
by Akbari et al. [STOC 2025] to also hold in randomized online-LOCAL. However,
both proofs heavily rely on the assumption that the algorithm does not have
access to the orientation of the underlying grid. In this paper, we show how to
lift this requirement and obtain the same lower bound (against either model)
even when the algorithm is explicitly given a globally consistent orientation
of the grid.
\\ ( https://arxiv.org/abs/2509.22233 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21789
Date: Fri, 26 Sep 2025 02:43:24 GMT   (13105kb)

Title: Visual Multi-Agent System: Mitigating Hallucination Snowballing via
  Visual Flow
Authors: Xinlei Yu, Chengming Xu, Guibin Zhang, Yongbo He, Zhangquan Chen,
  Zhucun Xue, Jiangning Zhang, Yue Liao, Xiaobin Hu, Yu-Gang Jiang, Shuicheng
  Yan
Categories: cs.MA cs.CV
\\
  Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables
challenging tasks but suffers from a novel failure term, multi-agent visual
hallucination snowballing, where hallucinations are seeded in a single agent
and amplified by following ones due to the over-reliance on textual flow to
relay visual information. Through turn-, layer-, and token-wise attention
analyses, we provide detailed insights into the essence of hallucination
snowballing regarding the reduction of visual attention allocation. It leads us
to identify a subset of vision tokens with a unimodal attention peak in middle
layers that best preserve visual evidence but gradually diminish in deeper
agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we
propose ViF, a lightweight, plug-and-play mitigation paradigm that relays
inter-agent messages with Visual Flow powered by the selected visual relay
tokens and applies attention reallocation to amplify this pattern. The
experiment results demonstrate that our method markedly reduces hallucination
snowballing, consistently improving the performance across eight benchmarks
based on four common MAS structures and ten base models. The source code will
be available at: https://github.com/YU-deep/ViF.git.
\\ ( https://arxiv.org/abs/2509.21789 ,  13105kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21834
Date: Fri, 26 Sep 2025 03:49:41 GMT   (581kb)

Title: RobustFlow: Towards Robust Agentic Workflow Generation
Authors: Shengxiang Xu, Jiayi Zhang, Shimin Di, Yuyu Luo, Liang Yao, Hanmo Liu,
  Jia Zhu, Fan Liu, Min-Ling Zhang
Categories: cs.MA
\\
  The automated generation of agentic workflows is a promising frontier for
enabling large language models (LLMs) to solve complex tasks. However, our
investigation reveals that the robustness of agentic workflow remains a
critical, unaddressed challenge. Current methods often generate wildly
inconsistent workflows when provided with instructions that are semantically
identical but differently phrased. This brittleness severely undermines their
reliability and trustworthiness for real-world applications. To quantitatively
diagnose this instability, we propose metrics based on nodal and topological
similarity to evaluate workflow consistency against common semantic variations
such as paraphrasing and noise injection. Subsequently, we further propose a
novel training framework, RobustFlow, that leverages preference optimization to
teach models invariance to instruction variations. By training on sets of
synonymous task descriptions, RobustFlow boosts workflow robustness scores to
70\% - 90\%, which is a substantial improvement over existing approaches. The
code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.
\\ ( https://arxiv.org/abs/2509.21834 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22130
Date: Fri, 26 Sep 2025 09:53:40 GMT   (1690kb)

Title: Multi-Agent Path Finding via Offline RL and LLM Collaboration
Authors: Merve Atasever, Matthew Hong, Mihir Nitin Kulkarni, Qingpei Li,
  Jyotirmoy V. Deshmukh
Categories: cs.MA cs.AI cs.LG
\\
  Multi-Agent Path Finding (MAPF) poses a significant and challenging problem
critical for applications in robotics and logistics, particularly due to its
combinatorial complexity and the partial observability inherent in realistic
environments. Decentralized reinforcement learning methods commonly encounter
two substantial difficulties: first, they often yield self-centered behaviors
among agents, resulting in frequent collisions, and second, their reliance on
complex communication modules leads to prolonged training times, sometimes
spanning weeks. To address these challenges, we propose an efficient
decentralized planning framework based on the Decision Transformer (DT),
uniquely leveraging offline reinforcement learning to substantially reduce
training durations from weeks to mere hours. Crucially, our approach
effectively handles long-horizon credit assignment and significantly improves
performance in scenarios with sparse and delayed rewards. Furthermore, to
overcome adaptability limitations inherent in standard RL methods under dynamic
environmental changes, we integrate a large language model (GPT-4o) to
dynamically guide agent policies. Extensive experiments in both static and
dynamically changing environments demonstrate that our DT-based approach,
augmented briefly by GPT-4o, significantly enhances adaptability and
performance.
\\ ( https://arxiv.org/abs/2509.22130 ,  1690kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22216
Date: Fri, 26 Sep 2025 11:29:54 GMT   (7128kb)

Title: Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic
  Dynamics: A Multi-Agent Reinforcement Learning Approach
Authors: Ahmet Onur Akman, Anastasia Psarou, Zolt\'an Gy\"orgy Varga, Grzegorz
  Jamr\'oz, Rafa{\l} Kucharski
Categories: cs.MA cs.AI
Comments: Work presented at the European Workshop on Reinforcement Learning
  (EWRL 2024)
\\
  This study examines the potential impact of reinforcement learning
(RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic
environment. We focus on a simplified day-to-day route choice problem in a
multi-agent setting. We consider a city network where human drivers travel
through their chosen routes to reach their destinations in minimum travel time.
Then, we convert one-third of the population into AVs, which are RL agents
employing Deep Q-learning algorithm. We define a set of optimization targets,
or as we call them behaviors, namely selfish, collaborative, competitive,
social, altruistic, and malicious. We impose a selected behavior on AVs through
their rewards. We run our simulations using our in-house developed RL framework
PARCOUR. Our simulations reveal that AVs optimize their travel times by up to
5\%, with varying impacts on human drivers' travel times depending on the AV
behavior. In all cases where AVs adopt a self-serving behavior, they achieve
shorter travel times than human drivers. Our findings highlight the complexity
differences in learning tasks of each target behavior. We demonstrate that the
multi-agent RL setting is applicable for collective routing on traffic
networks, though their impact on coexisting parties greatly varies with the
behaviors adopted.
\\ ( https://arxiv.org/abs/2509.22216 ,  7128kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22218
Date: Fri, 26 Sep 2025 11:31:00 GMT   (1165kb)

Title: VizGen: Data Exploration and Visualization from Natural Language via a
  Multi-Agent AI Architecture
Authors: Sandaru Fernando, Imasha Jayarathne, Sithumini Abeysekara, Shanuja
  Sithamparanthan, Thushari Silva, Deshan Jayawardana
Categories: cs.MA cs.AI cs.DB
\\
  Data visualization is essential for interpreting complex datasets, yet
traditional tools often require technical expertise, limiting accessibility.
VizGen is an AI-assisted graph generation system that empowers users to create
meaningful visualizations using natural language. Leveraging advanced NLP and
LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries
into SQL and recommends suitable graph types. Built on a multi-agent
architecture, VizGen handles SQL generation, graph creation, customization, and
insight extraction. Beyond visualization, it analyzes data for patterns,
anomalies, and correlations, and enhances user understanding by providing
explanations enriched with contextual information gathered from the internet.
The system supports real-time interaction with SQL databases and allows
conversational graph refinement, making data analysis intuitive and accessible.
VizGen democratizes data visualization by bridging the gap between technical
complexity and user-friendly design.
\\ ( https://arxiv.org/abs/2509.22218 ,  1165kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22596
Date: Fri, 26 Sep 2025 17:16:34 GMT   (416kb)

Title: Effective Policy Learning for Multi-Agent Online Coordination Beyond
  Submodular Objectives
Authors: Qixin Zhang, Yan Sun, Can Jin, Xikun Zhang, Yao Shu, Puning Zhao, Li
  Shen, Dacheng Tao
Categories: cs.MA cs.LG math.OC
Comments: Accepted to NeurIPS 2025
\\
  In this paper, we present two effective policy learning algorithms for
multi-agent online coordination(MA-OC) problem. The first one, \texttt{MA-SPL},
not only can achieve the optimal $(1-\frac{c}{e})$-approximation guarantee for
the MA-OC problem with submodular objectives but also can handle the unexplored
$\alpha$-weakly DR-submodular and $(\gamma,\beta)$-weakly submodular scenarios,
where $c$ is the curvature of the investigated submodular functions, $\alpha$
denotes the diminishing-return(DR) ratio and the tuple $(\gamma,\beta)$
represents the submodularity ratios. Subsequently, in order to reduce the
reliance on the unknown parameters $\alpha,\gamma,\beta$ inherent in the
\texttt{MA-SPL} algorithm, we further introduce the second online algorithm
named \texttt{MA-MPL}. This \texttt{MA-MPL} algorithm is entirely
\emph{parameter-free} and simultaneously can maintain the same approximation
ratio as the first \texttt{MA-SPL} algorithm. The core of our \texttt{MA-SPL}
and \texttt{MA-MPL} algorithms is a novel continuous-relaxation technique
termed as \emph{policy-based continuous extension}. Compared with the
well-established \emph{multi-linear extension}, a notable advantage of this new
\emph{policy-based continuous extension} is its ability to provide a lossless
rounding scheme for any set function, thereby enabling us to tackle the
challenging weakly submodular objectives. Finally, extensive simulations are
conducted to validate the effectiveness of our proposed algorithms.
\\ ( https://arxiv.org/abs/2509.22596 ,  416kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22620
Date: Fri, 26 Sep 2025 17:46:07 GMT   (178kb)

Title: Voting-Bloc Entropy: A New Metric for DAO Decentralization
Authors: Andr\'es F\'abrega, Amy Zhao, Jay Yu, James Austgen, Sarah Allen,
  Kushal Babel, Mahimna Kelkar, Ari Juels
Categories: cs.MA cs.CR
Comments: Full version of the paper published in USENIX Security 2025
\\
  Decentralized Autonomous Organizations (DAOs) use smart contracts to foster
communities working toward common goals. Existing definitions of
decentralization, however -- the 'D' in DAO -- fall short of capturing the key
properties characteristic of diverse and equitable participation. This work
proposes a new framework for measuring DAO decentralization called Voting-Bloc
Entropy (VBE, pronounced ''vibe''). VBE is based on the idea that voters with
closely aligned interests act as a centralizing force and should be modeled as
such. VBE formalizes this notion by measuring the similarity of participants'
utility functions across a set of voting rounds. Unlike prior, ad hoc
definitions of decentralization, VBE derives from first principles: We
introduce a simple (yet powerful) reinforcement learning-based conceptual model
for voting, that in turn implies VBE. We first show VBE's utility as a
theoretical tool. We prove a number of results about the (de)centralizing
effects of vote delegation, proposal bundling, bribery, etc. that are
overlooked in previous notions of DAO decentralization. Our results lead to
practical suggestions for enhancing DAO decentralization. We also show how VBE
can be used empirically by presenting measurement studies and VBE-based
governance experiments. We make the tools we developed for these results
available to the community in the form of open-source artifacts in order to
facilitate future study of DAO decentralization.
\\ ( https://arxiv.org/abs/2509.22620 ,  178kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2509.21324 (*cross-listing*)
Date: Wed, 27 Aug 2025 21:43:03 GMT   (2514kb)

Title: From Search to Reasoning: A Five-Level RAG Capability Framework for
  Enterprise Data
Authors: Gurbinder Gill, Ritvik Gupta, Denis Lusson, Anand Chandrashekar and
  Donald Nguyen
Categories: cs.IR cs.AI
ACM-class: I.2.1; I.2.7; I.2.8
\\
  Retrieval-Augmented Generation (RAG) has emerged as the standard paradigm for
answering questions on enterprise data. Traditionally, RAG has centered on
text-based semantic search and re-ranking. However, this approach falls short
when dealing with questions beyond data summarization or non-text data. This
has led to various attempts to supplement RAG to bridge the gap between RAG,
the implementation paradigm, and the question answering problem that enterprise
users expect it to solve. Given that contemporary RAG is a collection of
techniques rather than a defined implementation, discussion of RAG and related
question-answering systems benefits from a problem-oriented understanding.
  We propose a new classification framework (L1-L5) to categorize systems based
on data modalities and task complexity of the underlying question answering
problems: L1 (Surface Knowledge of Unstructured Data) through L4 (Reflective
and Reasoned Knowledge) and the aspirational L5 (General Intelligence). We also
introduce benchmarks aligned with these levels and evaluate four
state-of-the-art platforms: LangChain, Azure AI Search, OpenAI, and Corvic AI.
Our experiments highlight the value of multi-space retrieval and dynamic
orchestration for enabling L1-L4 capabilities. We empirically validate our
findings using diverse datasets indicative of enterprise use cases.
\\ ( https://arxiv.org/abs/2509.21324 ,  2514kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21325 (*cross-listing*)
Date: Mon, 1 Sep 2025 07:28:35 GMT   (1170kb)

Title: PIR-RAG: A System for Private Information Retrieval in
  Retrieval-Augmented Generation
Authors: Baiqiang Wang, Qian Lou, Mengxin Zheng, Dongfang Zhao
Categories: cs.IR cs.AI cs.CR
\\
  Retrieval-Augmented Generation (RAG) has become a foundational component of
modern AI systems, yet it introduces significant privacy risks by exposing user
queries to service providers. To address this, we introduce PIR-RAG, a
practical system for privacy-preserving RAG. PIR-RAG employs a novel
architecture that uses coarse-grained semantic clustering to prune the search
space, combined with a fast, lattice-based Private Information Retrieval (PIR)
protocol. This design allows for the efficient retrieval of entire document
clusters, uniquely optimizing for the end-to-end RAG workflow where full
document content is required. Our comprehensive evaluation against strong
baseline architectures, including graph-based PIR and Tiptoe-style private
scoring, demonstrates PIR-RAG's scalability and its superior performance in
terms of "RAG-Ready Latency"-the true end-to-end time required to securely
fetch content for an LLM. Our work establishes PIR-RAG as a viable and highly
efficient solution for privacy in large-scale AI systems.
\\ ( https://arxiv.org/abs/2509.21325 ,  1170kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21327 (*cross-listing*)
Date: Fri, 5 Sep 2025 20:20:19 GMT   (17364kb)

Title: Assessment of deep learning models integrated with weather and
  environmental variables for wildfire spread prediction and a case study of
  the 2023 Maui fires
Authors: Jiyeon Kim and Yingjie Hu and Negar Elhami-Khorasani and Kai Sun and
  Ryan Zhenqi Zhou
Categories: physics.soc-ph cs.AI cs.LG
\\
  Predicting the spread of wildfires is essential for effective fire management
and risk assessment. With the fast advancements of artificial intelligence
(AI), various deep learning models have been developed and utilized for
wildfire spread prediction. However, there is limited understanding of the
advantages and limitations of these models, and it is also unclear how deep
learning-based fire spread models can be compared with existing non-AI fire
models. In this work, we assess the ability of five typical deep learning
models integrated with weather and environmental variables for wildfire spread
prediction based on over ten years of wildfire data in the state of Hawaii. We
further use the 2023 Maui fires as a case study to compare the best deep
learning models with a widely-used fire spread model, FARSITE. The results show
that two deep learning models, i.e., ConvLSTM and ConvLSTM with attention,
perform the best among the five tested AI models. FARSITE shows higher
precision, lower recall, and higher F1-score than the best AI models, while the
AI models offer higher flexibility for the input data. By integrating AI models
with an explainable AI method, we further identify important weather and
environmental factors associated with the 2023 Maui wildfires.
\\ ( https://arxiv.org/abs/2509.21327 ,  17364kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21331 (*cross-listing*)
Date: Sun, 7 Sep 2025 14:41:39 GMT   (2324kb)

Title: Seismic Velocity Inversion from Multi-Source Shot Gathers Using Deep
  Segmentation Networks: Benchmarking U-Net Variants and SeismoLabV3+
Authors: Mahedi Hasan
Categories: physics.geo-ph cs.AI cs.LG
\\
  Seismic velocity inversion is a key task in geophysical exploration, enabling
the reconstruction of subsurface structures from seismic wave data. It is
critical for high-resolution seismic imaging and interpretation. Traditional
physics-driven methods, such as Full Waveform Inversion (FWI), are
computationally demanding, sensitive to initialization, and limited by the
bandwidth of seismic data. Recent advances in deep learning have led to
data-driven approaches that treat velocity inversion as a dense prediction
task. This research benchmarks three advanced encoder-decoder architectures --
U-Net, U-Net++, and DeepLabV3+ -- together with SeismoLabV3+, an optimized
variant of DeepLabV3+ with a ResNeXt50 32x4d backbone and task-specific
modifications -- for seismic velocity inversion using the ThinkOnward 2025
Speed \& Structure dataset, which consists of five-channel seismic shot gathers
paired with high-resolution velocity maps. Experimental results show that
SeismoLabV3+ achieves the best performance, with MAPE values of 0.03025 on the
internal validation split and 0.031246 on the hidden test set as scored via the
official ThinkOnward leaderboard. These findings demonstrate the suitability of
deep segmentation networks for seismic velocity inversion and underscore the
value of tailored architectural refinements in advancing geophysical AI models.
\\ ( https://arxiv.org/abs/2509.21331 ,  2324kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21339 (*cross-listing*)
Date: Mon, 15 Sep 2025 08:55:15 GMT   (3009kb)

Title: Cross-Modal Retrieval with Cauchy-Schwarz Divergence
Authors: Jiahao Zhang, Wenzhe Yin, Shujian Yu
Categories: cs.IR cs.AI cs.CV cs.MM
Comments: Accepted by ACMMM-25
\\
  Effective cross-modal retrieval requires robust alignment of heterogeneous
data types. Most existing methods focus on bi-modal retrieval tasks and rely on
distributional alignment techniques such as Kullback-Leibler divergence,
Maximum Mean Discrepancy, and correlation alignment. However, these methods
often suffer from critical limitations, including numerical instability,
sensitivity to hyperparameters, and their inability to capture the full
structure of the underlying distributions. In this paper, we introduce the
Cauchy-Schwarz (CS) divergence, a hyperparameter-free measure that improves
both training stability and retrieval performance. We further propose a novel
Generalized CS (GCS) divergence inspired by H\"older's inequality. This
extension enables direct alignment of three or more modalities within a unified
mathematical framework through a bidirectional circular comparison scheme,
eliminating the need for exhaustive pairwise comparisons. Extensive experiments
on six benchmark datasets demonstrate the effectiveness of our method in both
bi-modal and tri-modal retrieval tasks. The code of our CS/GCS divergence is
publicly available at https://github.com/JiahaoZhang666/CSD.
\\ ( https://arxiv.org/abs/2509.21339 ,  3009kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21340 (*cross-listing*)
Date: Mon, 15 Sep 2025 21:48:30 GMT   (104kb)

Title: Cycle is All You Need: More Is Different
Authors: Xin Li
Categories: cs.NE cs.AI cs.LG q-bio.NC
\\
  We propose an information-topological framework in which cycle closure is the
fundamental mechanism of memory and consciousness. Memory is not a static store
but the ability to re-enter latent cycles in neural state space, with invariant
cycles serving as carriers of meaning by filtering order-specific noise and
preserving what persists across contexts. The dot-cycle dichotomy captures
this: transient dots scaffold exploration, while nontrivial cycles encode
low-entropy content invariants that stabilize memory. Biologically,
polychronous neural groups realize 1-cycles through delay-locked spiking
reinforced by STDP, nested within theta-gamma rhythms that enforce boundary
cancellation. These micro-cycles compose hierarchically, extending navigation
loops into general memory and cognition. The perception-action cycle introduces
high-order invariance: closure holds even across sense-act alternations,
generalizing ancestral homing behavior. Sheaf-cosheaf duality formalizes this
process: sheaves glue perceptual fragments into global sections, cosheaves
decompose global plans into actions and closure aligns top-down predictions
with bottom-up cycles. Consciousness then arises as the persistence of
high-order invariants that integrate (unity) yet differentiate (richness)
across contexts. We conclude that cycle is all you need: persistent invariants
enable generalization in non-ergodic environments with long-term coherence at
minimal energetic cost.
\\ ( https://arxiv.org/abs/2509.21340 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21341 (*cross-listing*)
Date: Tue, 16 Sep 2025 02:17:04 GMT   (7456kb)

Title: From Embeddings to Equations: Genetic-Programming Surrogates for
  Interpretable Transformer Classification
Authors: Mohammad Sadegh Khorshidi, Navid Yazdanjue, Hassan Gharoun, Mohammad
  Reza Nikoo, Fang Chen, Amir H. Gandomi
Categories: cs.NE cs.AI cs.LG
Comments: 20 pages, 8 tables, 7 figures
MSC-class: 68T07
\\
  We study symbolic surrogate modeling of frozen Transformer embeddings to
obtain compact, auditable classifiers with calibrated probabilities. For five
benchmarks (SST2G, 20NG, MNIST, CIFAR10, MSC17), embeddings from ModernBERT,
DINOv2, and SigLIP are partitioned on the training set into disjoint,
information-preserving views via semantic-preserving feature partitioning
(SPFP). A cooperative multi-population genetic program (MEGP) then learns
additive, closed-form logit programs over these views. Across 30 runs per
dataset we report F1, AUC, log-loss, Brier, expected calibration error (ECE),
and symbolic complexity; a canonical model is chosen by a one-standard-error
rule on validation F1 with a parsimony tie-break. Temperature scaling fitted on
validation yields substantial ECE reductions on test. The resulting surrogates
achieve strong discrimination (up to F1 around 0.99 on MNIST, CIFAR10, MSC17;
around 0.95 on SST2G), while 20NG remains most challenging. We provide
reliability diagrams, dimension usage and overlap statistics,
contribution-based importances, and global effect profiles (PDP and ALE),
demonstrating faithful, cross-modal explanations grounded in explicit programs.
\\ ( https://arxiv.org/abs/2509.21341 ,  7456kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21342 (*cross-listing*)
Date: Tue, 16 Sep 2025 08:22:01 GMT   (2461kb)

Title: SGNNBench: A Holistic Evaluation of Spiking Graph Neural Network on
  Large-scale Graph
Authors: Huizhe Zhang, Jintang Li, Yuchang Zhu, Liang Chen and Li Kuang
Categories: cs.NE cs.AI cs.LG
Comments: The code is available at https://github.com/Zhhuizhe/SGNNBench
\\
  Graph Neural Networks (GNNs) are exemplary deep models designed for graph
data. Message passing mechanism enables GNNs to effectively capture graph
topology and push the performance boundaries across various graph tasks.
However, the trend of developing such complex machinery for graph
representation learning has become unsustainable on large-scale graphs. The
computational and time overhead make it imperative to develop more
energy-efficient GNNs to cope with the explosive growth of real-world graphs.
Spiking Graph Neural Networks (SGNNs), which integrate biologically plausible
learning via unique spike-based neurons, have emerged as a promising
energy-efficient alternative. Different layers communicate with sparse and
binary spikes, which facilitates computation and storage of intermediate graph
representations. Despite the proliferation of SGNNs proposed in recent years,
there is no systematic benchmark to explore the basic design principles of
these brain-inspired networks on the graph data. To bridge this gap, we present
SGNNBench to quantify progress in the field of SGNNs. Specifically, SGNNBench
conducts an in-depth investigation of SGNNs from multiple perspectives,
including effectiveness, energy efficiency, and architectural design. We
comprehensively evaluate 9 state-of-the-art SGNNs across 18 datasets. Regarding
efficiency, we empirically compare these baselines w.r.t model size, memory
usage, and theoretical energy consumption to reveal the often-overlooked energy
bottlenecks of SGNNs. Besides, we elaborately investigate the design space of
SGNNs to promote the development of a general SGNN paradigm.
\\ ( https://arxiv.org/abs/2509.21342 ,  2461kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21355 (*cross-listing*)
Date: Sat, 20 Sep 2025 03:32:31 GMT   (1435kb)

Title: Domain-Informed Genetic Superposition Programming: A Case Study on SFRC
  Beams
Authors: Mohammad Sadegh Khorshidi, Navid Yazdanjue, Hassan Gharoun, Mohammad
  Reza Nikoo, Fang Chen, Amir H. Gandomi
Categories: cs.NE cs.AI
Comments: 11 pages, 6 tables, 4 figures
MSC-class: 68T20, 68W50
\\
  This study presents domain-informed genetic superposition programming
(DIGSP), a symbolic regression framework tailored for engineering systems
governed by separable physical mechanisms. DIGSP partitions the input space
into domain-specific feature subsets and evolves independent genetic
programming (GP) populations to model material-specific effects. Early
evolution occurs in isolation, while ensemble fitness promotes inter-population
cooperation. To enable symbolic superposition, an adaptive hierarchical
symbolic abstraction mechanism (AHSAM) is triggered after stagnation across all
populations. AHSAM performs analysis of variance- (ANOVA) based filtering to
identify statistically significant individuals, compresses them into symbolic
constructs, and injects them into all populations through a validation-guided
pruning cycle. The DIGSP is benchmarked against a baseline multi-gene genetic
programming (BGP) model using a dataset of steel fiber-reinforced concrete
(SFRC) beams. Across 30 independent trials with 65% training, 10% validation,
and 25% testing splits, DIGSP consistently outperformed BGP in training and
test root mean squared error (RMSE). The Wilcoxon rank-sum test confirmed
statistical significance (p < 0.01), and DIGSP showed tighter error
distributions and fewer outliers. No significant difference was observed in
validation RMSE due to limited sample size. These results demonstrate that
domain-informed structural decomposition and symbolic abstraction improve
convergence and generalization. DIGSP offers a principled and interpretable
modeling strategy for systems where symbolic superposition aligns with the
underlying physical structure.
\\ ( https://arxiv.org/abs/2509.21355 ,  1435kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21367 (*cross-listing*)
Date: Mon, 22 Sep 2025 11:40:29 GMT   (33kb)

Title: Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart
  Tourism Customer Service: Defending Against Prompt Injection Attacks -- A
  Case Study of Hsinchu, Taiwan
Authors: Yu-Kai Shih, You-Kai Kang
Categories: cs.CR cs.AI
Comments: 12 pages, 7 figures, 5 tables
\\
  As smart tourism evolves, AI-powered chatbots have become indispensable for
delivering personalized, real-time assistance to travelers while promoting
sustainability and efficiency. However, these systems are increasingly
vulnerable to prompt injection attacks, where adversaries manipulate inputs to
elicit unintended behaviors such as leaking sensitive information or generating
harmful content. This paper presents a case study on the design and
implementation of a secure retrieval-augmented generation (RAG) chatbot for
Hsinchu smart tourism services. The system integrates RAG with API function
calls, multi-layered linguistic analysis, and guardrails against injections,
achieving high contextual awareness and security. Key features include a tiered
response strategy, RAG-driven knowledge grounding, and intent decomposition
across lexical, semantic, and pragmatic levels. Defense mechanisms include
system norms, gatekeepers for intent judgment, and reverse RAG text to
prioritize verified data. We also benchmark a GPT-5 variant (released
2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial
prompts and 223 benign queries show over 95% accuracy on benign tasks and
substantial detection of injection attacks. GPT-5 blocked about 85% of attacks,
showing progress yet highlighting the need for layered defenses. Findings
emphasize contributions to sustainable tourism, multilingual accessibility, and
ethical AI deployment. This work offers a practical framework for deploying
secure chatbots in smart tourism and contributes to resilient, trustworthy AI
applications.
\\ ( https://arxiv.org/abs/2509.21367 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21371 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:47:57 GMT   (9047kb)

Title: ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational
  Recommender Systems
Authors: Dayu Yang and Hui Fang
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: Accepted by WISE 2025: 26th International Web Information Systems
  Engineering conference. Our code is publicly available at the link:
  https://github.com/dayuyang1999/ReGeS
\\
  Connecting conversation with external domain knowledge is vital for
conversational recommender systems (CRS) to correctly understand user
preferences. However, existing solutions either require domain-specific
engineering, which limits flexibility, or rely solely on large language models,
which increases the risk of hallucination. While Retrieval-Augmented Generation
(RAG) holds promise, its naive use in CRS is hindered by noisy dialogues that
weaken retrieval and by overlooked nuances among similar items. We propose
ReGeS, a reciprocal Retrieval-Generation Synergy framework that unifies
generation-augmented retrieval to distill informative user intent from
conversations and retrieval-augmented generation to differentiate subtle item
features. This synergy obviates the need for extra annotations, reduces
hallucinations, and simplifies continuous updates. Experiments on multiple CRS
benchmarks show that ReGeS achieves state-of-the-art performance in
recommendation accuracy, demonstrating the effectiveness of reciprocal synergy
for knowledge-intensive CRS tasks.
\\ ( https://arxiv.org/abs/2509.21371 ,  9047kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21381 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:52:11 GMT   (7093kb)

Title: Toward a Realistic Encoding Model of Auditory Affective Understanding in
  the Brain
Authors: Guandong Pan, Yaqian Yang, Shi Chen, Xin Wang, Longzhao Liu, Hongwei
  Zheng, Shaoting Tang
Categories: eess.AS cs.AI cs.HC
\\
  In affective neuroscience and emotion-aware AI, understanding how complex
auditory stimuli drive emotion arousal dynamics remains unresolved. This study
introduces a computational framework to model the brain's encoding of
naturalistic auditory inputs into dynamic behavioral/neural responses across
three datasets (SEED, LIRIS, self-collected BAVE). Guided by neurobiological
principles of parallel auditory hierarchy, we decompose audio into multilevel
auditory features (through classical algorithms and wav2vec 2.0/Hubert) from
the original and isolated human voice/background soundtrack elements, mapping
them to emotion-related responses via cross-dataset analyses. Our analysis
reveals that high-level semantic representations (derived from the final layer
of wav2vec 2.0/Hubert) exert a dominant role in emotion encoding, outperforming
low-level acoustic features with significantly stronger mappings to behavioral
annotations and dynamic neural synchrony across most brain regions ($p <
0.05$). Notably, middle layers of wav2vec 2.0/hubert (balancing
acoustic-semantic information) surpass the final layers in emotion induction
across datasets. Moreover, human voices and soundtracks show dataset-dependent
emotion-evoking biases aligned with stimulus energy distribution (e.g., LIRIS
favors soundtracks due to higher background energy), with neural analyses
indicating voices dominate prefrontal/temporal activity while soundtracks excel
in limbic regions. By integrating affective computing and neuroscience, this
work uncovers hierarchical mechanisms of auditory-emotion encoding, providing a
foundation for adaptive emotion-aware systems and cross-disciplinary
explorations of audio-affective interactions.
\\ ( https://arxiv.org/abs/2509.21381 ,  7093kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21389 (*cross-listing*)
Date: Wed, 24 Sep 2025 01:39:34 GMT   (2660kb)

Title: Towards Adapting Federated & Quantum Machine Learning for Network
  Intrusion Detection: A Survey
Authors: Devashish Chaudhary, Sutharshan Rajasegarar and Shiva Raj Pokhrel
Categories: cs.CR cs.AI
Comments: 34 pages, 16 figures, IEEE Communication Surveys and Tutorials
\\
  This survey explores the integration of Federated Learning (FL) with Network
Intrusion Detection Systems (NIDS), with particular emphasis on deep learning
and quantum machine learning approaches. FL enables collaborative model
training across distributed devices while preserving data privacy-a critical
requirement in network security contexts where sensitive traffic data cannot be
centralized. Our comprehensive analysis systematically examines the full
spectrum of FL architectures, deployment strategies, communication protocols,
and aggregation methods specifically tailored for intrusion detection. We
provide an in-depth investigation of privacy-preserving techniques, model
compression approaches, and attack-specific federated solutions for threats
including DDoS, MITM, and botnet attacks. The survey further delivers a
pioneering exploration of Quantum FL (QFL), discussing quantum feature
encoding, quantum machine learning algorithms, and quantum-specific aggregation
methods that promise exponential speedups for complex pattern recognition in
network traffic. Through rigorous comparative analysis of classical and quantum
approaches, identification of research gaps, and evaluation of real-world
deployments, we outline a concrete roadmap for industrial adoption and future
research directions. This work serves as an authoritative reference for
researchers and practitioners seeking to enhance privacy, efficiency, and
robustness of federated intrusion detection systems in increasingly complex
network environments, while preparing for the quantum-enhanced cybersecurity
landscape of tomorrow.
\\ ( https://arxiv.org/abs/2509.21389 ,  2660kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21391 (*cross-listing*)
Date: Wed, 24 Sep 2025 02:44:57 GMT   (887kb)

Title: MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual
  Graph Understanding and Question Answering
Authors: Lihui Liu, Carl J. Yang
Categories: cs.IR cs.AI
\\
  Large Language Models (LLMs) have achieved impressive performance across a
wide range of applications. However, they often suffer from hallucinations in
knowledge-intensive domains due to their reliance on static pretraining
corpora. To address this limitation, Retrieval-Augmented Generation (RAG)
enhances LLMs by incorporating external knowledge sources during inference.
Among these sources, textual graphs provide structured and semantically rich
information that supports more precise and interpretable reasoning. This has
led to growing interest in graph-based RAG systems. Despite their potential,
most existing approaches rely on a single retriever to identify relevant
subgraphs, which limits their ability to capture the diverse aspects of complex
queries. Moreover, these systems often struggle to accurately judge the
relevance of retrieved content, making them prone to distraction by irrelevant
noise. To address these challenges, in this paper, we propose MIXRAG, a
Mixture-of-Experts Graph-RAG framework that introduces multiple specialized
graph retrievers and a dynamic routing controller to better handle diverse
query intents. Each retriever is trained to focus on a specific aspect of graph
semantics, such as entities, relations, or subgraph topology. A
Mixture-of-Experts module adaptively selects and fuses relevant retrievers
based on the input query. To reduce noise in the retrieved information, we
introduce a query-aware GraphEncoder that carefully analyzes relationships
within the retrieved subgraphs, highlighting the most relevant parts while
down-weighting unnecessary noise. Empirical results demonstrate that our method
achieves state-of-the-art performance and consistently outperforms various
baselines. MIXRAG is effective across a wide range of graph-based tasks in
different domains. The code will be released upon paper acceptance.
\\ ( https://arxiv.org/abs/2509.21391 ,  887kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21423 (*cross-listing*)
Date: Thu, 25 Sep 2025 09:34:24 GMT   (760kb)

Title: Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models
Authors: Ehsan Sharifian, Saber Salehkaleybar, Negar Kiyavash
Categories: stat.ML cs.AI cs.LG
\\
  We study the problem of causal structure learning from a combination of
observational and interventional data generated by a linear non-Gaussian
structural equation model that might contain cycles. Recent results show that
using mere observational data identifies the causal graph only up to a
permutation-equivalence class. We obtain a combinatorial characterization of
this class by showing that each graph in an equivalence class corresponds to a
perfect matching in a bipartite graph. This bipartite representation allows us
to analyze how interventions modify or constrain the matchings. Specifically,
we show that each atomic intervention reveals one edge of the true matching and
eliminates all incompatible causal graphs. Consequently, we formalize the
optimal experiment design task as an adaptive stochastic optimization problem
over the set of equivalence classes with a natural reward function that
quantifies how many graphs are eliminated from the equivalence class by an
intervention. We show that this reward function is adaptive submodular and
provide a greedy policy with a provable near-optimal performance guarantee. A
key technical challenge is to efficiently estimate the reward function without
having to explicitly enumerate all the graphs in the equivalence class. We
propose a sampling-based estimator using random matchings and analyze its bias
and concentration behavior. Our simulation results show that performing a small
number of interventions guided by our stochastic optimization framework
recovers the true underlying causal structure.
\\ ( https://arxiv.org/abs/2509.21423 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21424 (*cross-listing*)
Date: Thu, 25 Sep 2025 09:37:19 GMT   (8711kb)

Title: PhenoMoler: Phenotype-Guided Molecular Optimization via Chemistry Large
  Language Model
Authors: Ran Song, Hui Liu
Categories: physics.chem-ph cs.AI
\\
  Current molecular generative models primarily focus on improving drug-target
binding affinity and specificity, often neglecting the system-level phenotypic
effects elicited by compounds. Transcriptional profiles, as molecule-level
readouts of drug-induced phenotypic shifts, offer a powerful opportunity to
guide molecular design in a phenotype-aware manner. We present PhenoMoler, a
phenotype-guided molecular generation framework that integrates a chemistry
large language model with expression profiles to enable biologically informed
drug design. By conditioning the generation on drug-induced differential
expression signatures, PhenoMoler explicitly links transcriptional responses to
chemical structure. By selectively masking and reconstructing specific
substructures-scaffolds, side chains, or linkers-PhenoMoler supports
fine-grained, controllable molecular optimization. Extensive experiments
demonstrate that PhenoMoler generates chemically valid, novel, and diverse
molecules aligned with desired phenotypic profiles. Compared to FDA-approved
drugs, the generated compounds exhibit comparable or enhanced drug-likeness
(QED), optimized physicochemical properties, and superior binding affinity to
key cancer targets. These findings highlight PhenoMoler's potential for
phenotype-guided and structure-controllable molecular optimization.
\\ ( https://arxiv.org/abs/2509.21424 ,  8711kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21434 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:03:37 GMT   (186kb)

Title: Foundation models for high-energy physics
Authors: Anna Hallin
Categories: hep-ph cs.AI cs.LG hep-ex physics.data-an
Comments: To be submitted to SciPost Physics Proceedings (EuCAIFCon 2025)
\\
  The rise of foundation models -- large, pretrained machine learning models
that can be finetuned to a variety of tasks -- has revolutionized the fields of
natural language processing and computer vision. In high-energy physics, the
question of whether these models can be implemented directly in physics
research, or even built from scratch, tailored for particle physics data, has
generated an increasing amount of attention. This review, which is the first on
the topic of foundation models in high-energy physics, summarizes and discusses
the research that has been published in the field so far.
\\ ( https://arxiv.org/abs/2509.21434 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21447 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:18:35 GMT   (682kb)

Title: ARTI-6: Towards Six-dimensional Articulatory Speech Encoding
Authors: Jihwan Lee, Sean Foley, Thanathai Lertpetchpun, Kevin Huang, Yoonjeong
  Lee, Tiantian Feng, Louis Goldstein, Dani Byrd, Shrikanth Narayanan
Categories: eess.AS cs.AI cs.CL
\\
  We propose ARTI-6, a compact six-dimensional articulatory speech encoding
framework derived from real-time MRI data that captures crucial vocal tract
regions including the velum, tongue root, and larynx. ARTI-6 consists of three
components: (1) a six-dimensional articulatory feature set representing key
regions of the vocal tract; (2) an articulatory inversion model, which predicts
articulatory features from speech acoustics leveraging speech foundation
models, achieving a prediction correlation of 0.87; and (3) an articulatory
synthesis model, which reconstructs intelligible speech directly from
articulatory features, showing that even a low-dimensional representation can
generate natural-sounding speech. Together, ARTI-6 provides an interpretable,
computationally efficient, and physiologically grounded framework for advancing
articulatory inversion, synthesis, and broader speech technology applications.
The source code and speech samples are publicly available.
\\ ( https://arxiv.org/abs/2509.21447 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21463 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:29:25 GMT   (1186kb)

Title: Enhanced Generative Machine Listener
Authors: Vishnu Raj, Gouthaman KV, Shiv Gehlot, Lars Villemoes, Arijit Biswas
Categories: eess.AS cs.AI cs.LG
\\
  We present GMLv2, a reference-based model designed for the prediction of
subjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta
distribution-based loss to model the listener ratings and incorporates
additional neural audio coding (NAC) subjective datasets to extend its
generalization and applicability. Extensive evaluations on diverse testset
demonstrate that proposed GMLv2 consistently outperforms widely used metrics,
such as PEAQ and ViSQOL, both in terms of correlation with subjective scores
and in reliably predicting these scores across diverse content types and codec
configurations. Consequently, GMLv2 offers a scalable and automated framework
for perceptual audio quality evaluation, poised to accelerate research and
development in modern audio coding technologies.
\\ ( https://arxiv.org/abs/2509.21463 ,  1186kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21470 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:36:10 GMT   (1797kb)

Title: Score-based Idempotent Distillation of Diffusion Models
Authors: Shehtab Zaman, Chengyan Liu, Kenneth Chiu
Categories: cs.LG cs.AI
\\
  Idempotent generative networks (IGNs) are a new line of generative models
based on idempotent mapping to a target manifold. IGNs support both single-and
multi-step generation, allowing for a flexible trade-off between computational
cost and sample quality. But similar to Generative Adversarial Networks (GANs),
conventional IGNs require adversarial training and are prone to training
instabilities and mode collapse. Diffusion and score-based models are popular
approaches to generative modeling that iteratively transport samples from one
distribution, usually a Gaussian, to a target data distribution. These models
have gained popularity due to their stable training dynamics and high-fidelity
generation quality. However, this stability and quality come at the cost of
high computational cost, as the data must be transported incrementally along
the entire trajectory. New sampling methods, model distillation, and
consistency models have been developed to reduce the sampling cost and even
perform one-shot sampling from diffusion models. In this work, we unite
diffusion and IGNs by distilling idempotent models from diffusion model scores,
called SIGN. Our proposed method is highly stable and does not require
adversarial losses. We provide a theoretical analysis of our proposed
score-based training methods and empirically show that IGNs can be effectively
distilled from a pre-trained diffusion model, enabling faster inference than
iterative score-based models. SIGNs can perform multi-step sampling, allowing
users to trade off quality for efficiency. These models operate directly on the
source domain; they can project corrupted or alternate distributions back onto
the target manifold, enabling zero-shot editing of inputs. We validate our
models on multiple image datasets, achieving state-of-the-art results for
idempotent models on the CIFAR and CelebA datasets.
\\ ( https://arxiv.org/abs/2509.21470 ,  1797kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21473 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:39:09 GMT   (191kb)

Title: Are Hallucinations Bad Estimations?
Authors: Hude Liu, Jerry Yao-Chieh Hu, Jennifer Yuntong Zhang, Zhao Song, Han
  Liu
Categories: cs.LG cs.AI cs.CL cs.CV stat.ML
Comments: Code is available at https://github.com/MAGICS-LAB/hallucination
\\
  We formalize hallucinations in generative models as failures to link an
estimate to any plausible cause. Under this interpretation, we show that even
loss-minimizing optimal estimators still hallucinate. We confirm this with a
general high probability lower bound on hallucinate rate for generic data
distributions. This reframes hallucination as structural misalignment between
loss minimization and human-acceptable outputs, and hence estimation errors
induced by miscalibration. Experiments on coin aggregation, open-ended QA, and
text-to-image support our theory.
\\ ( https://arxiv.org/abs/2509.21473 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21485 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:45:07 GMT   (3377kb)

Title: Neural Operators for Mathematical Modeling of Transient Fluid Flow in
  Subsurface Reservoir Systems
Authors: Daniil D. Sirota, Sergey A. Khan, Sergey L. Kostikov, Kirill A. Butov
Categories: cs.LG cs.AI physics.flu-dyn physics.geo-ph
Comments: 10 pages, 6 figures
MSC-class: 93A30 (Primary) 68T07, 93-10 (Secondary)
ACM-class: I.6; I.2.6
\\
  This paper presents a method for modeling transient fluid flow in subsurface
reservoir systems based on the developed neural operator architecture
(TFNO-opt). Reservoir systems are complex dynamic objects with distributed
parameters described by systems of partial differential equations (PDEs).
Traditional numerical methods for modeling such systems, despite their high
accuracy, are characterized by significant time costs for performing
calculations, which limits their applicability in control and decision support
problems. The proposed architecture (TFNO-opt) is based on Fourier neural
operators, which allow approximating PDE solutions in infinite-dimensional
functional spaces, providing invariance to discretization and the possibility
of generalization to various implementations of equations. The developed
modifications are aimed at increasing the accuracy and stability of the trained
neural operator, which is especially important for control problems. These
include adjustable internal time resolution of the integral Fourier operator,
tensor decomposition of parameters in the spectral domain, use of the Sobolev
norm in the error function, and separation of approximation errors and
reconstruction of initial conditions for more accurate reproduction of physical
processes. The effectiveness of the proposed improvements is confirmed by
computational experiments. The practical significance is confirmed by
computational experiments using the example of the problem of hydrodynamic
modeling of an underground gas storage (UGS), where the acceleration of
calculations by six orders of magnitude was achieved, compared to traditional
methods. This opens up new opportunities for the effective control of complex
reservoir systems.
\\ ( https://arxiv.org/abs/2509.21485 ,  3377kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21500 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:57:39 GMT   (1428kb)

Title: Chasing the Tail: Effective Rubric-based Reward Modeling for Large
  Language Model Post-Training
Authors: Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra,
  Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, Lifeng Jin
Categories: cs.LG cs.AI
MSC-class: 68T50
ACM-class: I.2
\\
  Reinforcement fine-tuning (RFT) often suffers from \emph{reward
over-optimization}, where a policy model hacks the reward signals to achieve
high scores while producing low-quality outputs. Our theoretical analysis shows
that the key lies in reward misspecification at the high-reward tail: the
inability to reliably distinguish Excellent responses from merely Great ones.
This motivate us to focus on the high-reward region. However, such tail
examples are scarce under the base LLM. While off-policy exemplars (e.g. from
stronger models or rewrites) are easier to obtain, naively training on them
yields a misspecified reward for the policy we aim to align. To address this,
we study rubric-based rewards. By design, rubrics can leverage off-policy
examples while remaining insensitive to their artifacts. To elicit rubrics that
capture the high-reward tail, we highlight the importance of distinguishing
among great and diverse responses, and introduce a workflow to implement this
idea. We empirically demonstrate that rubric-based rewards substantially
mitigate reward over-optimization and deliver effective LLM post-training
improvements. Our code can be accessed at
https://github.com/Jun-Kai-Zhang/rubrics.git .
\\ ( https://arxiv.org/abs/2509.21500 ,  1428kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21502 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:58:06 GMT   (71kb)

Title: New Algorithmic Directions in Optimal Transport and Applications for
  Product Spaces
Authors: Salman Beigi, Omid Etesami, Mohammad Mahmoody, Amir Najafi
Categories: cs.DS cs.AI cs.IT cs.LG math.IT
\\
  We study optimal transport between two high-dimensional distributions
$\mu,\nu$ in $R^n$ from an algorithmic perspective: given $x \sim \mu$, find a
close $y \sim \nu$ in $poly(n)$ time, where $n$ is the dimension of $x,y$.
Thus, running time depends on the dimension rather than the full representation
size of $\mu,\nu$. Our main result is a general algorithm for transporting any
product distribution $\mu$ to any $\nu$ with cost $\Delta + \delta$ under
$\ell_p^p$, where $\Delta$ is the Knothe-Rosenblatt transport cost and $\delta$
is a computational error decreasing with runtime. This requires $\nu$ to be
"sequentially samplable" with bounded average sampling cost, a new but natural
notion.
  We further prove:
  An algorithmic version of Talagrand's inequality for transporting the
standard Gaussian $\Phi^n$ to arbitrary $\nu$ under squared Euclidean cost. For
$\nu = \Phi^n$ conditioned on a set $\mathcal{S}$ of measure $\varepsilon$, we
construct the sequential sampler in expected time $poly(n/\varepsilon)$ using
membership oracle access to $\mathcal{S}$. This yields an algorithmic transport
from $\Phi^n$ to $\Phi^n|\mathcal{S}$ in $poly(n/\varepsilon)$ time and
expected squared distance $O(\log 1/\varepsilon)$, optimal for general
$\mathcal{S}$ of measure $\varepsilon$.
  As corollary, we obtain the first computational concentration result (Etesami
et al. SODA 2020) for Gaussian measure under Euclidean distance with
dimension-independent transportation cost, resolving an open question of
Etesami et al. Specifically, for any $\mathcal{S}$ of Gaussian measure
$\varepsilon$, most $\Phi^n$ samples can be mapped to $\mathcal{S}$ within
distance $O(\sqrt{\log 1/\varepsilon})$ in $poly(n/\varepsilon)$ time.
\\ ( https://arxiv.org/abs/2509.21502 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21513 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:04:41 GMT   (3856kb)

Title: DistillKac: Few-Step Image Generation via Damped Wave Equations
Authors: Weiqiao Han, Chenlin Meng, Christopher D. Manning, Stefano Ermon
Categories: cs.LG cs.AI cs.CV math.PR stat.ML
\\
  We present DistillKac, a fast image generator that uses the damped wave
equation and its stochastic Kac representation to move probability mass at
finite speed. In contrast to diffusion models whose reverse time velocities can
become stiff and implicitly allow unbounded propagation speed, Kac dynamics
enforce finite speed transport and yield globally bounded kinetic energy.
Building on this structure, we introduce classifier-free guidance in velocity
space that preserves square integrability under mild conditions. We then
propose endpoint only distillation that trains a student to match a frozen
teacher over long intervals. We prove a stability result that promotes
supervision at the endpoints to closeness along the entire path. Experiments
demonstrate DistillKac delivers high quality samples with very few function
evaluations while retaining the numerical stability benefits of finite speed
probability flows.
\\ ( https://arxiv.org/abs/2509.21513 ,  3856kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21519 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:08:09 GMT   (349kb)

Title: $\mathbf{Li_2}$: A Framework on Dynamics of Feature Emergence and
  Delayed Generalization
Authors: Yuandong Tian
Categories: cs.LG cs.AI
\\
  While the phenomenon of grokking, i.e., delayed generalization, has been
studied extensively, it remains an open question whether there is a
mathematical framework to characterize what kind of features emerge, how and in
which conditions it happens from training, for complex structured inputs. We
propose a novel framework, named $\mathbf{Li_2}$, that captures three key
stages for the grokking behavior of 2-layer nonlinear networks: (I) Lazy
learning, (II) independent feature learning and (III) interactive feature
learning, characterized by the structure of backpropagated gradient $G_F$
across layers. In (I), $G_F$ is random, and top layer overfits to random hidden
representation. In (II), the gradient of each node (column of $G_F$) only
depends on its own activation, and thus each hidden node learns their
representation independently from $G_F$, which now carries information about
target labels, thanks to weight decay. Interestingly, the independent dynamics
follows exactly the gradient ascent of an energy function $E$, and its local
maxima are precisely the emerging features. We study whether these local-optima
induced features are generalizable, their representation power, and how they
change on sample size, in group arithmetic tasks. Finally, in (III), we
provably show how hidden nodes interact, and how $G_F$ changes to focus on
missing features that need to be learned. Our study sheds lights on roles
played by key hyperparameters such as weight decay, learning rate and sample
sizes in grokking, leads to provable scaling laws of memorization and
generalization, and reveals the underlying cause why recent optimizers such as
Muon can be effective, from the first principles of gradient dynamics. Our
analysis can be extended to multi-layer architectures.
\\ ( https://arxiv.org/abs/2509.21519 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21522 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:09:05 GMT   (223kb)

Title: Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via
  single stage training
Authors: Naisong Zhou, Saisamarth Rajesh Phaye, Milos Cernak, Tijana Stojkovic,
  Andy Pearce, Andrea Cavallaro, Andy Harper
Categories: cs.SD cs.AI
Comments: 5 pages, 2 figures, submitted to ICASSP2026
\\
  Diffusion-based generative models have achieved state-of-the-art performance
for perceptual quality in speech enhancement (SE). However, their iterative
nature requires numerous Neural Function Evaluations (NFEs), posing a challenge
for real-time applications. On the contrary, flow matching offers a more
efficient alternative by learning a direct vector field, enabling high-quality
synthesis in just a few steps using deterministic ordinary differential
equation~(ODE) solvers. We thus introduce Shortcut Flow Matching for Speech
Enhancement (SFMSE), a novel approach that trains a single, step-invariant
model. By conditioning the velocity field on the target time step during a
one-stage training process, SFMSE can perform single, few, or multi-step
denoising without any architectural changes or fine-tuning. Our results
demonstrate that a single-step SFMSE inference achieves a real-time factor
(RTF) of 0.013 on a consumer GPU while delivering perceptual quality comparable
to a strong diffusion baseline requiring 60 NFEs. This work also provides an
empirical analysis of the role of stochasticity in training and inference,
bridging the gap between high-quality generative SE and low-latency
constraints.
\\ ( https://arxiv.org/abs/2509.21522 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21528 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:15:29 GMT   (719kb)

Title: Preemptive Detection and Steering of LLM Misalignment via Latent
  Reachability
Authors: Sathwik Karnik, Somil Bansal
Categories: cs.LG cs.AI
\\
  Large language models (LLMs) are now ubiquitous in everyday tools, raising
urgent safety concerns about their tendency to generate harmful content. The
dominant safety approach -- reinforcement learning from human feedback (RLHF)
-- effectively shapes model behavior during training but offers no safeguards
at inference time, where unsafe continuations may still arise. We propose
BRT-Align, a reachability-based framework that brings control-theoretic safety
tools to LLM inference. BRT-Align models autoregressive generation as a
dynamical system in latent space and learn a safety value function via backward
reachability, estimating the worst-case evolution of a trajectory. This enables
two complementary mechanisms: (1) a runtime monitor that forecasts unsafe
completions several tokens in advance, and (2) a least-restrictive steering
filter that minimally perturbs latent states to redirect generation away from
unsafe regions. Experiments across multiple LLMs and toxicity benchmarks
demonstrate that BRT-Align provides more accurate and earlier detection of
unsafe continuations than baselines. Moreover, for LLM safety alignment,
BRT-Align substantially reduces unsafe generations while preserving sentence
diversity and coherence. Qualitative results further highlight emergent
alignment properties: BRT-Align consistently produces responses that are less
violent, less profane, less offensive, and less politically biased. Together,
these findings demonstrate that reachability analysis provides a principled and
practical foundation for inference-time LLM safety.
\\ ( https://arxiv.org/abs/2509.21528 ,  719kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21542 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:29:36 GMT   (1018kb)

Title: Psychological and behavioural responses in human-agent vs. human-human
  interactions: a systematic review and meta-analysis
Authors: Jianan Zhou, Fleur Corbett, Joori Byun, Talya Porat, Nejra van Zalk
Categories: cs.HC cs.AI
\\
  Interactive intelligent agents are being integrated across society. Despite
achieving human-like capabilities, humans' responses to these agents remain
poorly understood, with research fragmented across disciplines. We conducted a
first systematic synthesis comparing a range of psychological and behavioural
responses in matched human-agent vs. human-human dyadic interactions. A total
of 162 eligible studies (146 contributed to the meta-analysis; 468 effect
sizes) were included in the systematic review and meta-analysis, which
integrated frequentist and Bayesian approaches. Our results indicate that
individuals exhibited less prosocial behaviour and moral engagement when
interacting with agents vs. humans. They attributed less agency and
responsibility to agents, perceiving them as less competent, likeable, and
socially present. In contrast, individuals' social alignment (i.e., alignment
or adaptation of internal states and behaviours with partners), trust in
partners, personal agency, task performance, and interaction experiences were
generally comparable when interacting with agents vs. humans. We observed high
effect-size heterogeneity for many subjective responses (i.e., social
perceptions of partners, subjective trust, and interaction experiences),
suggesting context-dependency of partner effects. By examining the
characteristics of studies, participants, partners, interaction scenarios, and
response measures, we also identified several moderators shaping partner
effects. Overall, functional behaviours and interactive experiences with agents
can resemble those with humans, whereas fundamental social attributions and
moral/prosocial concerns lag in human-agent interactions. Agents are thus
afforded instrumental value on par with humans but lack comparable intrinsic
value, providing practical implications for agent design and regulation.
\\ ( https://arxiv.org/abs/2509.21542 ,  1018kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21617 (*cross-listing*)
Date: Thu, 25 Sep 2025 21:33:40 GMT   (423kb)

Title: LANCE: Low Rank Activation Compression for Efficient On-Device Continual
  Learning
Authors: Marco Paul E. Apolinario, Kaushik Roy
Categories: cs.LG cs.AI cs.NE
Comments: 16 pages, 3 figures
\\
  On-device learning is essential for personalization, privacy, and long-term
adaptation in resource-constrained environments. Achieving this requires
efficient learning, both fine-tuning existing models and continually acquiring
new tasks without catastrophic forgetting. Yet both settings are constrained by
high memory cost of storing activations during backpropagation. Existing
activation compression methods reduce this cost but relying on repeated
low-rank decompositions, introducing computational overhead. Also, such methods
have not been explored for continual learning. We propose LANCE (Low-rank
Activation Compression), a framework that performs one-shot higher-order
Singular Value Decompsoition (SVD) to obtain a reusable low-rank subspace for
activation projection. This eliminates repeated decompositions, reducing both
memory and computation. Moreover, fixed low-rank subspaces further enable
on-device continual learning by allocating tasks to orthogonal subspaces
without storing large task-specific matrices. Experiments show that LANCE
reduces activation storage up to 250$\times$ while maintaining accuracy
comparable to full backpropagation on CIFAR-10/100, Oxford-IIIT Pets,
Flowers102, and CUB-200 datasets. On continual learning benchmarks (Split
CIFAR-100, Split MiniImageNet, 5-Datasets), it achieves performance competitive
with orthogonal gradient projection methods at a fraction of the memory cost.
These results position LANCE as a practical and scalable solution for efficient
fine-tuning and continual learning on edge devices.
\\ ( https://arxiv.org/abs/2509.21617 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21625 (*cross-listing*)
Date: Thu, 25 Sep 2025 21:43:45 GMT   (28346kb)

Title: Guiding Audio Editing with Audio Language Model
Authors: Zitong Lan, Yiduo Hao, Mingmin Zhao
Categories: cs.SD cs.AI cs.LG eess.AS
\\
  Audio editing plays a central role in VR/AR immersion, virtual conferencing,
sound design, and other interactive media. However, recent generative audio
editing models depend on template-like instruction formats and are restricted
to mono-channel audio. These models fail to deal with declarative audio
editing, where the user declares what the desired outcome should be, while
leaving the details of editing operations to the system. We introduce SmartDJ,
a novel framework for stereo audio editing that combines the reasoning
capability of audio language models with the generative power of latent
diffusion. Given a high-level instruction, SmartDJ decomposes it into a
sequence of atomic edit operations, such as adding, removing, or spatially
relocating events. These operations are then executed by a diffusion model
trained to manipulate stereo audio. To support this, we design a data synthesis
pipeline that produces paired examples of high-level instructions, atomic edit
operations, and audios before and after each edit operation. Experiments
demonstrate that SmartDJ achieves superior perceptual quality, spatial realism,
and semantic alignment compared to prior audio editing methods. Demos are
available at https://zitonglan.github.io/project/smartdj/smartdj.html.
\\ ( https://arxiv.org/abs/2509.21625 ,  28346kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21629 (*cross-listing*)
Date: Thu, 25 Sep 2025 21:47:02 GMT   (153kb)

Title: InvBench: Can LLMs Accelerate Program Verification with Invariant
  Synthesis?
Authors: Anjiang Wei, Tarun Suresh, Tianran Sun, Haoze Wu, Ke Wang, Alex Aiken
Categories: cs.PL cs.AI cs.CL cs.LG
\\
  Program verification relies on loop invariants, yet automatically discovering
strong invariants remains a long-standing challenge. We introduce a principled
framework for evaluating LLMs on invariant synthesis. Our approach uses a
verifier-based decision procedure with a formal soundness guarantee and
assesses not only correctness but also the speedup that invariants provide in
verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based
verifiers against the traditional solver UAutomizer. While LLM-based verifiers
represent a promising direction, they do not yet offer a significant advantage
over UAutomizer. Model capability also proves critical, as shown by sharp
differences in speedups across models, and our benchmark remains an open
challenge for current LLMs. Finally, we show that supervised fine-tuning and
Best-of-N sampling can improve performance: fine-tuning on 3589 instances
raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,
and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.
\\ ( https://arxiv.org/abs/2509.21629 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21634 (*cross-listing*)
Date: Thu, 25 Sep 2025 21:49:43 GMT   (733kb)

Title: MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G
  Open RANs
Authors: Prakhar Sharma, Haohuang Wen, Vinod Yegneswaran, Ashish Gehani,
  Phillip Porras, Zhiqiang Lin
Categories: cs.CR cs.AI cs.LG cs.NI
\\
  The evolution toward 6G networks is being accelerated by the Open Radio
Access Network (O-RAN) paradigm -- an open, interoperable architecture that
enables intelligent, modular applications across public telecom and private
enterprise domains. While this openness creates unprecedented opportunities for
innovation, it also expands the attack surface, demanding resilient, low-cost,
and autonomous security solutions. Legacy defenses remain largely reactive,
labor-intensive, and inadequate for the scale and complexity of next-generation
systems. Current O-RAN applications focus mainly on network optimization or
passive threat detection, with limited capability for closed-loop, automated
response.
  To address this critical gap, we present an agentic AI framework for fully
automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM
orchestrates security workflows through a modular multi-agent system powered by
Large Language Models (LLMs). The framework features a Threat Analysis Agent
for real-time data triage, a Threat Classification Agent that uses
Retrieval-Augmented Generation (RAG) to map anomalies to specific
countermeasures, and a Threat Response Agent that safely operationalizes
mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge
bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped
with robust safety guardrails, MobiLLM provides a blueprint for trustworthy
AI-driven network security. Initial evaluations demonstrate that MobiLLM can
effectively identify and orchestrate complex mitigation strategies,
significantly reducing response latency and showcasing the feasibility of
autonomous security operations in 6G.
\\ ( https://arxiv.org/abs/2509.21634 ,  733kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21654 (*cross-listing*)
Date: Thu, 25 Sep 2025 22:16:38 GMT   (135kb)

Title: Limitations on Safe, Trusted, Artificial General Intelligence
Authors: Rina Panigrahy, Vatsal Sharan
Categories: cs.LG cs.AI cs.CC
Comments: 17 pages, 1 figure
\\
  Safety, trust and Artificial General Intelligence (AGI) are aspirational
goals in artificial intelligence (AI) systems, and there are several informal
interpretations of these notions. In this paper, we propose strict,
mathematical definitions of safety, trust, and AGI, and demonstrate a
fundamental incompatibility between them. We define safety of a system as the
property that it never makes any false claims, trust as the assumption that the
system is safe, and AGI as the property of an AI system always matching or
exceeding human capability. Our core finding is that -- for our formal
definitions of these notions -- a safe and trusted AI system cannot be an AGI
system: for such a safe, trusted system there are task instances which are
easily and provably solvable by a human but not by the system. We note that we
consider strict mathematical definitions of safety and trust, and it is
possible for real-world deployments to instead rely on alternate, practical
interpretations of these notions. We show our results for program verification,
planning, and graph reachability. Our proofs draw parallels to G\"odel's
incompleteness theorems and Turing's proof of the undecidability of the halting
problem, and can be regarded as interpretations of G\"odel's and Turing's
results.
\\ ( https://arxiv.org/abs/2509.21654 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21663 (*cross-listing*)
Date: Thu, 25 Sep 2025 22:31:43 GMT   (318kb)

Title: Logic of Hypotheses: from Zero to Full Knowledge in Neurosymbolic
  Integration
Authors: Davide Bizzaro and Alessandro Daniele
Categories: cs.LG cs.AI cs.LO
\\
  Neurosymbolic integration (NeSy) blends neural-network learning with symbolic
reasoning. The field can be split between methods injecting hand-crafted rules
into neural models, and methods inducing symbolic rules from data. We introduce
Logic of Hypotheses (LoH), a novel language that unifies these strands,
enabling the flexible integration of data-driven rule learning with symbolic
priors and expert knowledge. LoH extends propositional logic syntax with a
choice operator, which has learnable parameters and selects a subformula from a
pool of options. Using fuzzy logic, formulas in LoH can be directly compiled
into a differentiable computational graph, so the optimal choices can be
learned via backpropagation. This framework subsumes some existing NeSy models,
while adding the possibility of arbitrary degrees of knowledge specification.
Moreover, the use of Goedel fuzzy logic and the recently developed Goedel trick
yields models that can be discretized to hard Boolean-valued functions without
any loss in performance. We provide experimental analysis on such models,
showing strong results on tabular data and on the Visual Tic-Tac-Toe NeSy task,
while producing interpretable decision rules.
\\ ( https://arxiv.org/abs/2509.21663 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21666 (*cross-listing*)
Date: Thu, 25 Sep 2025 22:35:57 GMT   (28kb)

Title: DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks
Authors: Joshua Salim, Jordan Yu, Xilei Zhao
Categories: cs.LG cs.AI
\\
  While deep learning models excel at predictive tasks, they often overfit due
to their complex structure and large number of parameters, causing them to
memorize training data, including noise, rather than learn patterns that
generalize to new data. To tackle this challenge, this paper proposes a new
regularization method, i.e., Enforcing Domain-Informed Monotonicity in Deep
Neural Networks (DIM), which maintains domain-informed monotonic relationships
in complex deep learning models to further improve predictions. Specifically,
our method enforces monotonicity by penalizing violations relative to a linear
baseline, effectively encouraging the model to follow expected trends while
preserving its predictive power. We formalize this approach through a
comprehensive mathematical framework that establishes a linear reference,
measures deviations from monotonic behavior, and integrates these measurements
into the training objective. We test and validate the proposed methodology
using a real-world ridesourcing dataset from Chicago and a synthetically
created dataset. Experiments across various neural network architectures show
that even modest monotonicity constraints consistently enhance model
performance. DIM enhances the predictive performance of deep neural networks by
applying domain-informed monotonicity constraints to regularize model behavior
and mitigate overfitting
\\ ( https://arxiv.org/abs/2509.21666 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21673 (*cross-listing*)
Date: Thu, 25 Sep 2025 22:41:43 GMT   (3490kb)

Title: SlotFM: A Motion Foundation Model with Slot Attention for Diverse
  Downstream Tasks
Authors: Junyong Park, Oron Levy, Rebecca Adaimi, Asaf Liberman, Gierad Laput,
  Abdelkareem Bedri
Categories: cs.LG cs.AI
\\
  Wearable accelerometers are used for a wide range of applications, such as
gesture recognition, gait analysis, and sports monitoring. Yet most existing
foundation models focus primarily on classifying common daily activities such
as locomotion and exercise, limiting their applicability to the broader range
of tasks that rely on other signal characteristics. We present SlotFM, an
accelerometer foundation model that generalizes across diverse downstream
tasks. SlotFM uses Time-Frequency Slot Attention, an extension of Slot
Attention that processes both time and frequency representations of the raw
signals. It generates multiple small embeddings (slots), each capturing
different signal components, enabling task-specific heads to focus on the most
relevant parts of the data. We also introduce two loss regularizers that
capture local structure and frequency patterns, which improve reconstruction of
fine-grained details and helps the embeddings preserve task-relevant
information. We evaluate SlotFM on 16 classification and regression downstream
tasks that extend beyond standard human activity recognition. It outperforms
existing self-supervised approaches on 13 of these tasks and achieves
comparable results to the best performing approaches on the remaining tasks. On
average, our method yields a 4.5% performance gain, demonstrating strong
generalization for sensing foundation models.
\\ ( https://arxiv.org/abs/2509.21673 ,  3490kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21674 (*cross-listing*)
Date: Thu, 25 Sep 2025 22:48:49 GMT   (619kb)

Title: QueryGym: Step-by-Step Interaction with Relational Databases
Authors: Haritha Ananthakrishanan, Harsha Kokel, Kelsey Sikes, Debarun
  Bhattacharjya, Michael Katz, Shirin Sohrabi, Kavitha Srinivas
Categories: cs.DB cs.AI
\\
  We introduce QueryGym, an interactive environment for building, testing, and
evaluating LLM-based query planning agents. Existing frameworks often tie
agents to specific query language dialects or obscure their reasoning; QueryGym
instead requires agents to construct explicit sequences of relational algebra
operations, ensuring engine-agnostic evaluation and transparent step-by-step
planning. The environment is implemented as a Gymnasium interface that supplies
observations -- including schema details, intermediate results, and execution
feedback -- and receives actions that represent database exploration (e.g.,
previewing tables, sampling column values, retrieving unique values) as well as
relational algebra operations (e.g., filter, project, join). We detail the
motivation and the design of the environment. In the demo, we showcase the
utility of the environment by contrasting it with contemporary LLMs that query
databases. QueryGym serves as a practical testbed for research in error
remediation, transparency, and reinforcement learning for query generation. For
the associated demo, see https://ibm.biz/QueryGym.
\\ ( https://arxiv.org/abs/2509.21674 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21709 (*cross-listing*)
Date: Fri, 26 Sep 2025 00:10:02 GMT   (2824kb)

Title: Optimizing the non-Clifford-count in unitary synthesis using
  Reinforcement Learning
Authors: David Kremer, Ali Javadi-Abhari and Priyanka Mukhopadhyay
Categories: quant-ph cs.AI
\\
  An efficient implementation of unitary operators is important in order to
practically realize the computational advantages claimed by quantum algorithms
over their classical counterparts. In this paper we study the potential of
using reinforcement learning (RL) in order to synthesize quantum circuits,
while optimizing the T-count and CS-count, of unitaries that are exactly
implementable by the Clifford+T and Clifford+CS gate sets, respectively. In
general, the complexity of existing algorithms depend exponentially on the
number of qubits and the non-Clifford-count of unitaries. We have designed our
RL framework to work with channel representation of unitaries, that enables us
to perform matrix operations efficiently, using integers only. We have also
incorporated pruning heuristics and a canonicalization of operators, in order
to reduce the search complexity. As a result, compared to previous works, we
are able to implement significantly larger unitaries, in less time, with much
better success rate and improvement factor. Our results for Clifford+T
synthesis on two qubits achieve close-to-optimal decompositions for up to 100 T
gates, 5 times more than previous RL algorithms and to the best of our
knowledge, the largest instances achieved with any method to date. Our RL
algorithm is able to recover previously-known optimal linear complexity
algorithm for T-count-optimal decomposition of 1 qubit unitaries. For 2-qubit
Clifford+CS unitaries, our algorithm achieves a linear complexity, something
that could only be accomplished by a previous algorithm using $SO(6)$
representation.
\\ ( https://arxiv.org/abs/2509.21709 ,  2824kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21712 (*cross-listing*)
Date: Fri, 26 Sep 2025 00:20:30 GMT   (5768kb)

Title: Not My Agent, Not My Boundary? Elicitation of Personal Privacy
  Boundaries in AI-Delegated Information Sharing
Authors: Bingcan Guo, Eryue Xu, Zhiping Zhang, Tianshi Li
Categories: cs.CR cs.AI
\\
  Aligning AI systems with human privacy preferences requires understanding
individuals' nuanced disclosure behaviors beyond general norms. Yet eliciting
such boundaries remains challenging due to the context-dependent nature of
privacy decisions and the complex trade-offs involved. We present an AI-powered
elicitation approach that probes individuals' privacy boundaries through a
discriminative task. We conducted a between-subjects study that systematically
varied communication roles and delegation conditions, resulting in 1,681
boundary specifications from 169 participants for 61 scenarios. We examined how
these contextual factors and individual differences influence the boundary
specification. Quantitative results show that communication roles influence
individuals' acceptance of detailed and identifiable disclosure, AI delegation
and individuals' need for privacy heighten sensitivity to disclosed
identifiers, and AI delegation results in less consensus across individuals.
Our findings highlight the importance of situating privacy preference
elicitation within real-world data flows. We advocate using nuanced privacy
boundaries as an alignment goal for future AI systems.
\\ ( https://arxiv.org/abs/2509.21712 ,  5768kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21713 (*cross-listing*)
Date: Fri, 26 Sep 2025 00:23:11 GMT   (1362kb)

Title: Developing Strategies to Increase Capacity in AI Education
Authors: Noah Q. Cowit, Sri Yash Tadimalla, Stephanie T. Jones, Mary Lou Maher,
  Tracy Camp, Enrico Pontelli
Categories: cs.CY cs.AI
Comments: This is a 40 page report prepared by the CRA based on 32 virtual
  roundtable discussions with 202 experts committed to developing AI Education
  from varied backgrounds
\\
  Many institutions are currently grappling with teaching artificial
intelligence (AI) in the face of growing demand and relevance in our world. The
Computing Research Association (CRA) has conducted 32 moderated virtual
roundtable discussions of 202 experts committed to improving AI education.
These discussions slot into four focus areas: AI Knowledge Areas and Pedagogy,
Infrastructure Challenges in AI Education, Strategies to Increase Capacity in
AI Education, and AI Education for All. Roundtables were organized around
institution type to consider the particular goals and resources of different AI
education environments. We identified the following high-level community needs
to increase capacity in AI education. A significant digital divide creates
major infrastructure hurdles, especially for smaller and under-resourced
institutions. These challenges manifest as a shortage of faculty with AI
expertise, who also face limited time for reskilling; a lack of computational
infrastructure for students and faculty to develop and test AI models; and
insufficient institutional technical support. Compounding these issues is the
large burden associated with updating curricula and creating new programs. To
address the faculty gap, accessible and continuous professional development is
crucial for faculty to learn about AI and its ethical dimensions. This support
is particularly needed for under-resourced institutions and must extend to
faculty both within and outside of computing programs to ensure all students
have access to AI education. We have compiled and organized a list of resources
that our participant experts mentioned throughout this study. These resources
contribute to a frequent request heard during the roundtables: a central
repository of AI education resources for institutions to freely use across
higher education.
\\ ( https://arxiv.org/abs/2509.21713 ,  1362kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21735 (*cross-listing*)
Date: Fri, 26 Sep 2025 01:02:34 GMT   (12026kb)

Title: Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal
  Graph Deep Learning on Longitudinal Brain Networks
Authors: Houliang Zhou, Rong Zhou, Yangying Liu, Kanhao Zhao, Li Shen, Brian Y.
  Chen, Yu Zhang, Lifang He, and Alzheimer's Disease Neuroimaging Initiative
Categories: cs.LG cs.AI
\\
  Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease
(AD) progression is crucial for timely intervention. However, this task remains
challenging due to the complex dysfunctions in the spatio-temporal
characteristics of underlying brain networks, which are often overlooked by
existing methods. To address these limitations, we develop an interpretable
spatio-temporal graph neural network framework to predict future AD
progression, leveraging dual Stochastic Differential Equations (SDEs) to model
the irregularly-sampled longitudinal functional magnetic resonance imaging
(fMRI) data. We validate our approach on two independent cohorts, including the
Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease
Neuroimaging Initiative (ADNI). Our framework effectively learns sparse
regional and connective importance probabilities, enabling the identification
of key brain circuit abnormalities associated with disease progression.
Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal
lobule as salient regions, with significant disruptions in the ventral
attention, dorsal attention, and default mode networks. These abnormalities
correlate strongly with longitudinal AD-related clinical symptoms. Moreover,
our interpretability strategy reveals both established and novel neural
systems-level and sex-specific biomarkers, offering new insights into the
neurobiological mechanisms underlying AD progression. Our findings highlight
the potential of spatio-temporal graph-based learning for early, individualized
prediction of AD progression, even in the context of irregularly-sampled
longitudinal imaging data.
\\ ( https://arxiv.org/abs/2509.21735 ,  12026kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21737 (*cross-listing*)
Date: Fri, 26 Sep 2025 01:06:58 GMT   (805kb)

Title: POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead
  Optimization
Authors: Ziqing Wang, Yibo Wen, William Pattie, Xiao Luo, Weimin Wu, Jerry
  Yao-Chieh Hu, Abhishek Pandey, Han Liu, Kaize Ding
Categories: cs.LG cs.AI
\\
  Lead optimization in drug discovery requires efficiently navigating vast
chemical space through iterative cycles to enhance molecular properties while
preserving structural similarity to the original lead compound. Despite recent
advances, traditional optimization methods struggle with sample
efficiency-achieving good optimization performance with limited oracle
evaluations. Large Language Models (LLMs) provide a promising approach through
their in-context learning and instruction following capabilities, which align
naturally with these iterative processes. However, existing LLM-based methods
fail to leverage this strength, treating each optimization step independently.
To address this, we present POLO (Preference-guided multi-turn Optimization for
Lead Optimization), which enables LLMs to learn from complete optimization
trajectories rather than isolated steps. At its core, POLO introduces
Preference-Guided Policy Optimization (PGPO), a novel reinforcement learning
algorithm that extracts learning signals at two complementary levels:
trajectory-level optimization reinforces successful strategies, while
turn-level preference learning provides dense comparative feedback by ranking
intermediate molecules within each trajectory. Through this dual-level learning
from intermediate evaluation, POLO achieves superior sample efficiency by fully
exploiting each costly oracle call. Extensive experiments demonstrate that POLO
achieves 84% average success rate on single-property tasks (2.3x better than
baselines) and 50% on multi-property tasks using only 500 oracle evaluations,
significantly advancing the state-of-the-art in sample-efficient molecular
optimization.
\\ ( https://arxiv.org/abs/2509.21737 ,  805kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21742 (*cross-listing*)
Date: Fri, 26 Sep 2025 01:17:05 GMT   (14274kb)

Title: Brain PathoGraph Learning
Authors: Ciyuan Peng, Nguyen Linh Dan Le, Shan Jin, Dexuan Ding, Shuo Yu, Feng
  Xia
Categories: cs.LG cs.AI
\\
  Brain graph learning has demonstrated significant achievements in the fields
of neuroscience and artificial intelligence. However, existing methods struggle
to selectively learn disease-related knowledge, leading to heavy parameters and
computational costs. This challenge diminishes their efficiency, as well as
limits their practicality for real-world clinical applications. To this end, we
propose a lightweight Brain PathoGraph Learning (BrainPoG) model that enables
efficient brain graph learning by pathological pattern filtering and
pathological feature distillation. Specifically, BrainPoG first contains a
filter to extract the pathological pattern formulated by highly
disease-relevant subgraphs, achieving graph pruning and lesion localization. A
PathoGraph is therefore constructed by dropping less disease-relevant subgraphs
from the whole brain graph. Afterwards, a pathological feature distillation
module is designed to reduce disease-irrelevant noise features and enhance
pathological features of each node in the PathoGraph. BrainPoG can exclusively
learn informative disease-related knowledge while avoiding less relevant
information, achieving efficient brain graph learning. Extensive experiments on
four benchmark datasets demonstrate that BrainPoG exhibits superiority in both
model performance and computational efficiency across various brain disease
detection tasks.
\\ ( https://arxiv.org/abs/2509.21742 ,  14274kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21746 (*cross-listing*)
Date: Fri, 26 Sep 2025 01:24:16 GMT   (141kb)

Title: HyperCore: Coreset Selection under Noise via Hypersphere Models
Authors: Brian B. Moser, Arundhati S. Shanbhag, Tobias C. Nauen, Stanislav
  Frolov, Federico Raue, Joachim Folz, Andreas Dengel
Categories: cs.LG cs.AI
\\
  The goal of coreset selection methods is to identify representative subsets
of datasets for efficient model training. Yet, existing methods often ignore
the possibility of annotation errors and require fixed pruning ratios, making
them impractical in real-world settings. We present HyperCore, a robust and
adaptive coreset selection framework designed explicitly for noisy
environments. HyperCore leverages lightweight hypersphere models learned per
class, embedding in-class samples close to a hypersphere center while naturally
segregating out-of-class samples based on their distance. By using Youden's J
statistic, HyperCore can adaptively select pruning thresholds, enabling
automatic, noise-aware data pruning without hyperparameter tuning. Our
experiments reveal that HyperCore consistently surpasses state-of-the-art
coreset selection methods, especially under noisy and low-data regimes.
HyperCore effectively discards mislabeled and ambiguous points, yielding
compact yet highly informative subsets suitable for scalable and noise-free
learning.
\\ ( https://arxiv.org/abs/2509.21746 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21748 (*cross-listing*)
Date: Fri, 26 Sep 2025 01:26:45 GMT   (249kb)

Title: SubZeroCore: A Submodular Approach with Zero Training for Coreset
  Selection
Authors: Brian B. Moser, Tobias C. Nauen, Arundhati S. Shanbhag, Federico Raue,
  Stanislav Frolov, Joachim Folz, Andreas Dengel
Categories: cs.LG cs.AI
\\
  The goal of coreset selection is to identify representative subsets of
datasets for efficient model training. Yet, existing approaches paradoxically
require expensive training-based signals, e.g., gradients, decision boundary
estimates or forgetting counts, computed over the entire dataset prior to
pruning, which undermines their very purpose by requiring training on samples
they aim to avoid. We introduce SubZeroCore, a novel, training-free coreset
selection method that integrates submodular coverage and density into a single,
unified objective. To achieve this, we introduce a sampling strategy based on a
closed-form solution to optimally balance these objectives, guided by a single
hyperparameter that explicitly controls the desired coverage for local density
measures. Despite no training, extensive evaluations show that SubZeroCore
matches training-based baselines and significantly outperforms them at high
pruning rates, while dramatically reducing computational overhead. SubZeroCore
also demonstrates superior robustness to label noise, highlighting its
practical effectiveness and scalability for real-world scenarios.
\\ ( https://arxiv.org/abs/2509.21748 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21761 (*cross-listing*)
Date: Fri, 26 Sep 2025 01:45:25 GMT   (8197kb)

Title: Backdoor Attribution: Elucidating and Controlling Backdoor in Language
  Models
Authors: Miao Yu, Zhenhong Zhou, Moayad Aloqaily, Kun Wang, Biwei Huang,
  Stephen Wang, Yueming Jin, Qingsong Wen
Categories: cs.CR cs.AI
\\
  Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks
through data poisoning, yet the internal mechanisms governing these attacks
remain a black box. Previous research on interpretability for LLM safety tends
to focus on alignment, jailbreak, and hallucination, but overlooks backdoor
mechanisms, making it difficult to understand and fully eliminate the backdoor
threat. In this paper, aiming to bridge this gap, we explore the interpretable
mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a
tripartite causal analysis framework. We first introduce the Backdoor Probe
that proves the existence of learnable backdoor features encoded within the
representations. Building on this insight, we further develop Backdoor
Attention Head Attribution (BAHA), efficiently pinpointing the specific
attention heads responsible for processing these features. Our primary
experiments reveals these heads are relatively sparse; ablating a minimal
\textbf{$\sim$ 3%} of total heads is sufficient to reduce the Attack Success
Rate (ASR) by \textbf{over 90%}. More importantly, we further employ these
findings to construct the Backdoor Vector derived from these attributed heads
as a master controller for the backdoor. Through only \textbf{1-point}
intervention on \textbf{single} representation, the vector can either boost ASR
up to \textbf{$\sim$ 100% ($\uparrow$)} on clean inputs, or completely
neutralize backdoor, suppressing ASR down to \textbf{$\sim$ 0% ($\downarrow$)}
on triggered inputs. In conclusion, our work pioneers the exploration of
mechanistic interpretability in LLM backdoors, demonstrating a powerful method
for backdoor control and revealing actionable insights for the community.
\\ ( https://arxiv.org/abs/2509.21761 ,  8197kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21778 (*cross-listing*)
Date: Fri, 26 Sep 2025 02:30:23 GMT   (1203kb)

Title: Beyond Structure: Invariant Crystal Property Prediction with
  Pseudo-Particle Ray Diffraction
Authors: Bin Cao, Yang Liu, Longhan Zhang, Yifan Wu, Zhixun Li, Yuyu Luo, Hong
  Cheng, Yang Ren, Tong-Yi Zhang
Categories: cond-mat.mtrl-sci cs.AI
\\
  Crystal property prediction, governed by quantum mechanical principles, is
computationally prohibitive to solve exactly for large many-body systems using
traditional density functional theory. While machine learning models have
emerged as efficient approximations for large-scale applications, their
performance is strongly influenced by the choice of atomic representation.
Although modern graph-based approaches have progressively incorporated more
structural information, they often fail to capture long-term atomic
interactions due to finite receptive fields and local encoding schemes. This
limitation leads to distinct crystals being mapped to identical
representations, hindering accurate property prediction. To address this, we
introduce PRDNet that leverages unique reciprocal-space diffraction besides
graph representations. To enhance sensitivity to elemental and environmental
variations, we employ a data-driven pseudo-particle to generate a synthetic
diffraction pattern. PRDNet ensures full invariance to crystallographic
symmetries. Extensive experiments are conducted on Materials Project,
JARVIS-DFT, and MatBench, demonstrating that the proposed model achieves
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2509.21778 ,  1203kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21785 (*cross-listing*)
Date: Fri, 26 Sep 2025 02:42:25 GMT   (193kb)

Title: Unbiased Binning: Fairness-aware Attribute Representation
Authors: Abolfazl Asudeh, Zeinab (Mila) Asoodeh, Bita Asoodeh, Omid Asudeh
Categories: cs.DB cs.AI
\\
  Discretizing raw features into bucketized attribute representations is a
popular step before sharing a dataset. It is, however, evident that this step
can cause significant bias in data and amplify unfairness in downstream tasks.
  In this paper, we address this issue by introducing the unbiased binning
problem that, given an attribute to bucketize, finds its closest discretization
to equal-size binning that satisfies group parity across different buckets.
Defining a small set of boundary candidates, we prove that unbiased binning
must select its boundaries from this set. We then develop an efficient dynamic
programming algorithm on top of the boundary candidates to solve the unbiased
binning problem.
  Finding an unbiased binning may sometimes result in a high price of fairness,
or it may not even exist, especially when group values follow different
distributions. Considering that a small bias in the group ratios may be
tolerable in such settings, we introduce the epsilon-biased binning problem
that bounds the group disparities across buckets to a small value epsilon. We
first develop a dynamic programming solution, DP, that finds the optimal
binning in quadratic time. The DP algorithm, while polynomial, does not scale
to very large settings. Therefore, we propose a practically scalable algorithm,
based on local search (LS), for epsilon-biased binning. The key component of
the LS algorithm is a divide-and-conquer (D&C) algorithm that finds a
near-optimal solution for the problem in near-linear time. We prove that D&C
finds a valid solution for the problem unless none exists. The LS algorithm
then initiates a local search, using the D&C solution as the upper bound, to
find the optimal solution.
\\ ( https://arxiv.org/abs/2509.21785 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21792 (*cross-listing*)
Date: Fri, 26 Sep 2025 02:48:41 GMT   (434kb)

Title: FastGRPO: Accelerating Policy Optimization via Concurrency-aware
  Speculative Decoding and Online Draft Learning
Authors: Yizhou Zhang, Ning Lv, Teng Wang, Jisheng Dang
Categories: cs.LG cs.AI
Comments: Submitted to ICLR 2026
ACM-class: I.2.6; I.2.7
\\
  Group relative policy optimization (GRPO) has demonstrated significant
potential in improving the reasoning capabilities of large language models
(LLMs) via reinforcement learning. However, its practical deployment is impeded
by an excessively slow training process, primarily attributed to the
computationally intensive autoregressive generation of multiple responses per
query, which makes the generation phase the primary performance bottleneck.
Although speculative decoding presents a promising direction for acceleration,
its direct application in GRPO achieves limited speedup under high-concurrency
training conditions. To overcome this limitation, we propose a
concurrency-aware speculative decoding framework that dynamically adjusts the
drafting and verification strategy according to real-time concurrency levels,
thereby maximizing the acceleration of the generation process. Furthermore, to
address performance degradation arising from distributional drift between the
evolving target model and the fixed draft model during training, we introduce
an online draft learning mechanism that enables the draft model to continuously
adapt using feedback signals from the target model. Experimental results across
multiple mathematical reasoning datasets and models demonstrate that the
proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly
surpassing baseline approaches in efficiency. The code is available at
https://github.com/yedaotian9/GRPO_speculative.
\\ ( https://arxiv.org/abs/2509.21792 ,  434kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21802 (*cross-listing*)
Date: Fri, 26 Sep 2025 02:59:12 GMT   (2617kb)

Title: ChaosNexus: A Foundation Model for Universal Chaotic System Forecasting
  with Multi-scale Representations
Authors: Chang Liu, Bohao Zhao, Jingtao Ding, Yong Li
Categories: cs.LG cs.AI
\\
  Accurately forecasting chaotic systems, prevalent in domains such as weather
prediction and fluid dynamics, remains a significant scientific challenge. The
inherent sensitivity of these systems to initial conditions, coupled with a
scarcity of observational data, severely constrains traditional modeling
approaches. Since these models are typically trained for a specific system,
they lack the generalization capacity necessary for real-world applications,
which demand robust zero-shot or few-shot forecasting on novel or data-limited
scenarios. To overcome this generalization barrier, we propose ChaosNexus, a
foundation model pre-trained on a diverse corpus of chaotic dynamics.
ChaosNexus employs a novel multi-scale architecture named ScaleFormer augmented
with Mixture-of-Experts layers, to capture both universal patterns and
system-specific behaviors. The model demonstrates state-of-the-art zero-shot
generalization across both synthetic and real-world benchmarks. On a
large-scale testbed comprising over 9,000 synthetic chaotic systems, it
improves the fidelity of long-term attractor statistics by more than 40%
compared to the leading baseline. This robust performance extends to real-world
applications with exceptional data efficiency. For instance, in 5-day global
weather forecasting, ChaosNexus achieves a competitive zero-shot mean error
below 1 degree, a result that further improves with few-shot fine-tuning.
Moreover, experiments on the scaling behavior of ChaosNexus provide a guiding
principle for scientific foundation models: cross-system generalization stems
from the diversity of training systems, rather than sheer data volume.
\\ ( https://arxiv.org/abs/2509.21802 ,  2617kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21840 (*cross-listing*)
Date: Fri, 26 Sep 2025 04:01:48 GMT   (172kb)

Title: Can Large Language Models Autoformalize Kinematics?
Authors: Aditi Kabra, Jonathan Laurent, Sagar Bharadwaj, Ruben Martins, Stefan
  Mitsch, Andr\'e Platzer
Categories: cs.LO cs.AI
\\
  Autonomous cyber-physical systems like robots and self-driving cars could
greatly benefit from using formal methods to reason reliably about their
control decisions. However, before a problem can be solved it needs to be
stated. This requires writing a formal physics model of the cyber-physical
system, which is a complex task that traditionally requires human expertise and
becomes a bottleneck.
  This paper experimentally studies whether Large Language Models (LLMs) can
automate the formalization process. A 20 problem benchmark suite is designed
drawing from undergraduate level physics kinematics problems. In each problem,
the LLM is provided with a natural language description of the objects' motion
and must produce a model in differential game logic (dGL). The model is (1)
syntax checked and iteratively refined based on parser feedback, and (2)
semantically evaluated by checking whether symbolically executing the dGL
formula recovers the solution to the original physics problem. A success rate
of 70% (best over 5 samples) is achieved. We analyze failing cases, identifying
directions for future improvement. This provides a first quantitative baseline
for LLM-based autoformalization from natural language to a hybrid games logic
with continuous dynamics.
\\ ( https://arxiv.org/abs/2509.21840 ,  172kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21847 (*cross-listing*)
Date: Fri, 26 Sep 2025 04:15:29 GMT   (2402kb)

Title: Beyond Johnson-Lindenstrauss: Uniform Bounds for Sketched Bilinear Forms
Authors: Rohan Deb, Qiaobo Li, Mayank Shrivastava, Arindam Banerjee
Categories: cs.LG cs.AI stat.ML
\\
  Uniform bounds on sketched inner products of vectors or matrices underpin
several important computational and statistical results in machine learning and
randomized algorithms, including the Johnson-Lindenstrauss (J-L) lemma, the
Restricted Isometry Property (RIP), randomized sketching, and approximate
linear algebra. However, many modern analyses involve *sketched bilinear
forms*, for which existing uniform bounds either do not apply or are not sharp
on general sets. In this work, we develop a general framework to analyze such
sketched bilinear forms and derive uniform bounds in terms of geometric
complexities of the associated sets. Our approach relies on generic chaining
and introduces new techniques for handling suprema over pairs of sets. We
further extend these results to the setting where the bilinear form involves a
sum of $T$ independent sketching matrices and show that the deviation scales as
$\sqrt{T}$. This unified analysis recovers known results such as the J-L lemma
as special cases, while extending RIP-type guarantees. Additionally, we obtain
improved convergence bounds for sketched Federated Learning algorithms where
such cross terms arise naturally due to sketched gradient compression, and
design sketched variants of bandit algorithms with sharper regret bounds that
depend on the geometric complexity of the action and parameter sets, rather
than the ambient dimension.
\\ ( https://arxiv.org/abs/2509.21847 ,  2402kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21848 (*cross-listing*)
Date: Fri, 26 Sep 2025 04:15:40 GMT   (720kb)

Title: Graph of Agents: Principled Long Context Modeling by Emergent
  Multi-Agent Collaboration
Authors: Taejong Joo, Shu Ishida, Ivan Sosnovik, Bryan Lim, Sahand
  Rezaei-Shoshtari, Adam Gaier, Robert Giaquinto
Categories: cs.LG cs.AI
Comments: Preprint
\\
  As a model-agnostic approach to long context modeling, multi-agent systems
can process inputs longer than a large language model's context window without
retraining or architectural modifications. However, their performance often
heavily relies on hand-crafted multi-agent collaboration strategies and prompt
engineering, which limit generalizability. In this work, we introduce a
principled framework that formalizes the model-agnostic long context modeling
problem as a compression problem, yielding an information-theoretic compression
objective. Building on this framework, we propose Graph of Agents (GoA), which
dynamically constructs an input-dependent collaboration structure that
maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document
question answering benchmarks, GoA improves the average $F_1$ score of
retrieval-augmented generation by 5.7\% and a strong multi-agent baseline using
a fixed collaboration structure by 16.35\%, respectively. Even with only a 2K
context window, GoA surpasses the 128K context window Llama 3.1 8B on
LongBench, showing a dramatic increase in effective context length. Our source
code is available at https://github.com/tjoo512/graph-of-agents.
\\ ( https://arxiv.org/abs/2509.21848 ,  720kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21882 (*cross-listing*)
Date: Fri, 26 Sep 2025 05:06:25 GMT   (1756kb)

Title: Position: The Hidden Costs and Measurement Gaps of Reinforcement
  Learning with Verifiable Rewards
Authors: Aaron Tu, Weihao Xuan, Heli Qi, Xu Huang, Qingcheng Zeng, Shayan
  Talaei, Yijia Xiao, Peng Xia, Xiangru Tang, Yuchen Zhuang, Bing Hu, Hanqun
  Cao, Wenqi Shi, Tianang Leng, Rui Yang, Yingjian Chen, Ziqi Wang, Irene Li,
  Nan Liu, Huaxiu Yao, Li Erran Li, Ge Liu, Amin Saberi, Naoto Yokoya, Jure
  Leskovec, Yejin Choi, Fang Wu
Categories: cs.LG cs.AI
\\
  Reinforcement learning with verifiable rewards (RLVR) is a practical and
scalable approach to enhancing large language models in areas such as math,
code, and other structured tasks. Two questions motivate this paper: how much
of the reported gains survive under strictly parity-controlled evaluation, and
whether RLVR is cost-free or exacts a measurable tax. We argue that progress is
real, but gains are often overstated due to three forces - an RLVR tax,
evaluation pitfalls, and data contamination. Using a partial-prompt
contamination audit and matched-budget reproductions across base and RL models,
we show that several headline gaps shrink or vanish under clean,
parity-controlled evaluation. We then propose a tax-aware training and
evaluation protocol that co-optimizes accuracy, grounding, and calibrated
abstention and standardizes budgeting and provenance checks. Applied to recent
RLVR setups, this protocol yields more reliable estimates of reasoning gains
and, in several cases, revises prior conclusions. Our position is constructive:
RLVR is valuable and industry-ready; we advocate keeping its practical benefits
while prioritizing reliability, safety, and measurement.
\\ ( https://arxiv.org/abs/2509.21882 ,  1756kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21884 (*cross-listing*)
Date: Fri, 26 Sep 2025 05:17:38 GMT   (497kb)

Title: You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System
  Vectors
Authors: Bochuan Cao, Changjiang Li, Yuanpu Cao, Yameng Ge, Ting Wang, Jinghui
  Chen
Categories: cs.CR cs.AI cs.CL
Comments: 29 pages, 10 tables, 6figures, accepted by CCS 25
\\
  Large language models (LLMs) have been widely adopted across various
applications, leveraging customized system prompts for diverse tasks. Facing
potential system prompt leakage risks, model developers have implemented
strategies to prevent leakage, primarily by disabling LLMs from repeating their
context when encountering known attack patterns. However, it remains vulnerable
to new and unforeseen prompt-leaking techniques. In this paper, we first
introduce a simple yet effective prompt leaking attack to reveal such risks.
Our attack is capable of extracting system prompts from various LLM-based
application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our
findings further inspire us to search for a fundamental solution to the
problems by having no system prompt in the context. To this end, we propose
SysVec, a novel method that encodes system prompts as internal representation
vectors rather than raw text. By doing so, SysVec minimizes the risk of
unauthorized disclosure while preserving the LLM's core language capabilities.
Remarkably, this approach not only enhances security but also improves the
model's general instruction-following abilities. Experimental results
demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves
the LLM's functional integrity, and helps alleviate the forgetting issue in
long-context scenarios.
\\ ( https://arxiv.org/abs/2509.21884 ,  497kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21913 (*cross-listing*)
Date: Fri, 26 Sep 2025 05:51:59 GMT   (329kb)

Title: EqDiff-CT: Equivariant Conditional Diffusion model for CT Image
  Synthesis from CBCT
Authors: Alzahra Altalib, Chunhui Li, Alessandro Perelli
Categories: physics.med-ph cs.AI
Comments: 12 pages, 8 figures, 3 tables, submitted to IEEE Transactions on
  Radiation and Plasma Medical Sciences
MSC-class: 68T07
ACM-class: J.2
\\
  Cone-beam computed tomography (CBCT) is widely used for image-guided
radiotherapy (IGRT). It provides real time visualization at low cost and dose.
However, photon scattering and beam hindrance cause artifacts in CBCT. These
include inaccurate Hounsfield Units (HU), reducing reliability for dose
calculation, and adaptive planning. By contrast, computed tomography (CT)
offers better image quality and accurate HU calibration but is usually acquired
offline and fails to capture intra-treatment anatomical changes. Thus, accurate
CBCT-to-CT synthesis is needed to close the imaging-quality gap in adaptive
radiotherapy workflows.
  To cater to this, we propose a novel diffusion-based conditional generative
model, coined EqDiff-CT, to synthesize high-quality CT images from CBCT.
EqDiff-CT employs a denoising diffusion probabilistic model (DDPM) to
iteratively inject noise and learn latent representations that enable
reconstruction of anatomically consistent CT images. A group-equivariant
conditional U-Net backbone, implemented with e2cnn steerable layers, enforces
rotational equivariance (cyclic C4 symmetry), helping preserve fine structural
details while minimizing noise and artifacts.
  The system was trained and validated on the SynthRAD2025 dataset, comprising
CBCT-CT scans across multiple head-and-neck anatomical sites, and we compared
it with advanced methods such as CycleGAN and DDPM. EqDiff-CT provided
substantial gains in structural fidelity, HU accuracy and quantitative metrics.
Visual findings further confirm the improved recovery, sharper soft tissue
boundaries, and realistic bone reconstructions. The findings suggest that the
diffusion model has offered a robust and generalizable framework for CBCT
improvements. The proposed solution helps in improving the image quality as
well as the clinical confidence in the CBCT-guided treatment planning and dose
calculations.
\\ ( https://arxiv.org/abs/2509.21913 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21925 (*cross-listing*)
Date: Fri, 26 Sep 2025 06:13:03 GMT   (2330kb)

Title: Generation Properties of Stochastic Interpolation under Finite Training
  Set
Authors: Yunchen Li, Shaohui Lin, Zhou Yu
Categories: cs.LG cs.AI
\\
  This paper investigates the theoretical behavior of generative models under
finite training populations. Within the stochastic interpolation generative
framework, we derive closed-form expressions for the optimal velocity field and
score function when only a finite number of training samples are available. We
demonstrate that, under some regularity conditions, the deterministic
generative process exactly recovers the training samples, while the stochastic
generative process manifests as training samples with added Gaussian noise.
Beyond the idealized setting, we consider model estimation errors and introduce
formal definitions of underfitting and overfitting specific to generative
models. Our theoretical analysis reveals that, in the presence of estimation
errors, the stochastic generation process effectively produces convex
combinations of training samples corrupted by a mixture of uniform and Gaussian
noise. Experiments on generation tasks and downstream tasks such as
classification support our theory.
\\ ( https://arxiv.org/abs/2509.21925 ,  2330kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21928 (*cross-listing*)
Date: Fri, 26 Sep 2025 06:14:55 GMT   (5035kb)

Title: SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon
  Manipulation Tasks
Authors: Jialiang Li, Wenzheng Wu, Gaojing Zhang, Yifan Han and Wenzhao Lian
Categories: cs.RO cs.AI
\\
  Successfully solving long-horizon manipulation tasks remains a fundamental
challenge. These tasks involve extended action sequences and complex object
interactions, presenting a critical gap between high-level symbolic planning
and low-level continuous control. To bridge this gap, two essential
capabilities are required: robust long-horizon task planning and effective
goal-conditioned manipulation. Existing task planning methods, including
traditional and LLM-based approaches, often exhibit limited generalization or
sparse semantic reasoning. Meanwhile, image-conditioned control methods
struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a
novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon
Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural
representation for scene states. A structural scene graph enables bridging
task-level semantic reasoning and pixel-level visuo-motor control. This also
facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE
consists of two key components: (1) a scene graph-based task planner that uses
VLMs and LLMs to parse the environment and reason about physically-grounded
scene state transition sequences, and (2) a decoupled structural image editing
pipeline that controllably converts each target sub-goal graph into a
corresponding image through image inpainting and composition. Extensive
experiments have demonstrated that SAGE achieves state-of-the-art performance
on distinct long-horizon tasks.
\\ ( https://arxiv.org/abs/2509.21928 ,  5035kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21945 (*cross-listing*)
Date: Fri, 26 Sep 2025 06:25:19 GMT   (1226kb)

Title: Unveiling Many Faces of Surrogate Models for Configuration Tuning: A
  Fitness Landscape Analysis Perspective
Authors: Pengzhou Chen and Hongyuan Liang and Tao Chen
Categories: cs.SE cs.AI
Comments: This paper is under review
\\
  To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.
\\ ( https://arxiv.org/abs/2509.21945 ,  1226kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21947 (*cross-listing*)
Date: Fri, 26 Sep 2025 06:27:00 GMT   (1243kb)

Title: Active Attacks: Red-teaming LLMs via Adaptive Environments
Authors: Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park, Yoshua Bengio,
  Minsu Kim
Categories: cs.LG cs.AI
Comments: 22 pages, 7 figures, 18 tables
\\
  We address the challenge of generating diverse attack prompts for large
language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual
content) and are used for safety fine-tuning. Rather than relying on manual
prompt engineering, attacker LLMs can be trained with reinforcement learning
(RL) to automatically generate such prompts using only a toxicity classifier as
a reward. However, capturing a wide range of harmful behaviors is a significant
challenge that requires explicit diversity objectives. Existing
diversity-seeking RL methods often collapse to limited modes: once high-reward
prompts are found, exploration of new regions is discouraged. Inspired by the
active learning paradigm that encourages adaptive exploration, we introduce
\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its
attacks as the victim evolves. By periodically safety fine-tuning the victim
LLM with collected attack prompts, rewards in exploited regions diminish, which
forces the attacker to seek unexplored vulnerabilities. This process naturally
induces an easy-to-hard exploration curriculum, where the attacker progresses
beyond easy modes toward increasingly difficult ones. As a result, Active
Attacks uncovers a wide range of local attack modes step by step, and their
combination achieves wide coverage of the multi-mode distribution. Active
Attacks, a simple plug-and-play module that seamlessly integrates into existing
RL objectives, unexpectedly outperformed prior RL-based methods -- including
GFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates
against GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a
relative gain greater than $400\ \times$) with only a 6% increase in
computation. Our code is publicly available
\href{https://github.com/dbsxodud-11/active_attacks}{here}.
\\ ( https://arxiv.org/abs/2509.21947 ,  1243kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21961 (*cross-listing*)
Date: Fri, 26 Sep 2025 06:49:22 GMT   (2256kb)

Title: FlowDrive: moderated flow matching with data balancing for trajectory
  planning
Authors: Lingguang Wang, \"Omer \c{S}ahin Ta\c{s}, Marlon Steiner, Christoph
  Stiller
Categories: cs.RO cs.AI cs.LG
\\
  Learning-based planners are sensitive to the long-tailed distribution of
driving data. Common maneuvers dominate datasets, while dangerous or rare
scenarios are sparse. This imbalance can bias models toward the frequent cases
and degrade performance on critical scenarios. To tackle this problem, we
compare balancing strategies for sampling training data and find reweighting by
trajectory pattern an effective approach. We then present FlowDrive, a
flow-matching trajectory planner that learns a conditional rectified flow to
map noise directly to trajectory distributions with few flow-matching steps. We
further introduce moderated, in-the-loop guidance that injects small
perturbation between flow steps to systematically increase trajectory diversity
while remaining scene-consistent. On nuPlan and the interaction-focused
interPlan benchmarks, FlowDrive achieves state-of-the-art results among
learning-based planners and approaches methods with rule-based refinements.
After adding moderated guidance and light post-processing (FlowDrive*), it
achieves overall state-of-the-art performance across nearly all benchmark
splits.
\\ ( https://arxiv.org/abs/2509.21961 ,  2256kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21972 (*cross-listing*)
Date: Fri, 26 Sep 2025 06:59:36 GMT   (998kb)

Title: From Superficial Outputs to Superficial Learning: Risks of Large
  Language Models in Education
Authors: Iris Delikoura, Yi.R (May) Fung, Pan Hui
Categories: cs.CY cs.AI
\\
  Large Language Models (LLMs) are transforming education by enabling
personalization, feedback, and knowledge access, while also raising concerns
about risks to students and learning systems. Yet empirical evidence on these
risks remains fragmented. This paper presents a systematic review of 70
empirical studies across computer science, education, and psychology. Guided by
four research questions, we examine: (i) which applications of LLMs in
education have been most frequently explored; (ii) how researchers have
measured their impact; (iii) which risks stem from such applications; and (iv)
what mitigation strategies have been proposed. We find that research on LLMs
clusters around three domains: operational effectiveness, personalized
applications, and interactive learning tools. Across these, model-level risks
include superficial understanding, bias, limited robustness, anthropomorphism,
hallucinations, privacy concerns, and knowledge constraints. When learners
interact with LLMs, these risks extend to cognitive and behavioural outcomes,
including reduced neural activity, over-reliance, diminished independent
learning skills, and a loss of student agency. To capture this progression, we
propose an LLM-Risk Adapted Learning Model that illustrates how technical risks
cascade through interaction and interpretation to shape educational outcomes.
As the first synthesis of empirically assessed risks, this review provides a
foundation for responsible, human-centred integration of LLMs in education.
\\ ( https://arxiv.org/abs/2509.21972 ,  998kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21983 (*cross-listing*)
Date: Fri, 26 Sep 2025 07:06:26 GMT   (41267kb)

Title: Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning
Authors: Sigmund Hennum H{\o}eg, Aksel Vaaler, Chaoqi Liu, Olav Egeland, and
  Yilun Du
Categories: cs.RO cs.AI
Comments: 10 pages, 11 figures. This work has been submitted to the IEEE for
  possible publication. See https://sigmundhh.com/hybrid_diffusion/ for the
  project website
\\
  Constructing robots to accomplish long-horizon tasks is a long-standing
challenge within artificial intelligence. Approaches using generative methods,
particularly Diffusion Models, have gained attention due to their ability to
model continuous robotic trajectories for planning and control. However, we
show that these models struggle with long-horizon tasks that involve complex
decision-making and, in general, are prone to confusing different modes of
behavior, leading to failure. To remedy this, we propose to augment continuous
trajectory generation by simultaneously generating a high-level symbolic plan.
We show that this requires a novel mix of discrete variable diffusion and
continuous diffusion, which dramatically outperforms the baselines. In
addition, we illustrate how this hybrid diffusion process enables flexible
trajectory synthesis, allowing us to condition synthesized actions on partial
and complete symbolic conditions.
\\ ( https://arxiv.org/abs/2509.21983 ,  41267kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21986 (*cross-listing*)
Date: Fri, 26 Sep 2025 07:09:33 GMT   (5111kb)

Title: Developing Vision-Language-Action Model from Egocentric Videos
Authors: Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori
Categories: cs.RO cs.AI
\\
  Egocentric videos capture how humans manipulate objects and tools, providing
diverse motion cues for learning object manipulation. Unlike the costly,
expert-driven manual teleoperation commonly used in training
Vision-Language-Action models (VLAs), egocentric videos offer a scalable
alternative. However, prior studies that leverage such videos for training
robot policies typically rely on auxiliary annotations, such as detailed
hand-pose recordings. Consequently, it remains unclear whether VLAs can be
trained directly from raw egocentric videos. In this work, we address this
challenge by leveraging EgoScaler, a framework that extracts 6DoF object
manipulation trajectories from egocentric videos without requiring auxiliary
recordings. We apply EgoScaler to four large-scale egocentric video datasets
and automatically refine noisy or incomplete trajectories, thereby constructing
a new large-scale dataset for VLA pre-training. Our experiments with a
state-of-the-art $\pi_0$ architecture in both simulated and real-robot
environments yield three key findings: (i) pre-training on our dataset improves
task success rates by over 20\% compared to training from scratch, (ii) the
performance is competitive with that achieved using real-robot datasets, and
(iii) combining our dataset with real-robot data yields further improvements.
These results demonstrate that egocentric videos constitute a promising and
scalable resource for advancing VLA research.
\\ ( https://arxiv.org/abs/2509.21986 ,  5111kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22038 (*cross-listing*)
Date: Fri, 26 Sep 2025 08:15:58 GMT   (6658kb)

Title: Latent Diffusion : Multi-Dimension Stable Diffusion Latent Space
  Explorer
Authors: Zhihua Zhong and Xuanyang Huang
Categories: cs.LG cs.AI
\\
  Latent space is one of the key concepts in generative AI, offering powerful
means for creative exploration through vector manipulation. However, diffusion
models like Stable Diffusion lack the intuitive latent vector control found in
GANs, limiting their flexibility for artistic expression. This paper introduces
\workname, a framework for integrating customizable latent space operations
into the diffusion process. By enabling direct manipulation of conceptual and
spatial representations, this approach expands creative possibilities in
generative art. We demonstrate the potential of this framework through two
artworks, \textit{Infinitepedia} and \textit{Latent Motion}, highlighting its
use in conceptual blending and dynamic motion generation. Our findings reveal
latent space structures with semantic and meaningless regions, offering
insights into the geometry of diffusion models and paving the way for further
explorations of latent space.
\\ ( https://arxiv.org/abs/2509.22038 ,  6658kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22058 (*cross-listing*)
Date: Fri, 26 Sep 2025 08:40:53 GMT   (2722kb)

Title: An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose
Authors: Qifeng Wang, Weigang Li, Lei Nie, Xin Xu, Wenping Liu, Zhe Xu
Categories: cs.RO cs.AI
DOI: 10.1109/TIM.2025.3571148
\\
  As a key technology for autonomous navigation and positioning in mobile
robots, light detection and ranging (LiDAR) odometry is widely used in
autonomous driving applications. The Iterative Closest Point (ICP)-based
methods have become the core technique in LiDAR odometry due to their efficient
and accurate point cloud registration capability. However, some existing
ICP-based methods do not consider the reliability of the initial pose, which
may cause the method to converge to a local optimum. Furthermore, the absence
of an adaptive mechanism hinders the effective handling of complex dynamic
environments, resulting in a significant degradation of registration accuracy.
To address these issues, this paper proposes an adaptive ICP-based LiDAR
odometry method that relies on a reliable initial pose. First, distributed
coarse registration based on density filtering is employed to obtain the
initial pose estimation. The reliable initial pose is then selected by
comparing it with the motion prediction pose, reducing the initial error
between the source and target point clouds. Subsequently, by combining the
current and historical errors, the adaptive threshold is dynamically adjusted
to accommodate the real-time changes in the dynamic environment. Finally, based
on the reliable initial pose and the adaptive threshold, point-to-plane
adaptive ICP registration is performed from the current frame to the local map,
achieving high-precision alignment of the source and target point clouds.
Extensive experiments on the public KITTI dataset demonstrate that the proposed
method outperforms existing approaches and significantly enhances the accuracy
of LiDAR odometry.
\\ ( https://arxiv.org/abs/2509.22058 ,  2722kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22060 (*cross-listing*)
Date: Fri, 26 Sep 2025 08:42:59 GMT   (746kb)

Title: Decoding Deception: Understanding Automatic Speech Recognition
  Vulnerabilities in Evasion and Poisoning Attacks
Authors: Aravindhan G, Yuvaraj Govindarajulu, Parin Shah
Categories: cs.SD cs.AI cs.CR
\\
  Recent studies have demonstrated the vulnerability of Automatic Speech
Recognition systems to adversarial examples, which can deceive these systems
into misinterpreting input speech commands. While previous research has
primarily focused on white-box attacks with constrained optimizations, and
transferability based black-box attacks against commercial Automatic Speech
Recognition devices, this paper explores cost efficient white-box attack and
non transferability black-box adversarial attacks on Automatic Speech
Recognition systems, drawing insights from approaches such as Fast Gradient
Sign Method and Zeroth-Order Optimization. Further, the novelty of the paper
includes how poisoning attack can degrade the performances of state-of-the-art
models leading to misinterpretation of audio signals. Through experimentation
and analysis, we illustrate how hybrid models can generate subtle yet impactful
adversarial examples with very little perturbation having Signal Noise Ratio of
35dB that can be generated within a minute. These vulnerabilities of
state-of-the-art open source model have practical security implications, and
emphasize the need for adversarial security.
\\ ( https://arxiv.org/abs/2509.22060 ,  746kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22067 (*cross-listing*)
Date: Fri, 26 Sep 2025 08:49:47 GMT   (1639kb)

Title: The Rogue Scalpel: Activation Steering Compromises LLM Safety
Authors: Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Y. Rogov, Ivan
  Oseledets, Elena Tutubalina
Categories: cs.LG cs.AI
\\
  Activation steering is a promising technique for controlling LLM behavior by
adding semantically meaningful vectors directly into a model's hidden states
during inference. It is often framed as a precise, interpretable, and
potentially safer alternative to fine-tuning. We demonstrate the opposite:
steering systematically breaks model alignment safeguards, making it comply
with harmful requests. Through extensive experiments on different model
families, we show that even steering in a random direction can increase the
probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign
features from a sparse autoencoder (SAE), a common source of interpretable
directions, increases these rates by a further 2-4%. Finally, we show that
combining 20 randomly sampled vectors that jailbreak a single prompt creates a
universal attack, significantly increasing harmful compliance on unseen
requests. These results challenge the paradigm of safety through
interpretability, showing that precise control over model internals does not
guarantee precise control over model behavior.
\\ ( https://arxiv.org/abs/2509.22067 ,  1639kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22093 (*cross-listing*)
Date: Fri, 26 Sep 2025 09:13:02 GMT   (29304kb)

Title: Action-aware Dynamic Pruning for Efficient Vision-Language-Action
  Manipulation
Authors: Xiaohuan Pei, Yuxing Chen, Siyu Xu, Yunke Wang, Yuheng Shi, Chang Xu
Categories: cs.RO cs.AI
\\
  Robotic manipulation with Vision-Language-Action models requires efficient
inference over long-horizon multi-modal context, where attention to dense
visual tokens dominates computational cost. Existing methods optimize inference
speed by reducing visual redundancy within VLA models, but they overlook the
varying redundancy across robotic manipulation stages. We observe that the
visual token redundancy is higher in coarse manipulation phase than in
fine-grained operations, and is strongly correlated with the action dynamic.
Motivated by this observation, we propose \textbf{A}ction-aware
\textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning
framework that integrates text-driven token selection with action-aware
trajectory gating. Our method introduces a gating mechanism that conditions the
pruning signal on recent action trajectories, using past motion windows to
adaptively adjust token retention ratios in accordance with dynamics, thereby
balancing computational efficiency and perceptual precision across different
manipulation stages. Extensive experiments on the LIBERO suites and diverse
real-world scenarios demonstrate that our method significantly reduces FLOPs
and action inference latency (\textit{e.g.} $1.35 \times$ speed up on
OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\%
improvements with OpenVLA) compared to baselines, thereby providing a simple
plug-in path to efficient robot policies that advances the efficiency and
performance frontier of robotic manipulation. Our project website is:
\href{https://vla-adp.github.io/}{ADP.com}.
\\ ( https://arxiv.org/abs/2509.22093 ,  29304kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22097 (*cross-listing*)
Date: Fri, 26 Sep 2025 09:18:57 GMT   (661kb)

Title: SecureAgentBench: Benchmarking Secure Code Generation under Realistic
  Vulnerability Scenarios
Authors: Junkai Chen, Huihui Huang, Yunbo Lyu, Junwen An, Jieke Shi, Chengran
  Yang, Ting Zhang, Haoye Tian, Yikun Li, Zhenhao Li, Xin Zhou, Xing Hu, David
  Lo
Categories: cs.SE cs.AI cs.CL cs.CR
\\
  Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.
\\ ( https://arxiv.org/abs/2509.22097 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22102 (*cross-listing*)
Date: Fri, 26 Sep 2025 09:24:12 GMT   (746kb)

Title: Reinforcement Learning for Durable Algorithmic Recourse
Authors: Marina Ceccon, Alessandro Fabris, Goran Radanovi\'c, Asia J. Biega,
  Gian Antonio Susto
Categories: cs.LG cs.AI
\\
  Algorithmic recourse seeks to provide individuals with actionable
recommendations that increase their chances of receiving favorable outcomes
from automated decision systems (e.g., loan approvals). While prior research
has emphasized robustness to model updates, considerably less attention has
been given to the temporal dynamics of recourse--particularly in competitive,
resource-constrained settings where recommendations shape future applicant
pools. In this work, we present a novel time-aware framework for algorithmic
recourse, explicitly modeling how candidate populations adapt in response to
recommendations. Additionally, we introduce a novel reinforcement learning
(RL)-based recourse algorithm that captures the evolving dynamics of the
environment to generate recommendations that are both feasible and valid. We
design our recommendations to be durable, supporting validity over a predefined
time horizon T. This durability allows individuals to confidently reapply after
taking time to implement the suggested changes. Through extensive experiments
in complex simulation environments, we show that our approach substantially
outperforms existing baselines, offering a superior balance between feasibility
and long-term validity. Together, these results underscore the importance of
incorporating temporal and behavioral dynamics into the design of practical
recourse systems.
\\ ( https://arxiv.org/abs/2509.22102 ,  746kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22115 (*cross-listing*)
Date: Fri, 26 Sep 2025 09:36:53 GMT   (1659kb)

Title: Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework
  for Efficient Policy Optimization
Authors: Chao Wang, Tao Yang, Hongtao Tian, Yunsheng Shi, Qiyao Ma, Xiaotao
  Liu, Ting Yao, Wenbo Ding
Categories: cs.LG cs.AI
Comments: 18 pages, 5 figures, Under review as a conference paper at ICLR 2026
\\
  Critic-free methods like GRPO reduce memory demands by estimating advantages
from multiple rollouts but tend to converge slowly, as critical learning
signals are diluted by an abundance of uninformative samples and tokens. To
tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling
(D$^3$S)} framework that prioritizes the most informative samples and tokens
across groups to improve the efficient of policy optimization. D$^3$S operates
along two levels: (1) the sample-level, which selects a subset of rollouts to
maximize advantage variance ($\text{Var}(A)$). We theoretically proven that
this selection is positively correlated with the upper bound of the policy
gradient norms, yielding higher policy gradients. (2) the token-level, which
prioritizes tokens with a high product of advantage magnitude and policy
entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the
policy is both uncertain and impactful. Moreover, to prevent overfitting to
high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by
curriculum learning. This schedule starts with aggressive down-sampling to
accelerate early learning and gradually relaxes to promote robust
generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that
integrating D$^3$S into advanced RL algorithms achieves state-of-the-art
performance and generalization while requiring \textit{fewer} samples and
tokens across diverse reasoning benchmarks. Our code is added in the
supplementary materials and will be made publicly available.
\\ ( https://arxiv.org/abs/2509.22115 ,  1659kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22161 (*cross-listing*)
Date: Fri, 26 Sep 2025 10:17:42 GMT   (84kb)

Title: Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse
  in Smoothed Vector Quantization
Authors: Takashi Morita
Categories: cs.LG cs.AI
\\
  Vector quantization, which discretizes a continuous vector space into a
finite set of representative vectors (a codebook), has been widely adopted in
modern machine learning. Despite its effectiveness, vector quantization poses a
fundamental challenge: the non-differentiable quantization step blocks gradient
backpropagation. Smoothed vector quantization addresses this issue by relaxing
the hard assignment of a codebook vector into a weighted combination of
codebook entries, represented as the matrix product of a simplex vector and the
codebook. Effective smoothing requires two properties: (1) smoothed quantizers
should remain close to a onehot vector, ensuring tight approximation, and (2)
all codebook entries should be utilized, preventing code collapse. Existing
methods typically address these desiderata separately. By contrast, the present
study introduces a simple and intuitive regularization that promotes both
simultaneously by minimizing the distance between each simplex vertex and its
$K$-nearest smoothed quantizers. Experiments on representative benchmarks,
including discrete image autoencoding and contrastive speech representation
learning, demonstrate that the proposed method achieves more reliable codebook
utilization and improves performance compared to prior approaches.
\\ ( https://arxiv.org/abs/2509.22161 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22166 (*cross-listing*)
Date: Fri, 26 Sep 2025 10:27:55 GMT   (766kb)

Title: Lightweight error mitigation strategies for post-training N:M activation
  sparsity in LLMs
Authors: Shirin Alanova, Kristina Kazistova, Ekaterina Galaeva, Alina
  Kostromina, Vladimir Smirnov, Redko Dmitry, Alexey Dontsov, Maxim Zhelnin,
  Evgeny Burnaev, Egor Shvetsov
Categories: cs.LG cs.AI
\\
  The demand for efficient large language model (LLM) inference has intensified
the focus on sparsification techniques. While semi-structured (N:M) pruning is
well-established for weights, its application to activation pruning remains
underexplored despite its potential for dynamic, input-adaptive compression and
reductions in I/O overhead. This work presents a comprehensive analysis of
methods for post-training N:M activation pruning in LLMs. Across multiple LLMs,
we demonstrate that pruning activations enables superior preservation of
generative capabilities compared to weight pruning at equivalent sparsity
levels. We evaluate lightweight, plug-and-play error mitigation techniques and
pruning criteria, establishing strong hardware-friendly baselines that require
minimal calibration. Furthermore, we explore sparsity patterns beyond NVIDIA's
standard 2:4, showing that the 16:32 pattern achieves performance nearly on par
with unstructured sparsity. However, considering the trade-off between
flexibility and hardware implementation complexity, we focus on the 8:16
pattern as a superior candidate. Our findings provide both effective practical
methods for activation pruning and a motivation for future hardware to support
more flexible sparsity patterns. Our code is available
https://anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md .
\\ ( https://arxiv.org/abs/2509.22166 ,  766kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22168 (*cross-listing*)
Date: Fri, 26 Sep 2025 10:28:56 GMT   (16274kb)

Title: Teaching AI to Feel: A Collaborative, Full-Body Exploration of Emotive
  Communication
Authors: Esen K. T\"ut\"unc\"u, Lissette Lemus, Kris Pilcher, Holger Sprengel,
  Jordi Sabater-Mir
Categories: cs.HC cs.AI
Comments: 9 pages, 10 Figures, ACM MM'25
DOI: 10.1145/3746027.3758186
\\
  Commonaiverse is an interactive installation exploring human emotions through
full-body motion tracking and real-time AI feedback. Participants engage in
three phases: Teaching, Exploration and the Cosmos Phase, collaboratively
expressing and interpreting emotions with the system. The installation
integrates MoveNet for precise motion tracking and a multi-recommender AI
system to analyze emotional states dynamically, responding with adaptive
audiovisual outputs. By shifting from top-down emotion classification to
participant-driven, culturally diverse definitions, we highlight new pathways
for inclusive, ethical affective computing. We discuss how this collaborative,
out-of-the-box approach pushes multimedia research beyond single-user facial
analysis toward a more embodied, co-created paradigm of emotional AI.
Furthermore, we reflect on how this reimagined framework fosters user agency,
reduces bias, and opens avenues for advanced interactive applications.
\\ ( https://arxiv.org/abs/2509.22168 ,  16274kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22174 (*cross-listing*)
Date: Fri, 26 Sep 2025 10:34:06 GMT   (3060kb)

Title: Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood
  Aggregation with Minimal Overhead
Authors: Durgesh Kalwar, Mayank Baranwal, Harshad Khadilkar
Categories: cs.LG cs.AI
\\
  In today's data-sensitive landscape, distributed learning emerges as a vital
tool, not only fortifying privacy measures but also streamlining computational
operations. This becomes especially crucial within fully decentralized
infrastructures where local processing is imperative due to the absence of
centralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to
information aggregation in multi-agent networks. DYNAWEIGHT offers substantial
acceleration in decentralized learning with minimal additional communication
and memory overhead. Unlike traditional static weight assignments, such as
Metropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring
servers based on their relative losses on local datasets. Consequently, it
favors servers possessing diverse information, particularly in scenarios of
substantial data heterogeneity. Our experiments on various datasets MNIST,
CIFAR10, and CIFAR100 incorporating various server counts and graph topologies,
demonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT
functions as an aggregation scheme compatible with any underlying server-level
optimization algorithm, underscoring its versatility and potential for
widespread integration.
\\ ( https://arxiv.org/abs/2509.22174 ,  3060kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22184 (*cross-listing*)
Date: Fri, 26 Sep 2025 10:44:26 GMT   (313kb)

Title: Learning Equivariant Functions via Quadratic Forms
Authors: Pavan Karjol, Vivek V Kashyap, Rohan Kashyap, Prathosh A P
Categories: cs.LG cs.AI
\\
  In this study, we introduce a method for learning group (known or unknown)
equivariant functions by learning the associated quadratic form $x^T A x$
corresponding to the group from the data. Certain groups, known as orthogonal
groups, preserve a specific quadratic form, and we leverage this property to
uncover the underlying symmetry group under the assumption that it is
orthogonal. By utilizing the corresponding unique symmetric matrix and its
inherent diagonal form, we incorporate suitable inductive biases into the
neural network architecture, leading to models that are both simplified and
efficient. Our approach results in an invariant model that preserves norms,
while the equivariant model is represented as a product of a norm-invariant
model and a scale-invariant model, where the ``product'' refers to the group
action.
  Moreover, we extend our framework to a more general setting where the
function acts on tuples of input vectors via a diagonal (or product) group
action. In this extension, the equivariant function is decomposed into an
angular component extracted solely from the normalized first vector and a
scale-invariant component that depends on the full Gram matrix of the tuple.
This decomposition captures the inter-dependencies between multiple inputs
while preserving the underlying group symmetry.
  We assess the effectiveness of our framework across multiple tasks, including
polynomial regression, top quark tagging, and moment of inertia matrix
prediction. Comparative analysis with baseline methods demonstrates that our
model consistently excels in both discovering the underlying symmetry and
efficiently learning the corresponding equivariant function.
\\ ( https://arxiv.org/abs/2509.22184 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22199 (*cross-listing*)
Date: Fri, 26 Sep 2025 11:05:10 GMT   (5223kb)

Title: MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA
  Training
Authors: Haoyun Li, Ivan Zhang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Zhiqin
  Yang, Zhentao Zhang, Boyuan Wang, Chaojun Ni, Wenkang Qin, Xinze Chen, Yun
  Ye, Guan Huang, Zhenbo Song, Xingang Wang
Categories: cs.RO cs.AI
\\
  Vision Language Action (VLA) models derive their generalization capability
from diverse training data, yet collecting embodied robot interaction data
remains prohibitively expensive. In contrast, human demonstration videos are
far more scalable and cost-efficient to collect, and recent studies confirm
their effectiveness in training VLA models. However, a significant domain gap
persists between human videos and robot-executed videos, including unstable
camera viewpoints, visual discrepancies between human hands and robotic arms,
and differences in motion dynamics. To bridge this gap, we propose
MimicDreamer, a framework that turns fast, low-cost human demonstrations into
robot-usable supervision by jointly aligning vision, viewpoint, and actions to
directly support policy training. For visual alignment, we propose H2R Aligner,
a video diffusion model that generates high-fidelity robot demonstration videos
by transferring motion from human manipulation footage. For viewpoint
stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos
via homography and inpaints occlusions and distortions caused by warping. For
action alignment, we map human hand trajectories to the robot frame and apply a
constrained inverse kinematics solver to produce feasible, low-jitter joint
commands with accurate pose tracking. Empirically, VLA models trained purely on
our synthesized human-to-robot videos achieve few-shot execution on real
robots. Moreover, scaling training with human data significantly boosts
performance compared to models trained solely on real robot data; our approach
improves the average success rate by 14.7\% across six representative
manipulation tasks.
\\ ( https://arxiv.org/abs/2509.22199 ,  5223kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22207 (*cross-listing*)
Date: Fri, 26 Sep 2025 11:21:31 GMT   (6264kb)

Title: Reversible GNS for Dissipative Fluids with Consistent Bidirectional
  Dynamics
Authors: Mu Huang, Linning Xu, Mingyue Dai, Yidi Shao, Bo Dai
Categories: cs.LG cs.AI physics.flu-dyn
Comments: 13 pages, 5 figures
ACM-class: I.2.6; I.6.9; I.6.5
\\
  Simulating physically plausible trajectories toward user-defined goals is a
fundamental yet challenging task in fluid dynamics. While particle-based
simulators can efficiently reproduce forward dynamics, inverse inference
remains difficult, especially in dissipative systems where dynamics are
irreversible and optimization-based solvers are slow, unstable, and often fail
to converge. In this work, we introduce the Reversible Graph Network Simulator
(R-GNS), a unified framework that enforces bidirectional consistency within a
single graph architecture. Unlike prior neural simulators that approximate
inverse dynamics by fitting backward data, R-GNS does not attempt to reverse
the underlying physics. Instead, we propose a mathematically invertible design
based on residual reversible message passing with shared parameters, coupling
forward dynamics with inverse inference to deliver accurate predictions and
efficient recovery of plausible initial states. Experiments on three
dissipative benchmarks (Water-3D, WaterRamps, and WaterDrop) show that R-GNS
achieves higher accuracy and consistency with only one quarter of the
parameters, and performs inverse inference more than 100 times faster than
optimization-based baselines. For forward simulation, R-GNS matches the speed
of strong GNS baselines, while in goal-conditioned tasks it eliminates
iterative optimization and achieves orders-of-magnitude speedups. On
goal-conditioned tasks, R-GNS further demonstrates its ability to complex
target shapes (e.g., characters "L" and "N") through vivid, physically
consistent trajectories. To our knowledge, this is the first reversible
framework that unifies forward and inverse simulation for dissipative fluid
systems.
\\ ( https://arxiv.org/abs/2509.22207 ,  6264kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22219 (*cross-listing*)
Date: Fri, 26 Sep 2025 11:31:38 GMT   (845kb)

Title: Automatic Discovery of One Parameter Subgroups of $SO(n)$
Authors: Pavan Karjol, Vivek V Kashyap, Rohan Kashyap, Prathosh A P
Categories: cs.LG cs.AI
\\
  We introduce a novel framework for the automatic discovery of one-parameter
subgroups ($H_{\gamma}$) of $SO(3)$ and, more generally, $SO(n)$. One-parameter
subgroups of $SO(n)$ are crucial in a wide range of applications, including
robotics, quantum mechanics, and molecular structure analysis. Our method
utilizes the standard Jordan form of skew-symmetric matrices, which define the
Lie algebra of $SO(n)$, to establish a canonical form for orbits under the
action of $H_{\gamma}$. This canonical form is then employed to derive a
standardized representation for $H_{\gamma}$-invariant functions. By learning
the appropriate parameters, the framework uncovers the underlying one-parameter
subgroup $H_{\gamma}$. The effectiveness of the proposed approach is
demonstrated through tasks such as double pendulum modeling, moment of inertia
prediction, top quark tagging and invariant polynomial regression, where it
successfully recovers meaningful subgroup structure and produces interpretable,
symmetry-aware representations.
\\ ( https://arxiv.org/abs/2509.22219 ,  845kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22222 (*cross-listing*)
Date: Fri, 26 Sep 2025 11:34:55 GMT   (13087kb)

Title: Rigidity-Aware 3D Gaussian Deformation from a Single Image
Authors: Jinhyeok Kim, Jaehun Bang, Seunghyun Seo, Kyungdon Joo
Categories: cs.GR cs.AI cs.CV
Comments: 10 pages, 11 figures, conference
\\
  Reconstructing object deformation from a single image remains a significant
challenge in computer vision and graphics. Existing methods typically rely on
multi-view video to recover deformation, limiting their applicability under
constrained scenarios. To address this, we propose DeformSplat, a novel
framework that effectively guides 3D Gaussian deformation from only a single
image. Our method introduces two main technical contributions. First, we
present Gaussian-to-Pixel Matching which bridges the domain gap between 3D
Gaussian representations and 2D pixel observations. This enables robust
deformation guidance from sparse visual cues. Second, we propose Rigid Part
Segmentation consisting of initialization and refinement. This segmentation
explicitly identifies rigid regions, crucial for maintaining geometric
coherence during deformation. By combining these two techniques, our approach
can reconstruct consistent deformations from a single image. Extensive
experiments demonstrate that our approach significantly outperforms existing
methods and naturally extends to various applications,such as frame
interpolation and interactive object manipulation.
\\ ( https://arxiv.org/abs/2509.22222 ,  13087kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22232 (*cross-listing*)
Date: Fri, 26 Sep 2025 11:42:14 GMT   (33636kb)

Title: Fairness-Aware Reinforcement Learning (FAReL): A Framework for
  Transparent and Balanced Sequential Decision-Making
Authors: Alexandra Cimpean, Nicole Orzan, Catholijn Jonker, Pieter Libin, Ann
  Now\'e
Categories: cs.LG cs.AI
\\
  Equity in real-world sequential decision problems can be enforced using
fairness-aware methods. Therefore, we require algorithms that can make suitable
and transparent trade-offs between performance and the desired fairness
notions. As the desired performance-fairness trade-off is hard to specify a
priori, we propose a framework where multiple trade-offs can be explored.
Insights provided by the reinforcement learning algorithm regarding the
obtainable performance-fairness trade-offs can then guide stakeholders in
selecting the most appropriate policy. To capture fairness, we propose an
extended Markov decision process, $f$MDP, that explicitly encodes individuals
and groups. Given this $f$MDP, we formalise fairness notions in the context of
sequential decision problems and formulate a fairness framework that computes
fairness measures over time. We evaluate our framework in two scenarios with
distinct fairness requirements: job hiring, where strong teams must be composed
while treating applicants equally, and fraud detection, where fraudulent
transactions must be detected while ensuring the burden on customers is fairly
distributed. We show that our framework learns policies that are more fair
across multiple scenarios, with only minor loss in performance reward.
Moreover, we observe that group and individual fairness notions do not
necessarily imply one another, highlighting the benefit of our framework in
settings where both fairness types are desired. Finally, we provide guidelines
on how to apply this framework across different problem settings.
\\ ( https://arxiv.org/abs/2509.22232 ,  33636kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22246 (*cross-listing*)
Date: Fri, 26 Sep 2025 12:02:58 GMT   (122kb)

Title: ASSESS: A Semantic and Structural Evaluation Framework for Statement
  Similarity
Authors: Xiaoyang Liu, Tao Zhu, Zineng Dong, Yuntian Liu, Qingfeng Guo,
  Zhaoxuan Liu, Yu Chen, Tao Luo
Categories: cs.LG cs.AI
\\
  Statement autoformalization, the automated translation of statements from
natural language into formal languages, has seen significant advancements, yet
the development of automated evaluation metrics remains limited. Existing
metrics for formal statement similarity often fail to balance semantic and
structural information. String-based approaches capture syntactic structure but
ignore semantic meaning, whereas proof-based methods validate semantic
equivalence but disregard structural nuances and, critically, provide no graded
similarity score in the event of proof failure. To address these issues, we
introduce ASSESS (A Semantic and Structural Evaluation Framework for Statement
Similarity), which comprehensively integrates semantic and structural
information to provide a continuous similarity score. Our framework first
transforms formal statements into Operator Trees to capture their syntactic
structure and then computes a similarity score using our novel TransTED
(Transformation Tree Edit Distance) Similarity metric, which enhances
traditional Tree Edit Distance by incorporating semantic awareness through
transformations. For rigorous validation, we present EPLA (Evaluating
Provability and Likeness for Autoformalization), a new benchmark of 524
expert-annotated formal statement pairs derived from miniF2F and ProofNet, with
labels for both semantic provability and structural likeness. Experiments on
EPLA demonstrate that TransTED Similarity outperforms existing methods,
achieving state-of-the-art accuracy and the highest Kappa coefficient. The
benchmark, and implementation code will be made public soon.
\\ ( https://arxiv.org/abs/2509.22246 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22256 (*cross-listing*)
Date: Fri, 26 Sep 2025 12:19:27 GMT   (1875kb)

Title: Secure and Efficient Access Control for Computer-Use Agents via Context
  Space
Authors: Haochen Gong, Chenxiao Li, Rui Chang, Wenbo Shen
Categories: cs.CR cs.AI cs.OS
\\
  Large language model (LLM)-based computer-use agents represent a convergence
of AI and OS capabilities, enabling natural language to control system- and
application-level functions. However, due to LLMs' inherent uncertainty issues,
granting agents control over computers poses significant security risks. When
agent actions deviate from user intentions, they can cause irreversible
consequences. Existing mitigation approaches, such as user confirmation and
LLM-based dynamic action validation, still suffer from limitations in
usability, security, and performance. To address these challenges, we propose
CSAgent, a system-level, static policy-based access control framework for
computer-use agents. To bridge the gap between static policy and dynamic
context and user intent, CSAgent introduces intent- and context-aware policies,
and provides an automated toolchain to assist developers in constructing and
refining them. CSAgent enforces these policies through an optimized OS service,
ensuring that agent actions can only be executed under specific user intents
and contexts. CSAgent supports protecting agents that control computers through
diverse interfaces, including API, CLI, and GUI. We implement and evaluate
CSAgent, which successfully defends against more than 99.36% of attacks while
introducing only 6.83% performance overhead.
\\ ( https://arxiv.org/abs/2509.22256 ,  1875kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22259 (*cross-listing*)
Date: Fri, 26 Sep 2025 12:20:18 GMT   (395kb)

Title: Wavelet-Induced Rotary Encodings: RoPE Meets Graphs
Authors: Isaac Reid, Arijit Sehanobish, Cedrik H\"ofs, Bruno Mlodozeniec,
  Leonhard Vulpius, Federico Barbero, Adrian Weller, Krzysztof Choromanski,
  Richard E. Turner, Petar Veli\v{c}kovi\'c
Categories: cs.LG cs.AI
\\
  We introduce WIRE: Wavelet-Induced Rotary Encodings. WIRE extends Rotary
Position Encodings (RoPE), a popular algorithm in LLMs and ViTs, to
graph-structured data. We demonstrate that WIRE is more general than RoPE,
recovering the latter in the special case of grid graphs. WIRE also enjoys a
host of desirable theoretical properties, including equivariance under node
ordering permutation, compatibility with linear attention, and (under select
assumptions) asymptotic dependence on graph resistive distance. We test WIRE on
a range of synthetic and real-world tasks, including identifying monochromatic
subgraphs, semantic segmentation of point clouds, and more standard graph
benchmarks. We find it to be effective in settings where the underlying graph
structure is important.
\\ ( https://arxiv.org/abs/2509.22259 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22280 (*cross-listing*)
Date: Fri, 26 Sep 2025 12:45:29 GMT   (141kb)

Title: A Global Analysis of Cyber Threats to the Energy Sector: "Currents of
  Conflict" from a Geopolitical Perspective
Authors: Gustavo S\'anchez, Ghada Elbez, Veit Hagenmeyer
Categories: cs.CR cs.AI
Comments: THIS IS A POSTPRINT OF A PEER-REVIEWED ARTICLE, PLEASE CITE IT IF
  USING THIS WORK: Gustavo Sanchez, Ghada Elbez, and Veit Hagenmeyer. "A Global
  Analysis of Cyber Threats to the Energy Sector:"Currents of Conflict" from a
  geopolitical perspective." atp magazin 67.9 (2025): 56-66.
  https://doi.org/10.17560/atp.v67i9.2797
Journal-ref: Gustavo Sanchez, Ghada Elbez, and Veit Hagenmeyer. "A Global
  Analysis of Cyber Threats to the Energy Sector:"Currents of Conflict" from a
  geopolitical perspective." atp magazin 67.9 (2025): 56-66
DOI: 10.17560/atp.v67i9.2797
\\
  The escalating frequency and sophistication of cyber threats increased the
need for their comprehensive understanding. This paper explores the
intersection of geopolitical dynamics, cyber threat intelligence analysis, and
advanced detection technologies, with a focus on the energy domain. We leverage
generative artificial intelligence to extract and structure information from
raw cyber threat descriptions, enabling enhanced analysis. By conducting a
geopolitical comparison of threat actor origins and target regions across
multiple databases, we provide insights into trends within the general threat
landscape. Additionally, we evaluate the effectiveness of cybersecurity tools
-- with particular emphasis on learning-based techniques -- in detecting
indicators of compromise for energy-targeted attacks. This analysis yields new
insights, providing actionable information to researchers, policy makers, and
cybersecurity professionals.
\\ ( https://arxiv.org/abs/2509.22280 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22287 (*cross-listing*)
Date: Fri, 26 Sep 2025 12:48:51 GMT   (562kb)

Title: Leveraging Large Language Models for Robot-Assisted Learning of
  Morphological Structures in Preschool Children with Language Vulnerabilities
Authors: Stina Sundstedt, Mattias Wingren, Susanne H\"agglund and Daniel Ventus
Categories: cs.RO cs.AI cs.HC
Comments: 12 pages, 2 figures, Preprint of: Sundstedt, S., Wingren, M.,
  H\"agglund, S. & Ventus, D. (2025). Leveraging Large Language Models for
  Robot-Assisted Learning of Morphological Structures in Preschool Children
  with Language Vulnerabilities. In: Stephanidis, C., Antona, M., Ntoa, S. &
  Salvendy, G. (eds.), Communications in Computer and Information Science, vol.
  2523, pp. 415-425. Springer
ACM-class: I.2.7; H.5.2; K.3.1; J.4
Journal-ref: Communications in Computer and Information Science(2025).
  Stephanidis, C., Antona, M., Ntoa, S. & Salvendy, G. (eds.). p. 415-425 11 p.
  ( Communications in Computer and Information Science; vol. 2523)
DOI: 10.1007/978-3-031-94153-5_41
\\
  Preschool children with language vulnerabilities -- such as developmental
language disorders or immigration related language challenges -- often require
support to strengthen their expressive language skills. Based on the principle
of implicit learning, speech-language therapists (SLTs) typically embed target
morphological structures (e.g., third person -s) into everyday interactions or
game-based learning activities. Educators are recommended by SLTs to do the
same. This approach demands precise linguistic knowledge and real-time
production of various morphological forms (e.g., "Daddy wears these when he
drives to work"). The task becomes even more demanding when educators or parent
also must keep children engaged and manage turn-taking in a game-based
activity. In the TalBot project our multiprofessional team have developed an
application in which the Furhat conversational robot plays the word retrieval
game "Alias" with children to improve language skills. Our application
currently employs a large language model (LLM) to manage gameplay, dialogue,
affective responses, and turn-taking. Our next step is to further leverage the
capacity of LLMs so the robot can generate and deliver specific morphological
targets during the game. We hypothesize that a robot could outperform humans at
this task. Novel aspects of this approach are that the robot could ultimately
serve as a model and tutor for both children and professionals and that using
LLM capabilities in this context would support basic communication needs for
children with language vulnerabilities. Our long-term goal is to create a
robust LLM-based Robot-Assisted Language Learning intervention capable of
teaching a variety of morphological structures across different languages.
\\ ( https://arxiv.org/abs/2509.22287 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22299 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:00:46 GMT   (335kb)

Title: HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space
Authors: Ke Li, Zheng Yang, Zhongbin Zhou, Feng Xue, Zhonglin Jiang, Wenxiao
  Wang
Categories: cs.LG cs.AI
\\
  Mixture-of-Experts (MoE) architectures in large language models (LLMs)
deliver exceptional performance and reduced inference costs compared to dense
LLMs. However, their large parameter counts result in prohibitive memory
requirements, limiting practical deployment. While existing pruning methods
primarily focus on expert-level pruning, this coarse granularity often leads to
substantial accuracy degradation. In this work, we introduce HEAPr, a novel
pruning algorithm that decomposes experts into smaller, indivisible atomic
experts, enabling more precise and flexible atomic expert pruning. To measure
the importance of each atomic expert, we leverage second-order information
based on principles similar to Optimal Brain Surgeon (OBS) theory. To address
the computational and storage challenges posed by second-order information,
HEAPr exploits the inherent properties of atomic experts to transform the
second-order information from expert parameters into that of atomic expert
parameters, and further simplifies it to the second-order information of atomic
expert outputs. This approach reduces the space complexity from $O(d^4)$, where
d is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward
passes and one backward pass on a small calibration set to compute the
importance of atomic experts. Extensive experiments on MoE models, including
DeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing
expert-level pruning methods across a wide range of compression ratios and
benchmarks. Specifically, HEAPr achieves nearly lossless compression at
compression ratios of 20% ~ 25% in most models, while also reducing FLOPs
nearly by 20%. The code can be found at
\href{https://github.com/LLIKKE/HEAPr}{https://github.com/LLIKKE/HEAPr}.
\\ ( https://arxiv.org/abs/2509.22299 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22310 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:14:03 GMT   (9760kb)

Title: Adaptive Policy Backbone via Shared Network
Authors: Bumgeun Park, Donghwan Lee
Categories: cs.LG cs.AI
\\
  Reinforcement learning (RL) has achieved impressive results across domains,
yet learning an optimal policy typically requires extensive interaction data,
limiting practical deployment. A common remedy is to leverage priors, such as
pre-collected datasets or reference policies, but their utility degrades under
task mismatch between training and deployment. While prior work has sought to
address this mismatch, it has largely been restricted to in-distribution
settings. To address this challenge, we propose Adaptive Policy Backbone (APB),
a meta-transfer RL method that inserts lightweight linear layers before and
after a shared backbone, thereby enabling parameter-efficient fine-tuning
(PEFT) while preserving prior knowledge during adaptation. Our results show
that APB improves sample efficiency over standard RL and adapts to
out-of-distribution (OOD) tasks where existing meta-RL baselines typically
fail.
\\ ( https://arxiv.org/abs/2509.22310 ,  9760kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22319 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:19:32 GMT   (1907kb)

Title: Progressive Weight Loading: Accelerating Initial Inference and Gradually
  Boosting Performance on Resource-Constrained Environments
Authors: Hyunwoo Kim, Junha Lee, Mincheol Choi, Jeonghwan Lee, Jaeshin Cho
Categories: cs.LG cs.AI
\\
  Deep learning models have become increasingly large and complex, resulting in
higher memory consumption and computational demands. Consequently, model
loading times and initial inference latency have increased, posing significant
challenges in mobile and latency-sensitive environments where frequent model
loading and unloading are required, which directly impacts user experience.
While Knowledge Distillation (KD) offers a solution by compressing large
teacher models into smaller student ones, it often comes at the cost of reduced
performance. To address this trade-off, we propose Progressive Weight Loading
(PWL), a novel technique that enables fast initial inference by first deploying
a lightweight student model, then incrementally replacing its layers with those
of a pre-trained teacher model. To support seamless layer substitution, we
introduce a training method that not only aligns intermediate feature
representations between student and teacher layers, but also improves the
overall output performance of the student model. Our experiments on VGG,
ResNet, and ViT architectures demonstrate that models trained with PWL maintain
competitive distillation performance and gradually improve accuracy as teacher
layers are loaded-matching the final accuracy of the full teacher model without
compromising initial inference speed. This makes PWL particularly suited for
dynamic, resource-constrained deployments where both responsiveness and
performance are critical.
\\ ( https://arxiv.org/abs/2509.22319 ,  1907kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22335 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:28:53 GMT   (9815kb)

Title: Spectral Collapse Drives Loss of Plasticity in Deep Continual Learning
Authors: Naicheng He, Kaicheng Guo, Arjun Prakash, Saket Tiwari, Ruo Yu Tao,
  Tyrone Serapio, Amy Greenwald, George Konidaris
Categories: cs.LG cs.AI
\\
  We investigate why deep neural networks suffer from \emph{loss of plasticity}
in deep continual learning, failing to learn new tasks without reinitializing
parameters. We show that this failure is preceded by Hessian spectral collapse
at new-task initialization, where meaningful curvature directions vanish and
gradient descent becomes ineffective. To characterize the necessary condition
for successful training, we introduce the notion of $\tau$-trainability and
show that current plasticity preserving algorithms can be unified under this
framework. Targeting spectral collapse directly, we then discuss the Kronecker
factored approximation of the Hessian, which motivates two regularization
enhancements: maintaining high effective feature rank and applying $L2$
penalties. Experiments on continual supervised and reinforcement learning tasks
confirm that combining these two regularizers effectively preserves plasticity.
\\ ( https://arxiv.org/abs/2509.22335 ,  9815kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22352 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:50:29 GMT   (5032kb)

Title: SurvDiff: A Diffusion Model for Generating Synthetic Data in Survival
  Analysis
Authors: Marie Brockschmidt, Maresa Schr\"oder, Stefan Feuerriegel
Categories: cs.LG cs.AI
\\
  Survival analysis is a cornerstone of clinical research by modeling
time-to-event outcomes such as metastasis, disease relapse, or patient death.
Unlike standard tabular data, survival data often come with incomplete event
information due to dropout, or loss to follow-up. This poses unique challenges
for synthetic data generation, where it is crucial for clinical research to
faithfully reproduce both the event-time distribution and the censoring
mechanism. In this paper, we propose SurvDiff, an end-to-end diffusion model
specifically designed for generating synthetic data in survival analysis.
SurvDiff is tailored to capture the data-generating mechanism by jointly
generating mixed-type covariates, event times, and right-censoring, guided by a
survival-tailored loss function. The loss encodes the time-to-event structure
and directly optimizes for downstream survival tasks, which ensures that
SurvDiff (i) reproduces realistic event-time distributions and (ii) preserves
the censoring mechanism. Across multiple datasets, we show that \survdiff
consistently outperforms state-of-the-art generative baselines in both
distributional fidelity and downstream evaluation metrics across multiple
medical datasets. To the best of our knowledge, SurvDiff is the first diffusion
model explicitly designed for generating synthetic survival data.
\\ ( https://arxiv.org/abs/2509.22352 ,  5032kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22353 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:50:32 GMT   (38333kb)

Title: Context and Diversity Matter: The Emergence of In-Context Learning in
  World Models
Authors: Fan Wang, Zhiyuan Chen, Yuxuan Zhong, Sunjian Zheng, Pengtao Shao, Bo
  Yu, Shaoshan Liu, Jianan Wang, Ning Ding, Yang Cao and Yu Kang
Categories: cs.LG cs.AI
\\
  The capability of predicting environmental dynamics underpins both biological
neural systems and general embodied AI in adapting to their surroundings. Yet
prevailing approaches rest on static world models that falter when confronted
with novel or rare configurations. We investigate in-context environment
learning (ICEL), shifting attention from zero-shot performance to the growth
and asymptotic limits of the world model. Our contributions are three-fold: (1)
we formalize in-context learning of a world model and identify two core
mechanisms: environment recognition and environment learning; (2) we derive
error upper-bounds for both mechanisms that expose how the mechanisms emerge;
and (3) we empirically confirm that distinct ICL mechanisms exist in the world
model, and we further investigate how data distribution and model architecture
affect ICL in a manner consistent with theory. These findings demonstrate the
potential of self-adapting world models and highlight the key factors behind
the emergence of ICEL, most notably the necessity of long context and diverse
environments.
\\ ( https://arxiv.org/abs/2509.22353 ,  38333kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22358 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:53:56 GMT   (237kb)

Title: Stochastic activations
Authors: Maria Lomeli and Matthijs Douze and Gergely Szilvasy and Loic Cabannes
  and Jade Copet and Sainbayar Sukhbaatar and Jason Weston and Gabriel Synnaeve
  and Pierre-Emmanuel Mazar\'e and Herv\'e J\'egou
Categories: cs.LG cs.AI
\\
  We introduce stochastic activations. This novel strategy randomly selects
between several non-linear functions in the feed-forward layer of a large
language model. In particular, we choose between SILU or RELU depending on a
Bernoulli draw. This strategy circumvents the optimization problem associated
with RELU, namely, the constant shape for negative inputs that prevents the
gradient flow. We leverage this strategy in two ways:
  (1) We use stochastic activations during pre-training and fine-tune the model
with RELU, which is used at inference time to provide sparse latent vectors.
This reduces the inference FLOPs and translates into a significant speedup in
the CPU. Interestingly, this leads to much better results than training from
scratch with the RELU activation function.
  (2) We evaluate stochastic activations for generation. This strategy performs
reasonably well: it is only slightly inferior to the best deterministic
non-linearity, namely SILU combined with temperature scaling. This offers an
alternative to existing strategies by providing a controlled way to increase
the diversity of the generated text.
\\ ( https://arxiv.org/abs/2509.22358 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22359 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:55:29 GMT   (14756kb)

Title: Forecasting the Future with Yesterday's Climate: Temperature Bias in AI
  Weather and Climate Models
Authors: Jacob B. Landsberg and Elizabeth A. Barnes
Categories: physics.ao-ph cs.AI
Comments: 13 pages, 5 figures
\\
  AI-based climate and weather models have rapidly gained popularity, providing
faster forecasts with skill that can match or even surpass that of traditional
dynamical models. Despite this success, these models face a key challenge:
predicting future climates while being trained only with historical data. In
this study, we investigate this issue by analyzing boreal winter land
temperature biases in AI weather and climate models. We examine two weather
models, FourCastNet V2 Small (FourCastNet) and Pangu Weather (Pangu),
evaluating their predictions for 2020-2025 and Ai2 Climate Emulator version 2
(ACE2) for 1996-2010. These time periods lie outside of the respective models'
training sets and are significantly more recent than the bulk of their training
data, allowing us to assess how well the models generalize to new, i.e. more
modern, conditions. We find that all three models produce cold-biased mean
temperatures, resembling climates from 15-20 years earlier than the period they
are predicting. In some regions, like the Eastern U.S., the predictions
resemble climates from as much as 20-30 years earlier. Further analysis shows
that FourCastNet's and Pangu's cold bias is strongest in the hottest predicted
temperatures, indicating limited training exposure to modern extreme heat
events. In contrast, ACE2's bias is more evenly distributed but largest in
regions, seasons, and parts of the temperature distribution where climate
change has been most pronounced. These findings underscore the challenge of
training AI models exclusively on historical data and highlight the need to
account for such biases when applying them to future climate prediction.
\\ ( https://arxiv.org/abs/2509.22359 ,  14756kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22378 (*cross-listing*)
Date: Fri, 26 Sep 2025 14:07:29 GMT   (1286kb)

Title: Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM
  Approach
Authors: Zijian Zhao, Dian Jin, Zijing Zhou
Categories: cs.SD cs.AI cs.MM eess.AS
\\
  Recently, Image-to-Music (I2M) generation has garnered significant attention,
with potential applications in fields such as gaming, advertising, and
multi-modal art creation. However, due to the ambiguous and subjective nature
of I2M tasks, most end-to-end methods lack interpretability, leaving users
puzzled about the generation results. Even methods based on emotion mapping
face controversy, as emotion represents only a singular aspect of art.
Additionally, most learning-based methods require substantial computational
resources and large datasets for training, hindering accessibility for common
users. To address these challenges, we propose the first Vision Language Model
(VLM)-based I2M framework that offers high interpretability and low
computational cost. Specifically, we utilize ABC notation to bridge the text
and music modalities, enabling the VLM to generate music using natural
language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and
self-refinement techniques to allow the VLM to produce high-quality music
without external training. Furthermore, we leverage the generated motivations
in text and the attention maps from the VLM to provide explanations for the
generated results in both text and image modalities. To validate our method, we
conduct both human studies and machine evaluations, where our method
outperforms others in terms of music quality and music-image consistency,
indicating promising results. Our code is available at
https://github.com/RS2002/Image2Music .
\\ ( https://arxiv.org/abs/2509.22378 ,  1286kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22387 (*cross-listing*)
Date: Fri, 26 Sep 2025 14:15:44 GMT   (23kb)

Title: SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly
Authors: Narada Maugin, Tristan Cazenave
Categories: cs.LG cs.AI cs.GT
Comments: Accepted at Advances in Computer Games (ACG) 2025, LNCS (Springer)
\\
  The Counterfactual Regret Minimization (CFR) algorithm and its variants have
enabled the development of pokerbots capable of beating the best human players
in heads-up (1v1) cash games and competing with them in six-player formats.
However, CFR's computational complexity rises exponentially with the number of
players. Furthermore, in games with three or more players, following Nash
equilibrium no longer guarantees a non-losing outcome. These limitations, along
with others, significantly restrict the applicability of CFR to the most
popular formats: tournaments. Motivated by the recent success of Large Language
Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored
to Spin & Go, a popular three-player online poker format. SpinGPT is trained in
two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions;
(2) Reinforcement Learning on 270k solver-generated hands. Our results show
that SpinGPT matches the solver's actions in 78% of decisions (tolerant
accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100
versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest
that LLMs could be a new way to deal with multi-player imperfect-information
games like poker.
\\ ( https://arxiv.org/abs/2509.22387 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22394 (*cross-listing*)
Date: Fri, 26 Sep 2025 14:22:15 GMT   (980kb)

Title: Deep Learning-Based Cross-Anatomy CT Synthesis Using Adapted nnResU-Net
  with Anatomical Feature Prioritized Loss
Authors: Javier Sequeiro Gonz\'alez, Arthur Longuefosse, Miguel D\'iaz Benito,
  \'Alvaro Garc\'ia Mart\'in and Fabien Baldacci
Categories: eess.IV cs.AI cs.CV
ACM-class: I.2; J.3
\\
  We present a patch-based 3D nnUNet adaptation for MR to CT and CBCT to CT
image translation using the multicenter SynthRAD2025 dataset, covering head and
neck (HN), thorax (TH), and abdomen (AB) regions. Our approach leverages two
main network configurations: a standard UNet and a residual UNet, both adapted
from nnUNet for image synthesis. The Anatomical Feature-Prioritized (AFP) loss
was introduced, which compares multilayer features extracted from a compact
segmentation network trained on TotalSegmentator labels, enhancing
reconstruction of clinically relevant structures. Input volumes were normalized
per-case using zscore normalization for MRIs, and clipping plus dataset level
zscore normalization for CBCT and CT. Training used 3D patches tailored to each
anatomical region without additional data augmentation. Models were trained for
1000 and 1500 epochs, with AFP fine-tuning performed for 500 epochs using a
combined L1+AFP objective. During inference, overlapping patches were
aggregated via mean averaging with step size of 0.3, and postprocessing
included reverse zscore normalization. Both network configurations were applied
across all regions, allowing consistent model design while capturing local
adaptations through residual learning and AFP loss. Qualitative and
quantitative evaluation revealed that residual networks combined with AFP
yielded sharper reconstructions and improved anatomical fidelity, particularly
for bone structures in MR to CT and lesions in CBCT to CT, while L1only
networks achieved slightly better intensity-based metrics. This methodology
provides a stable solution for cross modality medical image synthesis,
demonstrating the effectiveness of combining the automatic nnUNet pipeline with
residual learning and anatomically guided feature losses.
\\ ( https://arxiv.org/abs/2509.22394 ,  980kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22418 (*cross-listing*)
Date: Fri, 26 Sep 2025 14:39:44 GMT   (14944kb)

Title: Partial Parameter Updates for Efficient Distributed Training
Authors: Anastasiia Filippova, Angelos Katharopoulos, David Grangier, Ronan
  Collobert
Categories: cs.LG cs.AI
\\
  We introduce a memory- and compute-efficient method for low-communication
distributed training. Existing methods reduce communication by performing
multiple local updates between infrequent global synchronizations. We
demonstrate that their efficiency can be significantly improved by restricting
backpropagation: instead of updating all the parameters, each node updates only
a fixed subset while keeping the remainder frozen during local steps. This
constraint substantially reduces peak memory usage and training FLOPs, while a
full forward pass over all parameters eliminates the need for cross-node
activation exchange. Experiments on a $1.3$B-parameter language model trained
across $32$ nodes show that our method matches the perplexity of prior
low-communication approaches under identical token and bandwidth budgets while
reducing training FLOPs and peak memory.
\\ ( https://arxiv.org/abs/2509.22418 ,  14944kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22434 (*cross-listing*)
Date: Fri, 26 Sep 2025 14:53:08 GMT   (137kb)

Title: An Ontology for Unified Modeling of Tasks, Actions, Environments, and
  Capabilities in Personal Service Robotics
Authors: Margherita Martorana, Francesca Urgese, Ilaria Tiddi, Stefan Schlobach
Categories: cs.RO cs.AI
\\
  Personal service robots are increasingly used in domestic settings to assist
older adults and people requiring support. Effective operation involves not
only physical interaction but also the ability to interpret dynamic
environments, understand tasks, and choose appropriate actions based on
context. This requires integrating both hardware components (e.g. sensors,
actuators) and software systems capable of reasoning about tasks, environments,
and robot capabilities. Frameworks such as the Robot Operating System (ROS)
provide open-source tools that help connect low-level hardware with
higher-level functionalities. However, real-world deployments remain tightly
coupled to specific platforms. As a result, solutions are often isolated and
hard-coded, limiting interoperability, reusability, and knowledge sharing.
Ontologies and knowledge graphs offer a structured way to represent tasks,
environments, and robot capabilities. Existing ontologies, such as the
Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for
Linguistic and Cognitive Engineering (DOLCE), provide models for activities,
spatial relationships, and reasoning structures. However, they often focus on
specific domains and do not fully capture the connection between environment,
action, robot capabilities, and system-level integration. In this work, we
propose the Ontology for roBOts and acTions (OntoBOT), which extends existing
ontologies to provide a unified representation of tasks, actions, environments,
and capabilities. Our contributions are twofold: (1) we unify these aspects
into a cohesive ontology to support formal reasoning about task execution, and
(2) we demonstrate its generalizability by evaluating competency questions
across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how
OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge
sharing in service robotics.
\\ ( https://arxiv.org/abs/2509.22434 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22436 (*cross-listing*)
Date: Fri, 26 Sep 2025 14:54:48 GMT   (4006kb)

Title: Global Convergence in Neural ODEs: Impact of Activation Functions
Authors: Tianxiang Gao, Siyuan Sun, Hailiang Liu, Hongyang Gao
Categories: cs.LG cs.AI stat.ML
Comments: ICLR 2025 (Oral)
\\
  Neural Ordinary Differential Equations (ODEs) have been successful in various
applications due to their continuous nature and parameter-sharing efficiency.
However, these unique characteristics also introduce challenges in training,
particularly with respect to gradient computation accuracy and convergence
analysis. In this paper, we address these challenges by investigating the
impact of activation functions. We demonstrate that the properties of
activation functions, specifically smoothness and nonlinearity, are critical to
the training dynamics. Smooth activation functions guarantee globally unique
solutions for both forward and backward ODEs, while sufficient nonlinearity is
essential for maintaining the spectral properties of the Neural Tangent Kernel
(NTK) during training. Together, these properties enable us to establish the
global convergence of Neural ODEs under gradient descent in overparameterized
regimes. Our theoretical findings are validated by numerical experiments, which
not only support our analysis but also provide practical guidelines for scaling
Neural ODEs, potentially leading to faster training and improved performance in
real-world applications.
\\ ( https://arxiv.org/abs/2509.22436 ,  4006kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22442 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:02:05 GMT   (18613kb)

Title: Learning to Ball: Composing Policies for Long-Horizon Basketball Moves
Authors: Pei Xu, Zhen Wu, Ruocheng Wang, Vishnu Sarukkai, Kayvon Fatahalian,
  Ioannis Karamouzas, Victor Zordan, C. Karen Liu
Categories: cs.GR cs.AI cs.LG cs.RO
Comments: ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2025).
  Website: http://pei-xu.github.io/basketball. Video:
  https://youtu.be/2RBFIjjmR2I. Code: https://github.com/xupei0610/basketball
Journal-ref: ACM Transactions on Graphics (December 2025)
DOI: 10.1145/3763367
\\
  Learning a control policy for a multi-phase, long-horizon task, such as
basketball maneuvers, remains challenging for reinforcement learning approaches
due to the need for seamless policy composition and transitions between skills.
A long-horizon task typically consists of distinct subtasks with well-defined
goals, separated by transitional subtasks with unclear goals but critical to
the success of the entire task. Existing methods like the mixture of experts
and skill chaining struggle with tasks where individual policies do not share
significant commonly explored states or lack well-defined initial and terminal
states between different phases. In this paper, we introduce a novel policy
integration framework to enable the composition of drastically different motor
skills in multi-phase long-horizon tasks with ill-defined intermediate states.
Based on that, we further introduce a high-level soft router to enable seamless
and robust transitions between the subtasks. We evaluate our framework on a set
of fundamental basketball skills and challenging transitions. Policies trained
by our approach can effectively control the simulated character to interact
with the ball and accomplish the long-horizon task specified by real-time user
commands, without relying on ball trajectory references.
\\ ( https://arxiv.org/abs/2509.22442 ,  18613kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22445 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:02:24 GMT   (246kb)

Title: Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal
  Description Length Objectives for Transformers
Authors: Peter Shaw, James Cohan, Jacob Eisenstein, Kristina Toutanova
Categories: cs.LG cs.AI cs.CL
\\
  The Minimum Description Length (MDL) principle offers a formal framework for
applying Occam's razor in machine learning. However, its application to neural
networks such as Transformers is challenging due to the lack of a principled,
universal measure for model complexity. This paper introduces the theoretical
notion of asymptotically optimal description length objectives, grounded in the
theory of Kolmogorov complexity. We establish that a minimizer of such an
objective achieves optimal compression, for any dataset, up to an additive
constant, in the limit as model resource bounds increase. We prove that
asymptotically optimal objectives exist for Transformers, building on a new
demonstration of their computational universality. We further show that such
objectives can be tractable and differentiable by constructing and analyzing a
variational objective based on an adaptive Gaussian mixture prior. Our
empirical analysis shows that this variational objective selects for a
low-complexity solution with strong generalization on an algorithmic task, but
standard optimizers fail to find such solutions from a random initialization,
highlighting key optimization challenges. More broadly, by providing a
theoretical framework for identifying description length objectives with strong
asymptotic guarantees, we outline a potential path towards training neural
networks that achieve greater compression and generalization.
\\ ( https://arxiv.org/abs/2509.22445 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22458 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:09:26 GMT   (1272kb)

Title: Physics-informed GNN for medium-high voltage AC power flow with
  edge-aware attention and line search correction operator
Authors: Changhun Kim, Timon Conrad, Redwanul Karim, Julian Oelhaf, David
  Riebesel, Tom\'as Arias-Vergara, Andreas Maier, Johann J\"ager, Siming Bayer
Categories: cs.LG cs.AI
Comments: 5 pages, 2 figures. Submitted to ICASSP 2026. Code available at
  https://github.com/Kimchangheon/PIGNN-Attn-LS
\\
  Physics-informed graph neural networks (PIGNNs) have emerged as fast AC
power-flow solvers that can replace classic Newton--Raphson (NR) solvers,
especially when thousands of scenarios must be evaluated. However, current
PIGNNs still need accuracy improvements at parity speed; in particular, the
physics loss is inoperative at inference, which can deter operational adoption.
We address this with PIGNN-Attn-LS, combining an edge-aware attention mechanism
that explicitly encodes line physics via per-edge biases, capturing the grid's
anisotropy, with a backtracking line-search-based globalized correction
operator that restores an operative decrease criterion at inference. Training
and testing use a realistic High-/Medium-Voltage scenario generator, with NR
used only to construct reference states. On held-out HV cases consisting of
4--32-bus grids, PIGNN-Attn-LS achieves a test RMSE of 0.00033 p.u. in voltage
and 0.08$^\circ$ in angle, outperforming the PIGNN-MLP baseline by 99.5\% and
87.1\%, respectively. With streaming micro-batches, it delivers 2--5$\times$
faster batched inference than NR on 4--1024-bus grids.
\\ ( https://arxiv.org/abs/2509.22458 ,  1272kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22461 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:12:46 GMT   (2625kb)

Title: MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark
Authors: Hui Li, Changhao Jiang, Hongyu Wang, Ming Zhang, Jiajun Sun, Zhixiong
  Yang, Yifei Cao, Shihan Dou, Xiaoran Fan, Baoyu Fan, Tao Ji, Tao Gui, Qi
  Zhang, Xuanjing Huang
Categories: cs.SD cs.AI cs.CL eess.AS
Comments: 25 pages, 7 figures
\\
  The ability to reason from audio, including speech, paralinguistic cues,
environmental sounds, and music, is essential for AI agents to interact
effectively in real-world scenarios. Existing benchmarks mainly focus on static
or single-scene settings and do not fully capture scenarios where multiple
speakers, unfolding events, and heterogeneous audio sources interact. To
address these challenges, we introduce MDAR, a benchmark for evaluating models
on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR
comprises 3,000 carefully curated question-answer pairs linked to diverse audio
clips, covering five categories of complex reasoning and spanning three
question types. We benchmark 26 state-of-the-art audio language models on MDAR
and observe that they exhibit limitations in complex reasoning tasks. On
single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy,
whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio
substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice
and open-ended tasks. Across all three question types, no model achieves 80%
performance. These findings underscore the unique challenges posed by MDAR and
its value as a benchmark for advancing audio reasoning research.Code and
benchmark can be found at https://github.com/luckyerr/MDAR.
\\ ( https://arxiv.org/abs/2509.22461 ,  2625kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22468 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:16:20 GMT   (1869kb)

Title: Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised
  Molecular Graph Pretraining
Authors: Boshra Ariguib, Mathias Niepert, Andrei Manolache
Categories: cs.LG cs.AI
\\
  High-quality molecular representations are essential for property prediction
and molecular design, yet large labeled datasets remain scarce. While
self-supervised pretraining on molecular graphs has shown promise, many
existing approaches either depend on hand-crafted augmentations or complex
generative objectives, and often rely solely on 2D topology, leaving valuable
3D structural information underutilized. To address this gap, we introduce
C-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework
that integrates 2D graphs with ensembles of 3D conformers. C-FREE learns
molecular representations by predicting subgraph embeddings from their
complementary neighborhoods in the latent space, using fixed-radius ego-nets as
modeling units across different conformers. This design allows us to integrate
both geometric and topological information within a hybrid Graph Neural Network
(GNN)-Transformer backbone, without negatives, positional encodings, or
expensive pre-processing. Pretraining on the GEOM dataset, which provides rich
3D conformational diversity, C-FREE achieves state-of-the-art results on
MoleculeNet, surpassing contrastive, generative, and other multimodal
self-supervised methods. Fine-tuning across datasets with diverse sizes and
molecule types further demonstrates that pretraining transfers effectively to
new chemical domains, highlighting the importance of 3D-informed molecular
representations.
\\ ( https://arxiv.org/abs/2509.22468 ,  1869kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22483 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:31:32 GMT   (1087kb)

Title: OFMU: Optimization-Driven Framework for Machine Unlearning
Authors: Sadia Asif, Mohammad Mohammadi Amiri
Categories: cs.LG cs.AI
Comments: Under review at ICLR 2026
ACM-class: I.2.6; I.2.7
\\
  Large language models deployed in sensitive applications increasingly require
the ability to unlearn specific knowledge, such as user requests, copyrighted
materials, or outdated information, without retraining from scratch to ensure
regulatory compliance, user privacy, and safety. This task, known as machine
unlearning, aims to remove the influence of targeted data (forgetting) while
maintaining performance on the remaining data (retention). A common approach is
to formulate this as a multi-objective problem and reduce it to a
single-objective problem via scalarization, where forgetting and retention
losses are combined using a weighted sum. However, this often results in
unstable training dynamics and degraded model utility due to conflicting
gradient directions. To address these challenges, we propose OFMU, a
penalty-based bi-level optimization framework that explicitly prioritizes
forgetting while preserving retention through a hierarchical structure. Our
method enforces forgetting via an inner maximization step that incorporates a
similarity-aware penalty to decorrelate the gradients of the forget and
retention objectives, and restores utility through an outer minimization step.
To ensure scalability, we develop a two-loop algorithm with provable
convergence guarantees under both convex and non-convex regimes. We further
provide a rigorous theoretical analysis of convergence rates and show that our
approach achieves better trade-offs between forgetting efficacy and model
utility compared to prior methods. Extensive experiments across vision and
language benchmarks demonstrate that OFMU consistently outperforms existing
unlearning methods in both forgetting efficacy and retained utility.
\\ ( https://arxiv.org/abs/2509.22483 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22484 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:31:34 GMT   (2271kb)

Title: A Machine Learning Pipeline for Multiple Sclerosis Biomarker Discovery:
  Comparing explainable AI and Traditional Statistical Approaches
Authors: Samuele Punzo, Silvia Giulia Galfr\`e, Francesco Massafra, Alessandro
  Maglione, Corrado Priami, Alina S\^irbu
Categories: cs.LG cs.AI
Comments: Short paper presented at the 20th conference on Computational
  Intelligence methods for Bioinformatics and Biostatistics (CIBB2025)
\\
  We present a machine learning pipeline for biomarker discovery in Multiple
Sclerosis (MS), integrating eight publicly available microarray datasets from
Peripheral Blood Mononuclear Cells (PBMC). After robust preprocessing we
trained an XGBoost classifier optimized via Bayesian search. SHapley Additive
exPlanations (SHAP) were used to identify key features for model prediction,
indicating thus possible biomarkers. These were compared with genes identified
through classical Differential Expression Analysis (DEA). Our comparison
revealed both overlapping and unique biomarkers between SHAP and DEA,
suggesting complementary strengths. Enrichment analysis confirmed the
biological relevance of SHAP-selected genes, linking them to pathways such as
sphingolipid signaling, Th1/Th2/Th17 cell differentiation, and Epstein-Barr
virus infection all known to be associated with MS. This study highlights the
value of combining explainable AI (xAI) with traditional statistical methods to
gain deeper insights into disease mechanism.
\\ ( https://arxiv.org/abs/2509.22484 ,  2271kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22493 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:37:47 GMT   (1487kb)

Title: Ontological foundations for contrastive explanatory narration of robot
  plans
Authors: Alberto Olivares-Alarcos, Sergi Foix, J\'ulia Borr\`as, Gerard Canal
  and Guillem Aleny\`a
Categories: cs.RO cs.AI cs.IR cs.LO
Comments: This version was submitted to the journal Information Sciences and is
  under review since October 2024
\\
  Mutual understanding of artificial agents' decisions is key to ensuring a
trustworthy and successful human-robot interaction. Hence, robots are expected
to make reasonable decisions and communicate them to humans when needed. In
this article, the focus is on an approach to modeling and reasoning about the
comparison of two competing plans, so that robots can later explain the
divergent result. First, a novel ontological model is proposed to formalize and
reason about the differences between competing plans, enabling the
classification of the most appropriate one (e.g., the shortest, the safest, the
closest to human preferences, etc.). This work also investigates the
limitations of a baseline algorithm for ontology-based explanatory narration.
To address these limitations, a novel algorithm is presented, leveraging
divergent knowledge between plans and facilitating the construction of
contrastive narratives. Through empirical evaluation, it is observed that the
explanations excel beyond the baseline method.
\\ ( https://arxiv.org/abs/2509.22493 ,  1487kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22505 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:47:37 GMT   (1039kb)

Title: Mental Health Impacts of AI Companions: Triangulating Social Media
  Quasi-Experiments, User Perspectives, and Relational Theory
Authors: Yunhao Yuan, Jiaxun Zhang, Talayeh Aledavood, Renwen Zhang, and
  Koustuv Saha
Categories: cs.HC cs.AI cs.CL cs.CY stat.AP
\\
  AI-powered companion chatbots (AICCs) such as Replika are increasingly
popular, offering empathetic interactions, yet their psychosocial impacts
remain unclear. We examined how engaging with AICCs shaped wellbeing and how
users perceived these experiences. First, we conducted a large-scale
quasi-experimental study of longitudinal Reddit data, applying stratified
propensity score matching and Difference-in-Differences regression. Findings
revealed mixed effects -- greater affective and grief expression, readability,
and interpersonal focus, alongside increases in language about loneliness and
suicidal ideation. Second, we complemented these results with 15
semi-structured interviews, which we thematically analyzed and contextualized
using Knapp's relationship development model. We identified trajectories of
initiation, escalation, and bonding, wherein AICCs provided emotional
validation and social rehearsal but also carried risks of over-reliance and
withdrawal. Triangulating across methods, we offer design implications for AI
companions that scaffold healthy boundaries, support mindful engagement,
support disclosure without dependency, and surface relationship stages --
maximizing psychosocial benefits while mitigating risks.
\\ ( https://arxiv.org/abs/2509.22505 ,  1039kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22545 (*cross-listing*)
Date: Fri, 26 Sep 2025 16:21:24 GMT   (2837kb)

Title: Does AI Coaching Prepare us for Workplace Negotiations?
Authors: Veda Duddu, Jash Rajesh Parekh, Andy Mao, Hanyi Min, Ziang Xiao,
  Vedant Das Swain, and Koustuv Saha
Categories: cs.HC cs.AI cs.CL cs.CY
\\
  Workplace negotiations are undermined by psychological barriers, which can
even derail well-prepared tactics. AI offers personalized and always --
available negotiation coaching, yet its effectiveness for negotiation
preparedness remains unclear. We built Trucey, a prototype AI coach grounded in
Brett's negotiation model. We conducted a between-subjects experiment (N=267),
comparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by
in-depth interviews (N=15). While Trucey showed the strongest reductions in
fear relative to both comparison conditions, the Handbook outperformed both AIs
in usability and psychological empowerment. Interviews revealed that the
Handbook's comprehensive, reviewable content was crucial for participants'
confidence and preparedness. In contrast, although participants valued AI's
rehearsal capability, its guidance often felt verbose and fragmented --
delivered in bits and pieces that required additional effort -- leaving them
uncertain or overwhelmed. These findings challenge assumptions of AI
superiority and motivate hybrid designs that integrate structured,
theory-driven content with targeted rehearsal, clear boundaries, and adaptive
scaffolds to address psychological barriers and support negotiation
preparedness.
\\ ( https://arxiv.org/abs/2509.22545 ,  2837kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22551 (*cross-listing*)
Date: Fri, 26 Sep 2025 16:32:41 GMT   (257kb)

Title: ConQuER: Modular Architectures for Control and Bias Mitigation in IQP
  Quantum Generative Models
Authors: Xiaocheng Zou, Shijin Duan, Charles Fleming, Gaowen Liu, Ramana Rao
  Kompella, Shaolei Ren, Xiaolin Xu
Categories: quant-ph cs.AI cs.LG
\\
  Quantum generative models based on instantaneous quantum polynomial (IQP)
circuits show great promise in learning complex distributions while maintaining
classical trainability. However, current implementations suffer from two key
limitations: lack of controllability over generated outputs and severe
generation bias towards certain expected patterns. We present a Controllable
Quantum Generative Framework, ConQuER, which addresses both challenges through
a modular circuit architecture. ConQuER embeds a lightweight controller circuit
that can be directly combined with pre-trained IQP circuits to precisely
control the output distribution without full retraining. Leveraging the
advantages of IQP, our scheme enables precise control over properties such as
the Hamming Weight distribution with minimal parameter and gate overhead. In
addition, inspired by the controller design, we extend this modular approach
through data-driven optimization to embed implicit control paths in the
underlying IQP architecture, significantly reducing generation bias on
structured datasets. ConQuER retains efficient classical training properties
and high scalability. We experimentally validate ConQuER on multiple quantum
state datasets, demonstrating its superior control accuracy and balanced
generation performance, only with very low overhead cost over original IQP
circuits. Our framework bridges the gap between the advantages of quantum
computing and the practical needs of controllable generation modeling.
\\ ( https://arxiv.org/abs/2509.22551 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22562 (*cross-listing*)
Date: Fri, 26 Sep 2025 16:41:47 GMT   (5954kb)

Title: Activation Function Design Sustains Plasticity in Continual Learning
Authors: Lute Lillo and Nick Cheney
Categories: cs.LG cs.AI cs.CV
\\
  In independent, identically distributed (i.i.d.) training regimes, activation
functions have been benchmarked extensively, and their differences often shrink
once model size and optimization are tuned. In continual learning, however, the
picture is different: beyond catastrophic forgetting, models can progressively
lose the ability to adapt (referred to as loss of plasticity) and the role of
the non-linearity in this failure mode remains underexplored. We show that
activation choice is a primary, architecture-agnostic lever for mitigating
plasticity loss. Building on a property-level analysis of negative-branch shape
and saturation behavior, we introduce two drop-in nonlinearities (Smooth-Leaky
and Randomized Smooth-Leaky) and evaluate them in two complementary settings:
(i) supervised class-incremental benchmarks and (ii) reinforcement learning
with non-stationary MuJoCo environments designed to induce controlled
distribution and dynamics shifts. We also provide a simple stress protocol and
diagnostics that link the shape of the activation to the adaptation under
change. The takeaway is straightforward: thoughtful activation design offers a
lightweight, domain-general way to sustain plasticity in continual learning
without extra capacity or task-specific tuning.
\\ ( https://arxiv.org/abs/2509.22562 ,  5954kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22566 (*cross-listing*)
Date: Fri, 26 Sep 2025 16:42:52 GMT   (21086kb)

Title: From Parameters to Behavior: Unsupervised Compression of the Policy
  Space
Authors: Davide Tenedini, Riccardo Zamboni, Mirco Mutti, Marcello Restelli
Categories: cs.LG cs.AI
\\
  Despite its recent successes, Deep Reinforcement Learning (DRL) is
notoriously sample-inefficient. We argue that this inefficiency stems from the
standard practice of optimizing policies directly in the high-dimensional and
highly redundant parameter space $\Theta$. This challenge is greatly compounded
in multi-task settings. In this work, we develop a novel, unsupervised approach
that compresses the policy parameter space $\Theta$ into a low-dimensional
latent space $\mathcal{Z}$. We train a generative model
$g:\mathcal{Z}\to\Theta$ by optimizing a behavioral reconstruction loss, which
ensures that the latent space is organized by functional similarity rather than
proximity in parameterization. We conjecture that the inherent dimensionality
of this manifold is a function of the environment's complexity, rather than the
size of the policy network. We validate our approach in continuous control
domains, showing that the parameterization of standard policy networks can be
compressed up to five orders of magnitude while retaining most of its
expressivity. As a byproduct, we show that the learned manifold enables
task-specific adaptation via Policy Gradient operating in the latent space
$\mathcal{Z}$.
\\ ( https://arxiv.org/abs/2509.22566 ,  21086kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22601 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:20:38 GMT   (33557kb)

Title: Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive
  Exploration for Agentic Reinforcement Learning
Authors: Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li,
  Zihan Xu, Yuchen Shi, Siqi Cai, Renting Rui, Shaofei Cai, Yuzheng Cai, Xuan
  Zhang, Sheng Ye, Ke Li, Xing Sun
Categories: cs.LG cs.AI cs.CL cs.CV cs.MA
Comments: 26 pages, 11 figures
\\
  Reinforcement learning (RL) is the dominant paradigm for sharpening strategic
tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,
yet it faces a fundamental challenge of exploration-exploitation trade-off.
Existing studies stimulate exploration through the lens of policy entropy, but
such mechanical entropy maximization is prone to RL training instability due to
the multi-turn distribution shifting. In this paper, we target the progressive
exploration-exploitation balance under the guidance of the agent own
experiences without succumbing to either entropy collapsing or runaway
divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)
recipe for training agentic LLMs. It extends the vanilla SIL framework, where a
replay buffer stores self-generated promising trajectories for off-policy
update, by gradually steering the policy evolution within a well-balanced range
of entropy across stages. Specifically, our approach incorporates a curriculum
to manage the exploration process, utilizing intrinsic rewards to foster
skill-level exploration and facilitating action-level exploration through SIL.
At first, the auxiliary tool call reward plays a critical role in the
accumulation of tool-use skills, enabling broad exposure to the unfamiliar
distributions of the environment feedback with an upward entropy trend. As
training progresses, self-imitation gets strengthened to exploit existing
successful patterns from replayed experiences for comparative action-level
exploration, accelerating solution iteration without unbounded entropy growth.
To further stabilize training, we recalibrate the advantages of experiences in
the replay buffer to address the potential policy drift. Reugularizations such
as the clipping of tokens with high covariance between probability and
advantage are introduced to the trajectory-level entropy control to curb
over-confidence.
\\ ( https://arxiv.org/abs/2509.22601 ,  33557kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22611 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:37:52 GMT   (389kb)

Title: Quantile Advantage Estimation for Entropy-Safe Reasoning
Authors: Junkang Wu, Kexin Huang, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He
Categories: cs.LG cs.AI
\\
  Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM
reasoning, but training often oscillates between {entropy collapse} and
{entropy explosion}. We trace both hazards to the mean baseline used in
value-free RL (e.g., GRPO and DAPO), which improperly penalizes
negative-advantage samples under reward outliers. We propose {Quantile
Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile
baseline. QAE induces a response-level, two-regime gate: on hard queries (p <=
1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it
targets remaining failures. Under first-order softmax updates, we prove
{two-sided entropy safety}, giving lower and upper bounds on one-step entropy
change that curb explosion and prevent collapse. Empirically, this minimal
modification stabilizes entropy, sparsifies credit assignment (with tuned K,
roughly 80% of responses receive zero advantage), and yields sustained pass@1
gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results
identify {baseline design} -- rather than token-level heuristics -- as the
primary mechanism for scaling RLVR.
\\ ( https://arxiv.org/abs/2509.22611 ,  389kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22621 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:46:32 GMT   (2222kb)

Title: IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning
Authors: Aayush Mishra, Daniel Khashabi, Anqi Liu
Categories: cs.LG cs.AI cs.CL
\\
  Supervised Fine-Tuning (SFT) is used to specialize model behavior by training
weights to produce intended target responses for queries. In contrast,
In-Context Learning (ICL) adapts models during inference with instructions or
demonstrations in the prompt. ICL can offer better generalizability and more
calibrated responses compared to SFT in data scarce settings, at the cost of
more inference compute. In this work, we ask the question: Can ICL's internal
computations be used to improve the qualities of SFT? We first show that ICL
and SFT produce distinct activation patterns, indicating that the two methods
achieve adaptation through different functional mechanisms. Motivated by this
observation and to use ICL's rich functionality, we introduce ICL Activation
Alignment (IA2), a self-distillation technique which aims to replicate ICL's
activation patterns in SFT models and incentivizes ICL-like internal reasoning.
Performing IA2 as a priming step before SFT significantly improves the accuracy
and calibration of model outputs, as shown by our extensive empirical results
on 12 popular benchmarks and 2 model families. This finding is not only
practically useful, but also offers a conceptual window into the inner
mechanics of model adaptation.
\\ ( https://arxiv.org/abs/2509.22621 ,  2222kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22623 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:48:56 GMT   (66kb)

Title: A Theoretical Analysis of Discrete Flow Matching Generative Models
Authors: Maojiang Su, Mingcheng Lu, Jerry Yao-Chieh Hu, Shang Wu, Zhao Song,
  Alex Reneau, Han Liu
Categories: cs.LG cs.AI stat.ML
\\
  We provide a theoretical analysis for end-to-end training Discrete Flow
Matching (DFM) generative models. DFM is a promising discrete generative
modeling framework that learns the underlying generative dynamics by training a
neural network to approximate the transformative velocity field. Our analysis
establishes a clear chain of guarantees by decomposing the final distribution
estimation error. We first prove that the total variation distance between the
generated and target distributions is controlled by the risk of the learned
velocity field. We then bound this risk by analyzing its two primary sources:
(i) Approximation Error, where we quantify the capacity of the Transformer
architecture to represent the true velocity, and (ii) Estimation Error, where
we derive statistical convergence rates that bound the error from training on a
finite dataset. By composing these results, we provide the first formal proof
that the distribution generated by a trained DFM model provably converges to
the true data distribution as the training set size increases.
\\ ( https://arxiv.org/abs/2509.22623 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22626 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:51:26 GMT   (454kb)

Title: Learning Admissible Heuristics for A*: Theory and Practice
Authors: Ehsan Futuhi, Nathan R. Sturtevant
Categories: cs.LG cs.AI
\\
  Heuristic functions are central to the performance of search algorithms such
as A-star, where admissibility - the property of never overestimating the true
shortest-path cost - guarantees solution optimality. Recent deep learning
approaches often disregard admissibility and provide limited guarantees on
generalization beyond the training data. This paper addresses both of these
limitations. First, we pose heuristic learning as a constrained optimization
problem and introduce Cross-Entropy Admissibility (CEA), a loss function that
enforces admissibility during training. On the Rubik's Cube domain, this method
yields near-admissible heuristics with significantly stronger guidance than
compressed pattern database (PDB) heuristics. Theoretically, we study the
sample complexity of learning heuristics. By leveraging PDB abstractions and
the structural properties of graphs such as the Rubik's Cube, we tighten the
bound on the number of training samples needed for A-star to generalize.
Replacing a general hypothesis class with a ReLU neural network gives bounds
that depend primarily on the network's width and depth, rather than on graph
size. Using the same network, we also provide the first generalization
guarantees for goal-dependent heuristics.
\\ ( https://arxiv.org/abs/2509.22626 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22633 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:57:17 GMT   (26kb)

Title: Towards Efficient Online Exploration for Reinforcement Learning with
  Human Feedback
Authors: Gen Li, Yuling Yan
Categories: stat.ML cs.AI cs.CL cs.LG math.ST stat.TH
\\
  Reinforcement learning with human feedback (RLHF), which learns a reward
model from human preference data and then optimizes a policy to favor preferred
responses, has emerged as a central paradigm for aligning large language models
(LLMs) with human preferences. In this paper, we investigate exploration
principles for online RLHF, where one seeks to adaptively collect new
preference data to refine both the reward model and the policy in a
data-efficient manner. By examining existing optimism-based exploration
algorithms, we identify a drawback in their sampling protocol: they tend to
gather comparisons that fail to reduce the most informative uncertainties in
reward differences, and we prove lower bounds showing that such methods can
incur linear regret over exponentially long horizons. Motivated by this
insight, we propose a new exploration scheme that directs preference queries
toward reducing uncertainty in reward differences most relevant to policy
improvement. Under a multi-armed bandit model of RLHF, we establish regret
bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta>0$ is a hyperparameter
that balances reward maximization against mitigating distribution shift. To our
knowledge, this is the first online RLHF algorithm with regret scaling
polynomially in all model parameters.
\\ ( https://arxiv.org/abs/2509.22633 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22649 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:59:57 GMT   (2133kb)

Title: Toward a Physics of Deep Learning and Brains
Authors: Arsham Ghavasieh, Meritxell Vila-Minana, Akanksha Khurd, John Beggs,
  Gerardo Ortiz, Santo Fortunato
Categories: cond-mat.dis-nn cond-mat.stat-mech cs.AI nlin.AO physics.bio-ph
\\
  Deep neural networks and brains both learn and share superficial
similarities: processing nodes are likened to neurons and adjustable weights
are likened to modifiable synapses. But can a unified theoretical framework be
found to underlie them both? Here we show that the equations used to describe
neuronal avalanches in living brains can also be applied to cascades of
activity in deep neural networks. These equations are derived from
non-equilibrium statistical physics and show that deep neural networks learn
best when poised between absorbing and active phases. Because these networks
are strongly driven by inputs, however, they do not operate at a true critical
point but within a quasi-critical regime -- one that still approximately
satisfies crackling noise scaling relations. By training networks with
different initializations, we show that maximal susceptibility is a more
reliable predictor of learning than proximity to the critical point itself.
This provides a blueprint for engineering improved network performance.
Finally, using finite-size scaling we identify distinct universality classes,
including Barkhausen noise and directed percolation. This theoretical framework
demonstrates that universal features are shared by both biological and
artificial neural networks.
\\ ( https://arxiv.org/abs/2509.22649 ,  2133kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22653 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:59:59 GMT   (37954kb)

Title: See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned
  Aerial Navigation
Authors: Chih Yao Hu, Yang-Sen Lin, Yuna Lee, Chih-Hai Su, Jie-Ying Lee,
  Shr-Ruei Tsai, Chin-Yang Lin, Kuan-Wen Chen, Tsung-Wei Ke, Yu-Lun Liu
Categories: cs.RO cs.AI cs.CL cs.CV cs.LG
Comments: CoRL 2025. Project page: https://spf-web.pages.dev
\\
  We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev
\\ ( https://arxiv.org/abs/2509.22653 ,  37954kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21336 (*cross-listing*)
Date: Fri, 12 Sep 2025 06:12:59 GMT   (576kb)

Title: HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous
  Data Stores
Authors: Guohang Yan, Yue Zhang, Pinlong Cai, Ding Wang, Song Mao, Hongwei
  Zhang, Yaoze Zhang, Hairong Zhang, Xinyu Cai, Botian Shi
Categories: cs.IR cs.CL
Comments: 15 pages, 4 figures
\\
  Retrieval-augmented generation (RAG) has become a dominant paradigm for
mitigating knowledge hallucination and staleness in large language models
(LLMs) while preserving data security. By retrieving relevant evidence from
private, domain-specific corpora and injecting it into carefully engineered
prompts, RAG delivers trustworthy responses without the prohibitive cost of
fine-tuning. Traditional retrieval-augmented generation (RAG) systems are
text-only and often rely on a single storage backend, most commonly a vector
database. In practice, this monolithic design suffers from unavoidable
trade-offs: vector search captures semantic similarity yet loses global
context; knowledge graphs excel at relational precision but struggle with
recall; full-text indexes are fast and exact yet semantically blind; and
relational engines such as MySQL provide strong transactional guarantees but no
semantic understanding. We argue that these heterogeneous retrieval paradigms
are complementary, and propose a principled fusion scheme to orchestrate them
synergistically, mitigating the weaknesses of any single modality. In this work
we introduce HetaRAG, a hybrid, deep-retrieval augmented generation framework
that orchestrates cross-modal evidence from heterogeneous data stores. We plan
to design a system that unifies vector indices, knowledge graphs, full-text
engines, and structured databases into a single retrieval plane, dynamically
routing and fusing evidence to maximize recall, precision, and contextual
fidelity. To achieve this design goal, we carried out preliminary explorations
and constructed an initial RAG pipeline; this technical report provides a brief
overview. The partial code is available at
https://github.com/KnowledgeXLab/HetaRAG.
\\ ( https://arxiv.org/abs/2509.21336 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21403 (*cross-listing*)
Date: Wed, 24 Sep 2025 15:50:17 GMT   (190kb)

Title: LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?
Authors: Rushil Gupta, Jason Hartford, Bang Liu
Categories: cs.LG cs.CL
Comments: Accepted to EMNLP 2025
\\
  Large language models (LLMs) have recently been proposed as general-purpose
agents for experimental design, with claims that they can perform in-context
experimental design. We evaluate this hypothesis using both open- and
closed-source instruction-tuned LLMs applied to genetic perturbation and
molecular property discovery tasks. We find that LLM-based agents show no
sensitivity to experimental feedback: replacing true outcomes with randomly
permuted labels has no impact on performance. Across benchmarks, classical
methods such as linear bandits and Gaussian process optimization consistently
outperform LLM agents. We further propose a simple hybrid method, LLM-guided
Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with
nearest-neighbor sampling to guide the design of experiments. LLMNN achieves
competitive or superior performance across domains without requiring
significant in-context adaptation. These results suggest that current open- and
closed-source LLMs do not perform in-context experimental design in practice
and highlight the need for hybrid frameworks that decouple prior-based
reasoning from batch acquisition with updated posteriors.
\\ ( https://arxiv.org/abs/2509.21403 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21501 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:58:02 GMT   (4011kb)

Title: LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to
  Evaluate Agentic-AI-based Shopping Assistants?
Authors: Lu Sun, Shihan Fu, Bingsheng Yao, Yuxuan Lu, Wenbo Li, Hansu Gu, Jiri
  Gesi, Jing Huang, Chen Luo, Dakuo Wang
Categories: cs.HC cs.CL
\\
  Agentic AI is emerging, capable of executing tasks through natural language,
such as Copilot for coding or Amazon Rufus for shopping. Evaluating these
systems is challenging, as their rapid evolution outpaces traditional human
evaluation. Researchers have proposed LLM Agents to simulate participants as
digital twins, but it remains unclear to what extent a digital twin can
represent a specific customer in multi-turn interaction with an agentic AI
system. In this paper, we recruited 40 human participants to shop with Amazon
Rufus, collected their personas, interaction traces, and UX feedback, and then
created digital twins to repeat the task. Pairwise comparison of human and
digital-twin traces shows that while agents often explored more diverse
choices, their action patterns aligned with humans and yielded similar design
feedback. This study is the first to quantify how closely LLM agents can mirror
human multi-turn interaction with an agentic AI system, highlighting their
potential for scalable evaluation.
\\ ( https://arxiv.org/abs/2509.21501 ,  4011kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21514 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:06:02 GMT   (799kb)

Title: Uncertainty-Aware Knowledge Tracing Models
Authors: Joshua Mitton, Prarthana Bhattacharyya, Ralph Abboud, Simon Woodhead
Categories: cs.LG cs.CL
Comments: 10 pages, 7 figures. Joshua Mitton and Prarthana Bhattacharyya
  contributed equally to this paper
\\
  The main focus of research on Knowledge Tracing (KT) models is on model
developments with the aim of improving predictive accuracy. Most of these
models make the most incorrect predictions when students choose a distractor,
leading to student errors going undetected. We present an approach to add new
capabilities to KT models by capturing predictive uncertainty and demonstrate
that a larger predictive uncertainty aligns with model incorrect predictions.
We show that uncertainty in KT models is informative and that this signal would
be pedagogically useful for application in an educational learning platform
that can be used in a limited resource setting where understanding student
ability is necessary.
\\ ( https://arxiv.org/abs/2509.21514 ,  799kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21548 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:34:05 GMT   (1312kb)

Title: C-QUERI: Congressional Questions, Exchanges, and Responses in
  Institutions Dataset
Authors: Manjari Rudra and Daniel Magleby and Sujoy Sikdar
Categories: cs.CY cs.CL
\\
  Questions in political interviews and hearings serve strategic purposes
beyond information gathering including advancing partisan narratives and
shaping public perceptions. However, these strategic aspects remain
understudied due to the lack of large-scale datasets for studying such
discourse. Congressional hearings provide an especially rich and tractable site
for studying political questioning: Interactions are structured by formal
rules, witnesses are obliged to respond, and members with different political
affiliations are guaranteed opportunities to ask questions, enabling
comparisons of behaviors across the political spectrum.
  We develop a pipeline to extract question-answer pairs from unstructured
hearing transcripts and construct a novel dataset of committee hearings from
the 108th--117th Congress. Our analysis reveals systematic differences in
questioning strategies across parties, by showing the party affiliation of
questioners can be predicted from their questions alone. Our dataset and
methods not only advance the study of congressional politics, but also provide
a general framework for analyzing question-answering across interview-like
settings.
\\ ( https://arxiv.org/abs/2509.21548 ,  1312kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21579 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:56:13 GMT   (830kb)

Title: Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews
Authors: Mst Eshita Khatun, Halima Akter, Tasnimul Rehan, Toufiq Ahmed
Categories: cs.LG cs.CL
Comments: Accepted & presented at THE 16th INTERNATIONAL IEEE CONFERENCE ON
  COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT) 2025
Journal-ref: THE 16th INTERNATIONAL IEEE CONFERENCE ON COMPUTING, COMMUNICATION
  AND NETWORKING TECHNOLOGIES (ICCCNT) 2025
\\
  In this digital era, online shopping is common practice in our daily lives.
Product reviews significantly influence consumer buying behavior and help
establish buyer trust. However, the prevalence of fraudulent reviews undermines
this trust by potentially misleading consumers and damaging the reputations of
the sellers. This research addresses this pressing issue by employing advanced
big data analytics and machine learning approaches on a substantial dataset of
Amazon product reviews. The primary objective is to detect and classify spam
reviews accurately so that it enhances the authenticity of the review. Using a
scalable big data framework, we efficiently process and analyze a large scale
of review data, extracting key features indicative of fraudulent behavior. Our
study illustrates the utility of various machine learning classifiers in
detecting spam reviews, with Logistic Regression achieving an accuracy of
90.35%, thus contributing to a more trustworthy and transparent online shopping
environment.
\\ ( https://arxiv.org/abs/2509.21579 ,  830kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21597 (*cross-listing*)
Date: Thu, 25 Sep 2025 21:09:40 GMT   (2492kb)

Title: AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit
Authors: Yi Zhu, Heitor R. Guimar\~aes, Arthur Pimentel and Tiago Falk
Categories: eess.AS cs.CL cs.SD
\\
  With the prevalence of artificial intelligence (AI)-generated content, such
as audio deepfakes, a large body of recent work has focused on developing
deepfake detection techniques. However, most models are evaluated on a narrow
set of datasets, leaving their generalization to real-world conditions
uncertain. In this paper, we systematically review 28 existing audio deepfake
datasets and present an open-source benchmarking toolkit called AUDDT
(https://github.com/MuSAELab/AUDDT). The goal of this toolkit is to automate
the evaluation of pretrained detectors across these 28 datasets, giving users
direct feedback on the advantages and shortcomings of their deepfake detectors.
We start by showcasing the usage of the developed toolkit, the composition of
our benchmark, and the breakdown of different deepfake subgroups. Next, using a
widely adopted pretrained deepfake detector, we present in- and out-of-domain
detection results, revealing notable differences across conditions and audio
manipulation types. Lastly, we also analyze the limitations of these existing
datasets and their gap relative to practical deployment scenarios.
\\ ( https://arxiv.org/abs/2509.21597 ,  2492kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21793 (*cross-listing*)
Date: Fri, 26 Sep 2025 02:49:08 GMT   (48kb)

Title: Compiling by Proving: Language-Agnostic Automatic Optimization from
  Formal Semantics
Authors: Jianhong Zhao, Everett Hildenbrandt, Juan Conejero, and Yongwang Zhao
Categories: cs.PL cs.CL
\\
  Verification proofs encode complete program behavior, yet we discard them
after checking correctness. We present compiling by proving, a paradigm that
transforms these proofs into optimized execution rules. By constructing
All-Path Reachability Proofs through symbolic execution and compiling their
graph structure, we consolidate many semantic rewrites into single rules while
preserving correctness by construction. We implement this as a
language-agnostic extension to the K framework. Evaluation demonstrates
performance improvements across different compilation scopes: opcode-level
optimizations show consistent speedups, while whole-program compilation
achieves orders of magnitude greater performance gains.
\\ ( https://arxiv.org/abs/2509.21793 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21843 (*cross-listing*)
Date: Fri, 26 Sep 2025 04:03:53 GMT   (391kb)

Title: SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models
Authors: Jingkai Guo, Chaitali Chakrabarti, Deliang Fan
Categories: cs.CR cs.CL cs.LG
Comments: 10 pages, 4 figures, 5 tables, 2 equations. Topics: Bit-flip attacks,
  adversarial attacks, large language models (LLMs)
\\
  Model integrity of Large language models (LLMs) has become a pressing
security concern with their massive online deployment. Prior Bit-Flip Attacks
(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can
severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips
can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs
and reveal that, despite the intuition of better robustness from modularity and
redundancy, only a handful of adversarial bit flips can also cause LLMs'
catastrophic accuracy degradation. However, existing BFA methods typically
focus on either integer or floating-point models separately, limiting attack
flexibility. Moreover, in floating-point models, random bit flips often cause
perturbed parameters to extreme values (e.g., flipping in exponent bit), making
it not stealthy and leading to numerical runtime error (e.g., invalid tensor
values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky
Bit-Flip Attack), which collapses LLM performance with only one single bit flip
while keeping perturbed values within benign layer-wise weight distribution. It
is achieved through iterative searching and ranking through our defined
parameter sensitivity metric, ImpactScore, which combines gradient sensitivity
and perturbation range constrained by the benign layer-wise weight
distribution. A novel lightweight SKIP searching algorithm is also proposed to
greatly reduce searching complexity, which leads to successful SBFA searching
taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma
models, with only one single bit flip, SBFA successfully degrades accuracy to
below random levels on MMLU and SST-2 in both BF16 and INT8 data formats.
Remarkably, flipping a single bit out of billions of parameters reveals a
severe security concern of SOTA LLM models.
\\ ( https://arxiv.org/abs/2509.21843 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21868 (*cross-listing*)
Date: Fri, 26 Sep 2025 04:51:34 GMT   (2753kb)

Title: What Makes LLM Agent Simulations Useful for Policy? Insights From an
  Iterative Design Engagement in Emergency Preparedness
Authors: Yuxuan Li, Sauvik Das, Hirokazu Shirado
Categories: cs.HC cs.CL
\\
  There is growing interest in using Large Language Models as agents (LLM
agents) for social simulations to inform policy, yet real-world adoption
remains limited. This paper addresses the question: How can LLM agent
simulations be made genuinely useful for policy? We report on a year-long
iterative design engagement with a university emergency preparedness team.
Across multiple design iterations, we iteratively developed a system of 13,000
LLM agents that simulate crowd movement and communication during a large-scale
gathering under various emergency scenarios. These simulations informed actual
policy implementation, shaping volunteer training, evacuation protocols, and
infrastructure planning. Analyzing this process, we identify three design
implications: start with verifiable scenarios and build trust gradually, use
preliminary simulations to elicit tacit knowledge, and treat simulation and
policy development as evolving together. These implications highlight
actionable pathways to making LLM agent simulations that are genuinely useful
for policy.
\\ ( https://arxiv.org/abs/2509.21868 ,  2753kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21891 (*cross-listing*)
Date: Fri, 26 Sep 2025 05:28:22 GMT   (3158kb)

Title: AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans
Authors: Yangtian Zi, Zixuan Wu, Aleksander Boruch-Gruszecki, Jonathan Bell,
  Arjun Guha
Categories: cs.SE cs.CL
\\
  Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.
\\ ( https://arxiv.org/abs/2509.21891 ,  3158kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21949 (*cross-listing*)
Date: Fri, 26 Sep 2025 06:29:15 GMT   (312kb)

Title: Evaluating Open-Source Large Language Models for Technical Telecom
  Question Answering
Authors: Arina Caraus, Alessio Buscemi, Sumit Kumar, Ion Turcanu
Categories: cs.NI cs.CL
Comments: Accepted at the IEEE GLOBECOM Workshops 2025: "Large AI Model over
  Future Wireless Networks"
\\
  Large Language Models (LLMs) have shown remarkable capabilities across
various fields. However, their performance in technical domains such as
telecommunications remains underexplored. This paper evaluates two open-source
LLMs, Gemma 3 27B and DeepSeek R1 32B, on factual and reasoning-based questions
derived from advanced wireless communications material. We construct a
benchmark of 105 question-answer pairs and assess performance using lexical
metrics, semantic similarity, and LLM-as-a-judge scoring. We also analyze
consistency, judgment reliability, and hallucination through source attribution
and score variance. Results show that Gemma excels in semantic fidelity and
LLM-rated correctness, while DeepSeek demonstrates slightly higher lexical
consistency. Additional findings highlight current limitations in telecom
applications and the need for domain-adapted models to support trustworthy
Artificial Intelligence (AI) assistants in engineering.
\\ ( https://arxiv.org/abs/2509.21949 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22061 (*cross-listing*)
Date: Fri, 26 Sep 2025 08:43:25 GMT   (38kb)

Title: Speak Your Mind: The Speech Continuation Task as a Probe of Voice-Based
  Model Bias
Authors: Shree Harsha Bokkahalli Satish, Harm Lameris, Olivier Perrotin, Gustav
  Eje Henter, \'Eva Sz\'ekely
Categories: eess.AS cs.CL cs.SD
Comments: 6 pages, 1 figure, Submitted to IEEE ICASSP 2026
\\
  Speech Continuation (SC) is the task of generating a coherent extension of a
spoken prompt while preserving both semantic context and speaker identity.
Because SC is constrained to a single audio stream, it offers a more direct
setting for probing biases in speech foundation models than dialogue does. In
this work we present the first systematic evaluation of bias in SC,
investigating how gender and phonation type (breathy, creaky, end-creak) affect
continuation behaviour. We evaluate three recent models: SpiritLM (base and
expressive), VAE-GSLM, and SpeechGPT across speaker similarity, voice quality
preservation, and text-based bias metrics. Results show that while both speaker
similarity and coherence remain a challenge, textual evaluations reveal
significant model and gender interactions: once coherence is sufficiently high
(for VAE-GSLM), gender effects emerge on text-metrics such as agency and
sentence polarity. In addition, continuations revert toward modal phonation
more strongly for female prompts than for male ones, revealing a systematic
voice-quality bias. These findings highlight SC as a controlled probe of
socially relevant representational biases in speech foundation models, and
suggest that it will become an increasingly informative diagnostic as
continuation quality improves.
\\ ( https://arxiv.org/abs/2509.22061 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22202 (*cross-listing*)
Date: Fri, 26 Sep 2025 11:14:38 GMT   (85kb)

Title: Library Hallucinations in LLMs: Risk Analysis Grounded in Developer
  Queries
Authors: Lukas Twist, Jie M. Zhang, Mark Harman, Helen Yannakoudakis
Categories: cs.SE cs.CL
Comments: 23 pages, 5 tables
\\
  Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.
\\ ( https://arxiv.org/abs/2509.22202 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22325 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:23:01 GMT   (1348kb)

Title: Can Synthetic Query Rewrites Capture User Intent Better than Humans in
  Retrieval-Augmented Generation?
Authors: JiaYing Zheng, HaiNan Zhang, Liang Pang, YongXin Tong, ZhiMing Zheng
Categories: cs.IR cs.CL
Comments: 10 pages, 6 figures
\\
  Multi-turn RAG systems often face queries with colloquial omissions and
ambiguous references, posing significant challenges for effective retrieval and
generation. Traditional query rewriting relies on human annotators to clarify
queries, but due to limitations in annotators' expressive ability and depth of
understanding, manually rewritten queries often diverge from those needed in
real-world RAG systems, resulting in a gap between user intent and system
response. We observe that high-quality synthetic queries can better bridge this
gap, achieving superior performance in both retrieval and generation compared
to human rewrites. This raises an interesting question: Can rewriting models
trained on synthetic queries better capture user intent than human annotators?
In this paper, we propose SynRewrite, a synthetic data-driven query rewriting
model to generate high-quality synthetic rewrites more aligned with user
intent. To construct training data, we prompt GPT-4o with dialogue history,
current queries, positive documents, and answers to synthesize high-quality
rewrites. A Flan-T5 model is then finetuned on this dataset to map dialogue
history and queries to synthetic rewrites. Finally, we further enhance the
rewriter using the generator's feedback through the DPO algorithm to boost
end-task performance. Experiments on TopiOCQA and QRECC datasets show that
SynRewrite consistently outperforms human rewrites in both retrieval and
generation tasks. Our results demonstrate that synthetic rewrites can serve as
a scalable and effective alternative to human annotations.
\\ ( https://arxiv.org/abs/2509.22325 ,  1348kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22463 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:14:03 GMT   (388kb)

Title: IIET: Efficient Numerical Transformer via Implicit Iterative Euler
  Method
Authors: Xinyu Liu, Bei Li, Jiahao Liu, Junhao Ruan, Kechen Jiao, Hongyin Tang,
  Jingang Wang, Xiao Tong, Jingbo Zhu
Categories: cs.LG cs.CL
\\
  High-order numerical methods enhance Transformer performance in tasks like
NLP and CV, but introduce a performance-efficiency trade-off due to increased
computational overhead. Our analysis reveals that conventional efficiency
techniques, such as distillation, can be detrimental to the performance of
these models, exemplified by PCformer. To explore more optimizable ODE-based
Transformer architectures, we propose the \textbf{I}terative \textbf{I}mplicit
\textbf{E}uler \textbf{T}ransformer \textbf{(IIET)}, which simplifies
high-order methods using an iterative implicit Euler approach. This
simplification not only leads to superior performance but also facilitates
model compression compared to PCformer. To enhance inference efficiency, we
introduce \textbf{I}teration \textbf{I}nfluence-\textbf{A}ware
\textbf{D}istillation \textbf{(IIAD)}. Through a flexible threshold, IIAD
allows users to effectively balance the performance-efficiency trade-off. On
lm-evaluation-harness, IIET boosts average accuracy by 2.65\% over vanilla
Transformers and 0.8\% over PCformer. Its efficient variant, E-IIET,
significantly cuts inference overhead by 55\% while retaining 99.4\% of the
original task accuracy. Moreover, the most efficient IIET variant achieves an
average performance gain exceeding 1.6\% over vanilla Transformer with
comparable speed.
\\ ( https://arxiv.org/abs/2509.22463 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22576 (*cross-listing*)
Date: Fri, 26 Sep 2025 16:51:44 GMT   (14227kb)

Title: EPO: Entropy-regularized Policy Optimization for LLM Agents
  Reinforcement Learning
Authors: Xu Wujiang and Wentian Zhao and Zhenting Wang and Li Yu-Jhe and Jin
  Can and Jin Mingyu and Mei Kai and Wan Kun and Metaxas Dimitris
Categories: cs.LG cs.CL
\\
  Training LLM agents in multi-turn environments with sparse rewards, where
completing a single task requires 30+ turns of interaction within an episode,
presents a fundamental challenge for reinforcement learning. We identify a
critical failure mode unique to this setting: the exploration-exploitation
cascade failure. This cascade begins with early-stage policy premature
convergence, where sparse feedback causes agents to commit to flawed,
low-entropy strategies. Subsequently, agents enter late-stage policy collapse,
where conventional entropy regularization becomes counterproductive, promoting
chaotic exploration that destabilizes training. We propose Entropy-regularized
Policy Optimization (EPO), a general framework that breaks this failure cycle
through three synergistic mechanisms: (1) adopting entropy regularization in
multi-turn settings to enhance exploration, (2) an entropy smoothing
regularizer that bounds policy entropy within historical averages to prevent
abrupt fluctuations, and (3) adaptive phase-based weighting that balances
exploration and exploitation across training. Our analysis justifies that EPO
guarantees monotonically decreasing entropy variance while maintaining
convergence. EPO achieves up to 152% performance improvement on ScienceWorld
and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn
sparse-reward settings require fundamentally different entropy control than
traditional RL, with broad implications for LLM agent training.
\\ ( https://arxiv.org/abs/2509.22576 ,  14227kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20401 (*cross-listing*)
Date: Tue, 23 Sep 2025 18:31:29 GMT   (8978kb)

Title: SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment
Authors: Binod Singh, Sayan Deb Sarkar, Iro Armeni
Categories: cs.GR cs.CV cs.RO
\\
  Aligning 3D scene graphs is a crucial initial step for several applications
in robot navigation and embodied perception. Current methods in 3D scene graph
alignment often rely on single-modality point cloud data and struggle with
incomplete or noisy input. We introduce SGAligner++, a cross-modal,
language-aided framework for 3D scene graph alignment. Our method addresses the
challenge of aligning partially overlapping scene observations across
heterogeneous modalities by learning a unified joint embedding space, enabling
accurate alignment even under low-overlap conditions and sensor noise. By
employing lightweight unimodal encoders and attention-based fusion, SGAligner++
enhances scene understanding for tasks such as visual localization, 3D
reconstruction, and navigation, while ensuring scalability and minimal
computational overhead. Extensive evaluations on real-world datasets
demonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%
on noisy real-world reconstructions, while enabling cross-modal generalization.
\\ ( https://arxiv.org/abs/2509.20401 ,  8978kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21370 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:28:10 GMT   (44792kb)

Title: Language-in-the-Loop Culvert Inspection on the Erie Canal
Authors: Yashom Dighe, Yash Turkar and Karthik Dantu
Categories: cs.RO cs.CV
Comments: First two authors contributed equally
\\
  Culverts on canals such as the Erie Canal, built originally in 1825, require
frequent inspections to ensure safe operation. Human inspection of culverts is
challenging due to age, geometry, poor illumination, weather, and lack of easy
access. We introduce VISION, an end-to-end, language-in-the-loop autonomy
system that couples a web-scale vision-language model (VLM) with constrained
viewpoint planning for autonomous inspection of culverts. Brief prompts to the
VLM solicit open-vocabulary ROI proposals with rationales and confidences,
stereo depth is fused to recover scale, and a planner -- aware of culvert
constraints -- commands repositioning moves to capture targeted close-ups.
Deployed on a quadruped in a culvert under the Erie Canal, VISION closes the
see, decide, move, re-image loop on-board and produces high-resolution images
for detailed reporting without domain-specific fine-tuning. In an external
evaluation by New York Canal Corporation personnel, initial ROI proposals
achieved 61.4\% agreement with subject-matter experts, and final
post-re-imaging assessments reached 80\%, indicating that VISION converts
tentative hypotheses into grounded, expert-aligned findings.
\\ ( https://arxiv.org/abs/2509.21370 ,  44792kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21477 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:41:20 GMT   (12283kb)

Title: VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete
  Observations
Authors: Yuan Gao, Hao Wu, Qingsong Wen, Kun Wang, Xian Wu, Xiaomeng Huang
Categories: cs.LG cs.CV physics.ao-ph
\\
  Reconstructing subsurface ocean dynamics, such as vertical velocity fields,
from incomplete surface observations poses a critical challenge in Earth
science, a field long hampered by the lack of standardized, analysis-ready
benchmarks. To systematically address this issue and catalyze research, we
first build and release KD48, a high-resolution ocean dynamics benchmark
derived from petascale simulations and curated with expert-driven denoising.
Building on this benchmark, we introduce VISION, a novel reconstruction
paradigm based on Dynamic Prompting designed to tackle the core problem of
missing data in real-world observations. The essence of VISION lies in its
ability to generate a visual prompt on-the-fly from any available subset of
observations, which encodes both data availability and the ocean's physical
state. More importantly, we design a State-conditioned Prompting module that
efficiently injects this prompt into a universal backbone, endowed with
geometry- and scale-aware operators, to guide its adaptive adjustment of
computational strategies. This mechanism enables VISION to precisely handle the
challenges posed by varying input combinations. Extensive experiments on the
KD48 benchmark demonstrate that VISION not only substantially outperforms
state-of-the-art models but also exhibits strong generalization under extreme
data missing scenarios. By providing a high-quality benchmark and a robust
model, our work establishes a solid infrastructure for ocean science research
under data uncertainty. Our codes are available at:
https://github.com/YuanGao-YG/VISION.
\\ ( https://arxiv.org/abs/2509.21477 ,  12283kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21498 (*cross-listing*)
Date: Thu, 25 Sep 2025 19:56:17 GMT   (26129kb)

Title: SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of
  Diffusion Models
Authors: Arani Roy, Shristi Das Biswas, Kaushik Roy
Categories: cs.LG cs.CV
\\
  Diffusion models (DMs), lauded for their generative performance, are
computationally prohibitive due to their billion-scale parameters and iterative
denoising dynamics. Existing efficiency techniques, such as quantization,
timestep reduction, or pruning, offer savings in compute, memory, or runtime
but are strictly bottlenecked by reliance on fine-tuning or retraining to
recover performance. In this work, we introduce SlimDiff, an automated
activation-informed structural compression framework that reduces both
attention and feedforward dimensionalities in DMs, while being entirely
gradient-free. SlimDiff reframes DM compression as a spectral approximation
task, where activation covariances across denoising timesteps define low-rank
subspaces that guide dynamic pruning under a fixed compression budget. This
activation-aware formulation mitigates error accumulation across timesteps by
applying module-wise decompositions over functional weight groups: query--key
interactions, value--output couplings, and feedforward projections, rather than
isolated matrix factorizations, while adaptively allocating sparsity across
modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff
achieves up to 35\% acceleration and $\sim$100M parameter reduction over
baselines, with generation quality on par with uncompressed models without any
backpropagation. Crucially, our approach requires only about 500 calibration
samples, over 70$\times$ fewer than prior methods. To our knowledge, this is
the first closed-form, activation-guided structural compression of DMs that is
entirely training-free, providing both theoretical clarity and practical
efficiency.
\\ ( https://arxiv.org/abs/2509.21498 ,  26129kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21526 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:10:41 GMT   (1692kb)

Title: TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised
  Learning
Authors: Hongyang He, Xinyuan Song, Yangfan He, Zeyu Zhang, Yanshu Li, Haochen
  You, Lifan Sun, Wenqiao Zhang
Categories: cs.LG cs.CV
Comments: Accepted by NeurIPS 2025
\\
  We introduce TRiCo, a novel triadic game-theoretic co-training framework that
rethinks the structure of semi-supervised learning by incorporating a teacher,
two students, and an adversarial generator into a unified training paradigm.
Unlike existing co-training or teacher-student approaches, TRiCo formulates SSL
as a structured interaction among three roles: (i) two student classifiers
trained on frozen, complementary representations, (ii) a meta-learned teacher
that adaptively regulates pseudo-label selection and loss balancing via
validation-based feedback, and (iii) a non-parametric generator that perturbs
embeddings to uncover decision boundary weaknesses. Pseudo-labels are selected
based on mutual information rather than confidence, providing a more robust
measure of epistemic uncertainty. This triadic interaction is formalized as a
Stackelberg game, where the teacher leads strategy optimization and students
follow under adversarial perturbations. By addressing key limitations in
existing SSL frameworks, such as static view interactions, unreliable
pseudo-labels, and lack of hard sample modeling, TRiCo provides a principled
and generalizable solution. Extensive experiments on CIFAR-10, SVHN, STL-10,
and ImageNet demonstrate that TRiCo consistently achieves state-of-the-art
performance in low-label regimes, while remaining architecture-agnostic and
compatible with frozen vision backbones.
\\ ( https://arxiv.org/abs/2509.21526 ,  1692kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21531 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:18:56 GMT   (26028kb)

Title: Patch-Based Diffusion for Data-Efficient, Radiologist-Preferred MRI
  Reconstruction
Authors: Rohan Sanda, Asad Aali, Andrew Johnston, Eduardo Reis, Jonathan Singh,
  Gordon Wetzstein, Sara Fridovich-Keil
Categories: eess.IV cs.CV
Comments: Code is available at: https://github.com/voilalab/PaDIS-MRI
\\
  Magnetic resonance imaging (MRI) requires long acquisition times, raising
costs, reducing accessibility, and making scans more susceptible to motion
artifacts. Diffusion probabilistic models that learn data-driven priors can
potentially assist in reducing acquisition time. However, they typically
require large training datasets that can be prohibitively expensive to collect.
Patch-based diffusion models have shown promise in learning effective
data-driven priors over small real-valued datasets, but have not yet
demonstrated clinical value in MRI. We extend the Patch-based Diffusion Inverse
Solver (PaDIS) to complex-valued, multi-coil MRI reconstruction, and compare it
against a state-of-the-art whole-image diffusion baseline (FastMRI-EDM) for 7x
undersampled MRI reconstruction on the FastMRI brain dataset. We show that
PaDIS-MRI models trained on small datasets of as few as 25 k-space images
outperform FastMRI-EDM on image quality metrics (PSNR, SSIM, NRMSE),
pixel-level uncertainty, cross-contrast generalization, and robustness to
severe k-space undersampling. In a blinded study with three radiologists,
PaDIS-MRI reconstructions were chosen as diagnostically superior in 91.7% of
cases, compared to baselines (i) FastMRI-EDM and (ii) classical convex
reconstruction with wavelet sparsity. These findings highlight the potential of
patch-based diffusion priors for high-fidelity MRI reconstruction in
data-scarce clinical settings where diagnostic confidence matters.
\\ ( https://arxiv.org/abs/2509.21531 ,  26028kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21541 (*cross-listing*)
Date: Thu, 25 Sep 2025 20:29:05 GMT   (5466kb)

Title: ControlHair: Physically-based Video Diffusion for Controllable Dynamic
  Hair Rendering
Authors: Weikai Lin, Haoxiang Li, Yuhao Zhu
Categories: cs.GR cs.CV
Comments: 9 pages,Project website: https://ctrlhair-arxiv.netlify.app/
ACM-class: I.3; I.2; I.4
\\
  Hair simulation and rendering are challenging due to complex strand dynamics,
diverse material properties, and intricate light-hair interactions. Recent
video diffusion models can generate high-quality videos, but they lack
fine-grained control over hair dynamics. We present ControlHair, a hybrid
framework that integrates a physics simulator with conditional video diffusion
to enable controllable dynamic hair rendering. ControlHair adopts a three-stage
pipeline: it first encodes physics parameters (e.g., hair stiffness, wind) into
per-frame geometry using a simulator, then extracts per-frame control signals,
and finally feeds control signals into a video diffusion model to generate
videos with desired hair dynamics. This cascaded design decouples physics
reasoning from video generation, supports diverse physics, and makes training
the video diffusion model easy. Trained on a curated 10K video dataset,
ControlHair outperforms text- and pose-conditioned baselines, delivering
precisely controlled hair dynamics. We further demonstrate three use cases of
ControlHair: dynamic hairstyle try-on, bullet-time effects, and cinemagraphic.
ControlHair introduces the first physics-informed video diffusion framework for
controllable dynamics. We provide a teaser video and experimental results on
our website.
\\ ( https://arxiv.org/abs/2509.21541 ,  5466kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21854 (*cross-listing*)
Date: Fri, 26 Sep 2025 04:32:26 GMT   (1957kb)

Title: Perception-Consistency Multimodal Large Language Models Reasoning via
  Caption-Regularized Policy Optimization
Authors: Songjun Tu, Qichao Zhang, Jingbo Sun, Yuqian Fu, Linjing Li, Xiangyuan
  Lan, Dongmei Jiang, Yaowei Wang, Dongbin Zhao
Categories: cs.MM cs.CV
Comments: 12pages, 11 figures
MSC-class: 68T07, 68T45
ACM-class: I.2.6; I.2.7; I.2.10
\\
  While multimodal large language models excel at tasks that integrate visual
perception with symbolic reasoning, their performance is often undermined by a
critical vulnerability: perception-induced errors that propagate through the
reasoning chain. Current reinforcement learning (RL) fine-tuning methods, while
enhancing reasoning abilities, largely fail to address the underlying
misalignment between visual grounding and the subsequent reasoning process. To
address this challenge, we propose \textbf{Caption-Regularized Policy
Optimization (CapPO)}, a novel RL framework that explicitly enforces perceptual
consistency during policy optimization. CapPO integrates two key mechanisms:
(1) a caption-based consistency regularization, which minimizes the divergence
between responses conditioned on raw images and those conditioned on captions,
thereby anchoring reasoning to semantically faithful visual content; and (2) a
KL-weighted advantage estimation scheme, which adaptively scales reinforcement
signals to strengthen perceptually consistent trajectories while suppressing
spurious correlations. Extensive experiments on five math-focused and five
general reasoning benchmarks demonstrate that CapPO achieves competitive
performance, yielding gains of +6.0% accuracy on math-related tasks and +2.4%
on general reasoning tasks over the base Qwen2.5-VL-7B model. Moreover,
ablation studies further confirm the effectiveness of each component, while
error analysis reveals that CapPO significantly reduces perception-related
mistakes compared with baselines. Overall, CapPO provides a simple yet
effective framework for improving multimodal reasoning.
\\ ( https://arxiv.org/abs/2509.21854 ,  1957kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21898 (*cross-listing*)
Date: Fri, 26 Sep 2025 05:31:59 GMT   (739kb)

Title: Closing the Oracle Gap: Increment Vector Transformation for Class
  Incremental Learning
Authors: Zihuan Qiu, Yi Xu, Fanman Meng, Runtong Zhang, Linfeng Xu, Qingbo Wu,
  Hongliang Li
Categories: cs.LG cs.CV
\\
  Class Incremental Learning (CIL) aims to sequentially acquire knowledge of
new classes without forgetting previously learned ones. Despite recent
progress, current CIL methods still exhibit significant performance gaps
compared to their oracle counterparts-models trained with full access to
historical data. Inspired by recent insights on Linear Mode Connectivity (LMC),
we revisit the geometric properties of oracle solutions in CIL and uncover a
fundamental observation: these oracle solutions typically maintain low-loss
linear connections to the optimum of previous tasks. Motivated by this finding,
we propose Increment Vector Transformation (IVT), a novel plug-and-play
framework designed to mitigate catastrophic forgetting during training. Rather
than directly following CIL updates, IVT periodically teleports the model
parameters to transformed solutions that preserve linear connectivity to
previous task optimum. By maintaining low-loss along these connecting paths,
IVT effectively ensures stable performance on previously learned tasks. The
transformation is efficiently approximated using diagonal Fisher Information
Matrices, making IVT suitable for both exemplar-free and exemplar-based
scenarios, and compatible with various initialization strategies. Extensive
experiments on CIFAR-100, FGVCAircraft, ImageNet-Subset, and ImageNet-Full
demonstrate that IVT consistently enhances the performance of strong CIL
baselines. Specifically, on CIFAR-100, IVT improves the last accuracy of the
PASS baseline by +5.12% and reduces forgetting by 2.54%. For the
CLIP-pre-trained SLCA baseline on FGVCAircraft, IVT yields gains of +14.93% in
average accuracy and +21.95% in last accuracy. The code will be released.
\\ ( https://arxiv.org/abs/2509.21898 ,  739kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22049 (*cross-listing*)
Date: Fri, 26 Sep 2025 08:33:34 GMT   (5015kb)

Title: Comparative Analysis of GAN and Diffusion for MRI-to-CT translation
Authors: Emily Honey, Anders Helbo, Jens Petersen
Categories: eess.IV cs.CV cs.LG
\\
  Computed tomography (CT) is essential for treatment and diagnostics; In case
CT are missing or otherwise difficult to obtain, methods for generating
synthetic CT (sCT) images from magnetic resonance imaging (MRI) images are
sought after. Therefore, it is valuable to establish a reference for what
strategies are most effective for MRI-to-CT translation. In this paper, we
compare the performance of two frequently used architectures for MRI-to-CT
translation: a conditional generative adversarial network (cGAN) and a
conditional denoising diffusion probabilistic model (cDDPM). We chose
well-established implementations to represent each architecture: Pix2Pix for
cGAN, and Palette for cDDPM. We separate the classical 3D translation problem
into a sequence of 2D translations on the transverse plane, to investigate the
viability of a strategy that reduces the computational cost. We also
investigate the impact of conditioning the generative process on a single MRI
image/slice and on multiple MRI slices. The performance is assessed using a
thorough evaluation protocol, including a novel slice-wise metric Similarity Of
Slices (SIMOS), which measures the continuity between transverse slices when
compiling the sCTs into 3D format. Our comparative analysis revealed that
MRI-to-CT generative models benefit from multi-channel conditional input and
using cDDPM as an architecture.
\\ ( https://arxiv.org/abs/2509.22049 ,  5015kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22053 (*cross-listing*)
Date: Fri, 26 Sep 2025 08:35:34 GMT   (181kb)

Title: Enriching Knowledge Distillation with Intra-Class Contrastive Learning
Authors: Hua Yuan, Ning Xu, Xin Geng, Yong Rui
Categories: cs.LG cs.CV
\\
  Since the advent of knowledge distillation, much research has focused on how
the soft labels generated by the teacher model can be utilized effectively.
Existing studies points out that the implicit knowledge within soft labels
originates from the multi-view structure present in the data. Feature
variations within samples of the same class allow the student model to
generalize better by learning diverse representations. However, in existing
distillation methods, teacher models predominantly adhere to ground-truth
labels as targets, without considering the diverse representations within the
same class. Therefore, we propose incorporating an intra-class contrastive loss
during teacher training to enrich the intra-class information contained in soft
labels. In practice, we find that intra-class loss causes instability in
training and slows convergence. To mitigate these issues, margin loss is
integrated into intra-class contrastive learning to improve the training
stability and convergence speed. Simultaneously, we theoretically analyze the
impact of this loss on the intra-class distances and inter-class distances. It
has been proved that the intra-class contrastive loss can enrich the
intra-class diversity. Experimental results demonstrate the effectiveness of
the proposed method.
\\ ( https://arxiv.org/abs/2509.22053 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22126 (*cross-listing*)
Date: Fri, 26 Sep 2025 09:49:44 GMT   (19655kb)

Title: Guidance Watermarking for Diffusion Models
Authors: Enoal Gesny, Eva Giboulot, Teddy Furon, Vivien Chappelier
Categories: cs.CR cs.CV
\\
  This paper introduces a novel watermarking method for diffusion models. It is
based on guiding the diffusion process using the gradient computed from any
off-the-shelf watermark decoder. The gradient computation encompasses different
image augmentations, increasing robustness to attacks against which the decoder
was not originally robust, without retraining or fine-tuning. Our method
effectively convert any \textit{post-hoc} watermarking scheme into an
in-generation embedding along the diffusion process. We show that this approach
is complementary to watermarking techniques modifying the variational
autoencoder at the end of the diffusion process. We validate the methods on
different diffusion models and detectors. The watermarking guidance does not
significantly alter the generated image for a given seed and prompt, preserving
both the diversity and quality of generation.
\\ ( https://arxiv.org/abs/2509.22126 ,  19655kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22227 (*cross-listing*)
Date: Fri, 26 Sep 2025 11:38:42 GMT   (40336kb)

Title: Aerial Path Planning for Urban Geometry and Texture Co-Capture
Authors: Weidan Xiong, Bochuan Zeng, Ziyu Hu, Jianwei Guo, Ke Xie, Hui Huang
Categories: cs.GR cs.CV
Comments: ACM TOG and SIGGRAPH Asia 2025 (Patent Protected); Project page:
  https://vcc.tech/research/2025/DroneTex
DOI: 10.1145/3763292
\\
  Recent advances in image acquisition and scene reconstruction have enabled
the generation of high-quality structural urban scene geometry, given
sufficient site information. However, current capture techniques often overlook
the crucial importance of texture quality, resulting in noticeable visual
artifacts in the textured models. In this work, we introduce the urban geometry
and texture co-capture problem under limited prior knowledge before a site
visit. The only inputs are a 2D building contour map of the target area and a
safe flying altitude above the buildings. We propose an innovative aerial path
planning framework designed to co-capture images for reconstructing both
structured geometry and high-fidelity textures. To evaluate and guide view
planning, we introduce a comprehensive texture quality assessment system,
including two novel metrics tailored for building facades. Firstly, our method
generates high-quality vertical dipping views and horizontal planar views to
effectively capture both geometric and textural details. A multi-objective
optimization strategy is then proposed to jointly maximize texture fidelity,
improve geometric accuracy, and minimize the cost associated with aerial views.
Furthermore, we present a sequential path planning algorithm that accounts for
texture consistency during image capture. Extensive experiments on large-scale
synthetic and real-world urban datasets demonstrate that our approach
effectively produces image sets suitable for concurrent geometric and texture
reconstruction, enabling the creation of realistic, textured scene proxies at
low operational cost.
\\ ( https://arxiv.org/abs/2509.22227 ,  40336kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22240 (*cross-listing*)
Date: Fri, 26 Sep 2025 11:56:28 GMT   (8588kb)

Title: COMPASS: Robust Feature Conformal Prediction for Medical Segmentation
  Metrics
Authors: Matt Y. Cheung, Ashok Veeraraghavan, Guha Balakrishnan
Categories: eess.IV cs.CV cs.LG stat.AP stat.ML
\\
  In clinical applications, the utility of segmentation models is often based
on the accuracy of derived downstream metrics such as organ size, rather than
by the pixel-level accuracy of the segmentation masks themselves. Thus,
uncertainty quantification for such metrics is crucial for decision-making.
Conformal prediction (CP) is a popular framework to derive such principled
uncertainty guarantees, but applying CP naively to the final scalar metric is
inefficient because it treats the complex, non-linear segmentation-to-metric
pipeline as a black box. We introduce COMPASS, a practical framework that
generates efficient, metric-based CP intervals for image segmentation models by
leveraging the inductive biases of their underlying deep neural networks.
COMPASS performs calibration directly in the model's representation space by
perturbing intermediate features along low-dimensional subspaces maximally
sensitive to the target metric. We prove that COMPASS achieves valid marginal
coverage under exchangeability and nestedness assumptions. Empirically, we
demonstrate that COMPASS produces significantly tighter intervals than
traditional CP baselines on four medical image segmentation tasks for area
estimation of skin lesions and anatomical structures. Furthermore, we show that
leveraging learned internal features to estimate importance weights allows
COMPASS to also recover target coverage under covariate shifts. COMPASS paves
the way for practical, metric-based uncertainty quantification for medical
image segmentation.
\\ ( https://arxiv.org/abs/2509.22240 ,  8588kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22356 (*cross-listing*)
Date: Fri, 26 Sep 2025 13:53:25 GMT   (13216kb)

Title: RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic
  Manipulation
Authors: Enguang Liu, Siyuan Liang, Liming Lu, Xiyu Zeng, Xiaochun Cao, Aishan
  Liu, Shuchao Pang
Categories: cs.RO cs.CV
\\
  The safety and reliability of embodied agents rely on accurate and unbiased
visual perception. However, existing benchmarks mainly emphasize generalization
and robustness under perturbations, while systematic quantification of visual
bias remains scarce. This gap limits a deeper understanding of how perception
influences decision-making stability. To address this issue, we propose
RoboView-Bias, the first benchmark specifically designed to systematically
quantify visual bias in robotic manipulation, following a principle of factor
isolation. Leveraging a structured variant-generation framework and a
perceptual-fairness validation protocol, we create 2,127 task instances that
enable robust measurement of biases induced by individual visual factors and
their interactions. Using this benchmark, we systematically evaluate three
representative embodied agents across two prevailing paradigms and report three
key findings: (i) all agents exhibit significant visual biases, with camera
viewpoint being the most critical factor; (ii) agents achieve their highest
success rates on highly saturated colors, indicating inherited visual
preferences from underlying VLMs; and (iii) visual biases show strong,
asymmetric coupling, with viewpoint strongly amplifying color-related bias.
Finally, we demonstrate that a mitigation strategy based on a semantic
grounding layer substantially reduces visual bias by approximately 54.5\% on
MOKA. Our results highlight that systematic analysis of visual bias is a
prerequisite for developing safe and reliable general-purpose embodied agents.
\\ ( https://arxiv.org/abs/2509.22356 ,  13216kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22507 (*cross-listing*)
Date: Fri, 26 Sep 2025 15:48:14 GMT   (3321kb)

Title: Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable,
  Heterogeneous Federated Learning on Non-IID Data
Authors: Zahid Iqbal
Categories: cs.LG cs.CV
\\
  Federated Learning (FL) has emerged as a promising decentralized learning
(DL) approach that enables the use of distributed data without compromising
user privacy. However, FL poses several key challenges. First, it is frequently
assumed that every client can train the same machine learning models, however,
not all clients are able to meet this assumption because of differences in
their business needs and computational resources. Second, statistical
heterogeneity (a.k.a. non-IID data) poses a major challenge in FL, which can
lead to lower global model performance. Third, while addressing these
challenges, there is a need for a cost-effective incentive mechanism to
encourage clients to participate in FL training. In response to these
challenges, we propose several methodologies: DL-SH, which facilitates
efficient, privacy-preserving, and communication-efficient learning in the
context of statistical heterogeneity; DL-MH, designed to manage fully
heterogeneous models while tackling statistical disparities; and I-DL-MH, an
incentive-based extension of DL-MH that promotes client engagement in federated
learning training by providing incentives within this complex federated
learning framework. Comprehensive experiments were carried out to assess the
performance and scalability of the proposed approaches across a range of
complex experimental settings. This involved utilizing various model
architectures, in diverse data distributions, including IID and several non-IID
scenarios, as well as multiple datasets. Experimental results demonstrate that
the proposed approaches significantly enhance accuracy and decrease
communication costs while effectively addressing statistical heterogeneity and
model heterogeneity in comparison to existing state-of-the-art approaches and
baselines, with DL-SH improving global model accuracy by 153%, and I-DL-MH
achieving a 225% improvement under non-IID conditions.
\\ ( https://arxiv.org/abs/2509.22507 ,  3321kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22522 (*cross-listing*)
Date: Fri, 26 Sep 2025 16:04:00 GMT   (2980kb)

Title: JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory
  Generation
Authors: Guillem Capellera, Luis Ferraz, Antonio Rubio, Alexandre Alahi,
  Antonio Agudo
Categories: cs.LG cs.CV
\\
  Generative models often treat continuous data and discrete events as separate
processes, creating a gap in modeling complex systems where they interact
synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion
framework designed to unify these two processes by simultaneously generating
continuous spatio-temporal data and synchronous discrete events. We demonstrate
its efficacy in the sports domain by simultaneously modeling multi-agent
trajectories and key possession events. This joint modeling is validated with
non-controllable generation and two novel controllable generation scenarios:
weak-possessor-guidance, which offers flexible semantic control over game
dynamics through a simple list of intended ball possessors, and text-guidance,
which enables fine-grained, language-driven generation. To enable the
conditioning with these guidance signals, we introduce CrossGuid, an effective
conditioning operation for multi-agent domains. We also share a new unified
sports benchmark enhanced with textual descriptions for soccer and football
datasets. JointDiff achieves state-of-the-art performance, demonstrating that
joint modeling is crucial for building realistic and controllable generative
models for interactive systems.
\\ ( https://arxiv.org/abs/2509.22522 ,  2980kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22573 (*cross-listing*)
Date: Fri, 26 Sep 2025 16:49:40 GMT   (39685kb)

Title: MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction
  using Human Pose and Emotion Information from RGB-only Camera Data
Authors: Farida Mohsen, Ali Safa
Categories: cs.RO cs.CV
\\
  Efficiently detecting human intent to interact with ubiquitous robots is
crucial for effective human-robot interaction (HRI) and collaboration. Over the
past decade, deep learning has gained traction in this field, with most
existing approaches relying on multimodal inputs, such as RGB combined with
depth (RGB-D), to classify time-sequence windows of sensory data as interactive
or non-interactive. In contrast, we propose a novel RGB-only pipeline for
predicting human interaction intent with frame-level precision, enabling faster
robot responses and improved service quality. A key challenge in intent
prediction is the class imbalance inherent in real-world HRI datasets, which
can hinder the model's training and generalization. To address this, we
introduce MINT-RVAE, a synthetic sequence generation method, along with new
loss functions and training strategies that enhance generalization on
out-of-sample data. Our approach achieves state-of-the-art performance (AUROC:
0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB
input and supporting precise frame onset prediction. Finally, to support future
research, we openly release our new dataset with frame-level labeling of human
interaction intent.
\\ ( https://arxiv.org/abs/2509.22573 ,  39685kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22642 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:59:07 GMT   (36620kb)

Title: WoW: Towards a World omniscient World model Through Embodied Interaction
Authors: Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin
  Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Anthony
  Chen, Qiang Zhou, Yueru Jia, Jiaming Liu, Yong Dai, Qingpo Wuwu, Chengyu Bai,
  Yu-Kai Wang, Ying Li, Lizhang Chen, Yong Bao, Zhiyuan Jiang, Jiacheng Zhu,
  Kai Tang, Ruichuan An, Yulin Luo, Qiuxuan Feng, Siyuan Zhou, Chi-min Chan,
  Chengkai Hou, Wei Xue, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang
Categories: cs.RO cs.CV cs.MM
\\
  Humans develop an understanding of intuitive physics through active
interaction with the world. This approach is in stark contrast to current video
models, such as Sora, which rely on passive observation and therefore struggle
with grasping physical causality. This observation leads to our central
hypothesis: authentic physical intuition of the world model must be grounded in
extensive, causally rich interactions with the real world. To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories. Our findings reveal that
the model's understanding of physics is a probabilistic distribution of
plausible outcomes, leading to stochastic instabilities and physical
hallucinations. Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions. In addition, a
co-trained Inverse Dynamics Model translates these refined plans into
executable robotic actions, thus closing the imagination-to-action loop. We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence. Our work provides
systematic evidence that large-scale, real-world interaction is a cornerstone
for developing physical intuition in AI. Models, data, and benchmarks will be
open-sourced.
\\ ( https://arxiv.org/abs/2509.22642 ,  36620kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22652 (*cross-listing*)
Date: Fri, 26 Sep 2025 17:59:59 GMT   (3027kb)

Title: Pixel Motion Diffusion is What We Need for Robot Control
Authors: E-Ro Nguyen, Yichi Zhang, Kanchana Ranasinghe, Xiang Li, Michael S.
  Ryoo
Categories: cs.RO cs.CV
Comments: 16 pages, 7 figures
\\
  We present DAWN (Diffusion is All We Need for robot control), a unified
diffusion-based framework for language-conditioned robotic manipulation that
bridges high-level motion intent and low-level robot action via structured
pixel motion representation. In DAWN, both the high-level and low-level
controllers are modeled as diffusion processes, yielding a fully trainable,
end-to-end system with interpretable intermediate motion abstractions. DAWN
achieves state-of-the-art results on the challenging CALVIN benchmark,
demonstrating strong multi-task performance, and further validates its
effectiveness on MetaWorld. Despite the substantial domain gap between
simulation and reality and limited real-world data, we demonstrate reliable
real-world transfer with only minimal finetuning, illustrating the practical
viability of diffusion-based motion abstractions for robotic control. Our
results show the effectiveness of combining diffusion modeling with
motion-centric representations as a strong baseline for scalable and robust
robot learning. Project page: https://nero1342.github.io/DAWN/
\\ ( https://arxiv.org/abs/2509.22652 ,  3027kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22369 (*cross-listing*)
Date: Fri, 26 Sep 2025 14:02:20 GMT   (1284kb)

Title: Role-Aware Multi-modal federated learning system for detecting phishing
  webpages
Authors: Bo Wang, Imran Khan, Martin White, Natalia Beloff
Categories: cs.LG cs.DC
Comments: 22 pages, 9 figures
\\
  We present a federated, multi-modal phishing website detector that supports
URL, HTML, and IMAGE inputs without binding clients to a fixed modality at
inference: any client can invoke any modality head trained elsewhere.
Methodologically, we propose role-aware bucket aggregation on top of FedProx,
inspired by Mixture-of-Experts and FedMM. We drop learnable routing and use
hard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling
separate aggregation of modality-specific parameters to isolate cross-embedding
conflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc
97.5% with FPR 2.4% across two data types; on the image subset (ablation) it
attains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an
early three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc
96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results
indicate that bucket aggregation with hard-gated experts enables stable
federated training under strict privacy, while improving the usability and
flexibility of multi-modal phishing detection.
\\ ( https://arxiv.org/abs/2509.22369 ,  1284kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21828 (*cross-listing*)
Date: Fri, 26 Sep 2025 03:41:40 GMT   (941kb)

Title: Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement
  Learning
Authors: The Viet Bui and Tien Mai and Hong Thanh Nguyen
Categories: cs.LG cs.MA
\\
  We study the problem of online multi-agent reinforcement learning (MARL) in
environments with sparse rewards, where reward feedback is not provided at each
interaction but only revealed at the end of a trajectory. This setting, though
realistic, presents a fundamental challenge: the lack of intermediate rewards
hinders standard MARL algorithms from effectively guiding policy learning. To
address this issue, we propose a novel framework that integrates online inverse
preference learning with multi-agent on-policy optimization into a unified
architecture. At its core, our approach introduces an implicit multi-agent
reward learning model, built upon a preference-based value-decomposition
network, which produces both global and local reward signals. These signals are
further used to construct dual advantage streams, enabling differentiated
learning targets for the centralized critic and decentralized actors. In
addition, we demonstrate how large language models (LLMs) can be leveraged to
provide preference labels that enhance the quality of the learned reward model.
Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and
SMACv2, show that our method achieves superior performance compared to existing
baselines, highlighting its effectiveness in addressing sparse-reward
challenges in online MARL.
\\ ( https://arxiv.org/abs/2509.21828 ,  941kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22426 (*cross-listing*)
Date: Fri, 26 Sep 2025 14:46:21 GMT   (1878kb)

Title: Learning from Delayed Feedback in Games via Extra Prediction
Authors: Yuma Fujimoto, Kenshi Abe, and Kaito Ariu
Categories: cs.LG cs.GT cs.MA math.OC
Comments: 11 pages, 3 figures (main); 9 pages (appendix)
\\
  This study raises and addresses the problem of time-delayed feedback in
learning in games. Because learning in games assumes that multiple agents
independently learn their strategies, a discrepancy in optimization often
emerges among the agents. To overcome this discrepancy, the prediction of the
future reward is incorporated into algorithms, typically known as Optimistic
Follow-the-Regularized-Leader (OFTRL). However, the time delay in observing the
past rewards hinders the prediction. Indeed, this study firstly proves that
even a single-step delay worsens the performance of OFTRL from the aspects of
regret and convergence. This study proposes the weighted OFTRL (WOFTRL), where
the prediction vector of the next reward in OFTRL is weighted $n$ times. We
further capture an intuition that the optimistic weight cancels out this time
delay. We prove that when the optimistic weight exceeds the time delay, our
WOFTRL recovers the good performances that the regret is constant
($O(1)$-regret) in general-sum normal-form games, and the strategies converge
to the Nash equilibrium as a subsequence (best-iterate convergence) in
poly-matrix zero-sum games. The theoretical results are supported and
strengthened by our experiments.
\\ ( https://arxiv.org/abs/2509.22426 ,  1878kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2404.11973
replaced with revised version Fri, 26 Sep 2025 14:07:34 GMT   (1266kb)

Title: A critical review of methods and challenges in large language models
Authors: Milad Moradi, Ke Yan, David Colwell, Matthias Samwald, Rhona Asgari
Categories: cs.AI
DOI: 10.32604/cmc.2025.061263
\\ ( https://arxiv.org/abs/2404.11973 ,  1266kb)
------------------------------------------------------------------------------
\\
arXiv:2404.16957
replaced with revised version Thu, 25 Sep 2025 22:24:28 GMT   (8943kb)

Title: Attributing Responsibility in AI-Induced Incidents: A Computational
  Reflective Equilibrium Framework for Accountability
Authors: Yunfei Ge, Ya-Ting Yang, and Quanyan Zhu
Categories: cs.AI cs.CY
\\ ( https://arxiv.org/abs/2404.16957 ,  8943kb)
------------------------------------------------------------------------------
\\
arXiv:2409.18319
replaced with revised version Thu, 25 Sep 2025 20:44:19 GMT   (2830kb)

Title: Development and Validation of a Large Language Model for Generating
  Fully-Structured Radiology Reports
Authors: Chuang Niu, Md Sayed Tanveer, Md Zabirul Islam, Parisa Kaviani, Qing
  Lyu, Mannudeep K. Kalra, Christopher T. Whitlow, Ge Wang
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2409.18319 ,  2830kb)
------------------------------------------------------------------------------
\\
arXiv:2504.00907
replaced with revised version Fri, 26 Sep 2025 16:06:34 GMT   (13957kb)

Title: Grounding Multimodal LLMs to Embodied Agents that Ask for Help with
  Reinforcement Learning
Authors: Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira,
  Roozbeh Mottaghi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2504.00907 ,  13957kb)
------------------------------------------------------------------------------
\\
arXiv:2504.20924
replaced with revised version Fri, 26 Sep 2025 04:48:04 GMT   (1162kb)

Title: A Domain-Agnostic Scalable AI Safety Ensuring Framework
Authors: Beomjun Kim, Kangyeon Kim, Sunwoo Kim, Yeonsang Shin, and Heejin Ahn
Categories: cs.AI
\\ ( https://arxiv.org/abs/2504.20924 ,  1162kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12833
replaced with revised version Fri, 26 Sep 2025 03:12:20 GMT   (4619kb)

Title: Reasoning BO: Enhancing Bayesian Optimization with Long-Context
  Reasoning Power of LLMs
Authors: Zhuo Yang, Daolang Wang, Lingli Ge, Beilun Wang, Tianfan Fu, Yuqiang
  Li
Categories: cs.AI
\\ ( https://arxiv.org/abs/2505.12833 ,  4619kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12872
replaced with revised version Fri, 26 Sep 2025 02:22:11 GMT   (2105kb)

Title: From Grunts to Lexicons: Emergent Language from Cooperative Foraging
Authors: Maytus Piriyajitakonkij, Rujikorn Charakorn, Weicheng Tao, Wei Pan,
  Mingfei Sun, Cheston Tan, Mengmi Zhang
Categories: cs.AI cs.LG cs.MA
\\ ( https://arxiv.org/abs/2505.12872 ,  2105kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16448
replaced with revised version Fri, 26 Sep 2025 02:01:19 GMT   (4195kb)

Title: The First Impression Problem: Internal Bias Triggers Overthinking in
  Reasoning Models
Authors: Renfei Dang, Zhening Li, Shujian Huang, Jiajun Chen
Categories: cs.AI
\\ ( https://arxiv.org/abs/2505.16448 ,  4195kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21279
replaced with revised version Fri, 26 Sep 2025 09:13:44 GMT   (3753kb)

Title: XBOUND: Exploring Capability Boundaries of Device-Control Agents at the
  State Level
Authors: Shaoqing Zhang, Kehai Chen, Zhuosheng Zhang, Rumei Li, Rongxiang Weng,
  Yang Xiang, Min Zhang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2505.21279 ,  3753kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22954
replaced with revised version Fri, 26 Sep 2025 16:36:03 GMT   (2259kb)

Title: Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents
Authors: Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, Jeff Clune
Categories: cs.AI
Comments: Code at https://github.com/jennyzzt/dgm
\\ ( https://arxiv.org/abs/2505.22954 ,  2259kb)
------------------------------------------------------------------------------
\\
arXiv:2506.01299
replaced with revised version Fri, 26 Sep 2025 07:48:02 GMT   (2600kb)

Title: Scalable In-Context Q-Learning
Authors: Jinmei Liu, Fuhong Liu, Jianye Hao, Bo Wang, Huaxiong Li, Chunlin
  Chen, Zhi Wang
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2506.01299 ,  2600kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11712
replaced with revised version Fri, 26 Sep 2025 01:18:35 GMT   (3441kb)

Title: Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal
  Preference Optimization
Authors: Wenqi Liu, Xuemeng Song, Jiaxi Li, Yinwei Wei, Na Zheng, Jianhua Yin,
  Liqiang Nie
Categories: cs.AI
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2506.11712 ,  3441kb)
------------------------------------------------------------------------------
\\
arXiv:2506.12508
replaced with revised version Fri, 26 Sep 2025 11:51:17 GMT   (8442kb)

Title: AgentOrchestra: Orchestrating Hierarchical Multi-Agent Intelligence with
  the Tool-Environment-Agent(TEA) Protocol
Authors: Wentao Zhang, Liang Zeng, Yuzhen Xiao, Yongcong Li, Ce Cui, Yilei
  Zhao, Rui Hu, Yang Liu, Yahui Zhou, Bo An
Categories: cs.AI
\\ ( https://arxiv.org/abs/2506.12508 ,  8442kb)
------------------------------------------------------------------------------
\\
arXiv:2506.13841
replaced with revised version Fri, 26 Sep 2025 00:39:37 GMT   (209kb)

Title: LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning
Authors: Miho Koda, Yu Zheng, Ruixian Ma, Mingyang Sun, Devesh Pansare, Fabio
  Duarte, Paolo Santi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2506.13841 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13142
replaced with revised version Fri, 26 Sep 2025 13:43:14 GMT   (246kb)

Title: From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement
  Learning
Authors: Ahmed Bahloul, Simon Malberg
Categories: cs.AI cs.CL
Comments: RARA Workshop @ ICDM 2025
\\ ( https://arxiv.org/abs/2507.13142 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2508.02016
replaced with revised version Fri, 26 Sep 2025 01:28:35 GMT   (1432kb)

Title: Dynamic Context Adaptation for Consistent Role-Playing Agents with
  Retrieval-Augmented Generations
Authors: Jeiyoon Park, Yongshin Han, Minseop Kim, Kisu Yang
Categories: cs.AI
Comments: preprint
\\ ( https://arxiv.org/abs/2508.02016 ,  1432kb)
------------------------------------------------------------------------------
\\
arXiv:2508.03174
replaced with revised version Fri, 26 Sep 2025 08:12:46 GMT   (1536kb)

Title: InqEduAgent: Adaptive AI Learning Partners with Gaussian Process
  Augmentation
Authors: Wen-Xi Yang and Tian-Fang Zhao and Guan Liu and Liang Yang and Zi-Tao
  Liu and Wei-Neng Chen
Categories: cs.AI
\\ ( https://arxiv.org/abs/2508.03174 ,  1536kb)
------------------------------------------------------------------------------
\\
arXiv:2508.21475
replaced with revised version Fri, 26 Sep 2025 13:36:22 GMT   (19089kb)

Title: MMSearch-Plus: Benchmarking Provenance-Aware Search for Multimodal
  Browsing Agents
Authors: Xijia Tao, Yihua Teng, Xinxing Su, Xinyu Fu, Jihao Wu, Chaofan Tao,
  Ziru Liu, Haoli Bai, Rui Liu, Lingpeng Kong
Categories: cs.AI
Comments: Project Page: https://mmsearch-plus.github.io
\\ ( https://arxiv.org/abs/2508.21475 ,  19089kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01245
replaced with revised version Fri, 26 Sep 2025 07:10:59 GMT   (194kb)

Title: Towards Agentic OS: An LLM Agent Framework for Linux Schedulers
Authors: Yusheng Zheng, Yanpeng Hu, Wei Zhang, Andi Quinn
Categories: cs.AI cs.MA cs.OS
Journal-ref: MLforSystem 2025
\\ ( https://arxiv.org/abs/2509.01245 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01938
replaced with revised version Fri, 26 Sep 2025 01:58:19 GMT   (3296kb)

Title: EigenBench: A Comparative Behavioral Measure of Value Alignment
Authors: Jonathn Chang, Leonhard Piff, Suvadip Sana, Jasmine X. Li, Lionel
  Levine
Categories: cs.AI cs.CL cs.CY cs.LG
\\ ( https://arxiv.org/abs/2509.01938 ,  3296kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09498
replaced with revised version Fri, 26 Sep 2025 06:26:13 GMT   (183kb)

Title: SEDM: Scalable Self-Evolving Distributed Memory for Agents
Authors: Haoran Xu, Jiacong Hu, Ke Zhang, Lei Yu, Yuxin Tang, Xinyuan Song,
  Yiqun Duan, Lynn Ai, Bill Shi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.09498 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12934
replaced with revised version Thu, 25 Sep 2025 20:31:28 GMT   (825kb)

Title: The Anatomy of Alignment: Decomposing Preference Optimization by
  Steering Sparse Features
Authors: Jeremias Ferrao, Matthijs van der Lende, Ilija Lichkovski, Clement Neo
Categories: cs.AI
Comments: Spotlight at NeurIPS 2025 Mechanistic Interpretability Workshop (WIP)
\\ ( https://arxiv.org/abs/2509.12934 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17978
replaced with revised version Fri, 26 Sep 2025 17:49:26 GMT   (10246kb)

Title: The STAR-XAI Protocol: A Framework for Inducing and Verifying Agency,
  Reasoning, and Reliability in AI Agents
Authors: Antoni Guasch and Maria Isabel Valdez
Categories: cs.AI cs.LO
Comments: Version 2: This article consolidates and replaces a previous version
  to present the complete research in a single, comprehensive manuscript
\\ ( https://arxiv.org/abs/2509.17978 ,  10246kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19517
replaced with revised version Thu, 25 Sep 2025 21:42:07 GMT   (1190kb)

Title: Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop
  Reasoning
Authors: Sai Teja Reddy Adapala
Categories: cs.AI cs.CL cs.LG
ACM-class: I.2.7; I.2.6
\\ ( https://arxiv.org/abs/2509.19517 ,  1190kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20067
replaced with revised version Fri, 26 Sep 2025 02:33:31 GMT   (7539kb)

Title: MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM
Authors: Wenliang Li, Rui Yan, Xu Zhang, Li Chen, Hongji Zhu, Jing Zhao, Junjun
  Li, Mengru Li, Wei Cao, Zihang Jiang, Wei Wei, Kun Zhang, Shaohua Kevin Zhou
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.20067 ,  7539kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21117
replaced with revised version Fri, 26 Sep 2025 05:33:48 GMT   (299kb)

Title: TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them
Authors: Yidong Wang, Yunze Song, Tingyuan Zhu, Xuanwang Zhang, Zhuohao Yu, Hao
  Chen, Chiyu Song, Qiufeng Wang, Cunxiang Wang, Zhen Wu, Xinyu Dai, Yue Zhang,
  Wei Ye, Shikun Zhang
Categories: cs.AI cs.CL
Comments: 22 pages, 9 figures, 6 tables
\\ ( https://arxiv.org/abs/2509.21117 ,  299kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21124
replaced with revised version Fri, 26 Sep 2025 03:10:45 GMT   (6482kb)

Title: Expanding Reasoning Potential in Foundation Model by Learning Diverse
  Chains of Thought Patterns
Authors: Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang,
  Hongfei Yan, Jingang Wang, Xunliang Cai
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2509.21124 ,  6482kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19462
replaced with revised version Fri, 26 Sep 2025 10:39:10 GMT   (679kb)

Title: Constituency Parsing using LLMs
Authors: Xuefeng Bai, Jialong Wu, Yulong Chen, Zhongqing Wang, Kehai Chen, Min
  Zhang, Yue Zhang
Categories: cs.CL
Comments: Accepted at IEEE Transactions on Audio, Speech, and Language
  Processing (TASLP). See https://ieeexplore.ieee.org/document/11130901/ for
  the official version
DOI: 10.1109/taslpro.2025.3600867
\\ ( https://arxiv.org/abs/2310.19462 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12881
replaced with revised version Fri, 26 Sep 2025 14:50:48 GMT   (3546kb)

Title: TEXT2AFFORD: Probing Object Affordance Prediction abilities of Language
  Models solely from Text
Authors: Sayantan Adak, Daivik Agrawal, Animesh Mukherjee and Somak Aditya
Categories: cs.CL
Comments: Accepted at Conference on Computational Natural Language Learning
  2024
DOI: 10.18653/v1/2024.conll-1.27
\\ ( https://arxiv.org/abs/2402.12881 ,  3546kb)
------------------------------------------------------------------------------
\\
arXiv:2406.09265
replaced with revised version Fri, 26 Sep 2025 11:01:36 GMT   (1094kb)

Title: Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs
Authors: Weixuan Wang, Barry Haddow, Minghao Wu, Wei Peng, Alexandra Birch
Categories: cs.CL
\\ ( https://arxiv.org/abs/2406.09265 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2407.04020
replaced with revised version Fri, 26 Sep 2025 13:30:48 GMT   (1272kb)

Title: LLMAEL: Large Language Models are Good Context Augmenters for Entity
  Linking
Authors: Amy Xin, Yunjia Qi, Zijun Yao, Fangwei Zhu, Kaisheng Zeng, Xu Bin, Lei
  Hou, Juanzi Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2407.04020 ,  1272kb)
------------------------------------------------------------------------------
\\
arXiv:2409.14364
replaced with revised version Fri, 26 Sep 2025 15:19:34 GMT   (4078kb)

Title: Position IDs Matter: An Enhanced Position Layout for Efficient Context
  Compression in Large Language Models
Authors: Runsong Zhao, Xin Liu, Xinyu Liu, Pengcheng Huang, Chunyang Xiao, Tong
  Xiao and Jingbo Zhu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2409.14364 ,  4078kb)
------------------------------------------------------------------------------
\\
arXiv:2410.07145
replaced with revised version Fri, 26 Sep 2025 06:31:28 GMT   (349kb)

Title: Stuffed Mamba: Oversized States Lead to the Inability to Forget
Authors: Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong
  Sun
Categories: cs.CL cs.AI cs.LG
Comments: COLM 2025
\\ ( https://arxiv.org/abs/2410.07145 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2410.11020
replaced with revised version Fri, 26 Sep 2025 11:43:55 GMT   (513kb)

Title: Improving the Language Understanding Capabilities of Large Language
  Models Using Reinforcement Learning
Authors: Bokai Hu, Sai Ashish Somayajula, Xin Pan, Pengtao Xie
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2410.11020 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2410.20016
replaced with revised version Thu, 25 Sep 2025 22:38:32 GMT   (310kb)

Title: Vulnerability of LLMs to Vertically Aligned Text Manipulations
Authors: Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Zhen Xiong, Nanyun
  Peng, Kai-wei Chang
Categories: cs.CL
Comments: Accepted to ACL 2025 (Main)
\\ ( https://arxiv.org/abs/2410.20016 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2410.21054
replaced with revised version Fri, 26 Sep 2025 12:48:22 GMT   (5950kb)

Title: Semantic Component Analysis: Introducing Multi-Topic Distributions to
  Clustering-Based Topic Modeling
Authors: Florian Eichin, Carolin M. Schuster, Georg Groh, and Michael A.
  Hedderich
Categories: cs.CL
Comments: 5 pages, 3 figures, code:
  https://github.com/mainlp/semantic_components
\\ ( https://arxiv.org/abs/2410.21054 ,  5950kb)
------------------------------------------------------------------------------
\\
arXiv:2410.23261
replaced with revised version Thu, 25 Sep 2025 22:50:33 GMT   (693kb)

Title: $100K or 100 Days: Trade-offs when Pre-Training with Academic Resources
Authors: Apoorv Khandelwal, Tian Yun, Nihal V. Nayak, Jack Merullo, Stephen H.
  Bach, Chen Sun, Ellie Pavlick
Categories: cs.CL cs.LG
Comments: Published at COLM 2025
\\ ( https://arxiv.org/abs/2410.23261 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2411.16495
replaced with revised version Fri, 26 Sep 2025 13:16:28 GMT   (25716kb)

Title: AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous
  Knowledge Reasoning
Authors: Amy Xin, Jinxin Liu, Zijun Yao, Zhicheng Lee, Shulin Cao, Lei Hou,
  Juanzi Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2411.16495 ,  25716kb)
------------------------------------------------------------------------------
\\
arXiv:2411.17388
replaced with revised version Fri, 26 Sep 2025 14:25:50 GMT   (629kb)

Title: Can LLMs be Good Graph Judge for Knowledge Graph Construction?
Authors: Haoyu Huang, Chong Chen, Zeang Sheng, Yang Li, Wentao Zhang
Categories: cs.CL cs.AI
Comments: EMNLP 2025 Main
\\ ( https://arxiv.org/abs/2411.17388 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2501.04961
replaced with revised version Fri, 26 Sep 2025 00:45:18 GMT   (1762kb)

Title: Demystifying Domain-adaptive Post-training for Financial LLMs
Authors: Zixuan Ke, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty
Categories: cs.CL cs.AI cs.CE cs.LG
Comments: EMNLP 2025 (Oral)
\\ ( https://arxiv.org/abs/2501.04961 ,  1762kb)
------------------------------------------------------------------------------
\\
arXiv:2501.08413
replaced with revised version Fri, 26 Sep 2025 02:57:27 GMT   (5482kb)

Title: Labeling Free-text Data using Language Model Ensembles
Authors: Jiaxing Qiu, Dongliang Guo, Natalie Papini, Noelle Peace, Hannah F.
  Fitterman-Harris, Cheri A. Levinson, Tom Hartvigsen, Teague R. Henry
Categories: cs.CL
\\ ( https://arxiv.org/abs/2501.08413 ,  5482kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12663
replaced with revised version Fri, 26 Sep 2025 16:20:20 GMT   (124kb)

Title: Demystifying Multilingual Chain-of-Thought in Process Reward Modeling
Authors: Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch
Categories: cs.CL
\\ ( https://arxiv.org/abs/2502.12663 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13319
replaced with revised version Thu, 25 Sep 2025 20:51:06 GMT   (698kb)

Title: Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare
Authors: Hiba Ahsan, Arnab Sen Sharma, Silvio Amir, David Bau, Byron C. Wallace
Categories: cs.CL
Comments: Accepted in EMNLP (Findings)
\\ ( https://arxiv.org/abs/2502.13319 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2502.14538
replaced with revised version Fri, 26 Sep 2025 03:04:08 GMT   (339kb)

Title: LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via
  Momentum-Guided Perturbation Optimization
Authors: Yupeng Chang, Chenlu Guo, Yi Chang, and Yuan Wu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2502.14538 ,  339kb)
------------------------------------------------------------------------------
\\
arXiv:2502.21263
replaced with revised version Fri, 26 Sep 2025 14:49:44 GMT   (1127kb)

Title: RuCCoD: Towards Automated ICD Coding in Russian
Authors: Aleksandr Nesterov, Andrey Sakhovskiy, Ivan Sviridov, Airat Valiev,
  Vladimir Makharev, Petr Anokhin, Galina Zubkova, Elena Tutubalina
Categories: cs.CL cs.AI cs.DB
Comments: Accepted to EMNLP 2025 (Main Conference)
\\ ( https://arxiv.org/abs/2502.21263 ,  1127kb)
------------------------------------------------------------------------------
\\
arXiv:2503.01902
replaced with revised version Fri, 26 Sep 2025 03:10:11 GMT   (457kb)

Title: How LLMs Fail to Support Fact-Checking
Authors: Adiba Mahbub Proma, Neeley Pate, James Druckman, Gourab Ghoshal,
  Hangfeng He, Ehsan Hoque
Categories: cs.CL cs.AI
Comments: Adiba and Neeley contributed equally
\\ ( https://arxiv.org/abs/2503.01902 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2503.01986
replaced with revised version Thu, 25 Sep 2025 23:09:21 GMT   (3021kb)

Title: Adaptively profiling models with task elicitation
Authors: Davis Brown, Prithvi Balehannina, Helen Jin, Shreya Havaldar, Hamed
  Hassani, Eric Wong
Categories: cs.CL cs.AI cs.LG
Comments: EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2503.01986 ,  3021kb)
------------------------------------------------------------------------------
\\
arXiv:2503.02519
replaced with revised version Fri, 26 Sep 2025 02:48:59 GMT   (681kb)

Title: Generator-Assistant Stepwise Rollback Framework for Large Language Model
  Agent
Authors: Xingzuo Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yong Xu, Min Zhang
Categories: cs.CL
Comments: EMNLP 2025 Main
\\ ( https://arxiv.org/abs/2503.02519 ,  681kb)
------------------------------------------------------------------------------
\\
arXiv:2503.03064
replaced with revised version Fri, 26 Sep 2025 01:21:52 GMT   (107kb)

Title: Improving LLM-as-a-Judge Inference with the Judgment Distribution
Authors: Victor Wang, Michael J.Q. Zhang, Eunsol Choi
Categories: cs.CL
Comments: EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2503.03064 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2503.06692
replaced with revised version Fri, 26 Sep 2025 02:25:38 GMT   (538kb)

Title: InftyThink: Breaking the Length Limits of Long-Context Reasoning in
  Large Language Models
Authors: Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian
  Shao, Yueting Zhuang
Categories: cs.CL cs.AI
Comments: Project Page: https://zju-real.github.io/InftyThink Code:
  https://github.com/ZJU-REAL/InftyThink Dataset:
  https://huggingface.co/datasets/ZJU-REAL/InftyThink
\\ ( https://arxiv.org/abs/2503.06692 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2503.09579
replaced with revised version Fri, 26 Sep 2025 06:14:31 GMT   (767kb)

Title: Cost-Optimal Grouped-Query Attention for Long-Context Modeling
Authors: Yingfa Chen, Yutong Wu, Chenyang Song, Zhen Leng Thai, Xingyu Shen, Xu
  Han, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI cs.LG
Comments: EMNLP 2025 Main
\\ ( https://arxiv.org/abs/2503.09579 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2503.10150
replaced with revised version Fri, 26 Sep 2025 14:37:04 GMT   (2876kb)

Title: Retrieval-Augmented Generation with Hierarchical Knowledge
Authors: Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen,
  Kaili Ma, Hongzhi Chen, James Cheng
Categories: cs.CL cs.AI
Comments: EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2503.10150 ,  2876kb)
------------------------------------------------------------------------------
\\
arXiv:2503.23768
replaced with revised version Thu, 25 Sep 2025 22:24:21 GMT   (31749kb)

Title: Texture or Semantics? Vision-Language Models Get Lost in Font
  Recognition
Authors: Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei
  Wang
Categories: cs.CL cs.CV
Comments: Accepted to COLM 2025
\\ ( https://arxiv.org/abs/2503.23768 ,  31749kb)
------------------------------------------------------------------------------
\\
arXiv:2504.10823
replaced with revised version Fri, 26 Sep 2025 17:40:31 GMT   (3734kb)

Title: CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from
  Multiple Perspectives
Authors: Ayoung Lee, Ryan Sungmo Kwon, Peter Railton, Lu Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2504.10823 ,  3734kb)
------------------------------------------------------------------------------
\\
arXiv:2504.12633
replaced with revised version Thu, 25 Sep 2025 19:12:37 GMT   (1567kb)

Title: SOLAR: Towards Characterizing Subjectivity of Individuals through
  Modeling Value Conflicts and Trade-offs
Authors: Younghun Lee and Dan Goldwasser
Categories: cs.CL
Comments: Accepted to the Main Conference at EMNLP 2025. 9 pages
\\ ( https://arxiv.org/abs/2504.12633 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15241
replaced with revised version Fri, 26 Sep 2025 15:05:35 GMT   (4329kb)

Title: MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety
Authors: Yahan Yang, Soham Dan, Shuo Li, Dan Roth, Insup Lee
Categories: cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2504.15241 ,  4329kb)
------------------------------------------------------------------------------
\\
arXiv:2504.18346
replaced with revised version Fri, 26 Sep 2025 10:08:32 GMT   (332kb)

Title: Comparing Uncertainty Measurement and Mitigation Methods for Large
  Language Models: A Systematic Review
Authors: Toghrul Abbasli, Kentaroh Toyoda, Yuan Wang, Leon Witt, Muhammad Asif
  Ali, Yukai Miao, Dan Li, Qingsong Wei
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2504.18346 ,  332kb)
------------------------------------------------------------------------------
\\
arXiv:2505.02091
replaced with revised version Fri, 26 Sep 2025 04:15:55 GMT   (3843kb)

Title: LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for
  Non-Convex Problems in Wireless Communications
Authors: Xinyue Peng, Yanming Liu, Yihan Cang, Chaoqun Cao, Ming Chen
Categories: cs.CL cs.LG
Comments: 6 pages,4 figures
\\ ( https://arxiv.org/abs/2505.02091 ,  3843kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11140
replaced with revised version Fri, 26 Sep 2025 09:25:25 GMT   (435kb)

Title: Follow the Path: Reasoning over Knowledge Graph Paths to Improve LLM
  Factuality
Authors: Mike Zhang and Johannes Bjerva and Russa Biswas
Categories: cs.CL cs.AI
Comments: Updated version 26.9
\\ ( https://arxiv.org/abs/2505.11140 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11480
replaced with revised version Thu, 25 Sep 2025 21:58:56 GMT   (208kb)

Title: SuperCoder: Assembly Program Superoptimization with Large Language
  Models
Authors: Anjiang Wei, Tarun Suresh, Huanmi Tan, Yinglun Xu, Gagandeep Singh, Ke
  Wang, Alex Aiken
Categories: cs.CL cs.AI cs.PF cs.PL cs.SE
\\ ( https://arxiv.org/abs/2505.11480 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11556
replaced with revised version Thu, 25 Sep 2025 19:30:07 GMT   (564kb)

Title: HiddenBench: Assessing Collective Reasoning in Multi-Agent LLMs via
  Hidden Profile Tasks
Authors: Yuxuan Li, Aoi Naito, Hirokazu Shirado
Categories: cs.CL cs.AI cs.MA
\\ ( https://arxiv.org/abs/2505.11556 ,  564kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11739
replaced with revised version Fri, 26 Sep 2025 03:55:57 GMT   (3138kb)

Title: ZeroTuning: Unlocking the Initial Token's Power to Enhance Large
  Language Models Without Training
Authors: Feijiang Han, Xiaodong Yu, Jianheng Tang, Delip Rao, Weihua Du, Lyle
  Ungar
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.11739 ,  3138kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12300
replaced with revised version Fri, 26 Sep 2025 16:33:17 GMT   (332kb)

Title: HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language
  Models
Authors: Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.12300 ,  332kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12313
replaced with revised version Fri, 26 Sep 2025 16:35:13 GMT   (765kb)

Title: ExpertSteer: Intervening in LLMs through Expert Knowledge
Authors: Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.12313 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12716
replaced with revised version Fri, 26 Sep 2025 03:43:47 GMT   (16498kb)

Title: Shadow-FT: Tuning Instruct Model via Training on Paired Base Model
Authors: Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Yik-Chung Wu, Ngai
  Wong, Yujiu Yang
Categories: cs.CL cs.AI
Comments: 24 pages, 12 tables, 8 figures. Previous name: Shadow-FT: Tuning
  Instruct via Base
\\ ( https://arxiv.org/abs/2505.12716 ,  16498kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13141
replaced with revised version Fri, 26 Sep 2025 03:42:48 GMT   (2693kb)

Title: Language-Specific Latent Process Hinders Cross-Lingual Performance
Authors: Zheng Wei Lim, Alham Fikri Aji, Trevor Cohn
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.13141 ,  2693kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14679
replaced with revised version Fri, 26 Sep 2025 04:37:01 GMT   (632kb)

Title: UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in
  Language Models
Authors: Xiaojie Gu, Ziying Huang, Jia-Chen Gu, Kai Zhang
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2505.14679 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14874
replaced with revised version Thu, 25 Sep 2025 21:15:58 GMT   (273kb)

Title: Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric
  Speech Recognition in Low-Resource Languages
Authors: Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea P\'erez-Toro,
  Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave,
  Elmar N\"oth, David R. Mortensen
Categories: cs.CL cs.SD eess.AS
Comments: 5 pages, 1 figure, Proceedings of Interspeech
DOI: 10.21437/Interspeech.2025-512
\\ ( https://arxiv.org/abs/2505.14874 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15674
replaced with revised version Fri, 26 Sep 2025 01:52:07 GMT   (4460kb)

Title: UniErase: Towards Balanced and Precise Unlearning in Language Models
Authors: Miao Yu, Liang Lin, Guibin Zhang, Xinfeng Li, Junfeng Fang, Xingrui
  Yu, Ivor Tsang, Ningyu Zhang, Kun Wang, Yang Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.15674 ,  4460kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16134
replaced with revised version Fri, 26 Sep 2025 15:21:49 GMT   (2376kb)

Title: Beyond Early-Token Bias: Model-Specific and Language-Specific Position
  Effects in Multilingual LLMs
Authors: Mikhail Menschikov, Alexander Kharitonov, Maiia Kotyga, Vadim
  Porvatov, Anna Zhukovskaya, David Kagramanyan, Egor Shvetsov, Evgeny Burnaev
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2505.16134 ,  2376kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16415
replaced with revised version Fri, 26 Sep 2025 04:58:14 GMT   (13651kb)

Title: Attributing Response to Context: A Jensen-Shannon Divergence Driven
  Mechanistic Study of Context Attribution in Retrieval-Augmented Generation
Authors: Ruizhe Li, Chen Chen, Yuchen Hu, Yanjun Gao, Xi Wang, Emine Yilmaz
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at NeurIPS 2025 Mechanistic Interpretability Workshop
\\ ( https://arxiv.org/abs/2505.16415 ,  13651kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16429
replaced with revised version Fri, 26 Sep 2025 02:01:40 GMT   (3724kb)

Title: Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform
  for Dynamic Recommender Systems
Authors: Song Jin, Juntian Zhang, Yuhan Liu, Xun Zhang, Yufei Zhang, Guojun
  Yin, Fei Jiang, Wei Lin, Rui Yan
Categories: cs.CL cs.AI
Comments: EMNLP2025 Main
\\ ( https://arxiv.org/abs/2505.16429 ,  3724kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16831
replaced with revised version Fri, 26 Sep 2025 07:26:51 GMT   (1884kb)

Title: Unlearning Isn't Deletion: Investigating Reversibility of Machine
  Unlearning in LLMs
Authors: Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Huadi Zheng, Peizhao Hu,
  Minxin Du, Haibo Hu
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: 46 pages
\\ ( https://arxiv.org/abs/2505.16831 ,  1884kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16965
replaced with revised version Thu, 25 Sep 2025 19:51:05 GMT   (6873kb)

Title: BP-Seg: A graphical model approach to unsupervised and non-contiguous
  text segmentation using belief propagation
Authors: Fengyi Li, Kayhan Behdin, Natesh Pillai, Xiaofeng Wang, Zhipeng Wang,
  Ercan Yildiz
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2505.16965 ,  6873kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17117
replaced with revised version Thu, 25 Sep 2025 21:34:22 GMT   (5945kb)

Title: From Tokens to Thoughts: How LLMs and Humans Trade Compression for
  Meaning
Authors: Chen Shani, Liron Soffer, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv
Categories: cs.CL cs.AI cs.IT math.IT
\\ ( https://arxiv.org/abs/2505.17117 ,  5945kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19660
replaced with revised version Fri, 26 Sep 2025 03:49:19 GMT   (689kb)

Title: Prompting is not Enough: Exploring Knowledge Integration and
  Controllable Generation on Large Language Models
Authors: Tingjia Shen, Hao Wang, Chuan Qin, Ruijun Sun, Yang Song, Defu Lian,
  Hengshu Zhu, and Enhong Chen
Categories: cs.CL cs.AI
Comments: 13 pages, 5 figures
MSC-class: 68P20
ACM-class: H.3.4; I.2.6
\\ ( https://arxiv.org/abs/2505.19660 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20321
replaced with revised version Fri, 26 Sep 2025 15:14:24 GMT   (777kb)

Title: BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge
  Bases
Authors: Mathew J. Koretsky, Maya Willey, Adi Asija, Owen Bianchi, Chelsea X.
  Alvarado, Tanay Nayak, Nicole Kuznetsov, Sungwon Kim, Mike A. Nalls, Daniel
  Khashabi, Faraz Faghri
Categories: cs.CL cs.AI cs.LG
Comments: Under Review
\\ ( https://arxiv.org/abs/2505.20321 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23297
replaced with revised version Fri, 26 Sep 2025 09:31:00 GMT   (10055kb)

Title: EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian
Authors: Daryna Dementieva, Nikolay Babakov, and Alexander Fraser
Categories: cs.CL
Comments: EMNLP2025, Findings
\\ ( https://arxiv.org/abs/2505.23297 ,  10055kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23621
replaced with revised version Fri, 26 Sep 2025 02:16:13 GMT   (8960kb)

Title: Table-R1: Inference-Time Scaling for Table Reasoning
Authors: Zheyuan Yang, Lyuhao Chen, Arman Cohan, Yilun Zhao
Categories: cs.CL
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2505.23621 ,  8960kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23867
replaced with revised version Fri, 26 Sep 2025 03:34:37 GMT   (2433kb)

Title: InfiMed: Low-Resource Medical MLLMs with Advancing Understanding and
  Reasoning
Authors: Zeyu Liu, Zhitian Hou, Guanghao Zhu, Zhijie Sang, Congkai Xie, Hongxia
  Yang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.23867 ,  2433kb)
------------------------------------------------------------------------------
\\
arXiv:2506.00789
replaced with revised version Thu, 25 Sep 2025 20:17:09 GMT   (534kb)

Title: RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented
  Generation Systems
Authors: Yixiao Zeng, Tianyu Cao, Danqing Wang, Xinran Zhao, Zimeng Qiu,
  Morteza Ziyadi, Tongshuang Wu, Lei Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2506.00789 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2506.01042
replaced with revised version Thu, 25 Sep 2025 20:23:14 GMT   (2308kb)

Title: Probing Neural Topology of Large Language Models
Authors: Yu Zheng, Yuan Yuan, Yue Zhuo, Yong Li, and Paolo Santi
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2506.01042 ,  2308kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05154
replaced with revised version Fri, 26 Sep 2025 07:11:36 GMT   (324kb)

Title: Resisting Contextual Interference in RAG via Parametric-Knowledge
  Reinforcement
Authors: Chenyu Lin, Yilin Wen, Du Su, Hexiang Tan, Fei Sun, Muhan Chen, Chenfu
  Bao, Zhonghou Lyu
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2506.05154 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2506.08123
replaced with revised version Fri, 26 Sep 2025 14:57:59 GMT   (979kb)

Title: QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA
Authors: Jacob Dineen, Aswin RRV, Qin Liu, Zhikun Xu, Xiao Ye, Ming Shen,
  Zhaonan Li, Shijie Lu, Chitta Baral, Muhao Chen, Ben Zhou
Categories: cs.CL
Comments: Accepted to Findings of EMNLP 2025
\\ ( https://arxiv.org/abs/2506.08123 ,  979kb)
------------------------------------------------------------------------------
\\
arXiv:2506.12109
replaced with revised version Fri, 26 Sep 2025 05:35:55 GMT   (2842kb)

Title: Personalized LLM Decoding via Contrasting Personal Preference
Authors: Hyungjune Bu, Chanjoo Jung, Minjae Kang, Jaehyung Kim
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2506.12109 ,  2842kb)
------------------------------------------------------------------------------
\\
arXiv:2506.17046
replaced with revised version Fri, 26 Sep 2025 14:51:36 GMT   (37817kb)

Title: MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for
  Multimodal Large Language Models
Authors: Xiaolong Wang, Zhaolu Kang, Wangyuxuan Zhai, Xinyue Lou, Yunghwei Lai,
  Ziyue Wang, Yawen Wang, Kaiyu Huang, Yile Wang, Peng Li, Yang Liu
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2506.17046 ,  37817kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20923
replaced with revised version Fri, 26 Sep 2025 06:08:11 GMT   (5129kb)

Title: KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A
  Versatile Embedding Model
Authors: Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Xin
  Zhang, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, Youcheng Pan, Yang
  Xiang, Meishan Zhang, Haofen Wang, Jun Yu, Baotian Hu, Min Zhang
Categories: cs.CL
Comments: 32 pages, 16 tables, 5 figures
\\ ( https://arxiv.org/abs/2506.20923 ,  5129kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21556
replaced with revised version Fri, 26 Sep 2025 08:43:45 GMT   (2739kb)

Title: VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for
  Retrieval-Augmented Generation
Authors: Hyeongcheol Park, Jiyoung Seo, MinHyuk Jang, Hogun Park, Ha Dam Baek,
  Gyusam Chang, Hyeonsoo Im, Sangpil Kim
Categories: cs.CL
Comments: Project Page: https://vatkg.github.io/
\\ ( https://arxiv.org/abs/2506.21556 ,  2739kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21875
replaced with revised version Fri, 26 Sep 2025 09:14:14 GMT   (560kb)

Title: WildSpeech-Bench: Benchmarking End-to-End SpeechLLMs in the Wild
Authors: Linhao Zhang, Jian Zhang, Bokai Lei, Chuhan Wu, Aiwei Liu, Wei Jia,
  Xiao Zhou
Categories: cs.CL
\\ ( https://arxiv.org/abs/2506.21875 ,  560kb)
------------------------------------------------------------------------------
\\
arXiv:2506.23508
replaced with revised version Fri, 26 Sep 2025 05:33:22 GMT   (2141kb)

Title: Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge
  Better: A Data Perspective
Authors: Zhihao Zhang, Qiaole Dong, Qi Zhang, Jun Zhao, Enyu Zhou, Zhiheng Xi,
  Senjie Jin, Xiaoran Fan, Yuhao Zhou, Mingqi Wu, Yanwei Fu, Tao Ji, Tao Gui,
  Xuanjing Huang and Kai Chen
Categories: cs.CL cs.AI
Comments: 20 pages (Preprint.)
\\ ( https://arxiv.org/abs/2506.23508 ,  2141kb)
------------------------------------------------------------------------------
\\
arXiv:2507.04504
replaced with revised version Fri, 26 Sep 2025 01:46:39 GMT   (987kb)

Title: Unveiling the Potential of Diffusion Large Language Model in
  Controllable Generation
Authors: Zhen Xiong, Yujun Cai, Zhecheng Li, Yiwei Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2507.04504 ,  987kb)
------------------------------------------------------------------------------
\\
arXiv:2507.05257
replaced with revised version Fri, 26 Sep 2025 03:31:14 GMT   (2932kb)

Title: Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions
Authors: Yuanzhe Hu, Yu Wang, Julian McAuley
Categories: cs.CL cs.AI
Comments: Y. Hu and Y. Wang contribute equally
\\ ( https://arxiv.org/abs/2507.05257 ,  2932kb)
------------------------------------------------------------------------------
\\
arXiv:2507.05418
replaced with revised version Fri, 26 Sep 2025 17:57:11 GMT   (1479kb)

Title: Learn Globally, Speak Locally: Bridging the Gaps in Multilingual
  Reasoning
Authors: Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid
  Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2507.05418 ,  1479kb)
------------------------------------------------------------------------------
\\
arXiv:2507.08339
replaced with revised version Fri, 26 Sep 2025 07:02:45 GMT   (256kb)

Title: What Factors Affect LLMs and RLLMs in Financial Question Answering?
Authors: Peng Wang, Xuesi Hu, Jiageng Wu, Yuntao Zou, Qiancheng Zhang, Dagang
  Li
Categories: cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2507.08339 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2507.08799
replaced with revised version Fri, 26 Sep 2025 17:59:54 GMT   (218kb)

Title: KV Cache Steering for Controlling Frozen LLMs
Authors: Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb
  Mirza, James R. Glass, Cees G. M. Snoek, Yuki M. Asano
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2507.08799 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13332
replaced with revised version Fri, 26 Sep 2025 05:58:32 GMT   (4861kb)

Title: The Imitation Game: Turing Machine Imitator is Length Generalizable
  Reasoner
Authors: Zhouqi Hua, Wenwei Zhang, Chengqi Lyu, Yuzhe Gu, Songyang Gao, Kuikun
  Liu, Dahua Lin, Kai Chen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2507.13332 ,  4861kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13681
replaced with revised version Fri, 26 Sep 2025 07:14:44 GMT   (711kb)

Title: LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for
  Multi-Turn Dialogues
Authors: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin
  Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2507.13681 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2507.18578
replaced with revised version Fri, 26 Sep 2025 04:15:45 GMT   (1114kb)

Title: Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective
  DLLMs
Authors: Feng Hong, Geng Yu, Yushi Ye, Haicheng Huang, Huangjie Zheng, Ya
  Zhang, Yanfeng Wang, Jiangchao Yao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2507.18578 ,  1114kb)
------------------------------------------------------------------------------
\\
arXiv:2507.20673
replaced with revised version Fri, 26 Sep 2025 11:33:27 GMT   (5395kb)

Title: Geometric-Mean Policy Optimization
Authors: Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao,
  Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei
Categories: cs.CL
Comments: Code is available at https://github.com/callsys/GMPO
\\ ( https://arxiv.org/abs/2507.20673 ,  5395kb)
------------------------------------------------------------------------------
\\
arXiv:2507.22168
replaced with revised version Thu, 25 Sep 2025 21:37:43 GMT   (863kb)

Title: Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing
  Styles
Authors: Kimberly Le Truong, Riccardo Fogliato, Hoda Heidari, Zhiwei Steven Wu
Categories: cs.CL cs.AI
Comments: Accepted to EMNLP 2025
\\ ( https://arxiv.org/abs/2507.22168 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2508.00344
replaced with revised version Fri, 26 Sep 2025 07:04:34 GMT   (10395kb)

Title: PilotRL: Training Language Model Agents via Global Planning-Guided
  Progressive Reinforcement Learning
Authors: Keer Lu, Chong Chen, Bin Cui, Huang Leng, Wentao Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2508.00344 ,  10395kb)
------------------------------------------------------------------------------
\\
arXiv:2508.00719
replaced with revised version Thu, 25 Sep 2025 20:25:22 GMT   (593kb)

Title: DAMR: Efficient and Adaptive Context-Aware Knowledge Graph Question
  Answering with LLM-Guided MCTS
Authors: Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siyang Gao, Chao Wang, Nan Yin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2508.00719 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01832
replaced with revised version Fri, 26 Sep 2025 03:52:08 GMT   (1188kb)

Title: MLP Memory: A Retriever-Pretrained Memory for Large Language Models
Authors: Rubin Wei, Jiaqi Cao, Jiarui Wang, Jushi Kai, Qipeng Guo, Bowen Zhou,
  Zhouhan Lin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2508.01832 ,  1188kb)
------------------------------------------------------------------------------
\\
arXiv:2508.03860
replaced with revised version Fri, 26 Sep 2025 09:54:51 GMT   (4783kb)

Title: Hallucination to Truth: A Review of Fact-Checking and Factuality
  Evaluation in Large Language Models
Authors: Subhey Sadi Rahman, Md. Adnanul Islam, Md. Mahbub Alam, Musarrat Zeba,
  Md. Abdur Rahman, Sadia Sultana Chowa, Mohaimenul Azam Khan Raiaan, Sami Azam
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2508.03860 ,  4783kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04349
replaced with revised version Fri, 26 Sep 2025 14:04:07 GMT   (2055kb)

Title: GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy
  Entropy
Authors: Hongze Tan and Jianfei Pan and Jinghao Lin and Tao Chen and Zhihang
  Zheng and Zhihao Tang and Haihua Yang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2508.04349 ,  2055kb)
------------------------------------------------------------------------------
\\
arXiv:2508.05282
replaced with revised version Fri, 26 Sep 2025 11:53:35 GMT   (913kb)

Title: ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for
  Late-Stage Fragility in LLMs
Authors: Dongxu Zhang, Ning Yang, Jihua Zhu, Jinnan Yang, Miao Xin and Baoliang
  Tian
Categories: cs.CL
\\ ( https://arxiv.org/abs/2508.05282 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2508.12461
replaced with revised version Fri, 26 Sep 2025 12:42:33 GMT   (298kb)

Title: Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open
  Source Models
Authors: Ziqian Bi, Keyu Chen, Chiung-Yi Tseng, Danyang Zhang, Tianyang Wang,
  Hongying Luo, Lu Chen, Junming Huang, Jibin Guan, Junfeng Hao, Junhao Song
Categories: cs.CL
\\ ( https://arxiv.org/abs/2508.12461 ,  298kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15253
replaced with revised version Fri, 26 Sep 2025 09:07:03 GMT   (1114kb)

Title: Conflict-Aware Soft Prompting for Retrieval-Augmented Generation
Authors: Eunseong Choi, June Park, Hyeri Lee, Jongwuk Lee
Categories: cs.CL cs.AI
Comments: Accepted to EMNLP 2025; 15 pages; 5 figures, 11 tables; Code
  available at https://github.com/eunseongc/CARE
\\ ( https://arxiv.org/abs/2508.15253 ,  1114kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15475
replaced with revised version Fri, 26 Sep 2025 09:45:06 GMT   (3262kb)

Title: Influence-driven Curriculum Learning for Pre-training on Limited Data
Authors: Loris Schoenegger, Lukas Thoma, Terra Blevins, Benjamin Roth
Categories: cs.CL cs.LG
Comments: Added acknowledgments section. 9 pages, Accepted to the BabyLM
  Workshop at EMNLP 2025
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2508.15475 ,  3262kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16876
replaced with revised version Fri, 26 Sep 2025 02:30:43 GMT   (391kb)

Title: Dream to Chat: Model-based Reinforcement Learning on Dialogues with User
  Belief Modeling
Authors: Yue Zhao, Xiaoyu Wang, Dan Wang, Zhonglin Jiang, Qingqing Gu, Teng
  Chen, Ningyuan Xi, Jinxian Qu, Yong Chen, Luo Ji
Categories: cs.CL cs.AI
Comments: Accepted to EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2508.16876 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2509.02097
replaced with revised version Fri, 26 Sep 2025 02:22:22 GMT   (2084kb)

Title: JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with
  Agent-as-Interviewer
Authors: Zhichao Shi, Xuhui Jiang, Chengjin Xu, Cangli Yao, Zhenxin Huang,
  Shengjie Ma, Yinghan Shen, Jian Guo, Yuanzhuo Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2509.02097 ,  2084kb)
------------------------------------------------------------------------------
\\
arXiv:2509.02123
replaced with revised version Fri, 26 Sep 2025 07:25:26 GMT   (1044kb)

Title: CMRAG: Co-modality-based visual document retrieval and question
  answering
Authors: Wang Chen, Wenhan Yu, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha,
  Deguo Xia, Jizhou Huang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2509.02123 ,  1044kb)
------------------------------------------------------------------------------
\\
arXiv:2509.03918
replaced with revised version Fri, 26 Sep 2025 15:57:36 GMT   (1472kb)

Title: Chain or tree? Re-evaluating complex reasoning from the perspective of a
  matrix of thought
Authors: Fengxiao Tang, Yufeng Li, Zongzong Wu, Ming Zhao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2509.03918 ,  1472kb)
------------------------------------------------------------------------------
\\
arXiv:2509.04059
replaced with revised version Fri, 26 Sep 2025 05:37:24 GMT   (3916kb)

Title: Towards an AI Musician: Synthesizing Sheet Music Problems for Musical
  Reasoning
Authors: Zhilin Wang, Zhe Yang, Yun Luo, Yafu Li, Xiaoye Qu, Ziqian Qiao,
  Haoran Zhang, Runzhe Zhan, Derek F. Wong, Jizhe Zhou, Yu Cheng
Categories: cs.CL
Comments: 34 pages
\\ ( https://arxiv.org/abs/2509.04059 ,  3916kb)
------------------------------------------------------------------------------
\\
arXiv:2509.06501
replaced with revised version Fri, 26 Sep 2025 02:31:59 GMT   (266kb)

Title: WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents
Authors: Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu
  Cheng, Zijia Wu, Chengyu Du, Qidi Xu, Jiayuan Song, Zhengmao Zhu, Wenhu Chen,
  Pengyu Zhao, Junxian He
Categories: cs.CL
\\ ( https://arxiv.org/abs/2509.06501 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09381
replaced with revised version Fri, 26 Sep 2025 13:52:31 GMT   (79kb)

Title: Modelling Analogies and Analogical Reasoning: Connecting Cognitive
  Science Theory and NLP Research
Authors: Molly R Petersen, Claire E Stevenson, Lonneke van der Plas
Categories: cs.CL
Comments: Accepted to Transactions of the Association for Computational
  Linguistics (TACL)
\\ ( https://arxiv.org/abs/2509.09381 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10739
replaced with revised version Fri, 26 Sep 2025 15:28:16 GMT   (925kb)

Title: Reasoning Under Uncertainty: Exploring Probabilistic Reasoning
  Capabilities of LLMs
Authors: Mobina Pournemat, Keivan Rezaei, Gaurang Sriramanan, Arman Zarei,
  Jiaxiang Fu, Yang Wang, Hamid Eghbalzadeh, Soheil Feizi
Categories: cs.CL
Comments: 27 pages, 4 figures
\\ ( https://arxiv.org/abs/2509.10739 ,  925kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12635
replaced with revised version Thu, 25 Sep 2025 21:11:04 GMT   (237kb)

Title: Positional Encoding via Token-Aware Phase Attention
Authors: Yu Wang, Sheng Shen, R\'emi Munos, Hongyuan Zhan, Yuandong Tian
Categories: cs.CL cs.AI
Comments: 24 pages
\\ ( https://arxiv.org/abs/2509.12635 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15587
replaced with revised version Fri, 26 Sep 2025 07:57:51 GMT   (320kb)

Title: DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation
  in Large Language Models
Authors: Tsz Ting Chung, Lemao Liu, Mo Yu, Dit-Yan Yeung
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by EMNLP 2025. Project Page:
  https://ttchungc.github.io/projects/divlogiceval/
\\ ( https://arxiv.org/abs/2509.15587 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15888
replaced with revised version Fri, 26 Sep 2025 07:27:02 GMT   (841kb)

Title: Distribution-Aligned Decoding for Efficient LLM Task Adaptation
Authors: Senkang Hu, Xudong Han, Jinqi Jiang, Yihang Tao, Zihan Fang, Yong Dai,
  Sam Tak Wu Kwong, Yuguang Fang
Categories: cs.CL cs.AI
Comments: Accepted by NeurIPS'25
\\ ( https://arxiv.org/abs/2509.15888 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16198
replaced with revised version Fri, 26 Sep 2025 02:50:53 GMT   (5262kb)

Title: RPG: A Repository Planning Graph for Unified and Scalable Codebase
  Generation
Authors: Jane Luo, Xin Zhang, Steven Liu, Jie Wu, Yiming Huang, Yangyu Huang,
  Chengyu Yin, Ying Xin, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qi Chen, Scarlett
  Li, Mao Yang
Categories: cs.CL cs.AI cs.SE
\\ ( https://arxiv.org/abs/2509.16198 ,  5262kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17428
replaced with revised version Fri, 26 Sep 2025 11:23:43 GMT   (2547kb)

Title: QWHA: Quantization-Aware Walsh-Hadamard Adaptation for
  Parameter-Efficient Fine-Tuning on Large Language Models
Authors: Hyesung Jeon, Seojune Lee, Beomseok Kang, Yulhwa Kim, Jae-Joon Kim
Categories: cs.CL
Comments: 25 pages, 9 figures, 14 tables
\\ ( https://arxiv.org/abs/2509.17428 ,  2547kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17445
replaced with revised version Thu, 25 Sep 2025 02:28:29 GMT   (2419kb)

Title: Semantic Reformulation Entropy for Robust Hallucination Detection in QA
  Tasks
Authors: Chaodong Tong, Qi Zhang, Lei Jiang, Yanbing Liu, Nannan Sun, Wei Li
Categories: cs.CL
Comments: 5pages, 5 figures, submitted to ICASSP 2026,
\\ ( https://arxiv.org/abs/2509.17445 ,  2419kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19742
replaced with revised version Fri, 26 Sep 2025 07:05:22 GMT   (672kb)

Title: HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical
  Collaborative LoRA for Zero-Shot DST
Authors: Shuyu Zhang, Yifan Wei, Xinru Wang, Yanmin Zhu, Yangfan He, Yixuan
  Weng, Bin Li
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2509.19742 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20186
replaced with revised version Fri, 26 Sep 2025 08:29:15 GMT   (253kb)

Title: Thinking Augmented Pre-training
Authors: Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei
Categories: cs.CL cs.LG
Comments: 19 pages
\\ ( https://arxiv.org/abs/2509.20186 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20900
replaced with revised version Fri, 26 Sep 2025 16:37:45 GMT   (232kb)

Title: Learning to Summarize by Learning to Quiz: Adversarial Agentic
  Collaboration for Long Document Summarization
Authors: Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch
Categories: cs.CL
\\ ( https://arxiv.org/abs/2509.20900 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20916
replaced with revised version Fri, 26 Sep 2025 12:52:14 GMT   (382kb)

Title: Cross-Linguistic Analysis of Memory Load in Sentence Comprehension:
  Linear Distance and Structural Density
Authors: Krishna Aggarwal
Categories: cs.CL q-bio.NC
Comments: 7 pages, 4 figures (Figure 2 has 3 sub-divisions)
\\ ( https://arxiv.org/abs/2509.20916 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21155
replaced with revised version Fri, 26 Sep 2025 02:37:41 GMT   (768kb)

Title: Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in
  Language Models
Authors: Chantal Shaib, Vinith M. Suriyakumar, Levent Sagun, Byron C. Wallace,
  Marzyeh Ghassemi
Categories: cs.CL
Comments: NeurIPS 2025 Spotlight
\\ ( https://arxiv.org/abs/2509.21155 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21192
replaced with revised version Fri, 26 Sep 2025 12:02:21 GMT   (1583kb)

Title: GEP: A GCG-Based method for extracting personally identifiable
  information from chatbots built on small language models
Authors: Jieli Zhu and Vi Ngoc-Nha Tran
Categories: cs.CL
Comments: 16 pages, 5 figures, 4 tables
\\ ( https://arxiv.org/abs/2509.21192 ,  1583kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21305
replaced with revised version Fri, 26 Sep 2025 12:15:36 GMT   (380kb)

Title: Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors
  in LLMs
Authors: Daniel Vennemeyer, Phan Anh Duong, Tiffany Zhan, Tianyu Jiang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2509.21305 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2204.05798
replaced with revised version Fri, 26 Sep 2025 16:46:44 GMT   (10480kb)

Title: Multi-View Hypercomplex Learning for Breast Cancer Screening
Authors: Eleonora Lopez, Eleonora Grassucci, and Danilo Comminiello
Categories: cs.CV cs.AI cs.LG
Comments: This paper has been submitted to Expert Systems with Applications
\\ ( https://arxiv.org/abs/2204.05798 ,  10480kb)
------------------------------------------------------------------------------
\\
arXiv:2405.10014
replaced with revised version Fri, 26 Sep 2025 06:03:06 GMT   (3228kb)

Title: Frequency-Domain Refinement with Multiscale Diffusion for Super
  Resolution
Authors: Xingjian Wang, Li Chai, Jiming Chen
Categories: cs.CV eess.IV
\\ ( https://arxiv.org/abs/2405.10014 ,  3228kb)
------------------------------------------------------------------------------
\\
arXiv:2410.03039
replaced with revised version Fri, 26 Sep 2025 02:37:14 GMT   (23572kb)

Title: Leveraging Model Guidance to Extract Training Data from Personalized
  Diffusion Models
Authors: Xiaoyu Wu, Jiaru Zhang, Zhiwei Steven Wu
Categories: cs.CV cs.AI cs.LG
Comments: Accepted at the International Conference on Machine Learning (ICML)
  2025
\\ ( https://arxiv.org/abs/2410.03039 ,  23572kb)
------------------------------------------------------------------------------
\\
arXiv:2410.03858
replaced with revised version Fri, 26 Sep 2025 08:51:11 GMT   (5708kb)

Title: Pose Prior Learner: Unsupervised Categorical Prior Learning for Pose
  Estimation
Authors: Ziyu Wang, Shuangpeng Han, Mengmi Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2410.03858 ,  5708kb)
------------------------------------------------------------------------------
\\
arXiv:2410.13674
replaced with revised version Fri, 26 Sep 2025 16:24:32 GMT   (18028kb)

Title: Diffusion Curriculum: Synthetic-to-Real Data Curriculum via Image-Guided
  Diffusion
Authors: Yijun Liang, Shweta Bhardwaj, Tianyi Zhou
Categories: cs.CV cs.AI
Comments: Accepted in ICCV2025. 22 pages, including references and appendix.
  Code is available at http://github.com/tianyi-lab/DisCL
\\ ( https://arxiv.org/abs/2410.13674 ,  18028kb)
------------------------------------------------------------------------------
\\
arXiv:2410.21582
replaced with revised version Fri, 26 Sep 2025 17:57:05 GMT   (2095kb)

Title: Large Pre-Training Datasets Don't Always Guarantee Robustness after
  Fine-Tuning
Authors: Jaedong Hwang, Brian Cheung, Zhang-Wei Hong, Akhilan Boopathy, Pulkit
  Agrawal, Ila Fiete
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2410.21582 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2411.18671
replaced with revised version Fri, 26 Sep 2025 08:17:26 GMT   (15751kb)

Title: TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any
  Point in Long Video
Authors: Jinyuan Qu, Hongyang Li, Shilong Liu, Tianhe Ren, Zhaoyang Zeng, Lei
  Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2411.18671 ,  15751kb)
------------------------------------------------------------------------------
\\
arXiv:2412.03255
replaced with revised version Fri, 26 Sep 2025 09:04:37 GMT   (17462kb)

Title: DynamicControl: Adaptive Condition Selection for Improved Text-to-Image
  Generation
Authors: Qingdong He, Jinlong Peng, Pengcheng Xu, Boyuan Jiang, Xiaobin Hu,
  Donghao Luo, Yong Liu, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning
  Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2412.03255 ,  17462kb)
------------------------------------------------------------------------------
\\
arXiv:2412.05271
replaced with revised version Fri, 26 Sep 2025 12:52:41 GMT   (1850kb)

Title: Expanding Performance Boundaries of Open-Source Multimodal Models with
  Model, Data, and Test-Time Scaling
Authors: Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui,
  Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang,
  Qingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo
  Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao,
  Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen,
  Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin,
  Yu Qiao, Jifeng Dai, Wenhai Wang
Categories: cs.CV
Comments: Technical Report
\\ ( https://arxiv.org/abs/2412.05271 ,  1850kb)
------------------------------------------------------------------------------
\\
arXiv:2412.05827
replaced with revised version Fri, 26 Sep 2025 05:47:13 GMT   (8405kb)

Title: Self-Guidance: Boosting Flow and Diffusion Generation on Their Own
Authors: Tiancheng Li, Weijian Luo, Zhiyang Chen, Liyuan Ma, Guo-Jun Qi
Categories: cs.CV
Comments: 16 pages, 13 figures
\\ ( https://arxiv.org/abs/2412.05827 ,  8405kb)
------------------------------------------------------------------------------
\\
arXiv:2412.07385
replaced with revised version Thu, 25 Sep 2025 21:08:54 GMT   (1894kb)

Title: LOGen: Toward Lidar Object Generation by Point Diffusion
Authors: Ellington Kirby, Mickael Chen, Renaud Marlet, Nermin Samet
Categories: cs.CV
Comments: BMVC 2025
\\ ( https://arxiv.org/abs/2412.07385 ,  1894kb)
------------------------------------------------------------------------------
\\
arXiv:2412.15216
replaced with revised version Fri, 26 Sep 2025 17:59:56 GMT   (5860kb)

Title: UIP2P: Unsupervised Instruction-based Image Editing via Edit
  Reversibility Constraint
Authors: Enis Simsar and Alessio Tonioni and Yongqin Xian and Thomas Hofmann
  and Federico Tombari
Categories: cs.CV
Comments: Accepted to ICCV'25. Project page: https://uip2p.github.io/
\\ ( https://arxiv.org/abs/2412.15216 ,  5860kb)
------------------------------------------------------------------------------
\\
arXiv:2412.20761
replaced with revised version Fri, 26 Sep 2025 07:48:08 GMT   (3383kb)

Title: Unforgettable Lessons from Forgettable Images: Intra-Class Memorability
  Matters in Computer Vision
Authors: Jie Jing, Yongjian Huang, Serena J.-W. Wang, Shuangpeng Han, Lucia
  Schiatti, Yen-Ling Kuo, Qing Lin, Mengmi Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2412.20761 ,  3383kb)
------------------------------------------------------------------------------
\\
arXiv:2501.12976
replaced with revised version Fri, 26 Sep 2025 03:10:31 GMT   (3142kb)

Title: LiT: Delving into a Simple Linear Diffusion Transformer for Image
  Generation
Authors: Jiahao Wang, Ning Kang, Lewei Yao, Mengzhao Chen, Chengyue Wu,
  Songyang Zhang, Shuchen Xue, Yong Liu, Taiqiang Wu, Xihui Liu, Kaipeng Zhang,
  Shifeng Zhang, Wenqi Shao, Zhenguo Li, Ping Luo
Categories: cs.CV
Comments: 20 pages, 14 figures
\\ ( https://arxiv.org/abs/2501.12976 ,  3142kb)
------------------------------------------------------------------------------
\\
arXiv:2501.14182
replaced with revised version Fri, 26 Sep 2025 06:01:42 GMT   (6690kb)

Title: Single-weight Model Editing for Post-hoc Spurious Correlation
  Neutralization
Authors: Shahin Hakemi, Naveed Akhtar, Ghulam Mubashar Hassan, Ajmal Mian
Categories: cs.CV
\\ ( https://arxiv.org/abs/2501.14182 ,  6690kb)
------------------------------------------------------------------------------
\\
arXiv:2502.02588
replaced with revised version Fri, 26 Sep 2025 06:22:26 GMT   (25771kb)

Title: Calibrated Multi-Preference Optimization for Aligning Diffusion Models
Authors: Kyungmin Lee and Xiaohang Li and Qifei Wang and Junfeng He and Junjie
  Ke and Ming-Hsuan Yang and Irfan Essa and Jinwoo Shin and Feng Yang and
  Yinxiao Li
Categories: cs.CV
Comments: CVPR 2025, Project page: https://kyungmnlee.github.io/capo.github.io/
\\ ( https://arxiv.org/abs/2502.02588 ,  25771kb)
------------------------------------------------------------------------------
\\
arXiv:2502.07215
replaced with revised version Fri, 26 Sep 2025 01:06:35 GMT   (15563kb)

Title: PDV: Prompt Directional Vectors for Zero-shot Composed Image Retrieval
Authors: Osman Tursun, Sinan Kalkan, Simon Denman, Clinton Fookes
Categories: cs.CV
\\ ( https://arxiv.org/abs/2502.07215 ,  15563kb)
------------------------------------------------------------------------------
\\
arXiv:2502.07531
replaced with revised version Fri, 26 Sep 2025 05:44:52 GMT   (46841kb)

Title: VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video
  Generation
Authors: Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru
  Huang, Yanwei Fu
Categories: cs.CV cs.AI cs.LG cs.MM
\\ ( https://arxiv.org/abs/2502.07531 ,  46841kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11360
replaced with revised version Fri, 26 Sep 2025 00:10:53 GMT   (2608kb)

Title: GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder
Authors: Seunghyuk Cho, Zhenyue Qin, Yang Liu, Youngbin Choi, Seungbeom Lee,
  Dongwoo Kim
Categories: cs.CV cs.CL
Comments: Accepted to EMNLP-Findings 2025
\\ ( https://arxiv.org/abs/2502.11360 ,  2608kb)
------------------------------------------------------------------------------
\\
arXiv:2503.01199
replaced with revised version Fri, 26 Sep 2025 03:24:20 GMT   (4825kb)

Title: LiteGS: A High-performance Framework to Train 3DGS in Subminutes via
  System and Algorithm Codesign
Authors: Kaimin Liao, Hua Wang, Zhi Chen, Luchao Wang, Yaohua Tang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2503.01199 ,  4825kb)
------------------------------------------------------------------------------
\\
arXiv:2503.07392
replaced with revised version Fri, 26 Sep 2025 04:49:33 GMT   (11842kb)

Title: SPEED: Scalable, Precise, and Efficient Concept Erasure for Diffusion
  Models
Authors: Ouxiang Li, Yuan Wang, Xinting Hu, Houcheng Jiang, Tao Liang, Yanbin
  Hao, Guojun Ma, Fuli Feng
Categories: cs.CV
\\ ( https://arxiv.org/abs/2503.07392 ,  11842kb)
------------------------------------------------------------------------------
\\
arXiv:2503.21767
replaced with revised version Fri, 26 Sep 2025 17:36:37 GMT   (1786kb)

Title: Semantic Consistent Language Gaussian Splatting for Point-Level
  Open-vocabulary Querying
Authors: Hairong Yin, Huangying Zhan, Yi Xu, Raymond A. Yeh
Categories: cs.CV
\\ ( https://arxiv.org/abs/2503.21767 ,  1786kb)
------------------------------------------------------------------------------
\\
arXiv:2504.14108
replaced with revised version Fri, 26 Sep 2025 02:03:49 GMT   (20932kb)

Title: DanceText: A Training-Free Layered Framework for Controllable
  Multilingual Text Transformation in Images
Authors: Zhenyu Yu, Mohd Yamani Idna Idris, Hua Wang, Pei Wang, Rizwan Qureshi,
  Shaina Raza, Aman Chadha, Yong Xiang, Zhixiang Chen
Categories: cs.CV
\\ ( https://arxiv.org/abs/2504.14108 ,  20932kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15371
replaced with revised version Thu, 25 Sep 2025 15:07:18 GMT   (957kb)

Title: Event2Vec: Processing Neuromorphic Events directly by Representations in
  Vector Space
Authors: Wei Fang, Priyadarshini Panda
Categories: cs.CV cs.NE
\\ ( https://arxiv.org/abs/2504.15371 ,  957kb)
------------------------------------------------------------------------------
\\
arXiv:2504.19223
replaced with revised version Fri, 26 Sep 2025 08:56:26 GMT   (1623kb)

Title: CARL: Camera-Agnostic Representation Learning for Spectral Image
  Analysis
Authors: Alexander Baumann, Leonardo Ayala, Silvia Seidlitz, Jan Sellner,
  Alexander Studier-Fischer, Berkin \"Ozdemir, Lena Maier-Hein, Slobodan Ilic
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2504.19223 ,  1623kb)
------------------------------------------------------------------------------
\\
arXiv:2505.03113
replaced with revised version Fri, 26 Sep 2025 03:47:51 GMT   (20293kb)

Title: Image Recognition with Online Lightweight Vision Transformer: A Survey
Authors: Zherui Zhang, Rongtao Xu, Jie Zhou, Changwei Wang, Xingtian Pei,
  Wenhao Xu, Jiguang Zhang, Li Guo, Longxiang Gao, Wenbo Xu, Shibiao Xu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2505.03113 ,  20293kb)
------------------------------------------------------------------------------
\\
arXiv:2505.03318
replaced with revised version Fri, 26 Sep 2025 00:30:36 GMT   (3233kb)

Title: Unified Multimodal Chain-of-Thought Reward Model through Reinforcement
  Fine-Tuning
Authors: Yibin Wang and Zhimin Li and Yuhang Zang and Chunyu Wang and Qinglin
  Lu and Cheng Jin and Jiaqi Wang
Categories: cs.CV
Comments: [Accepted by NeurIPS2025] Project Page:
  https://codegoat24.github.io/UnifiedReward/think
\\ ( https://arxiv.org/abs/2505.03318 ,  3233kb)
------------------------------------------------------------------------------
\\
arXiv:2505.03334
replaced with revised version Fri, 26 Sep 2025 15:51:15 GMT   (10305kb)

Title: OS-W2S: An Automatic Labeling Engine for Language-Guided Open-Set Aerial
  Object Detection
Authors: Guoting Wei, Yu Liu, Xia Yuan, Xizhe Xue, Linlin Guo, Yifan Yang,
  Chunxia Zhao, Zongwen Bai, Haokui Zhang, Rong Xiao
Categories: cs.CV cs.DB
\\ ( https://arxiv.org/abs/2505.03334 ,  10305kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15197
replaced with revised version Fri, 26 Sep 2025 03:02:17 GMT   (7808kb)

Title: Intentional Gesture: Deliver Your Intentions with Gestures for Speech
Authors: Pinxin Liu, Haiyang Liu, Luchuan Song, Jason J. Corso, Chenliang Xu
Categories: cs.CV cs.AI cs.GR
\\ ( https://arxiv.org/abs/2505.15197 ,  7808kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15441
replaced with revised version Fri, 26 Sep 2025 08:59:43 GMT   (2723kb)

Title: Octic Vision Transformers: Quicker ViTs Through Equivariance
Authors: David Nordstr\"om, Johan Edstedt, Fredrik Kahl, Georg B\"okman
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2505.15441 ,  2723kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16456
replaced with revised version Thu, 25 Sep 2025 22:17:16 GMT   (6150kb)

Title: PhyMAGIC: Physical Motion-Aware Generative Inference with
  Confidence-guided LLM
Authors: Siwei Meng and Yawei Luo and Ping Liu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2505.16456 ,  6150kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17560
replaced with revised version Fri, 26 Sep 2025 07:25:28 GMT   (8403kb)

Title: Deeper Diffusion Models Amplify Bias
Authors: Shahin Hakemi, Naveed Akhtar, Ghulam Mubashar Hassan, Ajmal Mian
Categories: cs.CV
\\ ( https://arxiv.org/abs/2505.17560 ,  8403kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18663
replaced with revised version Fri, 26 Sep 2025 07:59:20 GMT   (5625kb)

Title: DVD-Quant: Data-free Video Diffusion Transformers Quantization
Authors: Zhiteng Li, Hanxuan Li, Junyi Wu, Kai Liu, Haotong Qin, Linghe Kong,
  Guihai Chen, Yulun Zhang and Xiaokang Yang
Categories: cs.CV
Comments: Code and models will be available at
  https://github.com/lhxcs/DVD-Quant
\\ ( https://arxiv.org/abs/2505.18663 ,  5625kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18668
replaced with revised version Fri, 26 Sep 2025 11:00:42 GMT   (43165kb)

Title: ChartGalaxy: A Dataset for Infographic Chart Understanding and
  Generation
Authors: Zhen Li, Duan Li, Yukai Guo, Xinyuan Guo, Bowen Li, Lanxi Xiao, Shenyu
  Qiao, Jiashu Chen, Zijian Wu, Hui Zhang, Xinhuan Shu, Shixia Liu
Categories: cs.CV cs.CL
Comments: 58 pages
\\ ( https://arxiv.org/abs/2505.18668 ,  43165kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19585
replaced with revised version Fri, 26 Sep 2025 14:22:59 GMT   (2462kb)

Title: CARE: Confidence-aware Ratio Estimation for Medical Biomarkers
Authors: Jiameng Li, Teodora Popordanoska, Aleksei Tiulpin, Sebastian G.
  Gruber, Frederik Maes, Matthew B. Blaschko
Categories: cs.CV
Comments: 9 pages
\\ ( https://arxiv.org/abs/2505.19585 ,  2462kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20611
replaced with revised version Fri, 26 Sep 2025 14:22:42 GMT   (11036kb)

Title: Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation
Authors: Zenghao Zheng, Lianping Yang, Jinshan Pan, Hegui Zhu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2505.20611 ,  11036kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21862
replaced with revised version Thu, 25 Sep 2025 21:14:52 GMT   (17440kb)

Title: Towards Scalable Language-Image Pre-training for 3D Medical Imaging
Authors: Chenhui Zhao, Yiwei Lyu, Asadur Chowdury, Edward Harake, Akhil
  Kondepudi, Akshay Rao, Xinhai Hou, Honglak Lee, Todd Hollon
Categories: cs.CV
\\ ( https://arxiv.org/abs/2505.21862 ,  17440kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22978
replaced with revised version Fri, 26 Sep 2025 06:08:53 GMT   (1331kb)

Title: Pose-free 3D Gaussian splatting via shape-ray estimation
Authors: Youngju Na, Taeyeon Kim, Jumin Lee, Kyu Beom Han, Woo Jae Kim,
  Sung-eui Yoon
Categories: cs.CV
Comments: ICIP 2025 (Best Student Paper Award)
\\ ( https://arxiv.org/abs/2505.22978 ,  1331kb)
------------------------------------------------------------------------------
\\
arXiv:2506.02244
replaced with revised version Thu, 25 Sep 2025 20:44:47 GMT   (7107kb)

Title: Physics-Guided Motion Loss for Video Generation Model
Authors: Bowen Xue, Giuseppe Claudio Guarnera, Shuang Zhao, Zahra Montazeri
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2506.02244 ,  7107kb)
------------------------------------------------------------------------------
\\
arXiv:2506.02459
replaced with revised version Thu, 25 Sep 2025 20:16:36 GMT   (11903kb)

Title: ReSpace: Text-Driven 3D Indoor Scene Synthesis and Editing with
  Preference Alignment
Authors: Martin JJ. Bucher, Iro Armeni
Categories: cs.CV
Comments: 22 pages, 17 figures (incl. appendix)
ACM-class: I.2.10; I.2.7
\\ ( https://arxiv.org/abs/2506.02459 ,  11903kb)
------------------------------------------------------------------------------
\\
arXiv:2506.03162
replaced with revised version Thu, 25 Sep 2025 21:26:24 GMT   (4456kb)

Title: Dual Branch VideoMamba with Gated Class Token Fusion for Violence
  Detection
Authors: Damith Chamalke Senadeera, Xiaoyun Yang, Shibo Li, Muhammad Awais,
  Dimitrios Kollias, Gregory Slabaugh
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2506.03162 ,  4456kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05096
replaced with revised version Fri, 26 Sep 2025 08:32:05 GMT   (39864kb)

Title: Astraea: A Token-wise Acceleration Framework for Video Diffusion
  Transformers
Authors: Haosong Liu, Yuge Cheng, Wenxuan Miao, Zihan Liu, Aiyue Chen, Jing
  Lin, Yiwu Yao, Chen Chen, Jingwen Leng, Yu Feng, Minyi Guo
Categories: cs.CV
\\ ( https://arxiv.org/abs/2506.05096 ,  39864kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05667
replaced with revised version Fri, 26 Sep 2025 04:33:48 GMT   (26519kb)

Title: DriveAction: A Benchmark for Exploring Human-like Driving Decisions in
  VLA Models
Authors: Yuhan Hao, Zhengning Li, Lei Sun, Weilong Wang, Naixin Yi, Sheng Song,
  Caihong Qin, Mofan Zhou, Yifei Zhan, Xianpeng Lang
Categories: cs.CV cs.AI
Comments: Benchmark:
  https://huggingface.co/datasets/LiAuto-DriveAction/drive-action
\\ ( https://arxiv.org/abs/2506.05667 ,  26519kb)
------------------------------------------------------------------------------
\\
arXiv:2506.08543
replaced with revised version Thu, 25 Sep 2025 21:21:43 GMT   (4326kb)

Title: Structure before the Machine: Input Space is the Prerequisite for
  Concepts
Authors: Bowei Tian, Xuntao Lyu, Meng Liu, Hongyi Wang, Ang Li
Categories: cs.CV
Comments: arXiv admin note: text overlap with arXiv:2503.22720
\\ ( https://arxiv.org/abs/2506.08543 ,  4326kb)
------------------------------------------------------------------------------
\\
arXiv:2506.08809
replaced with revised version Thu, 25 Sep 2025 19:35:48 GMT   (8672kb)

Title: HiSin: A Sinogram-Aware Framework for Efficient High-Resolution
  Inpainting
Authors: Jiaze E, Srutarshi Banerjee, Tekin Bicer, Guannan Wang, Yanfu Zhang,
  Bin Ren
Categories: cs.CV eess.IV
\\ ( https://arxiv.org/abs/2506.08809 ,  8672kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09079
replaced with revised version Fri, 26 Sep 2025 06:33:17 GMT   (7148kb)

Title: VidBridge-R1: Bridging QA and Captioning for RL-based Video
  Understanding Models with Intermediate Proxy Tasks
Authors: Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Weihong Lin, Zekun Wang,
  Bohan Zeng, Yang Shi, Sihan Yang, Qiang Liu, Pengfei Wan, Liang Wang, Tieniu
  Tan
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2506.09079 ,  7148kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09935
replaced with revised version Fri, 26 Sep 2025 13:16:53 GMT   (2306kb)

Title: LEO-VL: Efficient Scene Representation for Scalable 3D Vision-Language
  Learning
Authors: Jiangyong Huang, Xiaojian Ma, Xiongkun Linghu, Yue Fan, Junchao He,
  Wenxin Tan, Qing Li, Song-Chun Zhu, Yixin Chen, Baoxiong Jia, Siyuan Huang
Categories: cs.CV
Comments: Project page: https://leo-vl.github.io
\\ ( https://arxiv.org/abs/2506.09935 ,  2306kb)
------------------------------------------------------------------------------
\\
arXiv:2506.10821
replaced with revised version Fri, 26 Sep 2025 08:22:16 GMT   (20366kb)

Title: Think With Videos For Agentic Long-Video Understanding
Authors: Huaying Yuan, Zheng Liu, Junjie Zhou, Hongjin Qian, Yan Shu, Nicu
  Sebe, Ji-Rong Wen, Zhicheng Dou
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2506.10821 ,  20366kb)
------------------------------------------------------------------------------
\\
arXiv:2506.15220
replaced with revised version Fri, 26 Sep 2025 07:30:12 GMT   (547kb)

Title: video-SALMONN 2: Caption-Enhanced Audio-Visual Large Language Models
Authors: Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei
  Li, Zejun Ma, Chao Zhang
Categories: cs.CV cs.CL cs.SD
\\ ( https://arxiv.org/abs/2506.15220 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2506.18862
replaced with revised version Fri, 26 Sep 2025 17:35:39 GMT   (12111kb)

Title: TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change
  Understanding and Forecasting
Authors: Zhongbin Guo, Yuhao Wang, Ping Jian, Chengzhi Li, Xinyue Chen, Zhen
  Yang, Ertai E
Categories: cs.CV cs.AI
Comments: Submitted to The Fourteenth International Conference on Learning
  Representations (ICLR 2026). Our dataset can be found at
  https://huggingface.co/datasets/IceInPot/TAMMs
\\ ( https://arxiv.org/abs/2506.18862 ,  12111kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20294
replaced with revised version Fri, 26 Sep 2025 05:52:12 GMT   (31101kb)

Title: Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag
  Explorations
Authors: Shunqi Mao, Wei Guo, Chaoyi Zhang, Jieting Long, Ke Xie, Weidong Cai
Categories: cs.CV
Comments: 32 pages, 11 figures, 10 tables
\\ ( https://arxiv.org/abs/2506.20294 ,  31101kb)
------------------------------------------------------------------------------
\\
arXiv:2506.22432
replaced with revised version Fri, 26 Sep 2025 06:29:19 GMT   (23211kb)

Title: Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy
Authors: Yuhao Liu, Tengfei Wang, Fang Liu, Zhenwei Wang, Rynson W.H. Lau
Categories: cs.CV
Comments: Accepted by Siggraph Asia 2025
\\ ( https://arxiv.org/abs/2506.22432 ,  23211kb)
------------------------------------------------------------------------------
\\
arXiv:2507.01908
replaced with revised version Fri, 26 Sep 2025 09:15:21 GMT   (14851kb)

Title: Reasoning to Edit: Hypothetical Instruction-Based Image Editing with
  Visual Reasoning
Authors: Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye
  Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2507.01908 ,  14851kb)
------------------------------------------------------------------------------
\\
arXiv:2507.02705
replaced with revised version Fri, 26 Sep 2025 03:45:01 GMT   (12067kb)

Title: SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond
  Feature Alignment
Authors: Qi Xu, Dongxu Wei, Lingzhe Zhao, Wenpu Li, Zhangchi Huang, Shunping
  Ji, Peidong Liu
Categories: cs.CV
Comments: Accepted to NeurIPS'25 (Spotlight). Project page:
  https://insomniaaac.github.io/siu3r/
\\ ( https://arxiv.org/abs/2507.02705 ,  12067kb)
------------------------------------------------------------------------------
\\
arXiv:2507.03262
replaced with revised version Fri, 26 Sep 2025 07:45:03 GMT   (1054kb)

Title: Investigating Redundancy in Multimodal Large Language Models with
  Multiple Vision Encoders
Authors: Yizhou Wang, Song Mao, Yang Chen, Yufan Shen, Yinqiao Yan, Pinlong
  Cai, Ding Wang, Guohang Yan, Zhi Yu, Xuming Hu, Botian Shi
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2507.03262 ,  1054kb)
------------------------------------------------------------------------------
\\
arXiv:2507.05394
replaced with revised version Fri, 26 Sep 2025 10:36:44 GMT   (439kb)

Title: pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for
  Vision-Language Models
Authors: Sajjad Ghiasvand, Mahnoosh Alizadeh, Ramtin Pedarsani
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2507.05394 ,  439kb)
------------------------------------------------------------------------------
\\
arXiv:2507.11245
replaced with revised version Fri, 26 Sep 2025 14:21:12 GMT   (23757kb)

Title: NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long
  Video Generation
Authors: X. Feng, H. Yu, M. Wu, S. Hu, J. Chen, C. Zhu, J. Wu, X. Chu, K. Huang
Categories: cs.CV
Comments: Project Page: https://amap-ml.github.io/NarrLV-Website/
\\ ( https://arxiv.org/abs/2507.11245 ,  23757kb)
------------------------------------------------------------------------------
\\
arXiv:2507.17522
replaced with revised version Fri, 26 Sep 2025 02:27:47 GMT   (14505kb)

Title: STQE: Spatial-Temporal Attribute Quality Enhancement for G-PCC
  Compressed Dynamic Point Clouds
Authors: Tian Guo, Hui Yuan, Xiaolong Mao, Shiqi Jiang, Raouf Hamzaoui, and Sam
  Kwong
Categories: cs.CV eess.IV
\\ ( https://arxiv.org/abs/2507.17522 ,  14505kb)
------------------------------------------------------------------------------
\\
arXiv:2507.20879
replaced with revised version Fri, 26 Sep 2025 10:14:08 GMT   (13913kb)

Title: DriveAgent-R1: Advancing VLM-based Autonomous Driving with Active
  Perception and Hybrid Thinking
Authors: Weicheng Zheng, Xiaofei Mao, Nanfei Ye, Pengxiang Li, Kun Zhan,
  Xianpeng Lang, Hang Zhao
Categories: cs.CV
\\ ( https://arxiv.org/abs/2507.20879 ,  13913kb)
------------------------------------------------------------------------------
\\
arXiv:2507.20890
replaced with revised version Thu, 25 Sep 2025 22:30:03 GMT   (925kb)

Title: $A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with
  Attention-Guided Refinement
Authors: Zhecheng Li, Guoxian Song, Yiwei Wang, Zhen Xiong, Junsong Yuan, Yujun
  Cai
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2507.20890 ,  925kb)
------------------------------------------------------------------------------
\\
arXiv:2507.21567
replaced with revised version Fri, 26 Sep 2025 00:32:56 GMT   (16019kb)

Title: RelMap: Enhancing Online Map Construction with Class-Aware Spatial
  Relation and Semantic Priors
Authors: Tianhui Cai, Yun Zhang, Zewei Zhou, Zhiyu Huang, Jiaqi Ma
Categories: cs.CV
\\ ( https://arxiv.org/abs/2507.21567 ,  16019kb)
------------------------------------------------------------------------------
\\
arXiv:2508.02192
replaced with revised version Fri, 26 Sep 2025 09:32:40 GMT   (14011kb)

Title: Content-Aware Mamba for Learned Image Compression
Authors: Yunuo Chen, Zezheng Lyu, Bing He, Hongwei Hu, Qi Wang, Yuan Tian, Li
  Song, Wenjun Zhang, Guo Lu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.02192 ,  14011kb)
------------------------------------------------------------------------------
\\
arXiv:2508.03442
replaced with revised version Fri, 26 Sep 2025 05:05:23 GMT   (14671kb)

Title: RAAG: Ratio Aware Adaptive Guidance
Authors: Shangwen Zhu, Qianyu Peng, Yuting Hu, Zhantao Yang, Han Zhang, Zhao
  Pu, Andy Zheng, Zhilei Shu, Ruili Feng, Fan Cheng
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.03442 ,  14671kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04324
replaced with revised version Fri, 26 Sep 2025 05:28:04 GMT   (23648kb)

Title: TempFlow-GRPO: When Timing Matters for GRPO in Flow Models
Authors: Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin,
  Fengyun Rao, Bo Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.04324 ,  23648kb)
------------------------------------------------------------------------------
\\
arXiv:2508.14483
replaced with revised version Fri, 26 Sep 2025 08:11:21 GMT   (33257kb)

Title: Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer
  for Photorealistic Video Restoration
Authors: Haoran Bai, Xiaoxu Chen, Canqian Yang, Zongyao He, Sibin Deng, Ying
  Chen
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.14483 ,  33257kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15431
replaced with revised version Fri, 26 Sep 2025 10:47:11 GMT   (11003kb)

Title: Small Dents, Big Impact: A Dataset and Deep Learning Approach for
  Vehicle Dent Detection
Authors: Danish Zia Baig, Mohsin Kamal, Zahid Ullah
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.15431 ,  11003kb)
------------------------------------------------------------------------------
\\
arXiv:2508.20466
replaced with revised version Fri, 26 Sep 2025 03:40:04 GMT   (4355kb)

Title: Re-Densification Meets Cross-Scale Propagation: Real-Time Neural
  Compression of LiDAR Point Clouds
Authors: Pengpeng Yu, Haoran Li, Runqing Jiang, Jing Wang, Liang Lin, Yulan Guo
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.20466 ,  4355kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01986
replaced with revised version Fri, 26 Sep 2025 06:30:36 GMT   (7310kb)

Title: Draw-In-Mind: Rebalancing Designer-Painter Roles in Unified Multimodal
  Models Benefits Image Editing
Authors: Ziyun Zeng, Junhao Zhang, Wei Li, Mike Zheng Shou
Categories: cs.CV cs.AI
Comments: Tech Report
\\ ( https://arxiv.org/abs/2509.01986 ,  7310kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07450
replaced with revised version Fri, 26 Sep 2025 00:44:36 GMT   (3161kb)

Title: GLEAM: Learning to Match and Explain in Cross-View Geo-Localization
Authors: Xudong Lu, Zhi Zheng, Yi Wan, Yongxiang Yao, Annan Wang, Renrui Zhang,
  Panwang Xia, Qiong Wu, Qingyun Li, Weifeng Lin, Xiangyu Zhao, Peifeng Ma, Xue
  Yang, Hongsheng Li
Categories: cs.CV cs.CL
Comments: 18 pages
\\ ( https://arxiv.org/abs/2509.07450 ,  3161kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09368
replaced with revised version Fri, 26 Sep 2025 10:18:06 GMT   (2057kb)

Title: A Fully Automatic Framework for Intracranial Pressure Grading:
  Integrating Keyframe Identification, ONSD Measurement and Clinical Data
Authors: Pengxu Wen, Tingting Yu, Ziwei Nie, Cheng Jiang, Zhenyu Yin, Mingyang
  He, Bo Liao and Xiaoping Yang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.09368 ,  2057kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10779
replaced with revised version Fri, 26 Sep 2025 17:22:22 GMT   (25073kb)

Title: Group Evidence Matters: Tiling-based Semantic Gating for Dense Object
  Detection
Authors: Yilun Xiao
Categories: cs.CV
Comments: 8 pages, 7 figures
\\ ( https://arxiv.org/abs/2509.10779 ,  25073kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14981
replaced with revised version Fri, 26 Sep 2025 03:23:26 GMT   (45573kb)

Title: SPATIALGEN: Layout-guided 3D Indoor Scene Generation
Authors: Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu,
  Rui Tang, Zihan Zhou, Ping Tan
Categories: cs.CV
Comments: 3D scene generation; diffusion model; Scene reconstruction and
  understanding
\\ ( https://arxiv.org/abs/2509.14981 ,  45573kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15472
replaced with revised version Thu, 25 Sep 2025 22:23:18 GMT   (21839kb)

Title: Efficient Multimodal Dataset Distillation via Generative Models
Authors: Zhenghao Zhao, Haoxuan Wang, Junyi Wu, Yuzhang Shang, Gaowen Liu, Yan
  Yan
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.15472 ,  21839kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15548
replaced with revised version Fri, 26 Sep 2025 16:42:25 GMT   (33618kb)

Title: MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild
Authors: Deming Li, Kaiwen Jiang, Yutao Tang, Ravi Ramamoorthi, Rama Chellappa,
  Cheng Peng
Categories: cs.CV
Comments: fixed typos
\\ ( https://arxiv.org/abs/2509.15548 ,  33618kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16031
replaced with revised version Fri, 26 Sep 2025 14:09:42 GMT   (10410kb)

Title: GLip: A Global-Local Integrated Progressive Framework for Robust Visual
  Speech Recognition
Authors: Tianyue Wang, Shuang Yang, Shiguang Shan and Xilin Chen
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.16031 ,  10410kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17283
replaced with revised version Fri, 26 Sep 2025 11:31:47 GMT   (379kb)

Title: Automated Facility Enumeration for Building Compliance Checking using
  Door Detection and Large Language Models
Authors: Licheng Zhang, Bach Le, Naveed Akhtar and Tuan Ngo
Categories: cs.CV cs.AI cs.ET
Comments: Author name correction in the second version (same content as the
  first version)
\\ ( https://arxiv.org/abs/2509.17283 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17513
replaced with revised version Fri, 26 Sep 2025 04:46:59 GMT   (31147kb)

Title: 4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive
  Volumetric Video Streaming
Authors: Zihan Zheng, Zhenlong Wu, Houqiang Zhong, Yuan Tian, Ning Cao, Lan Xu,
  Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Wenjun Zhang
Categories: cs.CV
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2509.17513 ,  31147kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17792
replaced with revised version Fri, 26 Sep 2025 13:01:23 GMT   (14204kb)

Title: Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding
Authors: S M A Sharif, Abdur Rehman, Fayaz Ali Dharejo, Radu Timofte, Rizwan
  Ali Naqvi
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.17792 ,  14204kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19665
replaced with revised version Fri, 26 Sep 2025 04:44:55 GMT   (24089kb)

Title: Deep Learning for Clouds and Cloud Shadow Segmentation in Methane
  Satellite and Airborne Imaging Spectroscopy
Authors: Manuel Perez-Carrasco, Maya Nasr, Sebastien Roche, Chris Chan Miller,
  Zhan Zhang, Core Francisco Park, Eleanor Walker, Cecilia Garraffo, Douglas
  Finkbeiner, Ritesh Gautam, Steven Wofsy
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2509.19665 ,  24089kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20280
replaced with revised version Fri, 26 Sep 2025 06:51:38 GMT   (35289kb)

Title: HiPerformer: A High-Performance Global-Local Segmentation Model with
  Modular Hierarchical Fusion Strategy
Authors: Dayu Tan, Zhenpeng Xu, Yansen Su, Xin Peng, Chunhou Zheng, and Weimin
  Zhong
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.20280 ,  35289kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20295
replaced with revised version Fri, 26 Sep 2025 07:41:01 GMT   (6192kb)

Title: FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory
  for Segmentation-oriented Anomaly Synthesis
Authors: Xichen Xu, Yanshu Wang, Jinbao Wang, Xiaoning Lei, Guoyang Xie,
  Guannan Jiang, Zhichao Lu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.20295 ,  6192kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20360
replaced with revised version Thu, 25 Sep 2025 22:11:13 GMT   (12927kb)

Title: EditVerse: Unifying Image and Video Editing and Generation with
  In-Context Learning
Authors: Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao,
  Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin,
  Soo Ye Kim, Qiang Xu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.20360 ,  12927kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20745
replaced with revised version Fri, 26 Sep 2025 03:42:36 GMT   (8972kb)

Title: Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object
  Detection
Authors: Yu Guo, Shengfeng He, Yuxu Lu, Haonan An, Yihang Tao, Huilin Zhu,
  Jingxian Liu, Yuguang Fang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.20745 ,  8972kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20787
replaced with revised version Fri, 26 Sep 2025 02:12:39 GMT   (218kb)

Title: Real-Time Object Detection Meets DINOv3
Authors: Shihua Huang, Yongjie Hou, Longfei Liu, Xuanlong Yu, Xi Shen
Categories: cs.CV
Comments: Source code available at
  https://github.com/Intellindust-AI-Lab/DEIMv2
\\ ( https://arxiv.org/abs/2509.20787 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21113
replaced with revised version Fri, 26 Sep 2025 04:41:48 GMT   (1566kb)

Title: MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for
  Video Temporal Reasoning
Authors: Sicheng Tao, Jungang Li, Yibo Yan, Junyan Zhang, Yubo Gao, Hanqian Li,
  ShuHang Xun, Yuxuan Fan, Hong Chen, Jianxiang He, Xuming Hu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.21113 ,  1566kb)
------------------------------------------------------------------------------
\\
arXiv:2503.11023
replaced with revised version Fri, 26 Sep 2025 13:30:21 GMT   (158kb)

Title: Beyond A Single AI Cluster: A Survey of Decentralized LLM Training
Authors: Haotian Dong, Jingyan Jiang, Rongwei Lu, Jiajun Luo, Jiajun Song,
  Bowen Li, Ying Shen, Zhi Wang
Categories: cs.DC
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2503.11023 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2504.08791
replaced with revised version Fri, 26 Sep 2025 03:17:23 GMT   (308kb)

Title: Prima.cpp: Fast 30-70B LLM Inference on Heterogeneous and Low-Resource
  Home Clusters
Authors: Zonghang Li, Tao Li, Wenjiao Feng, Rongxing Xiao, Jianshu She, Hong
  Huang, Mohsen Guizani, Hongfang Yu, Qirong Ho, Wei Xiang, Steve Liu
Categories: cs.DC cs.AI
Comments: 26 pages, 10 figures, 10 tables
MSC-class: 68T50
ACM-class: I.2.7; I.2.11
\\ ( https://arxiv.org/abs/2504.08791 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2504.17096
replaced with revised version Fri, 26 Sep 2025 15:37:53 GMT   (1962kb)

Title: Sailor: Automating Distributed Training over Dynamic, Heterogeneous, and
  Geo-distributed Clusters
Authors: Foteini Strati, Zhendong Zhang, George Manos, Ixeia S\'anchez P\'eriz,
  Qinghao Hu, Tiancheng Chen, Berk Buzcu, Song Han, Pamela Delgado, Ana
  Klimovic
Categories: cs.DC
\\ ( https://arxiv.org/abs/2504.17096 ,  1962kb)
------------------------------------------------------------------------------
\\
arXiv:2504.19516
replaced with revised version Fri, 26 Sep 2025 09:35:48 GMT   (2402kb)

Title: Boosting LLM Serving through Spatial-Temporal GPU Resource Sharing
Authors: Zejia Lin, Hongxin Xu, Guanyi Chen, Zhiguang Chen, Yutong Lu, Xianwei
  Zhang
Categories: cs.DC
\\ ( https://arxiv.org/abs/2504.19516 ,  2402kb)
------------------------------------------------------------------------------
\\
arXiv:2505.06558
replaced with revised version Fri, 26 Sep 2025 13:02:22 GMT   (593kb)

Title: Data Version Management and Machine-Actionable Reproducibility for HPC
Authors: Andreas Kn\"upfer, Timothy J. Callow
Categories: cs.DC
Comments: Under review
\\ ( https://arxiv.org/abs/2505.06558 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2507.12106
replaced with revised version Fri, 26 Sep 2025 10:45:07 GMT   (1195kb)

Title: Urban Green Governance: IoT-Driven Management and Enhancement of Urban
  Green Spaces in Campobasso
Authors: Antonio Salis, Gabriele Troina, Gianluca Boanelli, Marco Ottaviano,
  Paola Fortini, Soraya Versace
Categories: cs.DC cs.CY
Comments: 18 pages, 6 Figures
\\ ( https://arxiv.org/abs/2507.12106 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2505.02861
replaced with revised version Fri, 26 Sep 2025 10:28:11 GMT   (30kb)

Title: Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework
  for Optimal Agent Selection in Multi-Domain Task Environments
Authors: Kushagra Agrawal, Nisharg Nargund
Categories: cs.MA cs.AI cs.NE
Comments: Accepted for Publication at PReMI 2025 - 11th International
  Conference on Pattern Recognition and Machine Intelligence
\\ ( https://arxiv.org/abs/2505.02861 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14276
replaced with revised version Fri, 26 Sep 2025 02:28:36 GMT   (2456kb)

Title: Constructive Conflict-Driven Multi-Agent Reinforcement Learning for
  Strategic Diversity
Authors: Yuxiang Mai, Qiyue Yin, Wancheng Ni, Pei Xu, Kaiqi Huang
Categories: cs.MA cs.AI
Comments: Accepted by IJCAI 2025
Journal-ref: Proceedings of the 34th International Joint Conference on
  Artificial Intelligence (IJCAI-25), 2025
\\ ( https://arxiv.org/abs/2509.14276 ,  2456kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13498
replaced with revised version Fri, 26 Sep 2025 14:57:20 GMT   (7116kb)

Title: Efficient Epistemic Uncertainty Estimation in Regression Ensemble Models
  Using Pairwise-Distance Estimators
Authors: Lucas Berry, David Meger
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.13498 ,  7116kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04615
replaced with revised version Thu, 25 Sep 2025 23:51:30 GMT   (26033kb)

Title: VDFD: Multi-Agent Value Decomposition Framework with Disentangled World
  Model
Authors: Zhizun Wang and David Meger
Categories: cs.LG cs.AI cs.MA
Comments: 43 pages
\\ ( https://arxiv.org/abs/2309.04615 ,  26033kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17805
replaced with revised version Fri, 26 Sep 2025 08:40:37 GMT   (29kb)

Title: Biospheric AI
Authors: Marcin Korecki
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2401.17805 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2406.00276
replaced with revised version Fri, 26 Sep 2025 00:32:54 GMT   (10375kb)

Title: Machine Learning-Assisted Sustainable Remanufacturing, Reusing and
  Recycling for Lithium-ion Batteries
Authors: Shengyu Tao
Categories: cs.LG cs.AI cs.CE physics.data-an
Comments: This is a PhD thesis from Dr. Shengyu Tao at Tinsghua University and
  University of California at Berkeley
ACM-class: J.2; G.3
\\ ( https://arxiv.org/abs/2406.00276 ,  10375kb)
------------------------------------------------------------------------------
\\
arXiv:2406.01086
replaced with revised version Fri, 26 Sep 2025 07:04:24 GMT   (9089kb)

Title: Diverse Subset Selection via Norm-Based Sampling and Orthogonality
Authors: Noga Bar and Raja Giryes
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2406.01086 ,  9089kb)
------------------------------------------------------------------------------
\\
arXiv:2406.14265
replaced with revised version Fri, 26 Sep 2025 10:57:18 GMT   (2136kb)

Title: VeriFlow: Modeling Distributions for Neural Network Verification
Authors: Faried Abu Zaid, Daniel Neider, Mustafa Yal\c{c}{\i}ner
Categories: cs.LG cs.AI cs.LO cs.SC
\\ ( https://arxiv.org/abs/2406.14265 ,  2136kb)
------------------------------------------------------------------------------
\\
arXiv:2409.02136
replaced with revised version Fri, 26 Sep 2025 12:00:01 GMT   (2552kb)

Title: Large Language Models versus Classical Machine Learning: Performance in
  COVID-19 Mortality Prediction Using High-Dimensional Tabular Data
Authors: Mohammadreza Ghaffarzadeh-Esfahani, Mahdi Ghaffarzadeh-Esfahani, Arian
  Salahi-Niri, Hossein Toreyhi, Zahra Atf, Amirali Mohsenzadeh-Kermani, Mahshad
  Sarikhani, Zohreh Tajabadi, Fatemeh Shojaeian, Mohammad Hassan Bagheri, Aydin
  Feyzi, Mohammadamin Tarighatpayma, Narges Gazmeh, Fateme Heydari, Hossein
  Afshar, Amirreza Allahgholipour, Farid Alimardani, Ameneh Salehi, Naghmeh
  Asadimanesh, Mohammad Amin Khalafi, Hadis Shabanipour, Ali Moradi, Sajjad
  Hossein Zadeh, Omid Yazdani, Romina Esbati, Moozhan Maleki, Danial Samiei
  Nasr, Amirali Soheili, Hossein Majlesi, Saba Shahsavan, Alireza Soheilipour,
  Nooshin Goudarzi, Erfan Taherifard, Hamidreza Hatamabadi, Jamil S Samaan,
  Thomas Savage, Ankit Sakhuja, Ali Soroush, Girish Nadkarni, Ilad Alavi
  Darazam, Mohamad Amin Pourhoseingholi, Seyed Amir Ahmad Safavi-Naini
Categories: cs.LG cs.AI cs.CL
Comments: Code is available at:
  https://github.com/mohammad-gh009/Large-Language-Models-vs-Classical-Machine-learning
  and https://github.com/Sdamirsa/Tehran_COVID_Cohort. The datasets are
  available from the corresponding author on reasonable request
  (sdamirsa@ymail.com)
MSC-class: 92C50, 68T50
ACM-class: J.3
\\ ( https://arxiv.org/abs/2409.02136 ,  2552kb)
------------------------------------------------------------------------------
\\
arXiv:2409.05305
replaced with revised version Fri, 26 Sep 2025 09:33:03 GMT   (1979kb)

Title: Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic
  Gradients
Authors: Sebastian J. Wetzel and Zakaria Patel
Categories: cs.LG cs.AI
Comments: Major Revision, new code for experiments, reflect author
  contributions
\\ ( https://arxiv.org/abs/2409.05305 ,  1979kb)
------------------------------------------------------------------------------
\\
arXiv:2409.16322 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 06:56:49 GMT   (2386kb)

Title: On the Within-class Variation Issue in Alzheimer's Disease Detection
Authors: Jiawen Kang, Dongrui Han, Lingwei Meng, Jingyan Zhou, Jinchao Li,
  Xixin Wu, Helen Meng
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD q-bio.NC
Comments: Accepted for publication in Proc. of Interspeech 2025 conference.
  Note: this is an extended version of the conference paper, with an additional
  section included
DOI: 10.21437/Interspeech.2025-2751
\\ ( https://arxiv.org/abs/2409.16322 ,  2386kb)
------------------------------------------------------------------------------
\\
arXiv:2409.19375
replaced with revised version Fri, 26 Sep 2025 03:17:15 GMT   (3216kb)

Title: DOTA: Distributional Test-Time Adaptation of Vision-Language Models
Authors: Zongbo Han, Jialong Yang, Guangyu Wang, Junfan Li, Qianli Xu, Mike
  Zheng Shou, Changqing Zhang
Categories: cs.LG cs.AI cs.CL cs.CV cs.HC
Comments: Accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2409.19375 ,  3216kb)
------------------------------------------------------------------------------
\\
arXiv:2410.06883
replaced with revised version Thu, 25 Sep 2025 20:31:16 GMT   (1022kb)

Title: Degree-Conscious Spiking Graph for Cross-Domain Adaptation
Authors: Yingxu Wang, Mengzhu Wang, Houcheng Su, Nan Yin, Quanming Yao, James
  Kwok
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2410.06883 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2410.21249
replaced with revised version Fri, 26 Sep 2025 16:27:30 GMT   (418kb)

Title: Capacity-Aware Planning and Scheduling in Budget-Constrained Multi-Agent
  MDPs: A Meta-RL Approach
Authors: Manav Vora, Ilan Shomorony, Melkior Ornik
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2410.21249 ,  418kb)
------------------------------------------------------------------------------
\\
arXiv:2410.22376
replaced with revised version Fri, 26 Sep 2025 06:22:42 GMT   (29310kb)

Title: Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion
  Models on Rare Concepts with LLM Guidance
Authors: Dongmin Park, Sebin Kim, Taehong Moon, Minkyu Kim, Kangwook Lee,
  Jaewoong Cho
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: ICLR 2025 (spotlight)
\\ ( https://arxiv.org/abs/2410.22376 ,  29310kb)
------------------------------------------------------------------------------
\\
arXiv:2411.01423 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 04:21:40 GMT   (2387kb)

Title: Conditional Latent Space Molecular Scaffold Optimization for Accelerated
  Molecular Design
Authors: Onur Boyar, Hiroyuki Hanada, Ichiro Takeuchi
Categories: q-bio.BM cs.AI cs.LG
Comments: Published in Transactions on Machine Learning Research
\\ ( https://arxiv.org/abs/2411.01423 ,  2387kb)
------------------------------------------------------------------------------
\\
arXiv:2412.05723 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 17:37:00 GMT   (312kb)

Title: Training-Free Bayesianization for Low-Rank Adapters of Large Language
  Models
Authors: Haizhou Shi, Yibin Wang, Ligong Han, Huan Zhang, Hao Wang
Categories: stat.ML cs.AI cs.CL cs.LG
Comments: Accepted at NeurIPS 2025
\\ ( https://arxiv.org/abs/2412.05723 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2501.16355
replaced with revised version Fri, 26 Sep 2025 17:21:50 GMT   (397kb)

Title: How Strategic Agents Respond: Comparing Analytical Models with
  LLM-Generated Responses in Strategic Classification
Authors: Tian Xie, Pavan Rauch, Xueru Zhang
Categories: cs.LG cs.AI
Comments: Add GPT 5 experiments
\\ ( https://arxiv.org/abs/2501.16355 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2502.00666
replaced with revised version Fri, 26 Sep 2025 15:18:48 GMT   (125kb)

Title: Avoiding $\mathbf{exp(R_{max})}$ scaling in RLHF through
  Preference-based Exploration
Authors: Mingyu Chen, Yiding Chen, Wen Sun, Xuezhou Zhang
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2502.00666 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01456
replaced with revised version Fri, 26 Sep 2025 09:25:31 GMT   (839kb)

Title: Process Reinforcement through Implicit Rewards
Authors: Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Yuchen Zhang, Jiacheng
  Chen, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen,
  Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu
  Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, Ning Ding
Categories: cs.LG cs.AI cs.CL
Comments: 24 pages. Model&Code&Data available at
  https://github.com/PRIME-RL/PRIME
\\ ( https://arxiv.org/abs/2502.01456 ,  839kb)
------------------------------------------------------------------------------
\\
arXiv:2502.08985
replaced with revised version Fri, 26 Sep 2025 01:50:30 GMT   (2034kb)

Title: Beyond Shallow Behavior: Task-Efficient Value-Based Multi-Task Offline
  MARL via Skill Discovery
Authors: Xun Wang, Zhuoran Li, Hai Zhong and Longbo Huang
Categories: cs.LG cs.AI cs.MA
\\ ( https://arxiv.org/abs/2502.08985 ,  2034kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16060
replaced with revised version Fri, 26 Sep 2025 05:48:12 GMT   (2590kb)

Title: Tokenizing Single-Channel EEG with Time-Frequency Motif Learning
Authors: Jathurshan Pradeepkumar, Xihao Piao, Zheng Chen and Jimeng Sun
Categories: cs.LG cs.AI eess.SP
\\ ( https://arxiv.org/abs/2502.16060 ,  2590kb)
------------------------------------------------------------------------------
\\
arXiv:2503.11947
replaced with revised version Thu, 25 Sep 2025 21:43:39 GMT   (689kb)

Title: Ethical AI for Young Digital Citizens: A Call to Action on Privacy
  Governance
Authors: Austin Shouli, Ankur Barthwal, Molly Campbell and Ajay Kumar Shrestha
Categories: cs.CY cs.AI cs.LG
Comments: Preprint Version | Submitted to journal "Security and Privacy", Wiley
\\ ( https://arxiv.org/abs/2503.11947 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2503.13562 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 14:02:05 GMT   (2033kb)

Title: Detecting Scarce and Sparse Anomalous: Solving Dual Imbalance in
  Multi-Instance Learning
Authors: Lin-Han Jia, Lan-Zhe Guo, Zhi Zhou, Si-Ye Han, Zi-Wen Li, Yu-Feng Li
Categories: stat.ML cs.AI cs.LG
\\ ( https://arxiv.org/abs/2503.13562 ,  2033kb)
------------------------------------------------------------------------------
\\
arXiv:2504.00220
replaced with revised version Fri, 26 Sep 2025 01:47:03 GMT   (8552kb)

Title: Can Diffusion Models Disentangle? A Theoretical Perspective
Authors: Liming Wang, Muhammad Jehanzeb Mirza, Yishu Gong, Yuan Gong, Jiaqi
  Zhang, Brian H. Tracey, Katerina Placek, Marco Vilela, James R. Glass
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2504.00220 ,  8552kb)
------------------------------------------------------------------------------
\\
arXiv:2504.03814
replaced with revised version Fri, 26 Sep 2025 17:11:25 GMT   (3801kb)

Title: Recursive Training Loops in LLMs: How training data properties modulate
  distribution shift in generated data?
Authors: Grgur Kova\v{c}, J\'er\'emy Perez, R\'emy Portelas, Peter Ford
  Dominey, Pierre-Yves Oudeyer
Categories: cs.LG cs.AI cs.CL
Comments: Accepted to EMNLP 2025 (Oral)
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2504.03814 ,  3801kb)
------------------------------------------------------------------------------
\\
arXiv:2504.18591
replaced with revised version Fri, 26 Sep 2025 10:06:23 GMT   (3469kb)

Title: Geometry aware inference of steady state PDEs using Equivariant Neural
  Fields representations
Authors: Giovanni Catalani, Michael Bauerheim, Fr\'ed\'eric Tost, Xavier
  Bertrand, Joseph Morlier
Categories: cs.LG cs.AI cs.CV
Comments: NeurIPS 2025 AI for Science workshop
\\ ( https://arxiv.org/abs/2504.18591 ,  3469kb)
------------------------------------------------------------------------------
\\
arXiv:2505.10426
replaced with revised version Thu, 25 Sep 2025 21:54:54 GMT   (933kb)

Title: Formalising Human-in-the-Loop: Computational Reductions, Failure Modes,
  and Legal-Moral Responsibility
Authors: Maurice Chiodo, Dennis M\"uller, Paul Siewert, Jean-Luc Wetherall,
  Zoya Yasmine, John Burden
Categories: cs.CY cs.AI cs.HC math.HO
Comments: 31 pages. Keywords: Human-in-the-loop, Automated decision making
  system, Human oversight in sociotechnical systems, Oracle machine, AI safety,
  Trustworthy AI
ACM-class: F.1; H.1.2; I.2.0; K.4.1
\\ ( https://arxiv.org/abs/2505.10426 ,  933kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11574
replaced with revised version Fri, 26 Sep 2025 02:27:57 GMT   (319kb)

Title: Quantization Meets Reasoning: Exploring and Mitigating Degradation of
  Low-Bit LLMs in Mathematical Reasoning
Authors: Zhen Li, Yupeng Su, Songmiao Wang, Runming Yang, Congkai Xie, Aofan
  Liu, Ming Li, Jiannong Cao, Yuan Xie, Ngai Wong, Hongxia Yang
Categories: cs.LG cs.AI
Comments: 23pages
\\ ( https://arxiv.org/abs/2505.11574 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11737
replaced with revised version Thu, 25 Sep 2025 21:44:35 GMT   (767kb)

Title: TokUR: Token-Level Uncertainty Estimation for Large Language Model
  Reasoning
Authors: Tunyu Zhang, Haizhou Shi, Yibin Wang, Hengyi Wang, Xiaoxiao He,
  Zhuowei Li, Haoxian Chen, Ligong Han, Kai Xu, Huan Zhang, Dimitris Metaxas,
  Hao Wang
Categories: cs.LG cs.AI cs.CL
Comments: Preprint; Work in progress
\\ ( https://arxiv.org/abs/2505.11737 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11756
replaced with revised version Fri, 26 Sep 2025 11:18:40 GMT   (6247kb)

Title: Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders
Authors: David Chanin, Tom\'a\v{s} Dulka, Adri\`a Garriga-Alonso
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2505.11756 ,  6247kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11824
replaced with revised version Fri, 26 Sep 2025 03:18:43 GMT   (2055kb)

Title: Latent Veracity Inference for Identifying Errors in Stepwise Reasoning
Authors: Minsu Kim, Jean-Pierre Falet, Oliver E. Richardson, Xiaoyin Chen,
  Moksh Jain, Sungjin Ahn, Sungsoo Ahn, Yoshua Bengio
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.11824 ,  2055kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12143
replaced with revised version Thu, 25 Sep 2025 19:14:28 GMT   (2327kb)

Title: Structured Relational Representations
Authors: Arun Kumar, Paul Schrater
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.12143 ,  2327kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13497
replaced with revised version Fri, 26 Sep 2025 07:32:46 GMT   (8789kb)

Title: Learning Hierarchical Domain Models Through Environment-Grounded
  Interaction
Authors: Claudius Kienle, Benjamin Alt, Oleg Arenz and Jan Peters
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2505.13497 ,  8789kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13577
replaced with revised version Thu, 25 Sep 2025 23:01:20 GMT   (796kb)

Title: VocalAgent: Large Language Models for Vocal Health Diagnostics with
  Safety-Aware Evaluation
Authors: Yubin Kim, Taehan Kim, Wonjune Kang, Eugene Park, Joonsik Yoon,
  Dongjae Lee, Xin Liu, Daniel McDuff, Hyeonhoon Lee, Cynthia Breazeal, Hae Won
  Park
Categories: cs.SD cs.AI eess.AS
Comments: Accepted by Proceedings of Interspeech 2025; Website:
  https://han811.github.io/VocalAgent2025/
DOI: 10.21437/Interspeech.2025-41
\\ ( https://arxiv.org/abs/2505.13577 ,  796kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16790
replaced with revised version Fri, 26 Sep 2025 09:42:34 GMT   (5072kb)

Title: Learning Flexible Forward Trajectories for Masked Molecular Diffusion
Authors: Hyunjin Seo, Taewon Kim, Sihyun Yu, SungSoo Ahn
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.16790 ,  5072kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16932
replaced with revised version Thu, 25 Sep 2025 20:39:00 GMT   (2294kb)

Title: The Polar Express: Optimal Matrix Sign Methods and Their Application to
  the Muon Algorithm
Authors: Noah Amsel, David Persson, Christopher Musco, Robert M. Gower
Categories: cs.LG cs.AI cs.CL cs.NA math.NA math.OC
Comments: 34 pages, 8 figures, 4 algorithms
MSC-class: 65F30, 68T07, 68N19
ACM-class: G.1.3; I.2.6; F.2.1; G.1.6
\\ ( https://arxiv.org/abs/2505.16932 ,  2294kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16950
replaced with revised version Fri, 26 Sep 2025 14:35:04 GMT   (1012kb)

Title: Bottlenecked Transformers: Periodic KV Cache Consolidation for
  Generalised Reasoning
Authors: Adnan Oomerjee, Zafeirios Fountas, Haitham Bou-Ammar, Jun Wang
Categories: cs.LG cs.AI cs.IT math.IT
\\ ( https://arxiv.org/abs/2505.16950 ,  1012kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17967
replaced with revised version Fri, 26 Sep 2025 17:42:23 GMT   (786kb)

Title: FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization
  of Large Language Models
Authors: Ionut-Vlad Modoranu, Mher Safaryan, Erik Schultheis, Max Ryabinin,
  Artem Chumachenko, Dan Alistarh
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.17967 ,  786kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18697
replaced with revised version Fri, 26 Sep 2025 15:09:18 GMT   (1932kb)

Title: Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning?
  A Systematic Study
Authors: Ziyang Cheng, Zhixun Li, Yuhan Li, Yixin Song, Kangyi Zhao, Dawei
  Cheng, Jia Li, Hong Cheng, Jeffrey Xu Yu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.18697 ,  1932kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18777
replaced with revised version Fri, 26 Sep 2025 15:49:44 GMT   (631kb)

Title: HD-PiSSA: High-Rank Distributed Orthogonal Adaptation
Authors: Yiding Wang and Fauxu Meng and Xuefeng Zhang and Fan Jiang and Pingzhi
  Tang and Muhan Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.18777 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20110
replaced with revised version Fri, 26 Sep 2025 13:08:31 GMT   (13299kb)

Title: Beyond the Proxy: Trajectory-Distilled Guidance for Offline GFlowNet
  Training
Authors: Ruishuo Chen, Xun Wang, Rui Hu, Zhuoran Li, Longbo Huang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.20110 ,  13299kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21573
replaced with revised version Fri, 26 Sep 2025 14:09:49 GMT   (8759kb)

Title: Spectral-inspired Operator Learning with Limited Data and Unknown
  Physics
Authors: Han Wan and Rui Zhang and Hao Sun
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.21573 ,  8759kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21893
replaced with revised version Thu, 25 Sep 2025 13:07:51 GMT   (0kb,I)

Title: SDPO: Importance-Sampled Direct Preference Optimization for Stable
  Diffusion Training
Authors: Xiaomeng Yang, Zhiyu Tan, Junyan Wang, Zhijian Zhou, Hao Li
Categories: cs.LG cs.AI
Comments: This version contains a critical error in the main theorem and proof
  design that affects the validity of the results
\\ ( https://arxiv.org/abs/2505.21893 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21969
replaced with revised version Thu, 25 Sep 2025 15:34:10 GMT   (55749kb)

Title: DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced
  Memory Oriented Navigation
Authors: Tianjun Gu, Linfeng Li, Xuhong Wang, Chenghua Gong, Jingyu Gong,
  Zhizhong Zhang, Yuan Xie, Lizhuang Ma, Xin Tan
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2505.21969 ,  55749kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22988
replaced with revised version Fri, 26 Sep 2025 02:30:13 GMT   (127kb)

Title: Model-Preserving Adaptive Rounding
Authors: Albert Tseng and Zhaofeng Sun and Christopher De Sa
Categories: cs.LG cs.AI
Comments: Preprint
\\ ( https://arxiv.org/abs/2505.22988 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23863
replaced with revised version Fri, 26 Sep 2025 10:35:24 GMT   (2629kb)

Title: Mamba Integrated with Physics Principles Masters Long-term Chaotic
  System Forecasting
Authors: Chang Liu, Bohao Zhao, Jingtao Ding, Huandong Wang, Yong Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.23863 ,  2629kb)
------------------------------------------------------------------------------
\\
arXiv:2506.02863 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 13:07:51 GMT   (886kb)

Title: CapSpeech: Enabling Downstream Applications in Style-Captioned
  Text-to-Speech
Authors: Helin Wang, Jiarui Hai, Dading Chong, Karan Thakkar, Tiantian Feng,
  Dongchao Yang, Junhyeok Lee, Thomas Thebaud, Laureano Moro Velazquez, Jesus
  Villalba, Zengyi Qin, Shrikanth Narayanan, Mounya Elhiali, Najim Dehak
Categories: eess.AS cs.AI cs.SD
\\ ( https://arxiv.org/abs/2506.02863 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05980
replaced with revised version Fri, 26 Sep 2025 08:14:01 GMT   (9133kb)

Title: AMPED: Adaptive Multi-objective Projection for balancing Exploration and
  skill Diversification
Authors: Geonwoo Cho, Jaemoon Lee, Jaegyun Im, Subi Lee, Jihwan Lee, Sundong
  Kim
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2506.05980 ,  9133kb)
------------------------------------------------------------------------------
\\
arXiv:2506.06958
replaced with revised version Fri, 26 Sep 2025 16:20:04 GMT   (548kb)

Title: Position: Simulating Society Requires Simulating Thought
Authors: Chance Jiajie Li, Jiayi Wu, Zhenze Mo, Ao Qu, Yuhan Tang, Kaiya Ivy
  Zhao, Yulu Gan, Jie Fan, Jiangbo Yu, Jinhua Zhao, Paul Liang, Luis Alonso,
  Kent Larson
Categories: cs.CY cs.AI cs.MA
Comments: To appear in NeurIPS 2025 (Position Paper Track)
\\ ( https://arxiv.org/abs/2506.06958 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11022
replaced with revised version Fri, 26 Sep 2025 02:44:27 GMT   (13kb)

Title: Security Degradation in Iterative AI Code Generation -- A Systematic
  Analysis of the Paradox
Authors: Shivani Shukla, Himanshu Joshi and Romilla Syed
Categories: cs.SE cs.AI cs.CL cs.CR cs.LG
Comments: Keywords - Large Language Models, Security Vulnerabilities,
  AI-Generated Code, Iterative Feedback, Software Security, Secure Coding
  Practices, Feedback Loops, LLM Prompting Strategies
\\ ( https://arxiv.org/abs/2506.11022 ,  13kb)
------------------------------------------------------------------------------
\\
arXiv:2506.12037
replaced with revised version Fri, 26 Sep 2025 02:22:24 GMT   (1170kb)

Title: Exploiting Block Coordinate Descent for Cost-Effective LLM Model
  Training
Authors: Zeyu Liu, Yan Li, Yunquan Zhang, Boyang Zhang, Guoyong Jiang, Xin
  Zhang, Limin Xiao, Weifeng Zhang, Daning Cheng
Categories: cs.LG cs.AI
Comments: We have revised certain details of the manuscript and incorporated
  new experimental
\\ ( https://arxiv.org/abs/2506.12037 ,  1170kb)
------------------------------------------------------------------------------
\\
arXiv:2506.16975
replaced with revised version Fri, 26 Sep 2025 13:37:33 GMT   (7264kb)

Title: Latent Concept Disentanglement in Transformer-based Language Models
Authors: Guan Zhe Hong, Bhavya Vasudeva, Vatsal Sharan, Cyrus Rashtchian,
  Prabhakar Raghavan, Rina Panigrahy
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2506.16975 ,  7264kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20893
replaced with revised version Thu, 25 Sep 2025 20:25:09 GMT   (1041kb)

Title: On the Necessity of Output Distribution Reweighting for Effective Class
  Unlearning
Authors: Ali Ebrahimpour-Boroojeny, Yian Wang, and Hari Sundaram
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2506.20893 ,  1041kb)
------------------------------------------------------------------------------
\\
arXiv:2506.22095
replaced with revised version Fri, 26 Sep 2025 06:37:50 GMT   (112kb)

Title: Beyond Simple Graphs: Neural Multi-Objective Routing on Multigraphs
Authors: Filip Rydin, Attila Lischka, Jiaming Wu, Morteza Haghir Chehreghani,
  Bal\'azs Kulcs\'ar
Categories: cs.LG cs.AI
Comments: 29 pages, 6 Figures
\\ ( https://arxiv.org/abs/2506.22095 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2507.03119
replaced with revised version Thu, 25 Sep 2025 22:11:31 GMT   (714kb)

Title: Neural-Network solver of ideal MHD equilibria
Authors: Timo Thun, Andrea Merlo, Rory Conlin, Dario Panici, Daniel
  B\"ockenhoff
Categories: cs.LG cs.AI physics.plasm-ph
Comments: Submitted to Nuclear Fusion, 16 pages, 7 figures
\\ ( https://arxiv.org/abs/2507.03119 ,  714kb)
------------------------------------------------------------------------------
\\
arXiv:2507.07032
replaced with revised version Thu, 25 Sep 2025 21:22:48 GMT   (11720kb)

Title: Lightweight MSA Design Advances Protein Folding From Evolutionary
  Embeddings
Authors: Hanqun Cao, Xinyi Zhou, Zijun Gao, Chenyu Wang, Xin Gao, Zhi Zhang,
  Cesar de la Fuente-Nunez, Chunbin Gu, Ge Liu, Pheng-Ann Heng
Categories: cs.LG cs.AI q-bio.QM
\\ ( https://arxiv.org/abs/2507.07032 ,  11720kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13019
replaced with revised version Fri, 26 Sep 2025 07:50:31 GMT   (17556kb)

Title: Rethinking the Embodied Gap in Vision-and-Language Navigation: A
  Holistic Study of Physical and Visual Disparities
Authors: Liuyi Wang, Xinyuan Xia, Hui Zhao, Hanqing Wang, Tai Wang, Yilun Chen,
  Chengju Liu, Qijun Chen, Jiangmiao Pang
Categories: cs.RO cs.AI cs.CL cs.CV
Comments: Accepted by ICCV 2025
\\ ( https://arxiv.org/abs/2507.13019 ,  17556kb)
------------------------------------------------------------------------------
\\
arXiv:2507.14270
replaced with revised version Fri, 26 Sep 2025 06:16:46 GMT   (56kb)

Title: APTx Neuron: A Unified Trainable Neuron Architecture Integrating
  Activation and Computation
Authors: Ravin Kumar
Categories: cs.NE cs.AI cs.CV cs.LG
Comments: 11 pages, 2 figures, 1 table. Includes a GitHub repository for MNIST
  experiments and a PyPI package for APTx Neuron implementation
\\ ( https://arxiv.org/abs/2507.14270 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2507.14843
replaced with revised version Fri, 26 Sep 2025 04:52:44 GMT   (415kb)

Title: The Invisible Leash: Why RLVR May or May Not Escape Its Origin
Authors: Fang Wu, Weihao Xuan, Ximing Lu, Mingjie Liu, Yi Dong, Zaid Harchaoui,
  Yejin Choi
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2507.14843 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2507.17307
replaced with revised version Fri, 26 Sep 2025 10:55:36 GMT   (901kb)

Title: R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning
Authors: Zhuokun Chen, Zeren Chen, Jiahao He, Lu Sheng, Mingkui Tan, Jianfei
  Cai, and Bohan Zhuang
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2507.17307 ,  901kb)
------------------------------------------------------------------------------
\\
arXiv:2507.21591
replaced with revised version Thu, 25 Sep 2025 19:46:49 GMT   (2580kb)

Title: Hierarchical Graph Neural Network for Compressed Speech Steganalysis
Authors: Mustapha Hemis, Hamza Kheddar, Mohamed Chahine Ghanem, Bachir Boudraa
Categories: cs.CR cs.AI cs.SD eess.AS
DOI: 10.1016/j.array.2025.100510
\\ ( https://arxiv.org/abs/2507.21591 ,  2580kb)
------------------------------------------------------------------------------
\\
arXiv:2508.00017
replaced with revised version Fri, 26 Sep 2025 11:33:45 GMT   (216kb)

Title: Generative Logic: A New Computer Architecture for Deterministic
  Reasoning and Knowledge Generation
Authors: Nikolai Sergeev
Categories: cs.LO cs.AI cs.AR
Comments: v2: Performance update (conjecturer ~250 s; CE filter ~30 s; prover
  ~7 s; peak RAM ~1 GB). Added Counterexample Filter section and workflow
  clarifications. Updated code/artifact links. 18 pages, 5 figures. Code and
  HTML proof graphs archived at Zenodo (DOI: 10.5281/zenodo.17206386)
\\ ( https://arxiv.org/abs/2508.00017 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01188
replaced with revised version Fri, 26 Sep 2025 02:39:16 GMT   (6176kb)

Title: SpectrumWorld: Artificial Intelligence Foundation for Spectroscopy
Authors: Zhuo Yang, Jiaqing Xie, Shuaike Shen, Daolang Wang, Yeyun Chen, Ben
  Gao, Shuzhou Sun, Biqing Qi, Dongzhan Zhou, Lei Bai, Linjiang Chen, Shufei
  Zhang, Qinying Gu, Jun Jiang, Tianfan Fu, Yuqiang Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2508.01188 ,  6176kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01522
replaced with revised version Fri, 26 Sep 2025 13:39:00 GMT   (4432kb)

Title: Decentralized Aerial Manipulation of a Cable-Suspended Load using
  Multi-Agent Reinforcement Learning
Authors: Jack Zeng, Andreu Matoses Gimenez, Eugene Vinitsky, Javier
  Alonso-Mora, Sihao Sun
Categories: cs.RO cs.AI cs.MA
ACM-class: I.2.9; I.2.11; I.2.6
\\ ( https://arxiv.org/abs/2508.01522 ,  4432kb)
------------------------------------------------------------------------------
\\
arXiv:2508.06477 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 16:53:43 GMT   (3234kb)

Title: Intuition emerges in Maximum Caliber models at criticality
Authors: Llu\'is Arola-Fern\'andez
Categories: physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG
\\ ( https://arxiv.org/abs/2508.06477 ,  3234kb)
------------------------------------------------------------------------------
\\
arXiv:2508.06588
replaced with revised version Fri, 26 Sep 2025 10:31:45 GMT   (230kb)

Title: Graph is a Natural Regularization: Revisiting Vector Quantization for
  Graph Representation Learning
Authors: Zian Zhai, Fan Li, Xingyu Tan, Xiaoyang Wang, Wenjie Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2508.06588 ,  230kb)
------------------------------------------------------------------------------
\\
arXiv:2508.08524
replaced with revised version Fri, 26 Sep 2025 13:19:50 GMT   (35384kb)

Title: StreetReaderAI: Making Street View Accessible Using Context-Aware
  Multimodal AI
Authors: Jon E. Froehlich, Alexander Fiannaca, Nimer Jaber, Victor Tsaran,
  Shaun Kane
Categories: cs.HC cs.AI
Comments: Accepted to UIST'25; v2. Fixed a missing word in the PDF; v3. Fixed a
  typo in an author's name; v4. Changed system name and title
ACM-class: H.5; I.2
DOI: 10.1145/3746059.3747756
\\ ( https://arxiv.org/abs/2508.08524 ,  35384kb)
------------------------------------------------------------------------------
\\
arXiv:2508.14134
replaced with revised version Fri, 26 Sep 2025 11:04:12 GMT   (2759kb)

Title: ERIS: An Energy-Guided Feature Disentanglement Framework for
  Out-of-Distribution Time Series Classification
Authors: Xin Wu, Fei Teng, Ji Zhang, Xingwang Li, Yuxuan Liang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2508.14134 ,  2759kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16560
replaced with revised version Fri, 26 Sep 2025 09:13:02 GMT   (8696kb)

Title: Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse
  Autoencoders
Authors: David Chanin, Adri\`a Garriga-Alonso
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2508.16560 ,  8696kb)
------------------------------------------------------------------------------
\\
arXiv:2508.17550
replaced with revised version Fri, 26 Sep 2025 15:04:11 GMT   (147kb)

Title: In-Context Algorithm Emulation in Fixed-Weight Transformers
Authors: Jerry Yao-Chieh Hu, Hude Liu, Jennifer Yuntong Zhang, Han Liu
Categories: cs.LG cs.AI stat.ML
Comments: Code is available at https://github.com/MAGICS-LAB/algo_emu
\\ ( https://arxiv.org/abs/2508.17550 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2508.19463
replaced with revised version Fri, 26 Sep 2025 00:50:33 GMT   (1048kb)

Title: "She was useful, but a bit too optimistic": Augmenting Design with
  Interactive Virtual Personas
Authors: Paluck Deep, Monica Bharadhidasan, A. Baki Kocaballi
Categories: cs.HC cs.AI
Comments: The version accepted for publication at International Journal of
  Human-Computer Studies
Journal-ref: International Journal of Human-Computer Studies (2025)
DOI: 10.1016/j.ijhcs.2025.103646
\\ ( https://arxiv.org/abs/2508.19463 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2509.00338
replaced with revised version Fri, 26 Sep 2025 02:53:47 GMT   (389kb)

Title: Scalable Option Learning in High-Throughput Environments
Authors: Mikael Henaff, Scott Fujimoto, Michael Matthews, Michael Rabbat
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2509.00338 ,  389kb)
------------------------------------------------------------------------------
\\
arXiv:2509.02627 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 06:20:20 GMT   (791kb)

Title: A Two-Stage Strategy for Mitosis Detection Using Improved YOLO11x
  Proposals and ConvNeXt Classification
Authors: Jie Xiao, Mengye Lyu, and Shaojun Liu
Categories: eess.IV cs.AI
\\ ( https://arxiv.org/abs/2509.02627 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13400
replaced with revised version Fri, 26 Sep 2025 11:58:54 GMT   (100kb)

Title: Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer
  Reviews
Authors: Sai Suresh Macharla Vasu, Ivaxi Sheth, Hui-Po Wang, Ruta Binkyte,
  Mario Fritz
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2509.13400 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13805
replaced with revised version Fri, 26 Sep 2025 05:59:55 GMT   (5879kb)

Title: Towards a Physics Foundation Model
Authors: Florian Wiesner, Matthias Wessling, Stephen Baek
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2509.13805 ,  5879kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15363 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 06:24:21 GMT   (1282kb)

Title: Recent Advancements in Microscopy Image Enhancement using Deep Learning:
  A Survey
Authors: Debasish Dutta, Neeharika Sonowal, Risheraj Barauh, Deepjyoti Chetia
  and Sanjib Kr Kalita
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 7 pages, 3 figures and 1 table. 2024 IEEE International Conference on
  Computer Vision and Machine Intelligence (CVMI). IEEE, 2024
DOI: 10.1109/CVMI61877.2024.10782829
\\ ( https://arxiv.org/abs/2509.15363 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16780
replaced with revised version Fri, 26 Sep 2025 05:24:50 GMT   (1156kb)

Title: Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering
  on Math Textbook
Authors: Eason Chen, Chuangji Li, Shizhuo Li, Zimo Xiao, Jionghao Lin, and
  Kenneth R. Koedinger
Categories: cs.IR cs.AI cs.HC
\\ ( https://arxiv.org/abs/2509.16780 ,  1156kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17186
replaced with revised version Fri, 26 Sep 2025 13:41:31 GMT   (2100kb)

Title: Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long
  Sequence Modeling
Authors: Dehao Zhang, Malu Zhang, Shuai Wang, Jingya Wang, Wenjie Wei, Zeyu Ma,
  Guoqing Wang, Yang Yang, Haizhou Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2509.17186 ,  2100kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19406
replaced with revised version Fri, 26 Sep 2025 09:33:25 GMT   (6220kb)

Title: TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via
  Adaptive Granularity Patch and Segment-wise Decoding
Authors: Kuiye Ding and Fanda Fan and Chunyi Hou and Zheya Wang and Lei Wang
  and Zhengxin Yang and Jianfeng Zhan
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2509.19406 ,  6220kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19515
replaced with revised version Fri, 26 Sep 2025 12:44:25 GMT   (427kb)

Title: A Longitudinal Randomized Control Study of Companion Chatbot Use:
  Anthropomorphism and Its Mediating Role on Social Impacts
Authors: Rose E. Guingrich, Michael S. A. Graziano
Categories: cs.HC cs.AI cs.CY
\\ ( https://arxiv.org/abs/2509.19515 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19767
replaced with revised version Thu, 25 Sep 2025 19:07:48 GMT   (3512kb)

Title: FusedANN: Convexified Hybrid ANN via Attribute-Vector Fusion
Authors: Alireza Heidari, Wei Zhang, Ying Xiong
Categories: cs.IR cs.AI cs.DB math.OC
Comments: 62 pages,12 figures
\\ ( https://arxiv.org/abs/2509.19767 ,  3512kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20048
replaced with revised version Thu, 25 Sep 2025 23:19:39 GMT   (564kb)

Title: Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for
  Biosignal Representations
Authors: Rami Zewail
Categories: cs.LG cs.AI eess.SP
\\ ( https://arxiv.org/abs/2509.20048 ,  564kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20253
replaced with revised version Fri, 26 Sep 2025 11:00:50 GMT   (352kb)

Title: AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory
  Anchors for End-to-End Driving
Authors: Jinhao Chai, Anqing Jiang, Hao Jiang, Shiyi Mu, Zichong Gu, Hao Sun
  and Shugong Xu
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2509.20253 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20380
replaced with revised version Fri, 26 Sep 2025 01:37:37 GMT   (534kb)

Title: ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma
  Generation
Authors: Samyak Jhaveri, Vanessa Klotzmann, Crista Lopes
Categories: cs.SE cs.AI cs.PL
\\ ( https://arxiv.org/abs/2509.20380 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2406.07780
replaced with revised version Fri, 26 Sep 2025 02:34:07 GMT   (288kb)

Title: A Critical Look At Tokenwise Reward-Guided Text Generation
Authors: Ahmad Rashid, Ruotian Wu, Julia Grosse, Agustinus Kristiadi and Pascal
  Poupart
Categories: cs.LG cs.CL
Comments: Work accepted at COLM 2025
\\ ( https://arxiv.org/abs/2406.07780 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2408.15176
replaced with revised version Fri, 26 Sep 2025 09:10:50 GMT   (1947kb)

Title: Unifying Symbolic Music Arrangement: Track-Aware Reconstruction and
  Structured Tokenization
Authors: Longshen Ou, Jingwei Zhao, Ziyu Wang, Gus Xia, Qihao Liang, Torin
  Hopkins Ye Wang
Categories: cs.SD cs.CL eess.AS
Comments: Accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2408.15176 ,  1947kb)
------------------------------------------------------------------------------
\\
arXiv:2412.18123
replaced with revised version Fri, 26 Sep 2025 07:15:59 GMT   (24808kb)

Title: Detecting and Interpreting NSFW Prompts in Text-to-Image Models through
  Uncovering Harmful Semantics
Authors: Yiming Wang, Jiahao Chen, Qingming Li, Tong Zhang, Rui Zeng, Xing
  Yang, Shouling Ji
Categories: cs.CR cs.CL
\\ ( https://arxiv.org/abs/2412.18123 ,  24808kb)
------------------------------------------------------------------------------
\\
arXiv:2502.19649
replaced with revised version Fri, 26 Sep 2025 11:29:51 GMT   (543kb)

Title: Taxonomy, Opportunities, and Challenges of Representation Engineering
  for Large Language Models
Authors: Jan Wehner, Sahar Abdelnabi, Daniel Tan, David Krueger, Mario Fritz
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2502.19649 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23537
replaced with revised version Thu, 25 Sep 2025 20:19:14 GMT   (3340kb)

Title: Domain-Aware Tensor Network Structure Search
Authors: Giorgos Iacovides, Wuyang Zhou, Chao Li, Qibin Zhao, Danilo Mandic
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2505.23537 ,  3340kb)
------------------------------------------------------------------------------
\\
arXiv:2506.00209
replaced with revised version Fri, 26 Sep 2025 11:50:53 GMT   (6781kb)

Title: Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare
  Foundation Models
Authors: Liwen Sun, Hao-Ren Yao, Gary Gao, Ophir Frieder, Chenyan Xiong
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2506.00209 ,  6781kb)
------------------------------------------------------------------------------
\\
arXiv:2507.10859
replaced with revised version Thu, 25 Sep 2025 21:32:50 GMT   (4228kb)

Title: MultiVox: A Benchmark for Evaluating Voice Assistants for Multimodal
  Interactions
Authors: Ramaneswaran Selvakumar, Ashish Seth, Nishit Anand, Utkarsh Tyagi,
  Sonal Kumar, Sreyan Ghosh, Dinesh Manocha
Categories: cs.MM cs.CL cs.HC
\\ ( https://arxiv.org/abs/2507.10859 ,  4228kb)
------------------------------------------------------------------------------
\\
arXiv:2507.18053
replaced with revised version Fri, 26 Sep 2025 04:25:36 GMT   (2284kb)

Title: Resource Consumption Red-Teaming for Large Vision-Language Models
Authors: Haoran Gao, Yuanhe Zhang, Zhenhong Zhou, Lei Jiang, Fanyu Meng, Yujia
  Xiao, Li Sun, Kun Wang, Yang Liu, Junlan Feng
Categories: cs.CR cs.CL
\\ ( https://arxiv.org/abs/2507.18053 ,  2284kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16378
replaced with revised version Fri, 26 Sep 2025 16:04:13 GMT   (1516kb)

Title: Longitudinal and Multimodal Recording System to Capture Real-World
  Patient-Clinician Conversations for AI and Encounter Research: Protocol
Authors: Misk Al Zahidy, Kerly Guevara Maldonado, Luis Vilatuna Andrango, Ana
  Cristina Proano, Ana Gabriela Claros, Maria Lizarazo Jimenez, David
  Toro-Tobon, Victor M. Montori, Oscar J. Ponce-Ponte, and Juan P. Brito
Categories: cs.CY cs.CL
Comments: 23 pages, 2 figures, 2 tables
\\ ( https://arxiv.org/abs/2509.16378 ,  1516kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04692
replaced with revised version Thu, 25 Sep 2025 21:46:44 GMT   (8845kb)

Title: Diffence: Fencing Membership Privacy With Diffusion Models
Authors: Yuefeng Peng, Ali Naseh, Amir Houmansadr
Categories: cs.CR cs.CV cs.LG
Comments: NDSS 2025
\\ ( https://arxiv.org/abs/2312.04692 ,  8845kb)
------------------------------------------------------------------------------
\\
arXiv:2404.15274
replaced with revised version Fri, 26 Sep 2025 17:05:48 GMT   (3485kb)

Title: Metric-Guided Conformal Bounds for Probabilistic Image Reconstruction
Authors: Matt Y Cheung, Tucker J Netherton, Laurence E Court, Ashok
  Veeraraghavan, Guha Balakrishnan
Categories: cs.LG cs.CV eess.IV physics.med-ph
Comments: Accepted as Long Oral at UNSURE @ MICCAI 2025. 11 pages, 4 figures, 1
  table, 2 algorithms. Code available at
  https://github.com/matthewyccheung/conformal-metric. Previously titled
  "Metric-guided Image Reconstruction Bounds via Conformal Prediction"
\\ ( https://arxiv.org/abs/2404.15274 ,  3485kb)
------------------------------------------------------------------------------
\\
arXiv:2405.20470
replaced with revised version Fri, 26 Sep 2025 03:27:13 GMT   (6815kb)

Title: STHN: Deep Homography Estimation for UAV Thermal Geo-localization with
  Satellite Imagery
Authors: Jiuhong Xiao, Ning Zhang, Daniel Tortei and Giuseppe Loianno
Categories: cs.RO cs.CV
Comments: 8 pages, 7 figures. Accepted for IEEE Robotics and Automation Letters
\\ ( https://arxiv.org/abs/2405.20470 ,  6815kb)
------------------------------------------------------------------------------
\\
arXiv:2409.18788
replaced with revised version Fri, 26 Sep 2025 16:03:00 GMT   (3709kb)

Title: Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation
Authors: Raphael Hagmanns, Peter Mortimer, Miguel Granero, Thorsten Luettel,
  Janko Petereit
Categories: cs.RO cs.CV
Comments: Accepted for publication at 2025 IEEE International Conference on
  Robotics and Automation (ICRA)
DOI: 10.1109/ICRA55743.2025.11127604
\\ ( https://arxiv.org/abs/2409.18788 ,  3709kb)
------------------------------------------------------------------------------
\\
arXiv:2410.08074
replaced with revised version Fri, 26 Sep 2025 04:06:23 GMT   (89667kb)

Title: Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion
  Models
Authors: Vinith M. Suriyakumar, Rohan Alur, Ayush Sekhari, Manish Raghavan,
  Ashia C. Wilson
Categories: cs.LG cs.CR cs.CV
Comments: 30 pages, 20 figures
\\ ( https://arxiv.org/abs/2410.08074 ,  89667kb)
------------------------------------------------------------------------------
\\
arXiv:2503.02904 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 13:33:43 GMT   (25208kb)

Title: Surgical Vision World Model
Authors: Saurabh Koju, Saurav Bastola, Prashant Shrestha, Sanskar Amgain, Yash
  Raj Shrestha, Rudra P. K. Poudel, Binod Bhattarai
Categories: eess.IV cs.CV cs.LG
Comments: This paper has been accepted at the Data Engineering in Medical
  Imaging Workshop, MICCAI 2025
\\ ( https://arxiv.org/abs/2503.02904 ,  25208kb)
------------------------------------------------------------------------------
\\
arXiv:2503.10434
replaced with revised version Fri, 26 Sep 2025 16:25:56 GMT   (6420kb)

Title: Learning Personalized Driving Styles via Reinforcement Learning from
  Human Feedback
Authors: Derun Li, Changye Li, Yue Wang, Jianwei Ren, Xin Wen, Pengxiang Li,
  Leimeng Xu, Kun Zhan, Peng Jia, Xianpeng Lang, Ningyi Xu, Hang Zhao
Categories: cs.RO cs.CV cs.LG
Comments: 20 pages, 6 figures
\\ ( https://arxiv.org/abs/2503.10434 ,  6420kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17659
replaced with revised version Fri, 26 Sep 2025 04:19:49 GMT   (6561kb)

Title: Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling
Authors: Xiaolong Tang, Meina Kan, Shiguang Shan, Xilin Chen
Categories: cs.RO cs.CV
\\ ( https://arxiv.org/abs/2505.17659 ,  6561kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23692
replaced with revised version Fri, 26 Sep 2025 16:17:30 GMT   (11305kb)

Title: Mobi-$\pi$: Mobilizing Your Robot Learning Policy
Authors: Jingyun Yang, Isabella Huang, Brandon Vu, Max Bajracharya, Rika
  Antonova, Jeannette Bohg
Categories: cs.RO cs.CV cs.LG
Comments: CoRL 2025. Project website: https://mobipi.github.io/
\\ ( https://arxiv.org/abs/2505.23692 ,  11305kb)
------------------------------------------------------------------------------
\\
arXiv:2506.08334
replaced with revised version Thu, 25 Sep 2025 21:24:31 GMT   (1968kb)

Title: iTACO: Interactable Digital Twins of Articulated Objects from Casually
  Captured RGBD Videos
Authors: Weikun Peng and Jun Lv and Cewu Lu and Manolis Savva
Categories: cs.GR cs.CV
Comments: Project website can be found at
  https://3dlg-hcvc.github.io/video2articulation/
\\ ( https://arxiv.org/abs/2506.08334 ,  1968kb)
------------------------------------------------------------------------------
\\
arXiv:2506.13050
replaced with revised version Thu, 25 Sep 2025 09:19:46 GMT   (55021kb)

Title: NeuVAS: Neural Implicit Surfaces for Variational Shape Modeling
Authors: Pengfei Wang, Qiujie Dong, Fangtian Liang, Hao Pan, Lei Yang, Congyi
  Zhang, Guying Lin, Caiming Zhang, Yuanfeng Zhou, Changhe Tu, Shiqing Xin,
  Alla Sheffer, Xin Li, Wenping Wang
Categories: cs.GR cs.CV
\\ ( https://arxiv.org/abs/2506.13050 ,  55021kb)
------------------------------------------------------------------------------
\\
arXiv:2507.17897 (*cross-listing*)
replaced with revised version Fri, 26 Sep 2025 15:29:40 GMT   (1029kb)

Title: Multimodal Recurrent Ensembles for Predicting Brain Responses to
  Naturalistic Movies (Algonauts 2025)
Authors: Semih Eren and Deniz Kucukahmetler and Nico Scherf
Categories: q-bio.NC cs.CV cs.LG
Comments: 8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts
  Project session (3rd-place team). Code:
  https://github.com/erensemih/Algonauts2025_ModalityRNN v3: Added equal
  contribution footnote to author list
\\ ( https://arxiv.org/abs/2507.17897 ,  1029kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20725
replaced with revised version Fri, 26 Sep 2025 03:01:38 GMT   (25871kb)

Title: SeamCrafter: Enhancing Mesh Seam Generation for Artist UV Unwrapping via
  Reinforcement Learning
Authors: Duoteng Xu and Yuguang Chen and Jing Li and Xinhai Liu and Xueqi Ma
  and Zhuo Chen and Dongyu Zhang and Chunchao Guo
Categories: cs.GR cs.CV
\\ ( https://arxiv.org/abs/2509.20725 ,  25871kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20793
replaced with revised version Fri, 26 Sep 2025 12:40:58 GMT   (1116kb)

Title: FERD: Fairness-Enhanced Data-Free Robustness Distillation
Authors: Zhengxiao Li, Liming Lu, Xu Zheng, Siyuan Liang, Zhenghan Chen,
  Yongbin Zhou, Shuchao Pang
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2509.20793 ,  1116kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21196
replaced with revised version Fri, 26 Sep 2025 10:35:12 GMT   (6628kb)

Title: Differential-Integral Neural Operator for Long-Term Turbulence
  Forecasting
Authors: Hao Wu, Yuan Gao, Fan Xu, Fan Zhang, Qingsong Wen, Kun Wang, Xiaomeng
  Huang, Xian Wu
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2509.21196 ,  6628kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02525
replaced with revised version Thu, 25 Sep 2025 20:11:21 GMT   (3452kb)

Title: QECO: A QoE-Oriented Computation Offloading Algorithm based on Deep
  Reinforcement Learning for Mobile Edge Computing
Authors: Iman Rahmaty, Hamed Shah-Mansouri, Ali Movaghar
Categories: cs.NI cs.DC cs.LG cs.PF
Journal-ref: IEEE Trans. Netw. Sci. Eng., vol. 12, no. 4, pp. 3118-3130, 2025
DOI: 10.1109/TNSE.2025.3556809
\\ ( https://arxiv.org/abs/2311.02525 ,  3452kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04526
replaced with revised version Fri, 26 Sep 2025 08:42:42 GMT   (787kb)

Title: Policy Design in Zero-Trust Distributed Networks: Challenges and
  Solutions
Authors: Fannya R. Sandjaja, Ayesha A. Majeed, Abdullah Abdullah, Gyan
  Wickremasinghe, Karen Rafferty, Vishal Sharma
Categories: cs.NI cs.DC
Comments: 11 pages, 6 Figures, 2 Tables
\\ ( https://arxiv.org/abs/2508.04526 ,  787kb)
------------------------------------------------------------------------------
\\
arXiv:2409.10395
replaced with revised version Fri, 26 Sep 2025 16:11:58 GMT   (121kb)

Title: Reducing Leximin Fairness to Utilitarian Optimization
Authors: Eden Hartman, Yonatan Aumann, Avinatan Hassidim, Erel Segal-Halevi
Categories: cs.GT cs.DS cs.MA
Comments: Accepted to AAAI-2025
DOI: 10.1609/aaai.v39i13.33521
\\ ( https://arxiv.org/abs/2409.10395 ,  121kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---

