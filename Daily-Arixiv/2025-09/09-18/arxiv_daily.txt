Gmail	李嘉维 <qetwe0000@gmail.com>
cs daily Subj-class mailing 8004a1 1
send mail ONLY to cs <no-reply@arxiv.org>	2025年9月22日 11:57
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Computer Vision and Pattern Recognition
Distributed, Parallel, and Cluster Computing
Multiagent Systems
 received from  Thu 18 Sep 25 18:00:00 GMT  to  Fri 19 Sep 25 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2509.15237
Date: Wed, 17 Sep 2025 14:36:38 GMT   (6041kb)

Title: MICA: Multi-Agent Industrial Coordination Assistant
Authors: Di Wen, Kunyu Peng, Junwei Zheng, Yufan Chen, Yitain Shi, Jiale Wei,
  Ruiping Liu, Kailun Yang, Rainer Stiefelhagen
Categories: cs.AI cs.CV cs.LG
Comments: The source code will be made publicly available at
  https://github.com/Kratos-Wen/MICA
\\
  Industrial workflows demand adaptive and trustworthy assistance that can
operate under limited computing, connectivity, and strict privacy constraints.
In this work, we present MICA (Multi-Agent Industrial Coordination Assistant),
a perception-grounded and speech-interactive system that delivers real-time
guidance for assembly, troubleshooting, part queries, and maintenance. MICA
coordinates five role-specialized language agents, audited by a safety checker,
to ensure accurate and compliant support. To achieve robust step understanding,
we introduce Adaptive Step Fusion (ASF), which dynamically blends expert
reasoning with online adaptation from natural speech feedback. Furthermore, we
establish a new multi-agent coordination benchmark across representative task
categories and propose evaluation metrics tailored to industrial assistance,
enabling systematic comparison of different coordination topologies. Our
experiments demonstrate that MICA consistently improves task success,
reliability, and responsiveness over baseline structures, while remaining
deployable on practical offline hardware. Together, these contributions
highlight MICA as a step toward deployable, privacy-preserving multi-agent
assistants for dynamic factory environments. The source code will be made
publicly available at https://github.com/Kratos-Wen/MICA.
\\ ( https://arxiv.org/abs/2509.15237 ,  6041kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15239
Date: Wed, 17 Sep 2025 15:44:25 GMT   (2238kb)

Title: KNARsack: Teaching Neural Algorithmic Reasoners to Solve
  Pseudo-Polynomial Problems
Authors: Stjepan Po\v{z}gaj, Dobrik Georgiev, Marin \v{S}ili\'c, Petar
  Veli\v{c}kovi\'c
Categories: cs.AI cs.LG
Comments: 14 pages, 10 figures
\\
  Neural algorithmic reasoning (NAR) is a growing field that aims to embed
algorithmic logic into neural networks by imitating classical algorithms. In
this extended abstract, we detail our attempt to build a neural algorithmic
reasoner that can solve Knapsack, a pseudo-polynomial problem bridging
classical algorithms and combinatorial optimisation, but omitted in standard
NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow
the two-phase pipeline for the Knapsack problem, which involves first
constructing the dynamic programming table and then reconstructing the solution
from it. The approach, which models intermediate states through dynamic
programming supervision, achieves better generalization to larger problem
instances than a direct-prediction baseline that attempts to select the optimal
subset only from the problem inputs.
\\ ( https://arxiv.org/abs/2509.15239 ,  2238kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15291
Date: Thu, 18 Sep 2025 17:24:08 GMT   (956kb)

Title: The Distribution Shift Problem in Transportation Networks using
  Reinforcement Learning and AI
Authors: Federico Taschin, Abderrahmane Lazaraq, Ozan K. Tonguz, Inci Ozgunes
Categories: cs.AI cs.SY eess.SY
\\
  The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart
transportation networks has increased significantly in the last few years.
Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to
be a very promising approach by several authors. However, a problem with using
Reinforcement Learning in Traffic Signal Control is the reliability of the
trained RL agents due to the dynamically changing distribution of the input
data with respect to the distribution of the data used for training. This
presents a major challenge and a reliability problem for the trained network of
AI agents and could have very undesirable and even detrimental consequences if
a suitable solution is not found. Several researchers have tried to address
this problem using different approaches. In particular, Meta Reinforcement
Learning (Meta RL) promises to be an effective solution. In this paper, we
evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and
show that, while under certain conditions MetaLight can indeed lead to
reasonably good results, under some other conditions it might not perform well
(with errors of up to 22%), suggesting that Meta RL schemes are often not
robust enough and can even pose major reliability problems.
\\ ( https://arxiv.org/abs/2509.15291 ,  956kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15292
Date: Thu, 18 Sep 2025 17:24:47 GMT   (858kb)

Title: An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for
  Rapid Literature
Authors: Abhiyan Dhakal (1), Kausik Paudel (1), Sanjog Sigdel (1) ((1)
  Kathmandu University, Dhulikhel, Nepal)
Categories: cs.AI
Comments: 8 pages, 6 figures, 1 table, National Conference on Computer
  Innovations
\\
  We propose an automated pipeline for performing literature reviews using
semantic similarity. Unlike traditional systematic review systems or
optimization based methods, this work emphasizes minimal overhead and high
relevance by using transformer based embeddings and cosine similarity. By
providing a paper title and abstract, it generates relevant keywords, fetches
relevant papers from open access repository, and ranks them based on their
semantic closeness to the input. Three embedding models were evaluated. A
statistical thresholding approach is then applied to filter relevant papers,
enabling an effective literature review pipeline. Despite the absence of
heuristic feedback or ground truth relevance labels, the proposed system shows
promise as a scalable and practical tool for preliminary research and
exploratory analysis.
\\ ( https://arxiv.org/abs/2509.15292 ,  858kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15336
Date: Thu, 18 Sep 2025 18:27:30 GMT   (86kb)

Title: Knowledge-Driven Hallucination in Large Language Models: An Empirical
  Study on Process Modeling
Authors: Humam Kourani, Anton Antonov, Alessandro Berti, Wil M.P. van der Aalst
Categories: cs.AI
Comments: The Version of Record of this contribution will be published in the
  proceedings of the 2nd International Workshop on Generative AI for Process
  Mining (GenAI4PM 2025). This preprint has not undergone peer review or any
  post-submission improvements or corrections
\\
  The utility of Large Language Models (LLMs) in analytical tasks is rooted in
their vast pre-trained knowledge, which allows them to interpret ambiguous
inputs and infer missing information. However, this same capability introduces
a critical risk of what we term knowledge-driven hallucination: a phenomenon
where the model's output contradicts explicit source evidence because it is
overridden by the model's generalized internal knowledge. This paper
investigates this phenomenon by evaluating LLMs on the task of automated
process modeling, where the goal is to generate a formal business process model
from a given source artifact. The domain of Business Process Management (BPM)
provides an ideal context for this study, as many core business processes
follow standardized patterns, making it likely that LLMs possess strong
pre-trained schemas for them. We conduct a controlled experiment designed to
create scenarios with deliberate conflict between provided evidence and the
LLM's background knowledge. We use inputs describing both standard and
deliberately atypical process structures to measure the LLM's fidelity to the
provided evidence. Our work provides a methodology for assessing this critical
reliability issue and raises awareness of the need for rigorous validation of
AI-generated artifacts in any evidence-based domain.
\\ ( https://arxiv.org/abs/2509.15336 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15366
Date: Thu, 18 Sep 2025 19:08:03 GMT   (1180kb)

Title: Diagnostics of cognitive failures in multi-agent expert systems using
  dynamic evaluation protocols and subsequent mutation of the processing
  context
Authors: Andrejs Sorstkins, Josh Bailey, Dr Alistair Baron
Categories: cs.AI
Comments: Dissertation and research project created in collaboration with
  JobFair LTD
\\
  The rapid evolution of neural architectures - from multilayer perceptrons to
large-scale Transformer-based models - has enabled language models (LLMs) to
exhibit emergent agentic behaviours when equipped with memory, planning, and
external tool use. However, their inherent stochasticity and multi-step
decision processes render classical evaluation methods inadequate for
diagnosing agentic performance. This work introduces a diagnostic framework for
expert systems that not only evaluates but also facilitates the transfer of
expert behaviour into LLM-powered agents. The framework integrates (i) curated
golden datasets of expert annotations, (ii) silver datasets generated through
controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores
and prescribes targeted improvements. These prescriptions are embedded into a
vectorized recommendation map, allowing expert interventions to propagate as
reusable improvement trajectories across multiple system instances. We
demonstrate the framework on a multi-agent recruiter-assistant system, showing
that it uncovers latent cognitive failures - such as biased phrasing,
extraction drift, and tool misrouting - while simultaneously steering agents
toward expert-level reasoning and style. The results establish a foundation for
standardized, reproducible expert behaviour transfer in stochastic,
tool-augmented LLM agents, moving beyond static evaluation to active expert
system refinement.
\\ ( https://arxiv.org/abs/2509.15366 ,  1180kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15409
Date: Thu, 18 Sep 2025 20:36:22 GMT   (384kb)

Title: FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation
  Algorithms
Authors: Yu Shee, Anthony M. Smaldone, Anton Morgunov, Gregory W. Kyro, Victor
  S. Batista
Categories: cs.AI
\\
  Retrosynthesis, the process of deconstructing a target molecule into simpler
precursors, is crucial for computer-aided synthesis planning (CASP). Widely
adopted tree-search methods often suffer from exponential computational
complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic
method that leverages fragmentation algorithms, specifically BRICS and r-BRICS,
combined with stock-aware exploration and pattern fingerprint screening to
achieve quadratic complexity. FragmentRetro recursively combines molecular
fragments and verifies their presence in a building block set, providing sets
of fragment combinations as retrosynthetic solutions. We present the first
formal computational analysis of retrosynthetic methods, showing that tree
search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as
$O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number
of heavy atoms in the target molecule and $b$ is the branching factor for tree
search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate
that FragmentRetro achieves high solved rates with competitive runtime,
including cases where tree search fails. The method benefits from fingerprint
screening, which significantly reduces substructure matching complexity. While
FragmentRetro focuses on efficiently identifying fragment-based solutions
rather than full reaction pathways, its computational advantages and ability to
generate strategic starting candidates establish it as a powerful foundational
component for scalable and automated synthesis planning.
\\ ( https://arxiv.org/abs/2509.15409 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15541
Date: Fri, 19 Sep 2025 02:49:56 GMT   (6672kb)

Title: Stress Testing Deliberative Alignment for Anti-Scheming Training
Authors: Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni, Axel
  H{\o}jmark, Felix Hofst\"atter, J\'er\'emy Scheurer, Alexander Meinke, Jason
  Wolfe, Teun van der Weij, Alex Lloyd, Nicholas Goldowsky-Dill, Angela Fan,
  Andrei Matveiakin, Rusheb Shah, Marcus Williams, Amelia Glaese, Boaz Barak,
  Wojciech Zaremba, Marius Hobbhahn
Categories: cs.AI
\\
  Highly capable AI systems could secretly pursue misaligned goals -- what we
call "scheming". Because a scheming AI would deliberately try to hide its
misaligned goals and actions, measuring and mitigating scheming requires
different strategies than are typically used in ML. We propose that assessing
anti-scheming interventions requires at least (1) testing propensity to scheme
on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming
is driven by situational awareness, and (3) checking for robustness to
pre-existing misaligned goals. We use a broad category of "covert actions" --
such as secretly breaking rules or intentionally underperforming in tests -- as
a proxy for scheming, and design evaluations for covert actions. We then
stress-test deliberative alignment as a case study for anti-scheming. Across 26
OOD evaluations (180+ environments), deliberative alignment reduces covert
action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our
mitigation is also able to largely stop agents from pursuing a hidden goal
previously trained into the model, but we still find misbehavior after
additional red-teaming. We find that models' chain-of-thought (CoT) often
demonstrates awareness of being evaluated for alignment, and show causal
evidence that this awareness decreases covert behavior, while unawareness
increases it. Therefore, we cannot exclude that the observed reductions in
covert action rates are at least partially driven by situational awareness.
While we rely on human-legible CoT for training, studying situational
awareness, and demonstrating clear evidence of misalignment, our ability to
rely on this degrades as models continue to depart from reasoning in standard
English. We encourage research into alignment mitigations for scheming and
their assessment, especially for the adversarial case of deceptive alignment,
which this paper does not address.
\\ ( https://arxiv.org/abs/2509.15541 ,  6672kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15635
Date: Fri, 19 Sep 2025 05:57:03 GMT   (6391kb)

Title: MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large
  Language Model Agents
Authors: Pan Tang, Shixiang Tang, Huanqi Pu, Zhiqing Miao, Zhixing Wang
Categories: cs.AI
Comments: 18 pages, 22 figures
\\
  This paper presents MicroRCA-Agent, an innovative solution for microservice
root cause analysis based on large language model agents, which constructs an
intelligent fault root cause localization system with multimodal data fusion.
The technical innovations are embodied in three key aspects: First, we combine
the pre-trained Drain log parsing algorithm with multi-level data filtering
mechanism to efficiently compress massive logs into high-quality fault
features. Second, we employ a dual anomaly detection approach that integrates
Isolation Forest unsupervised learning algorithms with status code validation
to achieve comprehensive trace anomaly identification. Third, we design a
statistical symmetry ratio filtering mechanism coupled with a two-stage LLM
analysis strategy to enable full-stack phenomenon summarization across
node-service-pod hierarchies. The multimodal root cause analysis module
leverages carefully designed cross-modal prompts to deeply integrate multimodal
anomaly information, fully exploiting the cross-modal understanding and logical
reasoning capabilities of large language models to generate structured analysis
results encompassing fault components, root cause descriptions, and reasoning
trace. Comprehensive ablation studies validate the complementary value of each
modal data and the effectiveness of the system architecture. The proposed
solution demonstrates superior performance in complex microservice fault
scenarios, achieving a final score of 50.71. The code has been released at:
https://github.com/tangpan360/MicroRCA-Agent.
\\ ( https://arxiv.org/abs/2509.15635 ,  6391kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15690
Date: Fri, 19 Sep 2025 07:06:27 GMT   (674kb)

Title: CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning
  Framework for C++ Compilation Repair
Authors: Weixuan Sun, Jucai Zhai, Dengfeng Liu, Xin Zhang, Xiaojun Wu, Qiaobo
  Hao, AIMgroup, Yang Fang, Jiuyang Tang
Categories: cs.AI
\\
  The automated repair of C++ compilation errors presents a significant
challenge, the resolution of which is critical for developer productivity.
Progress in this domain is constrained by two primary factors: the scarcity of
large-scale, high-fidelity datasets and the limitations of conventional
supervised methods, which often fail to generate semantically correct
patches.This paper addresses these gaps by introducing a comprehensive
framework with three core contributions. First, we present CCrepair, a novel,
large-scale C++ compilation error dataset constructed through a sophisticated
generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)
paradigm guided by a hybrid reward signal, shifting the focus from mere
compilability to the semantic quality of the fix. Finally, we establish the
robust, two-stage evaluation system providing this signal, centered on an
LLM-as-a-Judge whose reliability has been rigorously validated against the
collective judgments of a panel of human experts. This integrated approach
aligns the training objective with generating high-quality, non-trivial patches
that are both syntactically and semantically correct. The effectiveness of our
approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct
model achieved performance comparable to a Qwen2.5-14B-Instruct model,
validating the efficiency of our training paradigm. Our work provides the
research community with a valuable new dataset and a more effective paradigm
for training and evaluating robust compilation repair models, paving the way
for more practical and reliable automated programming assistants.
\\ ( https://arxiv.org/abs/2509.15690 ,  674kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15730
Date: Fri, 19 Sep 2025 08:01:27 GMT   (717kb)

Title: A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process
  Automation
Authors: Lukas Laakmann, Seyyid A. Ciftci, Christian Janiesch
Categories: cs.AI cs.RO
Journal-ref: Business Process Management Forum. BPM 2024. Lecture Notes in
  Business Information Processing, vol 526. pp. 319-336
DOI: 10.1007/978-3-031-70418-5_19
\\
  Robotic process automation (RPA) is a lightweight approach to automating
business processes using software robots that emulate user actions at the
graphical user interface level. While RPA has gained popularity for its
cost-effective and timely automation of rule-based, well-structured tasks, its
symbolic nature has inherent limitations when approaching more complex tasks
currently performed by human agents. Machine learning concepts enabling
intelligent RPA provide an opportunity to broaden the range of automatable
tasks. In this paper, we conduct a literature review to explore the connections
between RPA and machine learning and organize the joint concept intelligent RPA
into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML
integration and RPA-ML interaction. Together, they comprise eight dimensions:
architecture and ecosystem, capabilities, data basis, intelligence level, and
technical depth of integration as well as deployment environment, lifecycle
phase, and user-robot relation.
\\ ( https://arxiv.org/abs/2509.15730 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15780
Date: Fri, 19 Sep 2025 09:10:29 GMT   (4518kb)

Title: Ontology Creation and Management Tools: the Case of Anatomical
  Connectivity
Authors: Natallia Kokash, Bernard de Bono and Tom Gillespie
Categories: cs.AI cs.DL
Comments: 14 pages
\\
  We are developing infrastructure to support researchers in mapping data
related to the peripheral nervous system and other physiological systems, with
an emphasis on their relevance to the organs under investigation. The nervous
system, a complex network of nerves and ganglia, plays a critical role in
coordinating and transmitting signals throughout the body. To aid in this, we
have created ApiNATOMY, a framework for the topological and semantic
representation of multiscale physiological circuit maps. ApiNATOMY integrates a
Knowledge Representation (KR) model and a suite of Knowledge Management (KM)
tools. The KR model enables physiology experts to easily capture interactions
between anatomical entities, while the KM tools help modelers convert
high-level abstractions into detailed models of physiological processes, which
can be integrated with external ontologies and knowledge graphs.
\\ ( https://arxiv.org/abs/2509.15780 ,  4518kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15786
Date: Fri, 19 Sep 2025 09:17:48 GMT   (704kb)

Title: Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage
  Approach via Semantic Clustering and Multi-Agent Collaboration
Authors: Nan Li, Bo Kang, Tijl De Bie
Categories: cs.AI cs.IR
\\
  Creating robust occupation taxonomies, vital for applications ranging from
job recommendation to labor market intelligence, is challenging. Manual
curation is slow, while existing automated methods are either not adaptive to
dynamic regional markets (top-down) or struggle to build coherent hierarchies
from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent
taxonomy Builder), a framework that fully automates the creation of
high-quality, data-driven taxonomies from raw job postings. CLIMB uses global
semantic clustering to distill core occupations, then employs a
reflection-based multi-agent system to iteratively build a coherent hierarchy.
On three diverse, real-world datasets, we show that CLIMB produces taxonomies
that are more coherent and scalable than existing methods and successfully
capture unique regional characteristics. We release our code and datasets at
https://anonymous.4open.science/r/CLIMB.
\\ ( https://arxiv.org/abs/2509.15786 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15848
Date: Fri, 19 Sep 2025 10:31:59 GMT   (205kb)

Title: A Comparative Study of Rule-Based and Data-Driven Approaches in
  Industrial Monitoring
Authors: Giovanni De Gasperis and Sante Dino Facchini
Categories: cs.AI
\\
  Industrial monitoring systems, especially when deployed in Industry 4.0
environments, are experiencing a shift in paradigm from traditional rule-based
architectures to data-driven approaches leveraging machine learning and
artificial intelligence. This study presents a comparison between these two
methodologies, analyzing their respective strengths, limitations, and
application scenarios, and proposes a basic framework to evaluate their key
properties. Rule-based systems offer high interpretability, deterministic
behavior, and ease of implementation in stable environments, making them ideal
for regulated industries and safety-critical applications. However, they face
challenges with scalability, adaptability, and performance in complex or
evolving contexts. Conversely, data-driven systems excel in detecting hidden
anomalies, enabling predictive maintenance and dynamic adaptation to new
conditions. Despite their high accuracy, these models face challenges related
to data availability, explainability, and integration complexity. The paper
suggests hybrid solutions as a possible promising direction, combining the
transparency of rule-based logic with the analytical power of machine learning.
Our hypothesis is that the future of industrial monitoring lies in intelligent,
synergic systems that leverage both expert knowledge and data-driven insights.
This dual approach enhances resilience, operational efficiency, and trust,
paving the way for smarter and more flexible industrial environments.
\\ ( https://arxiv.org/abs/2509.15848 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15957
Date: Fri, 19 Sep 2025 13:17:16 GMT   (968kb)

Title: EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by
  Large Language Models via Model Context Protocol
Authors: Kanato Masayoshi, Masahiro Hashimoto, Ryoichi Yokoyama, Naoki Toda,
  Yoshifumi Uwamino, Shogo Fukuda, Ho Namkoong, Masahiro Jinzaki
Categories: cs.AI cs.CL cs.HC cs.IR
\\
  Background: Large language models (LLMs) show promise in medicine, but their
deployment in hospitals is limited by restricted access to electronic health
record (EHR) systems. The Model Context Protocol (MCP) enables integration
between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP
can autonomously retrieve clinically relevant information in a real hospital
setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated
with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct
agent to interact with it. Six tasks were tested, derived from use cases of the
infection control team (ICT). Eight patients discussed at ICT conferences were
retrospectively analyzed. Agreement with physician-generated gold standards was
measured.
  Results: The LLM consistently selected and executed the correct MCP tools.
Except for two tasks, all tasks achieved near-perfect accuracy. Performance was
lower in the complex task requiring time-dependent calculations. Most errors
arose from incorrect arguments or misinterpretation of tool results. Responses
from EHR-MCP were reliable, though long and repetitive data risked exceeding
the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a
real hospital setting, achieving near-perfect performance in simple tasks while
highlighting challenges in complex ones. EHR-MCP provides an infrastructure for
secure, consistent data access and may serve as a foundation for hospital AI
agents. Future work should extend beyond retrieval to reasoning, generation,
and clinical impact assessment, paving the way for effective integration of
generative AI into clinical practice.
\\ ( https://arxiv.org/abs/2509.15957 ,  968kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15962
Date: Fri, 19 Sep 2025 13:20:34 GMT   (1324kb)

Title: Structured Information for Improving Spatial Relationships in
  Text-to-Image Generation
Authors: Sander Schildermans, Chang Tian, Ying Jiao, Marie-Francine Moens
Categories: cs.AI
Comments: text-to-image generation, structured information, spatial
  relationship
\\
  Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing
spatial relationships described in natural language prompts remains a major
challenge. Prior efforts have addressed this issue through prompt optimization,
spatially grounded generation, and semantic refinement. This work introduces a
lightweight approach that augments prompts with tuple-based structured
information, using a fine-tuned language model for automatic conversion and
seamless integration into T2I pipelines. Experimental results demonstrate
substantial improvements in spatial accuracy, without compromising overall
image quality as measured by Inception Score. Furthermore, the automatically
generated tuples exhibit quality comparable to human-crafted tuples. This
structured information provides a practical and portable solution to enhance
spatial relationships in T2I generation, addressing a key limitation of current
large-scale generative systems.
\\ ( https://arxiv.org/abs/2509.15962 ,  1324kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16058
Date: Fri, 19 Sep 2025 15:08:30 GMT   (18715kb)

Title: Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired
  Approach for Attention Management in Transformers
Authors: Krati Saxena, Federico Jurado Ruiz, Guido Manzi, Dianbo Liu and Alex
  Lamb
Categories: cs.AI cs.LG
\\
  Attention mechanisms have become integral in AI, significantly enhancing
model performance and scalability by drawing inspiration from human cognition.
Concurrently, the Attention Schema Theory (AST) in cognitive science posits
that individuals manage their attention by creating a model of the attention
itself, effectively allocating cognitive resources. Inspired by AST, we
introduce ASAC (Attention Schema-based Attention Control), which integrates the
attention schema concept into artificial neural networks. Our initial
experiments focused on embedding the ASAC module within transformer
architectures. This module employs a Vector-Quantized Variational AutoEncoder
(VQVAE) as both an attention abstractor and controller, facilitating precise
attention management. By explicitly modeling attention allocation, our approach
aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both
the vision and NLP domains, highlighting its ability to improve classification
accuracy and expedite the learning process. Our experiments with vision
transformers across various datasets illustrate that the attention controller
not only boosts classification accuracy but also accelerates learning.
Furthermore, we have demonstrated the model's robustness and generalization
capabilities across noisy and out-of-distribution datasets. In addition, we
have showcased improved performance in multi-task settings. Quick experiments
reveal that the attention schema-based module enhances resilience to
adversarial attacks, optimizes attention to improve learning efficiency, and
facilitates effective transfer learning and learning from fewer examples. These
promising results establish a connection between cognitive science and machine
learning, shedding light on the efficient utilization of attention mechanisms
in AI systems.
\\ ( https://arxiv.org/abs/2509.16058 ,  18715kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15248
Date: Wed, 17 Sep 2025 22:28:27 GMT   (234kb)

Title: Synthetic bootstrapped pretraining
Authors: Zitong Yang, Aonan Zhang, Hong Liu, Tatsunori Hashimoto, Emmanuel
  Cand\`es, Chong Wang, Ruoming Pang
Categories: cs.CL cs.AI
\\
  We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)
pretraining procedure that first learns a model of relations between documents
from the pretraining dataset and then leverages it to synthesize a vast new
corpus for joint training. While the standard pretraining teaches LMs to learn
causal correlations among tokens within a single document, it is not designed
to efficiently model the rich, learnable inter-document correlations that can
potentially lead to better performance. We validate SBP by designing a
compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T
tokens from scratch. We find SBP consistently improves upon a strong repetition
baseline and delivers a significant fraction of performance improvement
attainable by an oracle upper bound with access to 20x more unique data.
Qualitative analysis reveals that the synthesized documents go beyond mere
paraphrases -- SBP first abstracts a core concept from the seed material and
then crafts a new narration on top of it. Besides strong empirical performance,
SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns
to abstract the latent concepts shared between related documents.
\\ ( https://arxiv.org/abs/2509.15248 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15255
Date: Thu, 18 Sep 2025 07:02:55 GMT   (341kb)

Title: Comparative Analysis of Tokenization Algorithms for Low-Resource
  Language Dzongkha
Authors: Tandin Wangchuk and Tad Gonsalves
Categories: cs.CL
Comments: 10 Pages
\\
  Large Language Models (LLMs) are gaining popularity and improving rapidly.
Tokenizers are crucial components of natural language processing, especially
for LLMs. Tokenizers break down input text into tokens that models can easily
process while ensuring the text is accurately represented, capturing its
meaning and structure. Effective tokenizers enhance the capabilities of LLMs by
improving a model's understanding of context and semantics, ultimately leading
to better performance in various downstream tasks, such as translation,
classification, sentiment analysis, and text generation. Most pre-trained
tokenizers are suitable for high-resource languages like English but perform
poorly for low-resource languages. Dzongkha, Bhutan's national language spoken
by around seven hundred thousand people, is a low-resource language, and its
linguistic complexity poses unique NLP challenges. Despite some progress,
significant research in Dzongkha NLP is lacking, particularly in tokenization.
This study evaluates the training and performance of three common tokenization
algorithms in comparison to other popular methods. Specifically, Byte-Pair
Encoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their
suitability for Dzongkha. Performance was assessed using metrics like Subword
Fertility, Proportion of Continued Words, Normalized Sequence Length, and
execution time. The results show that while all three algorithms demonstrate
potential, SentencePiece is the most effective for Dzongkha tokenization,
paving the way for further NLP advancements. This underscores the need for
tailored approaches for low-resource languages and ongoing research. In this
study, we presented three tokenization algorithms for Dzongkha, paving the way
for building Dzongkha Large Language Models.
\\ ( https://arxiv.org/abs/2509.15255 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15260
Date: Thu, 18 Sep 2025 08:14:34 GMT   (1910kb)

Title: Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's
  Low-Resource Languages
Authors: Yujia Hu, Ming Shan Hee, Preslav Nakov, Roy Ka-Wei Lee
Categories: cs.CL
Comments: 9 pages, EMNLP 2025
\\
  The advancement of Large Language Models (LLMs) has transformed natural
language processing; however, their safety mechanisms remain under-explored in
low-resource, multilingual settings. Here, we aim to bridge this gap. In
particular, we introduce \textsf{SGToxicGuard}, a novel dataset and evaluation
framework for benchmarking LLM safety in Singapore's diverse linguistic
context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a
red-teaming approach to systematically probe LLM vulnerabilities in three
real-world scenarios: \textit{conversation}, \textit{question-answering}, and
\textit{content composition}. We conduct extensive experiments with
state-of-the-art multilingual LLMs, and the results uncover critical gaps in
their safety guardrails. By offering actionable insights into cultural
sensitivity and toxicity mitigation, we lay the foundation for safer and more
inclusive AI systems in linguistically diverse environments.\footnote{Link to
the dataset: https://github.com/Social-AI-Studio/SGToxicGuard.}
\textcolor{red}{Disclaimer: This paper contains sensitive content that may be
disturbing to some readers.}
\\ ( https://arxiv.org/abs/2509.15260 ,  1910kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15335
Date: Thu, 18 Sep 2025 18:26:53 GMT   (980kb)

Title: PolBiX: Detecting LLMs' Political Bias in Fact-Checking through
  X-phemisms
Authors: Charlott Jakob, David Harbecke, Patrick Parschan, Pia Wenzel Neves,
  Vera Schmitt
Categories: cs.CL
\\
  Large Language Models are increasingly used in applications requiring
objective assessment, which could be compromised by political bias. Many
studies found preferences for left-leaning positions in LLMs, but downstream
effects on tasks like fact-checking remain underexplored. In this study, we
systematically investigate political bias through exchanging words with
euphemisms or dysphemisms in German claims. We construct minimal pairs of
factually equivalent claims that differ in political connotation, to assess the
consistency of LLMs in classifying them as true or false. We evaluate six LLMs
and find that, more than political leaning, the presence of judgmental words
significantly influences truthfulness assessment. While a few models show
tendencies of political bias, this is not mitigated by explicitly calling for
objectivism in prompts.
\\ ( https://arxiv.org/abs/2509.15335 ,  980kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15339
Date: Thu, 18 Sep 2025 18:29:14 GMT   (1294kb)

Title: Quantifying Self-Awareness of Knowledge in Large Language Models
Authors: Yeongbin Seo and Dongha Lee and Jinyoung Yeo
Categories: cs.CL
MSC-class: 68T50
ACM-class: I.2.7
\\
  Hallucination prediction in large language models (LLMs) is often interpreted
as a sign of self-awareness. However, we argue that such performance can arise
from question-side shortcuts rather than true model-side introspection. To
disentangle these factors, we propose the Approximate Question-side Effect
(AQE), which quantifies the contribution of question-awareness. Our analysis
across multiple datasets reveals that much of the reported success stems from
exploiting superficial patterns in questions. We further introduce SCAO
(Semantic Compression by Answering in One word), a method that enhances the use
of model-side signals. Experiments show that SCAO achieves strong and
consistent performance, particularly in settings with reduced question-side
cues, highlighting its effectiveness in fostering genuine self-awareness in
LLMs.
\\ ( https://arxiv.org/abs/2509.15339 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15350
Date: Thu, 18 Sep 2025 18:41:57 GMT   (4213kb)

Title: Real, Fake, or Manipulated? Detecting Machine-Influenced Text
Authors: Yitong Wang, Zhongping Zhang, Margherita Piana, Zheng Zhou, Peter
  Gerstoft, Bryan A. Plummer
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Findings
\\
  Large Language Model (LLMs) can be used to write or modify documents,
presenting a challenge for understanding the intent behind their use. For
example, benign uses may involve using LLM on a human-written document to
improve its grammar or to translate it into another language. However, a
document entirely produced by a LLM may be more likely to be used to spread
misinformation than simple translation (\eg, from use by malicious actors or
simply by hallucinating). Prior works in Machine Generated Text (MGT) detection
mostly focus on simply identifying whether a document was human or machine
written, ignoring these fine-grained uses. In this paper, we introduce a
HiErarchical, length-RObust machine-influenced text detector (HERO), which
learns to separate text samples of varying lengths from four primary types:
human-written, machine-generated, machine-polished, and machine-translated.
HERO accomplishes this by combining predictions from length-specialist models
that have been trained with Subcategory Guidance. Specifically, for categories
that are easily confused (\eg, different source languages), our Subcategory
Guidance module encourages separation of the fine-grained categories, boosting
performance. Extensive experiments across five LLMs and six domains demonstrate
the benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on
average.
\\ ( https://arxiv.org/abs/2509.15350 ,  4213kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15361
Date: Thu, 18 Sep 2025 19:01:11 GMT   (1446kb)

Title: Beyond Spurious Signals: Debiasing Multimodal Large Language Models via
  Counterfactual Inference and Adaptive Expert Routing
Authors: Zichen Wu, Hsiu-Yuan Huang and Yunfang Wu
Categories: cs.CL cs.AI cs.MM
Comments: Accepted by EMNLP 2025 Findings
\\
  Multimodal Large Language Models (MLLMs) have shown substantial capabilities
in integrating visual and textual information, yet frequently rely on spurious
correlations, undermining their robustness and generalization in complex
multimodal reasoning tasks. This paper addresses the critical challenge of
superficial correlation bias in MLLMs through a novel causal mediation-based
debiasing framework. Specially, we distinguishing core semantics from spurious
textual and visual contexts via counterfactual examples to activate
training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture
with dynamic routing to selectively engages modality-specific debiasing
experts. Empirical evaluation on multimodal sarcasm detection and sentiment
analysis tasks demonstrates that our framework significantly surpasses unimodal
debiasing strategies and existing state-of-the-art models.
\\ ( https://arxiv.org/abs/2509.15361 ,  1446kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15362
Date: Thu, 18 Sep 2025 19:01:48 GMT   (794kb)

Title: Speech Language Models for Under-Represented Languages: Insights from
  Wolof
Authors: Yaya Sy and Dioula Doucour\'e and Christophe Cerisara and Irina Illina
Categories: cs.CL cs.SD eess.AS
\\
  We present our journey in training a speech language model for Wolof, an
underrepresented language spoken in West Africa, and share key insights. We
first emphasize the importance of collecting large-scale, spontaneous,
high-quality speech data, and show that continued pretraining HuBERT on this
dataset outperforms both the base model and African-centric models on ASR. We
then integrate this speech encoder into a Wolof LLM to train the first Speech
LLM for this language, extending its capabilities to tasks such as speech
translation. Furthermore, we explore training the Speech LLM to perform
multi-step Chain-of-Thought before transcribing or translating. Our results
show that the Speech LLM not only improves speech recognition but also performs
well in speech translation. The models and the code will be openly shared.
\\ ( https://arxiv.org/abs/2509.15362 ,  794kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15373
Date: Thu, 18 Sep 2025 19:20:37 GMT   (46kb)

Title: Frustratingly Easy Data Augmentation for Low-Resource ASR
Authors: Katsumi Ibaraki, David Chiang
Categories: cs.CL
Comments: 5 pages, 2 figures, 2 tables, submitted to ICASSP 2026
\\
  This paper introduces three self-contained data augmentation methods for
low-resource Automatic Speech Recognition (ASR). Our techniques first generate
novel text--using gloss-based replacement, random replacement, or an LLM-based
approach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We
apply these methods, which leverage only the original annotated data, to four
languages with extremely limited resources (Vatlongos, Nashta, Shinekhen
Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a
combination of the original audio and generated synthetic data yields
significant performance gains, including a 14.3% absolute WER reduction for
Nashta. The methods prove effective across all four low-resource languages and
also show utility for high-resource languages like English, demonstrating their
broad applicability.
\\ ( https://arxiv.org/abs/2509.15373 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15403
Date: Thu, 18 Sep 2025 20:29:48 GMT   (9508kb)

Title: Quantifying Uncertainty in Natural Language Explanations of Large
  Language Models for Question Answering
Authors: Yangyi Li, Mengdi Huai
Categories: cs.CL cs.LG
\\
  Large language models (LLMs) have shown strong capabilities, enabling
concise, context-aware answers in question answering (QA) tasks. The lack of
transparency in complex LLMs has inspired extensive research aimed at
developing methods to explain large language behaviors. Among existing
explanation methods, natural language explanations stand out due to their
ability to explain LLMs in a self-explanatory manner and enable the
understanding of model behaviors even when the models are closed-source.
However, despite these promising advancements, there is no existing work
studying how to provide valid uncertainty guarantees for these generated
natural language explanations. Such uncertainty quantification is critical in
understanding the confidence behind these explanations. Notably, generating
valid uncertainty estimates for natural language explanations is particularly
challenging due to the auto-regressive generation process of LLMs and the
presence of noise in medical inquiries. To bridge this gap, in this work, we
first propose a novel uncertainty estimation framework for these generated
natural language explanations, which provides valid uncertainty guarantees in a
post-hoc and model-agnostic manner. Additionally, we also design a novel robust
uncertainty estimation method that maintains valid uncertainty guarantees even
under noise. Extensive experiments on QA tasks demonstrate the desired
performance of our methods.
\\ ( https://arxiv.org/abs/2509.15403 ,  9508kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15419
Date: Thu, 18 Sep 2025 20:51:33 GMT   (533kb)

Title: Deep learning and abstractive summarisation for radiological reports: an
  empirical study for adapting the PEGASUS models' family with scarce data
Authors: Claudio Benzoni, Martina Langhals, Martin Boeker, Luise Modersohn,
  M\'at\'e E. Maros
Categories: cs.CL cs.AI cs.LG
Comments: 14 pages, 4 figures, and 3 tables
\\
  Regardless of the rapid development of artificial intelligence, abstractive
summarisation is still challenging for sensitive and data-restrictive domains
like medicine. With the increasing number of imaging, the relevance of
automated tools for complex medical text summarisation is expected to become
highly relevant. In this paper, we investigated the adaptation via fine-tuning
process of a non-domain-specific abstractive summarisation encoder-decoder
model family, and gave insights to practitioners on how to avoid over- and
underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological
reports public dataset. For each model, we comprehensively evaluated two
different checkpoints with varying sizes of the same training data. We
monitored the models' performances with lexical and semantic metrics during the
training history on the fixed-size validation set. PEGASUS exhibited different
phases, which can be related to epoch-wise double-descent, or
peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger
checkpoint led to a performance detriment. This work highlights the challenges
and risks of fine-tuning models with high expressivity when dealing with scarce
training data, and lays the groundwork for future investigations into more
robust fine-tuning strategies for summarisation models in specialised domains.
\\ ( https://arxiv.org/abs/2509.15419 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15430
Date: Thu, 18 Sep 2025 21:09:29 GMT   (410kb)

Title: BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised
  Speech Recognition
Authors: Liuyuan Jiang, Xiaodong Cui, Brian Kingsbury, Tianyi Chen, Lisha Chen
Categories: cs.CL cs.SD eess.AS
Comments: 5 pages including reference
\\
  Speech is a rich signal, and labeled audio-text pairs are costly, making
self-supervised learning essential for scalable representation learning. A core
challenge in speech SSL is generating pseudo-labels that are both informative
and efficient: strong labels, such as those used in HuBERT, improve downstream
performance but rely on external encoders and multi-stage pipelines, while
efficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.
We propose BiRQ, a bilevel SSL framework that combines the efficiency of
BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key
idea is to reuse part of the model itself as a pseudo-label generator:
intermediate representations are discretized by a random-projection quantizer
to produce enhanced labels, while anchoring labels derived directly from the
raw input stabilize training and prevent collapse. Training is formulated as an
efficient first-order bilevel optimization problem, solved end-to-end with
differentiable Gumbel-softmax selection. This design eliminates the need for
external label encoders, reduces memory cost, and enables iterative label
refinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ
while maintaining low complexity and computational efficiency. We validate our
method on various datasets, including 960-hour LibriSpeech, 150-hour AMI
meetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.
\\ ( https://arxiv.org/abs/2509.15430 ,  410kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15447
Date: Thu, 18 Sep 2025 21:43:28 GMT   (1561kb)

Title: PILOT: Steering Synthetic Data Generation with Psychological &
  Linguistic Output Targeting
Authors: Caitlin Cisar, Emily Sheffield, Joshua Drake, Alden Harrell,
  Subramanian Chidambaram, Nikita Nangia, Vinayak Arannil, Alex Williams
Categories: cs.CL cs.AI
\\
  Generative AI applications commonly leverage user personas as a steering
mechanism for synthetic data generation, but reliance on natural language
representations forces models to make unintended inferences about which
attributes to emphasize, limiting precise control over outputs. We introduce
PILOT (Psychological and Linguistic Output Targeting), a two-phase framework
for steering large language models with structured psycholinguistic profiles.
In Phase 1, PILOT translates natural language persona descriptions into
multidimensional profiles with normalized scores across linguistic and
psychological dimensions. In Phase 2, these profiles guide generation along
measurable axes of variation. We evaluate PILOT across three state-of-the-art
LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas
under three conditions: Natural-language Persona Steering (NPS), Schema-Based
Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate
that schema-based approaches significantly reduce artificial-sounding persona
repetition while improving output coherence, with silhouette scores increasing
from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals
a fundamental trade-off: SBS produces more concise outputs with higher topical
consistency, while NPS offers greater lexical diversity but reduced
predictability. HPS achieves a balance between these extremes, maintaining
output variety while preserving structural consistency. Expert linguistic
evaluation confirms that PILOT maintains high response quality across all
conditions, with no statistically significant differences between steering
approaches.
\\ ( https://arxiv.org/abs/2509.15447 ,  1561kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15476
Date: Thu, 18 Sep 2025 22:44:27 GMT   (1467kb)

Title: Evaluating Multimodal Large Language Models on Spoken Sarcasm
  Understanding
Authors: Zhu Li, Xiyuan Gao, Yuqing Zhang, Shekhar Nayak, Matt Coler
Categories: cs.CL cs.MM
\\
  Sarcasm detection remains a challenge in natural language understanding, as
sarcastic intent often relies on subtle cross-modal cues spanning text, speech,
and vision. While prior work has primarily focused on textual or visual-textual
sarcasm, comprehensive audio-visual-textual sarcasm understanding remains
underexplored. In this paper, we systematically evaluate large language models
(LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and
Chinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In
addition to direct classification, we explore models as feature encoders,
integrating their representations through a collaborative gating fusion module.
Experimental results show that audio-based models achieve the strongest
unimodal performance, while text-audio and audio-vision combinations outperform
unimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show
competitive zero-shot and fine-tuned performance. Our findings highlight the
potential of MLLMs for cross-lingual, audio-visual-textual sarcasm
understanding.
\\ ( https://arxiv.org/abs/2509.15476 ,  1467kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15478
Date: Thu, 18 Sep 2025 22:51:06 GMT   (751kb)

Title: Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt
  Modalities and Models
Authors: Madison Van Doren, Casey Ford, Emily Dix
Categories: cs.CL
\\
  Multimodal large language models (MLLMs) are increasingly used in real world
applications, yet their safety under adversarial conditions remains
underexplored. This study evaluates the harmlessness of four leading MLLMs
(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to
adversarial prompts across text-only and multimodal formats. A team of 26 red
teamers generated 726 prompts targeting three harm categories: illegal
activity, disinformation, and unethical behaviour. These prompts were submitted
to each model, and 17 annotators rated 2,904 model outputs for harmfulness
using a 5-point scale. Results show significant differences in vulnerability
across models and modalities. Pixtral 12B exhibited the highest rate of harmful
responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%).
Contrary to expectations, text-only prompts were slightly more effective at
bypassing safety mechanisms than multimodal ones. Statistical analysis
confirmed that both model type and input modality were significant predictors
of harmfulness. These findings underscore the urgent need for robust,
multimodal safety benchmarks as MLLMs are deployed more widely.
\\ ( https://arxiv.org/abs/2509.15478 ,  751kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15485
Date: Thu, 18 Sep 2025 23:14:51 GMT   (56kb)

Title: mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic
  Readability Assessment
Authors: Ahmed Abdou
Categories: cs.CL cs.AI
\\
  We present a simple, model-agnostic post-processing technique for
fine-grained Arabic readability classification in the BAREC 2025 Shared Task
(19 ordinal levels). Our method applies conformal prediction to generate
prediction sets with coverage guarantees, then computes weighted averages using
softmax-renormalized probabilities over the conformal sets. This
uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing
high-penalty misclassifications to nearer levels. Our approach shows consistent
QWK improvements of 1-3 points across different base models. In the strict
track, our submission achieves QWK scores of 84.9\%(test) and 85.7\% (blind
test) for sentence level, and 73.3\% for document level. For Arabic educational
assessment, this enables human reviewers to focus on a handful of plausible
levels, combining statistical guarantees with practical usability.
\\ ( https://arxiv.org/abs/2509.15485 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15515
Date: Fri, 19 Sep 2025 01:39:08 GMT   (448kb)

Title: LLM Cache Bandit Revisited: Addressing Query Heterogeneity for
  Cost-Effective LLM Inference
Authors: Hantao Yang, Hong Xie, Defu Lian, Enhong Chen
Categories: cs.CL
\\
  This paper revisits the LLM cache bandit problem, with a special focus on
addressing the query heterogeneity for cost-effective LLM inference. Previous
works often assume uniform query sizes. Heterogeneous query sizes introduce a
combinatorial structure for cache selection, making the cache replacement
process more computationally and statistically challenging. We treat optimal
cache selection as a knapsack problem and employ an accumulation-based strategy
to effectively balance computational overhead and cache updates. In theoretical
analysis, we prove that the regret of our algorithm achieves an $O(\sqrt{MNT})$
bound, improving the coefficient of $\sqrt{MN}$ compared to the $O(MN\sqrt{T})$
result in Berkeley, where $N$ is the total number of queries and $M$ is the
cache size. Additionally, we also provide a problem-dependent bound, which was
absent in previous works. The experiment rely on real-world data show that our
algorithm reduces the total cost by approximately 12\%.
\\ ( https://arxiv.org/abs/2509.15515 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15518
Date: Fri, 19 Sep 2025 01:49:27 GMT   (1196kb)

Title: How do Language Models Generate Slang: A Systematic Comparison between
  Human and Machine-Generated Slang Usages
Authors: Siyang Wu, Zhewei Sun
Categories: cs.CL cs.AI cs.LG
\\
  Slang is a commonly used type of informal language that poses a daunting
challenge to NLP systems. Recent advances in large language models (LLMs),
however, have made the problem more approachable. While LLM agents are becoming
more widely applied to intermediary tasks such as slang detection and slang
interpretation, their generalizability and reliability are heavily dependent on
whether these models have captured structural knowledge about slang that align
well with human attested slang usages. To answer this question, we contribute a
systematic comparison between human and machine-generated slang usages. Our
evaluative framework focuses on three core aspects: 1) Characteristics of the
usages that reflect systematic biases in how machines perceive slang, 2)
Creativity reflected by both lexical coinages and word reuses employed by the
slang usages, and 3) Informativeness of the slang usages when used as
gold-standard examples for model distillation. By comparing human-attested
slang usages from the Online Slang Dictionary (OSD) and slang generated by
GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our
results suggest that while LLMs have captured significant knowledge about the
creative aspects of slang, such knowledge does not align with humans
sufficiently to enable LLMs for extrapolative tasks such as linguistic
analyses.
\\ ( https://arxiv.org/abs/2509.15518 ,  1196kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15549
Date: Fri, 19 Sep 2025 03:07:59 GMT   (20037kb)

Title: A method for improving multilingual quality and diversity of instruction
  fine-tuning datasets
Authors: Chunguang Zhao, Yilun Liu, Pufan Zeng, Yuanchang Luo, Shimin Tao,
  Minggui He, Weibin Meng, Song Xu, Ziang Chen, Chen Liu, Hongxia Ma, Li Zhang,
  Boxing Chen, Daimeng Wei
Categories: cs.CL
\\
  Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large
language models (LLMs) to generalize effectively across diverse linguistic and
cultural contexts. However, the scarcity of high-quality multilingual training
data and corresponding building method remains a critical bottleneck. While
data selection has shown promise in English settings, existing methods often
fail to generalize across languages due to reliance on simplistic heuristics or
language-specific assumptions. In this work, we introduce Multilingual Data
Quality and Diversity (M-DaQ), a novel method for improving LLMs
multilinguality, by selecting high-quality and semantically diverse
multilingual IFT samples. We further conduct the first systematic investigation
of the Superficial Alignment Hypothesis (SAH) in multilingual setting.
Empirical results across 18 languages demonstrate that models fine-tuned with
M-DaQ method achieve significant performance gains over vanilla baselines over
60% win rate. Human evaluations further validate these gains, highlighting the
increment of cultural points in the response. We release the M-DaQ code to
support future research.
\\ ( https://arxiv.org/abs/2509.15549 ,  20037kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15550
Date: Fri, 19 Sep 2025 03:08:13 GMT   (10200kb)

Title: DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired
  Mutation-Repair Paradigm
Authors: Xiaowei Zhu, Yubing Ren, Fang Fang, Qingfeng Tan, Shi Wang, Yanan Cao
Categories: cs.CL
Comments: NeurIPS 2025 Spotlight
\\
  The rapid advancement of large language models (LLMs) has blurred the line
between AI-generated and human-written text. This progress brings societal
risks such as misinformation, authorship ambiguity, and intellectual property
concerns, highlighting the urgent need for reliable AI-generated text detection
methods. However, recent advances in generative language modeling have resulted
in significant overlap between the feature distributions of human-written and
AI-generated text, blurring classification boundaries and making accurate
detection increasingly challenging. To address the above challenges, we propose
a DNA-inspired perspective, leveraging a repair-based process to directly and
interpretably capture the intrinsic differences between human-written and
AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a
zero-shot detection method for distinguishing AI-generated and human-written
text. The method constructs an ideal AI-generated sequence for each input,
iteratively repairs non-optimal tokens, and quantifies the cumulative repair
effort as an interpretable detection signal. Empirical evaluations demonstrate
that our method achieves state-of-the-art detection performance and exhibits
strong robustness against various adversarial attacks and input lengths.
Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC
and 2.08% in F1 score across multiple public benchmark datasets.
\\ ( https://arxiv.org/abs/2509.15550 ,  10200kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15556
Date: Fri, 19 Sep 2025 03:34:34 GMT   (145kb)

Title: Exploring Polyglot Harmony: On Multilingual Data Allocation for Large
  Language Models Pretraining
Authors: Ping Guo, Yubing Ren, Binbin Liu, Fengze Liu, Haobin Lin, Yifan Zhang,
  Bingni Zhang, Taifeng Wang, Yin Zheng
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have become integral to a wide range of
applications worldwide, driving an unprecedented global demand for effective
multilingual capabilities. Central to achieving robust multilingual performance
is the strategic allocation of language proportions within training corpora.
However, determining optimal language ratios is highly challenging due to
intricate cross-lingual interactions and sensitivity to dataset scale. This
paper introduces Climb (Cross-Lingual Interaction-aware Multilingual
Balancing), a novel framework designed to systematically optimize multilingual
data allocation. At its core, Climb introduces a cross-lingual
interaction-aware language ratio, explicitly quantifying each language's
effective allocation by capturing inter-language dependencies. Leveraging this
ratio, Climb proposes a principled two-step optimization procedure--first
equalizing marginal benefits across languages, then maximizing the magnitude of
the resulting language allocation vectors--significantly simplifying the
inherently complex multilingual optimization problem. Extensive experiments
confirm that Climb can accurately measure cross-lingual interactions across
various multilingual settings. LLMs trained with Climb-derived proportions
consistently achieve state-of-the-art multilingual performance, even achieving
competitive performance with open-sourced LLMs trained with more tokens.
\\ ( https://arxiv.org/abs/2509.15556 ,  145kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15560
Date: Fri, 19 Sep 2025 03:45:44 GMT   (268kb)

Title: How important is language for human-like intelligence?
Authors: Gary Lupyan, Hunter Gentry, Martin Zettersten
Categories: cs.CL
\\
  We use language to communicate our thoughts. But is language merely the
expression of thoughts, which are themselves produced by other, nonlinguistic
parts of our minds? Or does language play a more transformative role in human
cognition, allowing us to have thoughts that we otherwise could (or would) not
have? Recent developments in artificial intelligence (AI) and cognitive science
have reinvigorated this old question. We argue that language may hold the key
to the emergence of both more general AI systems and central aspects of human
intelligence. We highlight two related properties of language that make it such
a powerful tool for developing domain--general abilities. First, language
offers compact representations that make it easier to represent and reason
about many abstract concepts (e.g., exact numerosity). Second, these compressed
representations are the iterated output of collective minds. In learning a
language, we learn a treasure trove of culturally evolved abstractions. Taken
together, these properties mean that a sufficiently powerful learning system
exposed to language--whether biological or artificial--learns a compressed
model of the world, reverse engineering many of the conceptual and causal
structures that support human (and human-like) thought.
\\ ( https://arxiv.org/abs/2509.15560 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15568
Date: Fri, 19 Sep 2025 04:07:46 GMT   (630kb)

Title: LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs
Authors: Junlong Jia, Xing Wu, Chaochen Gao, Ziyang Chen, Zijia Lin, Zhongzhi
  Li, Weinong Wang, Haotian Xu, Donghui Jin, Debing Zhang, Binghui Guo
Categories: cs.CL cs.AI
Comments: work in progress
\\
  High-quality long-context data is essential for training large language
models (LLMs) capable of processing extensive documents, yet existing synthesis
approaches using relevance-based aggregation face challenges of computational
efficiency. We present LiteLong, a resource-efficient method for synthesizing
long-context data through structured topic organization and multi-agent debate.
Our approach leverages the BISAC book classification system to provide a
comprehensive hierarchical topic organization, and then employs a debate
mechanism with multiple LLMs to generate diverse, high-quality topics within
this structure. For each topic, we use lightweight BM25 retrieval to obtain
relevant documents and concatenate them into 128K-token training samples.
Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves
competitive long-context performance and can seamlessly integrate with other
long-dependency enhancement methods. LiteLong makes high-quality long-context
data synthesis more accessible by reducing both computational and data
engineering costs, facilitating further research in long-context language
training.
\\ ( https://arxiv.org/abs/2509.15568 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15577
Date: Fri, 19 Sep 2025 04:24:57 GMT   (211kb)

Title: Relevance to Utility: Process-Supervised Rewrite for RAG
Authors: Jaeyoung Kim, Jongho Kim, Seung-won Hwang, Seoho Song, Young-In Song
Categories: cs.CL cs.AI
\\
  Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.
\\ ( https://arxiv.org/abs/2509.15577 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15579
Date: Fri, 19 Sep 2025 04:29:59 GMT   (1678kb)

Title: Chunk Based Speech Pre-training with High Resolution Finite Scalar
  Quantization
Authors: Yun Tang and Cindy Tseng
Categories: cs.CL cs.SD eess.AS
\\
  Low latency speech human-machine communication is becoming increasingly
necessary as speech technology advances quickly in the last decade. One of the
primary factors behind the advancement of speech technology is self-supervised
learning. Most self-supervised learning algorithms are designed with full
utterance assumption and compromises have to made if partial utterances are
presented, which are common in the streaming applications. In this work, we
propose a chunk based self-supervised learning (Chunk SSL) algorithm as an
unified solution for both streaming and offline speech pre-training. Chunk SSL
is optimized with the masked prediction loss and an acoustic encoder is
encouraged to restore indices of those masked speech frames with help from
unmasked frames in the same chunk and preceding chunks. A copy and append data
augmentation approach is proposed to conduct efficient chunk based
pre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to
discretize input speech features and our study shows a high resolution FSQ
codebook, i.e., a codebook with vocabulary size up to a few millions, is
beneficial to transfer knowledge from the pre-training task to the downstream
tasks. A group masked prediction loss is employed during pre-training to
alleviate the high memory and computation cost introduced by the large
codebook. The proposed approach is examined in two speech to text tasks, i.e.,
speech recognition and speech translation. Experimental results on the
\textsc{Librispeech} and \textsc{Must-C} datasets show that the proposed method
could achieve very competitive results for speech to text tasks at both
streaming and offline modes.
\\ ( https://arxiv.org/abs/2509.15579 ,  1678kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15587
Date: Fri, 19 Sep 2025 04:40:46 GMT   (319kb)

Title: DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation
  in Large Language Models
Authors: Tsz Ting Chung, Lemao Liu, Mo Yu, Dit-Yan Yeung
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by EMNLP 2025. Project Page:
  https://ttchungc.github.io/projects/divlogiceval/
\\
  Logic reasoning in natural language has been recognized as an important
measure of human intelligence for Large Language Models (LLMs). Popular
benchmarks may entangle multiple reasoning skills and thus provide unfaithful
evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning
benchmarks are limited in language diversity and their distributions are
deviated from the distribution of an ideal logic reasoning benchmark, which may
lead to biased evaluation results. This paper thereby proposes a new classical
logic benchmark DivLogicEval, consisting of natural sentences composed of
diverse statements in a counterintuitive way. To ensure a more reliable
evaluation, we also introduce a new evaluation metric that mitigates the
influence of bias and randomness inherent in LLMs. Through experiments, we
demonstrate the extent to which logical reasoning is required to answer the
questions in DivLogicEval and compare the performance of different popular LLMs
in conducting logical reasoning.
\\ ( https://arxiv.org/abs/2509.15587 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15620
Date: Fri, 19 Sep 2025 05:32:50 GMT   (1620kb)

Title: SciEvent: Benchmarking Multi-domain Scientific Event Extraction
Authors: Bofu Dong, Pritesh Shah, Sumedh Sonawane, Tiyasha Banerjee, Erin
  Brady, Xinya Du, Ming Jiang
Categories: cs.CL
Comments: 9 pages, 8 figures (main); 22 pages, 11 figures (appendix). Accepted
  to EMNLP 2025 (Main Conference)
\\
  Scientific information extraction (SciIE) has primarily relied on
entity-relation extraction in narrow domains, limiting its applicability to
interdisciplinary research and struggling to capture the necessary context of
scientific information, often resulting in fragmented or conflicting
statements. In this paper, we introduce SciEvent, a novel multi-domain
benchmark of scientific abstracts annotated via a unified event extraction (EE)
schema designed to enable structured and context-aware understanding of
scientific content. It includes 500 abstracts across five research domains,
with manual annotations of event segments, triggers, and fine-grained
arguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting
abstracts into core scientific activities--Background, Method, Result, and
Conclusion; and (2) extracting the corresponding triggers and arguments.
Experiments with fine-tuned EE models, large language models (LLMs), and human
annotators reveal a performance gap, with current models struggling in domains
such as sociology and humanities. SciEvent serves as a challenging benchmark
and a step toward generalizable, multi-domain SciIE.
\\ ( https://arxiv.org/abs/2509.15620 ,  1620kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15621
Date: Fri, 19 Sep 2025 05:34:45 GMT   (2427kb)

Title: Concept Unlearning in Large Language Models via Self-Constructed
  Knowledge Triplets
Authors: Tomoya Yamashita, Yuuki Yamanaka, Masanori Yamada, Takayuki Miura,
  Toshiki Shibahara, Tomoharu Iwata
Categories: cs.CL cs.LG
\\
  Machine Unlearning (MU) has recently attracted considerable attention as a
solution to privacy and copyright issues in large language models (LLMs).
Existing MU methods aim to remove specific target sentences from an LLM while
minimizing damage to unrelated knowledge. However, these approaches require
explicit target sentences and do not support removing broader concepts, such as
persons or events. To address this limitation, we introduce Concept Unlearning
(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to
represent the LLM's internal knowledge and define CU as removing the forgetting
target nodes and associated edges. This graph-based formulation enables a more
intuitive unlearning and facilitates the design of more effective methods. We
propose a novel method that prompts the LLM to generate knowledge triplets and
explanatory sentences about the forgetting target and applies the unlearning
process to these representations. Our approach enables more precise and
comprehensive concept removal by aligning the unlearning process with the LLM's
internal knowledge representations. Experiments on real-world and synthetic
datasets demonstrate that our method effectively achieves concept-level
unlearning while preserving unrelated knowledge.
\\ ( https://arxiv.org/abs/2509.15621 ,  2427kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15631
Date: Fri, 19 Sep 2025 05:48:12 GMT   (6539kb)

Title: Sparse-Autoencoder-Guided Internal Representation Unlearning for Large
  Language Models
Authors: Tomoya Yamashita, Akira Ito, Yuuki Yamanaka, Masanori Yamada, Takayuki
  Miura, Toshiki Shibahara
Categories: cs.CL cs.LG
\\
  As large language models (LLMs) are increasingly deployed across various
applications, privacy and copyright concerns have heightened the need for more
effective LLM unlearning techniques. Many existing unlearning methods aim to
suppress undesirable outputs through additional training (e.g., gradient
ascent), which reduces the probability of generating such outputs. While such
suppression-based approaches can control model outputs, they may not eliminate
the underlying knowledge embedded in the model's internal activations; muting a
response is not the same as forgetting it. Moreover, such suppression-based
methods often suffer from model collapse. To address these issues, we propose a
novel unlearning method that directly intervenes in the model's internal
activations. In our formulation, forgetting is defined as a state in which the
activation of a forgotten target is indistinguishable from that of ``unknown''
entities. Our method introduces an unlearning objective that modifies the
activation of the target entity away from those of known entities and toward
those of unknown entities in a sparse autoencoder latent space. By aligning the
target's internal activation with those of unknown entities, we shift the
model's recognition of the target entity from ``known'' to ``unknown'',
achieving genuine forgetting while avoiding over-suppression and model
collapse. Empirically, we show that our method effectively aligns the internal
activations of the forgotten target, a result that the suppression-based
approaches do not reliably achieve. Additionally, our method effectively
reduces the model's recall of target knowledge in question-answering tasks
without significant damage to the non-target knowledge.
\\ ( https://arxiv.org/abs/2509.15631 ,  6539kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15640
Date: Fri, 19 Sep 2025 06:06:36 GMT   (1884kb)

Title: Multilingual LLM Prompting Strategies for Medical English-Vietnamese
  Machine Translation
Authors: Nhu Vo, Nu-Uyen-Phuong Le, Dung D. Le, Massimo Piccardi, Wray Buntine
Categories: cs.CL
Comments: The work is under peer review
\\
  Medical English-Vietnamese machine translation (En-Vi MT) is essential for
healthcare access and communication in Vietnam, yet Vietnamese remains a
low-resource and under-studied language. We systematically evaluate prompting
strategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,
comparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,
an English-Vietnamese medical lexicon. Results show that model scale is the
primary driver of performance: larger LLMs achieve strong zero-shot results,
while few-shot prompting yields only marginal improvements. In contrast,
terminology-aware cues and embedding-based example retrieval consistently
improve domain-specific translation. These findings underscore both the promise
and the current limitations of multilingual LLMs for medical En-Vi MT.
\\ ( https://arxiv.org/abs/2509.15640 ,  1884kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15655
Date: Fri, 19 Sep 2025 06:29:33 GMT   (6385kb)

Title: Layer-wise Minimal Pair Probing Reveals Contextual
  Grammatical-Conceptual Hierarchy in Speech Representations
Authors: Linyang He, Qiaolin Wang, Xilin Jiang, Nima Mesgarani
Categories: cs.CL eess.AS
Comments: EMNLP 2025 Main Conference (Oral)
\\
  Transformer-based speech language models (SLMs) have significantly improved
neural speech recognition and understanding. While existing research has
examined how well SLMs encode shallow acoustic and phonetic features, the
extent to which SLMs encode nuanced syntactic and conceptual features remains
unclear. By drawing parallels with linguistic competence assessments for large
language models, this study is the first to systematically evaluate the
presence of contextual syntactic and semantic features across SLMs for
self-supervised learning (S3M), automatic speech recognition (ASR), speech
compression (codec), and as the encoder for auditory large language models
(AudioLLMs). Through minimal pair designs and diagnostic feature analysis
across 71 tasks spanning diverse linguistic levels, our layer-wise and
time-resolved analysis uncovers that 1) all speech encode grammatical features
more robustly than conceptual ones.
\\ ( https://arxiv.org/abs/2509.15655 ,  6385kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15667
Date: Fri, 19 Sep 2025 06:42:42 GMT   (1192kb)

Title: VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion
Authors: Dimitrios Damianos, Leon Voukoutis, Georgios Paraskevopoulos, Vassilis
  Katsouros
Categories: cs.CL cs.SD eess.AS
\\
  We present a multimodal fusion framework that bridges pre-trained
decoder-based large language models (LLM) and acoustic encoder-decoder
architectures such as Whisper, with the aim of building speech-enabled LLMs.
Instead of directly using audio embeddings, we explore an intermediate
audio-conditioned text space as a more effective mechanism for alignment. Our
method operates fully in continuous text representation spaces, fusing
Whisper's hidden decoder states with those of an LLM through cross-modal
attention, and supports both offline and streaming modes. We introduce
\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that
our approach effectively aligns representations across modalities. These
results highlight continuous space fusion as a promising path for multilingual
and low-resource speech LLMs, while achieving state-of-the-art results for
Automatic Speech Recognition in Greek, providing an average $\sim20\%$ relative
improvement across benchmarks.
\\ ( https://arxiv.org/abs/2509.15667 ,  1192kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15701
Date: Fri, 19 Sep 2025 07:23:25 GMT   (55kb)

Title: Fine-Tuning Large Multimodal Models for Automatic Pronunciation
  Assessment
Authors: Ke Wang, Wenning Wei, Yan Deng, Lei He, Sheng Zhao
Categories: cs.CL cs.SD eess.AS
Comments: submitted to ICASSP2026
\\
  Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted
Language Learning (CALL), requiring evaluation across multiple granularities
and aspects. Large Multimodal Models (LMMs) present new opportunities for APA,
but their effectiveness in fine-grained assessment remains uncertain. This work
investigates fine-tuning LMMs for APA using the Speechocean762 dataset and a
private corpus. Fine-tuning significantly outperforms zero-shot settings and
achieves competitive results on single-granularity tasks compared to public and
commercial systems. The model performs well at word and sentence levels, while
phoneme-level assessment remains challenging. We also observe that the Pearson
Correlation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation
Coefficient (SCC) remains around 0.6, suggesting that SCC better reflects
ordinal consistency. These findings highlight both the promise and limitations
of LMMs for APA and point to future work on fine-grained modeling and
rank-aware evaluation.
\\ ( https://arxiv.org/abs/2509.15701 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15714
Date: Fri, 19 Sep 2025 07:45:34 GMT   (784kb)

Title: Once Upon a Time: Interactive Learning for Storytelling with Small
  Language Models
Authors: Jonas Mayer Martins, Ali Hamza Bashir, Muhammad Rehan Khalid, Lisa
  Beinborn
Categories: cs.CL cs.AI
Comments: EMNLP 2025, BabyLM Challenge; 16 pages, 6 figures
\\
  Children efficiently acquire language not just by listening, but by
interacting with others in their social environment. Conversely, large language
models are typically trained with next-word prediction on massive amounts of
text. Motivated by this contrast, we investigate whether language models can be
trained with less data by learning not only from next-word prediction but also
from high-level, cognitively inspired feedback. We train a student model to
generate stories, which a teacher model rates on readability, narrative
coherence, and creativity. By varying the amount of pretraining before the
feedback loop, we assess the impact of this interactive learning on formal and
functional linguistic competence. We find that the high-level feedback is
highly data efficient: With just 1 M words of input in interactive learning,
storytelling skills can improve as much as with 410 M words of next-word
prediction.
\\ ( https://arxiv.org/abs/2509.15714 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15723
Date: Fri, 19 Sep 2025 07:53:51 GMT   (283kb)

Title: REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed
  Prompting
Authors: Nannan Huang, Haytham M. Fayek, and Xiuzhen Zhang
Categories: cs.CL
Comments: Accepted to the 5th New Frontiers in Summarization Workshop
  (NewSumm@EMNLP 2025)
\\
  Individuals express diverse opinions, a fair summary should represent these
viewpoints comprehensively. Previous research on fairness in opinion
summarisation using large language models (LLMs) relied on hyperparameter
tuning or providing ground truth distributional information in prompts.
However, these methods face practical limitations: end-users rarely modify
default model parameters, and accurate distributional information is often
unavailable. Building upon cognitive science research demonstrating that
frequency-based representations reduce systematic biases in human statistical
reasoning by making reference classes explicit and reducing cognitive load,
this study investigates whether frequency framed prompting (REFER) can
similarly enhance fairness in LLM opinion summarisation. Through systematic
experimentation with different prompting frameworks, we adapted techniques
known to improve human reasoning to elicit more effective information
processing in language models compared to abstract probabilistic
representations.Our results demonstrate that REFER enhances fairness in
language models when summarising opinions. This effect is particularly
pronounced in larger language models and using stronger reasoning instructions.
\\ ( https://arxiv.org/abs/2509.15723 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15739
Date: Fri, 19 Sep 2025 08:10:32 GMT   (1709kb)

Title: Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via
  Argumentation Theory Semantics
Authors: Reza Sanayei, Srdjan Vesic, Eduardo Blanco, Mihai Surdeanu
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Findings
\\
  Large Language Models (LLMs) excel at linear reasoning tasks but remain
underexplored on non-linear structures such as those found in natural debates,
which are best expressed as argument graphs. We evaluate whether LLMs can
approximate structured reasoning from Computational Argumentation Theory (CAT).
Specifically, we use Quantitative Argumentation Debate (QuAD) semantics, which
assigns acceptability scores to arguments based on their attack and support
relations. Given only dialogue-formatted debates from two NoDE datasets, models
are prompted to rank arguments without access to the underlying graph. We test
several LLMs under advanced instruction strategies, including Chain-of-Thought
and In-Context Learning. While models show moderate alignment with QuAD
rankings, performance degrades with longer inputs or disrupted discourse flow.
Advanced prompting helps mitigate these effects by reducing biases related to
argument length and position. Our findings highlight both the promise and
limitations of LLMs in modeling formal argumentation semantics and motivate
future work on graph-aware reasoning.
\\ ( https://arxiv.org/abs/2509.15739 ,  1709kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15763
Date: Fri, 19 Sep 2025 08:47:37 GMT   (619kb)

Title: UniGist: Towards General and Hardware-aligned Sequence-level Long
  Context Compression
Authors: Chenlong Deng, Zhisong Zhang, Kelong Mao, Shuaiyi Li, Tianqing Fang,
  Hongming Zhang, Haitao Mi, Dong Yu, Zhicheng Dou
Categories: cs.CL
Comments: 15 pages, 7 figures
\\
  Large language models are increasingly capable of handling long-context
inputs, but the memory overhead of key-value (KV) cache remains a major
bottleneck for general-purpose deployment. While various compression strategies
have been explored, sequence-level compression, which drops the full KV caches
for certain tokens, is particularly challenging as it can lead to the loss of
important contextual information. To address this, we introduce UniGist, a
sequence-level long-context compression framework that efficiently preserves
context information by replacing raw tokens with special compression tokens
(gists) in a fine-grained manner. We adopt a chunk-free training strategy and
design an efficient kernel with a gist shift trick, enabling optimized GPU
training. Our scheme also supports flexible inference by allowing the actual
removal of compressed tokens, resulting in real-time memory savings.
Experiments across multiple long-context tasks demonstrate that UniGist
significantly improves compression quality, with especially strong performance
in detail-recalling tasks and long-range dependency modeling.
\\ ( https://arxiv.org/abs/2509.15763 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15789
Date: Fri, 19 Sep 2025 09:21:13 GMT   (681kb)

Title: UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus
  from the United Nations
Authors: Qiuyang Lu, Fangjian Shen, Zhengkai Tang, Qiang Liu, Hexuan Cheng, Hui
  Liu, Wushao Wen
Categories: cs.CL cs.LG
Comments: 5 pages, 1 figure, submitted to ICASSP2026
\\
  The quality and accessibility of multilingual datasets are crucial for
advancing machine translation. However, previous corpora built from United
Nations documents have suffered from issues such as opaque process, difficulty
of reproduction, and limited scale. To address these challenges, we introduce a
complete end-to-end solution, from data acquisition via web scraping to text
alignment. The entire process is fully reproducible, with a minimalist
single-machine example and optional distributed computing steps for
scalability. At its core, we propose a new Graph-Aided Paragraph Alignment
(GAPA) algorithm for efficient and flexible paragraph-level alignment. The
resulting corpus contains over 713 million English tokens, more than doubling
the scale of prior work. To the best of our knowledge, this represents the
largest publicly available parallel corpus composed entirely of
human-translated, non-AI-generated content. Our code and corpus are accessible
under the MIT License.
\\ ( https://arxiv.org/abs/2509.15789 ,  681kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15793
Date: Fri, 19 Sep 2025 09:23:41 GMT   (33kb)

Title: RAVE: Retrieval and Scoring Aware Verifiable Claim Detection
Authors: Yufeng Li, Arkaitz Zubiaga
Categories: cs.CL
Comments: 5 pages, 1 figure
\\
  The rapid spread of misinformation on social media underscores the need for
scalable fact-checking tools. A key step is claim detection, which identifies
statements that can be objectively verified. Prior approaches often rely on
linguistic cues or claim check-worthiness, but these struggle with vague
political discourse and diverse formats such as tweets. We present RAVE
(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that
combines evidence retrieval with structured signals of relevance and source
credibility. Experiments on CT22-test and PoliClaim-test show that RAVE
consistently outperforms text-only and retrieval-based baselines in both
accuracy and F1.
\\ ( https://arxiv.org/abs/2509.15793 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15811
Date: Fri, 19 Sep 2025 09:38:54 GMT   (1320kb)

Title: Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning
Authors: Sara Rajaee, Rochelle Choenni, Ekaterina Shutova, Christof Monz
Categories: cs.CL cs.AI
\\
  While the reasoning abilities of large language models (LLMs) continue to
advance, it remains unclear how such ability varies across languages in
multilingual LLMs and whether different languages produce reasoning paths that
complement each other. To investigate this question, we train a reward model to
rank generated responses for a given question across languages. Our results
show that our cross-lingual reward model substantially improves mathematical
reasoning performance compared to using reward modeling within a single
language, benefiting even high-resource languages. While English often exhibits
the highest performance in multilingual models, we find that cross-lingual
sampling particularly benefits English under low sampling budgets. Our findings
reveal new opportunities to improve multilingual reasoning by leveraging the
complementary strengths of diverse languages.
\\ ( https://arxiv.org/abs/2509.15811 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15837
Date: Fri, 19 Sep 2025 10:16:58 GMT   (589kb)

Title: The Curious Case of Visual Grounding: Different Effects for Speech- and
  Text-based Language Encoders
Authors: Adrian Sauter, Willem Zuidema, Marianne de Heer Kloots
Categories: cs.CL
Comments: 5 pages, 3 figures, Submitted to ICASSP 2026
ACM-class: I.2.7
\\
  How does visual information included in training affect language processing
in audio- and text-based deep learning models? We explore how such visual
grounding affects model-internal representations of words, and find
substantially different effects in speech- vs. text-based language encoders.
Firstly, global representational comparisons reveal that visual grounding
increases alignment between representations of spoken and written language, but
this effect seems mainly driven by enhanced encoding of word identity rather
than meaning. We then apply targeted clustering analyses to probe for phonetic
vs. semantic discriminability in model representations. Speech-based
representations remain phonetically dominated with visual grounding, but in
contrast to text-based representations, visual grounding does not improve
semantic discriminability. Our findings could usefully inform the development
of more efficient methods to enrich speech-based models with visually-informed
semantics.
\\ ( https://arxiv.org/abs/2509.15837 ,  589kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15839
Date: Fri, 19 Sep 2025 10:18:48 GMT   (557kb)

Title: Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning
  on Chinese Multi-Subject Physics Problems
Authors: Zhongze Luo, Zhenshuai Yin, Yongxin Guo, Zhichao Wang, Jionghao Zhu,
  Xiaoying Tang
Categories: cs.CL
\\
  While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,
their application in specialized scientific domains like physics reveals
significant gaps in current evaluation benchmarks. Specifically, existing
benchmarks often lack fine-grained subject coverage, neglect the step-by-step
reasoning process, and are predominantly English-centric, failing to
systematically evaluate the role of visual information. Therefore, we introduce
\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive
benchmark that includes 5 difficulty levels, featuring 1,412 image-associated,
multiple-choice questions spanning 11 high-school physics subjects. We employ a
dual evaluation framework to evaluate 20 different MLLMs, analyzing both final
answer accuracy and the step-by-step integrity of their chain-of-thought.
Furthermore, we systematically study the impact of difficulty level and visual
information by comparing the model performance before and after changing the
input mode. Our work provides not only a fine-grained resource for the
community but also offers a robust methodology for dissecting the multimodal
reasoning process of state-of-the-art MLLMs, and our dataset and code have been
open-sourced: https://github.com/luozhongze/Multi-Physics.
\\ ( https://arxiv.org/abs/2509.15839 ,  557kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15888
Date: Fri, 19 Sep 2025 11:35:56 GMT   (841kb)

Title: Distribution-Aligned Decoding for Efficient LLM Task Adaptation
Authors: Senkang Hu, Xudong Han, Jinqi Jiang, Yihang Tao, Zihan Fang, Sam Tak
  Wu Kwong, Yuguang Fang
Categories: cs.CL cs.AI
Comments: Accepted by NeurIPS'25
\\
  Adapting billion-parameter language models to a downstream task is still
costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task
adaptation as output-distribution alignment: the objective is to steer the
output distribution toward the task distribution directly during decoding
rather than indirectly through weight updates. Building on this view, we
introduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and
theoretically grounded method. We start with a short warm-start fine-tune and
extract a task-aware steering vector from the Kullback-Leibler (KL) divergence
gradient between the output distribution of the warm-started and pre-trained
models. This steering vector is then used to guide the decoding process to
steer the model's output distribution towards the task distribution. We
theoretically prove that SVD is first-order equivalent to the gradient step of
full fine-tuning and derive a globally optimal solution for the strength of the
steering vector. Across three tasks and nine benchmarks, SVD paired with four
standard PEFT methods improves multiple-choice accuracy by up to 5 points and
open-ended truthfulness by 2 points, with similar gains (1-2 points) on
commonsense datasets without adding trainable parameters beyond the PEFT
adapter. SVD thus offers a lightweight, theoretically grounded path to stronger
task adaptation for large language models.
\\ ( https://arxiv.org/abs/2509.15888 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15896
Date: Fri, 19 Sep 2025 11:51:17 GMT   (395kb)

Title: The Psychology of Falsehood: A Human-Centric Survey of Misinformation
  Detection
Authors: Arghodeep Nandi, Megha Sundriyal, Euna Mehnaz Khan, Jikai Sun, Emily
  Vraga, Jaideep Srivastava, Tanmoy Chakraborty
Categories: cs.CL cs.CY
Comments: Accepted in EMNLP'25 Main
\\
  Misinformation remains one of the most significant issues in the digital age.
While automated fact-checking has emerged as a viable solution, most current
systems are limited to evaluating factual accuracy. However, the detrimental
effect of misinformation transcends simple falsehoods; it takes advantage of
how individuals perceive, interpret, and emotionally react to information. This
underscores the need to move beyond factuality and adopt more human-centered
detection frameworks. In this survey, we explore the evolving interplay between
traditional fact-checking approaches and psychological concepts such as
cognitive biases, social dynamics, and emotional responses. By analyzing
state-of-the-art misinformation detection systems through the lens of human
psychology and behavior, we reveal critical limitations of current methods and
identify opportunities for improvement. Additionally, we outline future
research directions aimed at creating more robust and adaptive frameworks, such
as neuro-behavioural models that integrate technological factors with the
complexities of human cognition and social influence. These approaches offer
promising pathways to more effectively detect and mitigate the societal harms
of misinformation.
\\ ( https://arxiv.org/abs/2509.15896 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15901
Date: Fri, 19 Sep 2025 11:58:17 GMT   (452kb)

Title: Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and
  Personalization via Questions
Authors: Frederic Kirstein, Sonu Kumar, Terry Ruas, Bela Gipp
Categories: cs.CL cs.AI
Comments: Accepted at EMNLP 2025
\\
  Meeting summarization with large language models (LLMs) remains error-prone,
often producing outputs with hallucinations, omissions, and irrelevancies. We
present FRAME, a modular pipeline that reframes summarization as a semantic
enrichment task. FRAME extracts and scores salient facts, organizes them
thematically, and uses these to enrich an outline into an abstractive summary.
To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that
has the model build a reasoning trace by answering nine questions before
content selection. For evaluation, we propose P-MESA, a multi-dimensional,
reference-free evaluation framework to assess if a summary fits a target
reader. P-MESA reliably identifies error instances, achieving >= 89% balanced
accuracy against human annotations and strongly aligns with human severity
ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and
omission by 2 out of 5 points (measured with MESA), while SCOPE improves
knowledge fit and goal alignment over prompt-only baselines. Our findings
advocate for rethinking summarization to improve control, faithfulness, and
personalization.
\\ ( https://arxiv.org/abs/2509.15901 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15926
Date: Fri, 19 Sep 2025 12:28:50 GMT   (21kb)

Title: Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay
  Assessment
Authors: Ahmed Karim, Qiao Wang (Judy), Zheng Yuan
Categories: cs.CL cs.LG
Comments: Accepted at EMNLP 2025 (Main Conference). Camera-ready version
\\
  Automated Essay Scoring (AES) systems now reach near human agreement on some
public benchmarks, yet real-world adoption, especially in high-stakes
examinations, remains limited. A principal obstacle is that most models output
a single score without any accompanying measure of confidence or explanation.
We address this gap with conformal prediction, a distribution-free wrapper that
equips any classifier with set-valued outputs and formal coverage guarantees.
Two open-source large language models (Llama-3 8B and Qwen-2.5 3B) are
fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and
calibrated at a 90 percent risk level. Reliability is assessed with UAcc, an
uncertainty-aware accuracy that rewards models for being both correct and
concise. To our knowledge, this is the first work to combine conformal
prediction and UAcc for essay scoring. The calibrated models consistently meet
the coverage target while keeping prediction sets compact, indicating that
open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we
discuss scaling and broader user studies as future work.
\\ ( https://arxiv.org/abs/2509.15926 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15958
Date: Fri, 19 Sep 2025 13:18:30 GMT   (472kb)

Title: Localmax dynamics for attention in transformers and its asymptotic
  behavior
Authors: Henri Cimeti\`ere, Maria Teresa Chiri, Bahman Gharesifard
Categories: cs.CL cs.LG math.DS math.OC
Comments: 28 pages, 5 figures
MSC-class: 68T07, 68T50, 37N35, 37B25
\\
  We introduce a new discrete-time attention model, termed the localmax
dynamics, which interpolates between the classic softmax dynamics and the
hardmax dynamics, where only the tokens that maximize the influence toward a
given token have a positive weight. As in hardmax, uniform weights are
determined by a parameter controlling neighbor influence, but the key extension
lies in relaxing neighborhood interactions through an alignment-sensitivity
parameter, which allows controlled deviations from pure hardmax behavior. As we
prove, while the convex hull of the token states still converges to a convex
polytope, its structure can no longer be fully described by a maximal alignment
set, prompting the introduction of quiescent sets to capture the invariant
behavior of tokens near vertices. We show that these sets play a key role in
understanding the asymptotic behavior of the system, even under time-varying
alignment sensitivity parameters. We further show that localmax dynamics does
not exhibit finite-time convergence and provide results for vanishing, nonzero,
time-varying alignment-sensitivity parameters, recovering the limiting behavior
of hardmax as a by-product. Finally, we adapt Lyapunov-based methods from
classical opinion dynamics, highlighting their limitations in the asymmetric
setting of localmax interactions and outlining directions for future research.
\\ ( https://arxiv.org/abs/2509.15958 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15974
Date: Fri, 19 Sep 2025 13:35:07 GMT   (390kb)

Title: BEFT: Bias-Efficient Fine-Tuning of Language Models
Authors: Baichuan Huang, Ananth Balashankar, Amir Aminifar
Categories: cs.CL cs.AI cs.LG
\\
  Fine-tuning all-bias-terms stands out among various parameter-efficient
fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and
competitive performance, especially in low-data regimes. Bias-only fine-tuning
has the potential for unprecedented parameter efficiency. However, the link
between fine-tuning different bias terms (i.e., bias terms in the query, key,
or value projections) and downstream performance remains unclear. The existing
approaches, e.g., based on the magnitude of bias change or empirical Fisher
information, provide limited guidance for selecting the particular bias term
for effective fine-tuning. In this paper, we propose an approach for selecting
the bias term to be fine-tuned, forming the foundation of our bias-efficient
fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against
other bias-selection approaches, across a wide range of large language models
(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B
parameters. Our results demonstrate the effectiveness and superiority of our
bias-efficient approach on diverse downstream tasks, including classification,
multiple-choice, and generation tasks.
\\ ( https://arxiv.org/abs/2509.15974 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16025
Date: Fri, 19 Sep 2025 14:33:05 GMT   (450kb)

Title: Session-Level Spoken Language Assessment with a Multimodal Foundation
  Model via Multi-Target Learning
Authors: Hong-Yun Lin, Jhen-Ke Lin, Chung-Chun Wang, Hao-Chien Lu, Berlin Chen
Categories: cs.CL cs.AI
Comments: Copyright 2025 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\
  Spoken Language Assessment (SLA) estimates a learner's oral proficiency from
spontaneous speech. The growing population of L2 English speakers has
intensified the demand for reliable SLA, a critical component of Computer
Assisted Language Learning (CALL). Existing efforts often rely on cascaded
pipelines, which are prone to error propagation, or end-to-end models that
often operate on a short audio window, which might miss discourse-level
evidence. This paper introduces a novel multimodal foundation model approach
that performs session-level evaluation in a single pass. Our approach couples
multi-target learning with a frozen, Whisper ASR model-based speech prior for
acoustic-aware calibration, allowing for jointly learning holistic and
trait-level objectives of SLA without resorting to handcrafted features. By
coherently processing the entire response session of an L2 speaker, the model
excels at predicting holistic oral proficiency. Experiments conducted on the
Speak & Improve benchmark demonstrate that our proposed approach outperforms
the previous state-of-the-art cascaded system and exhibits robust cross-part
generalization, producing a compact deployable grader that is tailored for CALL
applications.
\\ ( https://arxiv.org/abs/2509.16025 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16028
Date: Fri, 19 Sep 2025 14:34:22 GMT   (1044kb)

Title: Think, Verbalize, then Speak: Bridging Complex Thoughts and
  Comprehensible Speech
Authors: Sang Hoon Woo, Sehun Lee, Kang-wook Kim, Gunhee Kim
Categories: cs.CL cs.AI
Comments: EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT
\\
  Spoken dialogue systems increasingly employ large language models (LLMs) to
leverage their advanced reasoning capabilities. However, direct application of
LLMs in spoken communication often yield suboptimal results due to mismatches
between optimal textual and verbal delivery. While existing approaches adapt
LLMs to produce speech-friendly outputs, their impact on reasoning performance
remains underexplored. In this work, we propose Think-Verbalize-Speak, a
framework that decouples reasoning from spoken delivery to preserve the full
reasoning capacity of LLMs. Central to our method is verbalizing, an
intermediate step that translates thoughts into natural, speech-ready text. We
also introduce ReVerT, a latency-efficient verbalizer based on incremental and
asynchronous summarization. Experiments across multiple benchmarks show that
our method enhances speech naturalness and conciseness with minimal impact on
reasoning. The project page with the dataset and the source code is available
at https://yhytoto12.github.io/TVS-ReVerT
\\ ( https://arxiv.org/abs/2509.16028 ,  1044kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16093
Date: Fri, 19 Sep 2025 15:36:02 GMT   (412kb)

Title: Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM
  Responses
Authors: Fangyi Yu, Nabeel Seedat, Dasha Herrmannova, Frank Schilder, Jonathan
  Richard Schwarz
Categories: cs.CL cs.AI
\\
  Evaluating long-form answers in high-stakes domains such as law or medicine
remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to
capture semantic correctness, and current LLM-based evaluators often reduce
nuanced aspects of answer quality into a single undifferentiated score. We
introduce DeCE, a decomposed LLM evaluation framework that separates precision
(factual accuracy and relevance) and recall (coverage of required concepts),
using instance-specific criteria automatically extracted from gold answer
requirements. DeCE is model-agnostic and domain-general, requiring no
predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate
different LLMs on a real-world legal QA task involving multi-jurisdictional
reasoning and citation grounding. DeCE achieves substantially stronger
correlation with expert judgments ($r=0.78$), compared to traditional metrics
($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional
evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist
models favor recall, while specialized models favor precision. Importantly,
only 11.95% of LLM-generated criteria required expert revision, underscoring
DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation
framework in expert domains.
\\ ( https://arxiv.org/abs/2509.16093 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16105
Date: Fri, 19 Sep 2025 15:47:42 GMT   (589kb)

Title: DiEP: Adaptive Mixture-of-Experts Compression through Differentiable
  Expert Pruning
Authors: Sikai Bai, Haoxi Li, Jie Zhang, Zicong Hong, Song Guo
Categories: cs.CL
Comments: 18 pages
\\
  Despite the significant breakthrough of Mixture-of-Experts (MoE), the
increasing scale of these MoE models presents huge memory and storage
challenges. Existing MoE pruning methods, which involve reducing parameter size
with a uniform sparsity across all layers, often lead to suboptimal outcomes
and performance degradation due to varying expert redundancy in different MoE
layers. To address this, we propose a non-uniform pruning strategy, dubbed
\textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP}), which
adaptively adjusts pruning rates at the layer level while jointly learning
inter-layer importance, effectively capturing the varying redundancy across
different MoE layers. By transforming the global discrete search space into a
continuous one, our method handles exponentially growing non-uniform expert
combinations, enabling adaptive gradient-based pruning. Extensive experiments
on five advanced MoE models demonstrate the efficacy of our method across
various NLP tasks. Notably, \textbf{DiEP} retains around 92\% of original
performance on Mixtral 8$\times$7B with only half the experts, outperforming
other pruning methods by up to 7.1\% on the challenging MMLU dataset.
\\ ( https://arxiv.org/abs/2509.16105 ,  589kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16107
Date: Fri, 19 Sep 2025 15:49:26 GMT   (646kb)

Title: It Depends: Resolving Referential Ambiguity in Minimal Contexts with
  Commonsense Knowledge
Authors: Lukas Ellinger and Georg Groh
Categories: cs.CL
Comments: Accepted by UncertaiNLP workshop @ EMNLP 2025
\\
  Ambiguous words or underspecified references require interlocutors to resolve
them, often by relying on shared context and commonsense knowledge. Therefore,
we systematically investigate whether Large Language Models (LLMs) can leverage
commonsense to resolve referential ambiguity in multi-turn conversations and
analyze their behavior when ambiguity persists. Further, we study how requests
for simplified language affect this capacity. Using a novel multilingual
evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and
Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that
current LLMs struggle to resolve ambiguity effectively: they tend to commit to
a single interpretation or cover all possible references, rather than hedging
or seeking clarification. This limitation becomes more pronounced under
simplification prompts, which drastically reduce the use of commonsense
reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct
Preference Optimization substantially improves ambiguity resolution across all
request types. These results underscore the need for advanced fine-tuning to
improve LLMs' handling of ambiguity and to ensure robust performance across
diverse communication styles.
\\ ( https://arxiv.org/abs/2509.16107 ,  646kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16112
Date: Fri, 19 Sep 2025 15:57:40 GMT   (254kb)

Title: CodeRAG: Finding Relevant and Necessary Knowledge for
  Retrieval-Augmented Repository-Level Code Completion
Authors: Sheng Zhang, Yifan Ding, Shuquan Lian, Shun Song, Hui Li
Categories: cs.CL cs.IR cs.SE
Comments: EMNLP 2025
\\
  Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.
\\ ( https://arxiv.org/abs/2509.16112 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16188
Date: Fri, 19 Sep 2025 17:47:48 GMT   (7704kb)

Title: CultureScope: A Dimensional Lens for Probing Cultural Understanding in
  LLMs
Authors: Jinghao Zhang, Sihang Jiang, Shiwei Guo, Shisong Chen, Yanghua Xiao,
  Hongwei Feng, Jiaqing Liang, Minggui HE, Shimin Tao, Hongxia Ma
Categories: cs.CL cs.AI
\\
  As large language models (LLMs) are increasingly deployed in diverse cultural
environments, evaluating their cultural understanding capability has become
essential for ensuring trustworthy and culturally aligned applications.
However, most existing benchmarks lack comprehensiveness and are challenging to
scale and adapt across different cultural contexts, because their frameworks
often lack guidance from well-established cultural theories and tend to rely on
expert-driven manual annotations. To address these issues, we propose
CultureScope, the most comprehensive evaluation framework to date for assessing
cultural understanding in LLMs. Inspired by the cultural iceberg theory, we
design a novel dimensional schema for cultural knowledge classification,
comprising 3 layers and 140 dimensions, which guides the automated construction
of culture-specific knowledge bases and corresponding evaluation datasets for
any given languages and cultures. Experimental results demonstrate that our
method can effectively evaluate cultural understanding. They also reveal that
existing large language models lack comprehensive cultural competence, and
merely incorporating multilingual data does not necessarily enhance cultural
understanding. All code and data files are available at
https://github.com/HoganZinger/Culture
\\ ( https://arxiv.org/abs/2509.16188 ,  7704kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16198
Date: Fri, 19 Sep 2025 17:58:14 GMT   (5265kb)

Title: RPG: A Repository Planning Graph for Unified and Scalable Codebase
  Generation
Authors: Jane Luo, Xin Zhang, Steven Liu, Jie Wu, Yiming Huang, Yangyu Huang,
  Chengyu Yin, Ying Xin, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qi Chen, Scarlett
  Li, Mao Yang
Categories: cs.CL cs.AI cs.SE
\\
  Large language models excel at function- and file-level code generation, yet
generating complete repositories from scratch remains a fundamental challenge.
This process demands coherent and reliable planning across proposal- and
implementation-level stages, while natural language, due to its ambiguity and
verbosity, is ill-suited for faithfully representing complex software
structures. To address this, we introduce the Repository Planning Graph (RPG),
a persistent representation that unifies proposal- and implementation-level
planning by encoding capabilities, file structures, data flows, and functions
in one graph. RPG replaces ambiguous natural language with an explicit
blueprint, enabling long-horizon planning and scalable repository generation.
Building on RPG, we develop ZeroRepo, a graph-driven framework for repository
generation from scratch. It operates in three stages: proposal-level planning
and implementation-level refinement to construct the graph, followed by
graph-guided code generation with test validation. To evaluate this setting, we
construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.
On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly
3.9$\times$ the strongest baseline (Claude Code) and about 64$\times$ other
baselines. It attains 81.5% functional coverage and a 69.7% pass rate,
exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further
analysis shows that RPG models complex dependencies, enables progressively more
sophisticated planning through near-linear scaling, and enhances LLM
understanding of repositories, thereby accelerating agent localization.
\\ ( https://arxiv.org/abs/2509.16198 ,  5265kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15234
Date: Wed, 17 Sep 2025 09:44:59 GMT   (265kb)

Title: Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in
  Chest X-rays
Authors: Hanbin Ko, Gihun Cho, Inhyeok Baek, Donguk Kim, Joonbeom Koo, Changi
  Kim, Dongheon Lee, Chang Min Park
Categories: cs.CV
Comments: 24 pages, 2 figures, under review
MSC-class: 68T07, 68U10, 92C55
ACM-class: I.2.10; I.2.7
\\
  Vision-language pretraining has advanced image-text alignment, yet progress
in radiology remains constrained by the heterogeneity of clinical reports,
including abbreviations, impression-only notes, and stylistic variability.
Unlike general-domain settings where more data often leads to better
performance, naively scaling to large collections of noisy reports can plateau
or even degrade model learning. We ask whether large language model (LLM)
encoders can provide robust clinical representations that transfer across
diverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR,
a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a
dual-tower framework that couples this encoder with a vision backbone.
LLM2VEC4CXR improves clinical text understanding over BERT-based baselines,
handles abbreviations and style variation, and achieves strong clinical
alignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to
boost retrieval accuracy and clinically oriented scores, with stronger
cross-dataset generalization than prior medical CLIP variants. Trained on 1.6M
CXR studies from public and private sources with heterogeneous and noisy
reports, our models demonstrate that robustness -- not scale alone -- is the
key to effective multimodal learning. We release models to support further
research in medical image-text representation learning.
\\ ( https://arxiv.org/abs/2509.15234 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15235
Date: Wed, 17 Sep 2025 11:28:58 GMT   (2498kb)

Title: ViSpec: Accelerating Vision-Language Models with Vision-Aware
  Speculative Decoding
Authors: Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen
Categories: cs.CV cs.CL
Comments: 12 pages, 4 figures
\\
  Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), yet its application to vision-language models
(VLMs) remains underexplored, with existing methods achieving only modest
speedups (<1.5x). This gap is increasingly significant as multimodal
capabilities become central to large-scale models. We hypothesize that large
VLMs can effectively filter redundant image information layer by layer without
compromising textual comprehension, whereas smaller draft models struggle to do
so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a
novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor
module to compress image tokens into a compact representation, which is
seamlessly integrated into the draft model's attention mechanism while
preserving original image positional information. Additionally, we extract a
global feature vector for each input image and augment all subsequent text
tokens with this feature to enhance multimodal coherence. To overcome the
scarcity of multimodal datasets with long assistant responses, we curate a
specialized training dataset by repurposing existing datasets and generating
extended outputs using the target VLM with modified prompts. Our training
strategy mitigates the risk of the draft model exploiting direct access to the
target model's hidden states, which could otherwise lead to shortcut learning
when training solely on target model outputs. Extensive experiments validate
ViSpec, achieving, to our knowledge, the first substantial speedup in VLM
speculative decoding.
\\ ( https://arxiv.org/abs/2509.15235 ,  2498kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15241
Date: Wed, 17 Sep 2025 16:28:53 GMT   (500kb)

Title: M-PACE: Mother Child Framework for Multimodal Compliance
Authors: Shreyash Verma, Amit Kesari, Vinayak Trivedi, Anupam Purwar, Ratnesh
  Jamidar
Categories: cs.CV cs.CL
Comments: The M-PACE framework uses a "mother-child" AI model system to
  automate and unify compliance checks for ads, reducing costs while
  maintaining high accuracy
\\
  Ensuring that multi-modal content adheres to brand, legal, or
platform-specific compliance standards is an increasingly complex challenge
across domains. Traditional compliance frameworks typically rely on disjointed,
multi-stage pipelines that integrate separate modules for image classification,
text extraction, audio transcription, hand-crafted checks, and rule-based
merges. This architectural fragmentation increases operational overhead,
hampers scalability, and hinders the ability to adapt to dynamic guidelines
efficiently. With the emergence of Multimodal Large Language Models (MLLMs),
there is growing potential to unify these workflows under a single,
general-purpose framework capable of jointly processing visual and textual
content. In light of this, we propose Multimodal Parameter Agnostic Compliance
Engine (M-PACE), a framework designed for assessing attributes across
vision-language inputs in a single pass. As a representative use case, we apply
M-PACE to advertisement compliance, demonstrating its ability to evaluate over
15 compliance-related attributes. To support structured evaluation, we
introduce a human-annotated benchmark enriched with augmented samples that
simulate challenging real-world conditions, including visual obstructions and
profanity injection. M-PACE employs a mother-child MLLM setup, demonstrating
that a stronger parent MLLM evaluating the outputs of smaller child models can
significantly reduce dependence on human reviewers, thereby automating quality
control. Our analysis reveals that inference costs reduce by over 31 times,
with the most efficient models (Gemini 2.0 Flash as child MLLM selected by
mother MLLM) operating at 0.0005 per image, compared to 0.0159 for Gemini 2.5
Pro with comparable accuracy, highlighting the trade-off between cost and
output quality achieved in real time by M-PACE in real life deployment over
advertising data.
\\ ( https://arxiv.org/abs/2509.15241 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15242
Date: Wed, 17 Sep 2025 16:40:08 GMT   (2917kb)

Title: ProFusion: 3D Reconstruction of Protein Complex Structures from
  Multi-view AFM Images
Authors: Jaydeep Rade, Md Hasibul Hasan Hasib, Meric Ozturk, Baboucarr Faal,
  Sheng Yang, Dipali G. Sashital, Vincenzo Venditti, Baoyu Chen, Soumik Sarkar,
  Adarsh Krishnamurthy, Anwesha Sarkar
Categories: cs.CV
\\
  AI-based in silico methods have improved protein structure prediction but
often struggle with large protein complexes (PCs) involving multiple
interacting proteins due to missing 3D spatial cues. Experimental techniques
like Cryo-EM are accurate but costly and time-consuming. We present ProFusion,
a hybrid framework that integrates a deep learning model with Atomic Force
Microscopy (AFM), which provides high-resolution height maps from random
orientations, naturally yielding multi-view data for 3D reconstruction.
However, generating a large-scale AFM imaging data set sufficient to train deep
learning models is impractical. Therefore, we developed a virtual AFM framework
that simulates the imaging process and generated a dataset of ~542,000 proteins
with multi-view synthetic AFM images. We train a conditional diffusion model to
synthesize novel views from unposed inputs and an instance-specific Neural
Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D
protein structures achieve an average Chamfer Distance within the AFM imaging
resolution, reflecting high structural fidelity. Our method is extensively
validated on experimental AFM images of various PCs, demonstrating strong
potential for accurate, cost-effective protein complex structure prediction and
rapid iterative validation using AFM experiments.
\\ ( https://arxiv.org/abs/2509.15242 ,  2917kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15243
Date: Wed, 17 Sep 2025 18:18:59 GMT   (5914kb)

Title: Multi-Modal Interpretability for Enhanced Localization in
  Vision-Language Models
Authors: Muhammad Imran and Yugyung Lee
Categories: cs.CV
Comments: 8 pages, 6 figures, 3 tables
Journal-ref: Non-Archival track - The First Workshop on Multimodal Knowledge
  and Language Modeling IJCAI 2025 Workshop, August 16, 2025 IJCAI 2025
  Workshop, August 16, 2025 Room 516B, Palais des congr\`es, Montreal, Canada
\\
  Recent advances in vision-language models have significantly expanded the
frontiers of automated image analysis. However, applying these models in
safety-critical contexts remains challenging due to the complex relationships
between objects, subtle visual cues, and the heightened demand for transparency
and reliability. This paper presents the Multi-Modal Explainable Learning
(MMEL) framework, designed to enhance the interpretability of vision-language
models while maintaining high performance. Building upon prior work in
gradient-based explanations for transformer architectures (Grad-eclip), MMEL
introduces a novel Hierarchical Semantic Relationship Module that enhances
model interpretability through multi-scale feature processing, adaptive
attention weighting, and cross-modal alignment. Our approach processes features
at multiple semantic levels to capture relationships between image regions at
different granularities, applying learnable layer-specific weights to balance
contributions across the model's depth. This results in more comprehensive
visual explanations that highlight both primary objects and their contextual
relationships with improved precision. Through extensive experiments on
standard datasets, we demonstrate that by incorporating semantic relationship
information into gradient-based attribution maps, MMEL produces more focused
and contextually aware visualizations that better reflect how vision-language
models process complex scenes. The MMEL framework generalizes across various
domains, offering valuable insights into model decisions for applications
requiring high interpretability and reliability.
\\ ( https://arxiv.org/abs/2509.15243 ,  5914kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15250
Date: Thu, 18 Sep 2025 01:05:37 GMT   (2412kb)

Title: Walk and Read Less: Improving the Efficiency of Vision-and-Language
  Navigation via Tuning-Free Multimodal Token Pruning
Authors: Wenda Qin, Andrea Burns, Bryan A. Plummer, Margrit Betke
Categories: cs.CV cs.AI
Comments: Accepted to ACL 2024 Findings. Data and code to be released at
  https://github.com/wdqin/VLN-NAP
\\
  Large models achieve strong performance on Vision-and-Language Navigation
(VLN) tasks, but are costly to run in resource-limited environments. Token
pruning offers appealing tradeoffs for efficiency with minimal performance loss
by reducing model input size, but prior work overlooks VLN-specific challenges.
For example, information loss from pruning can effectively increase
computational cost due to longer walks. Thus, the inability to identify
uninformative tokens undermines the supposed efficiency gains from pruning. To
address this, we propose Navigation-Aware Pruning (NAP), which uses
navigation-specific traits to simplify the pruning process by pre-filtering
tokens into foreground and background. For example, image views are filtered
based on whether the agent can navigate in that direction. We also extract
navigation-relevant instructions using a Large Language Model. After filtering,
we focus pruning on background tokens, minimizing information loss. To further
help avoid increases in navigation length, we discourage backtracking by
removing low-importance navigation nodes. Experiments on standard VLN
benchmarks show NAP significantly outperforms prior work, preserving higher
success rates while saving more than 50% FLOPS.
\\ ( https://arxiv.org/abs/2509.15250 ,  2412kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15257
Date: Thu, 18 Sep 2025 07:48:46 GMT   (11754kb)

Title: RespoDiff: Dual-Module Bottleneck Transformation for Responsible &
  Faithful T2I Generation
Authors: Silpa Vadakkeeveetil Sreelatha, Sauradip Nag, Muhammad Awais, Serge
  Belongie, Anjan Dutta
Categories: cs.CV cs.LG
\\
  The rapid advancement of diffusion models has enabled high-fidelity and
semantically rich text-to-image generation; however, ensuring fairness and
safety remains an open challenge. Existing methods typically improve fairness
and safety at the expense of semantic fidelity and image quality. In this work,
we propose RespoDiff, a novel framework for responsible text-to-image
generation that incorporates a dual-module transformation on the intermediate
bottleneck representations of diffusion models. Our approach introduces two
distinct learnable modules: one focused on capturing and enforcing responsible
concepts, such as fairness and safety, and the other dedicated to maintaining
semantic alignment with neutral prompts. To facilitate the dual learning
process, we introduce a novel score-matching objective that enables effective
coordination between the modules. Our method outperforms state-of-the-art
methods in responsible generation by ensuring semantic alignment while
optimizing both objectives without compromising image fidelity. Our approach
improves responsible and semantically coherent generation by 20% across
diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale
models like SDXL, enhancing fairness and safety. Code will be released upon
acceptance.
\\ ( https://arxiv.org/abs/2509.15257 ,  11754kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15267
Date: Thu, 18 Sep 2025 10:09:04 GMT   (14874kb)

Title: Autoguided Online Data Curation for Diffusion Model Training
Authors: Valeria Pais, Luis Oala, Daniele Faccio and Marco Aversa
Categories: cs.CV cs.AI cs.LG
Comments: Accepted non-archival paper at ICCV 2025 Workshop on Curated Data for
  Efficient Learning (CDEL)
\\
  The costs of generative model compute rekindled promises and hopes for
efficient data curation. In this work, we investigate whether recently
developed autoguidance and online data selection methods can improve the time
and sample efficiency of training generative diffusion models. We integrate
joint example selection (JEST) and autoguidance into a unified code base for
fast ablation and benchmarking. We evaluate combinations of data curation on a
controlled 2-D synthetic data generation task as well as (3x64x64)-D image
generation. Our comparisons are made at equal wall-clock time and equal number
of samples, explicitly accounting for the overhead of selection. Across
experiments, autoguidance consistently improves sample quality and diversity.
Early AJEST (applying selection only at the beginning of training) can match or
modestly exceed autoguidance alone in data efficiency on both tasks. However,
its time overhead and added complexity make autoguidance or uniform random data
selection preferable in most situations. These findings suggest that while
targeted online selection can yield efficiency gains in early training, robust
sample quality improvements are primarily driven by autoguidance. We discuss
limitations and scope, and outline when data selection may be beneficial.
\\ ( https://arxiv.org/abs/2509.15267 ,  14874kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15270
Date: Thu, 18 Sep 2025 10:57:26 GMT   (7653kb)

Title: PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for
  fingerprinting AI-generated images
Authors: Emanuele Ricco, Elia Onofri, Lorenzo Cima, Stefano Cresci, Roberto Di
  Pietro
Categories: cs.CV cs.AI
\\
  A critical need has emerged for generative AI: attribution methods. That is,
solutions that can identify the model originating AI-generated content. This
feature, generally relevant in multimodal applications, is especially sensitive
in commercial settings where users subscribe to paid proprietary services and
expect guarantees about the source of the content they receive. To address
these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image
Signature Mapping framework for fingerprinting AI-generated images. PRISM is
based on a radial reduction of the discrete Fourier transform that leverages
amplitude and phase information to capture model-specific signatures. The
output of the above process is subsequently clustered via linear discriminant
analysis to achieve reliable model attribution in diverse settings, even if the
model's internal details are inaccessible. To support our work, we construct
PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN-
and diffusion-based models. On this dataset, PRISM achieves an attribution
accuracy of 92.04%. We additionally evaluate our method on four benchmarks from
the literature, reaching an average accuracy of 81.60%. Finally, we evaluate
our methodology also in the binary task of detecting real vs fake images,
achieving an average accuracy of 88.41%. We obtain our best result on GenImage
with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our
results demonstrate the effectiveness of frequency-domain fingerprinting for
cross-architecture and cross-dataset model attribution, offering a viable
solution for enforcing accountability and trust in generative AI systems.
\\ ( https://arxiv.org/abs/2509.15270 ,  7653kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15271
Date: Thu, 18 Sep 2025 11:18:28 GMT   (1211kb)

Title: Large Vision Models Can Solve Mental Rotation Problems
Authors: Sebastian Ray Mason, Anders Gj{\o}lbye, Phillip Chavarria H{\o}jbjerg,
  Lenka T\v{e}tkov\'a, Lars Kai Hansen
Categories: cs.CV cs.AI
\\
  Mental rotation is a key test of spatial reasoning in humans and has been
central to understanding how perception supports cognition. Despite the success
of modern vision transformers, it is still unclear how well these models
develop similar abilities. In this work, we present a systematic evaluation of
ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from
simple block structures similar to those used by Shepard and Metzler to study
human cognition, to more complex block figures, three types of text, and
photo-realistic objects. By probing model representations layer by layer, we
examine where and how these networks succeed. We find that i) self-supervised
ViTs capture geometric structure better than supervised ViTs; ii) intermediate
layers perform better than final layers; iii) task difficulty increases with
rotation complexity and occlusion, mirroring human reaction times and
suggesting similar constraints in embedding space representations.
\\ ( https://arxiv.org/abs/2509.15271 ,  1211kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15272
Date: Thu, 18 Sep 2025 11:46:07 GMT   (8388kb)

Title: Which Direction to Choose? An Analysis on the Representation Power of
  Self-Supervised ViTs in Downstream Tasks
Authors: Yannis Kaltampanidis, Alexandros Doumanoglou, Dimitrios Zarpalas
Categories: cs.CV
Comments: 24 pages, XAI 2025
\\
  Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently
demonstrated considerable potential as a pre-training strategy for a variety of
computer vision tasks, including image classification and segmentation, both in
standard and few-shot downstream contexts. Two pre-training objectives dominate
the landscape of SSL techniques: Contrastive Learning and Masked Image
Modeling. Features (or tokens) extracted from the final transformer attention
block -- specifically, the keys, queries, and values -- as well as features
obtained after the final block's feed-forward layer, have become a common
foundation for addressing downstream tasks. However, in many existing
approaches, these pre-trained ViT features are further processed through
additional transformation layers, often involving lightweight heads or combined
with distillation, to achieve superior task performance. Although such methods
can improve task outcomes, to the best of our knowledge, a comprehensive
analysis of the intrinsic representation capabilities of unaltered ViT features
has yet to be conducted. This study aims to bridge this gap by systematically
evaluating the use of these unmodified features across image classification and
segmentation tasks, in both standard and few-shot contexts. The classification
and segmentation rules that we use are either hyperplane based (as in logistic
regression) or cosine-similarity based, both of which rely on the presence of
interpretable directions in the ViT's latent space. Based on the previous rules
and without the use of additional feature transformations, we conduct an
analysis across token types, tasks, and pre-trained ViT models. This study
provides insights into the optimal choice for token type and decision rule
based on the task, context, and the pre-training objective, while reporting
detailed findings on two widely-used datasets.
\\ ( https://arxiv.org/abs/2509.15272 ,  8388kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15293
Date: Thu, 18 Sep 2025 17:56:30 GMT   (853kb)

Title: How Good are Foundation Models in Step-by-Step Embodied Reasoning?
Authors: Dinura Dissanayake and Ahmed Heakl and Omkar Thawakar and Noor Ahsan
  and Ritesh Thawkar and Ketan More and Jean Lahoud and Rao Anwer and Hisham
  Cholakkal and Ivan Laptev and Fahad Shahbaz Khan and Salman Khan
Categories: cs.CV cs.RO
\\
  Embodied agents operating in the physical world must make decisions that are
not only effective but also safe, spatially coherent, and grounded in context.
While recent advances in large multimodal models (LMMs) have shown promising
capabilities in visual understanding and language generation, their ability to
perform structured reasoning for real-world embodied tasks remains
underexplored. In this work, we aim to understand how well foundation models
can perform step-by-step reasoning in embodied environments. To this end, we
propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to
evaluate the reasoning capabilities of LMMs in complex embodied decision-making
scenarios. Our benchmark spans a diverse set of tasks that require agents to
interpret multimodal observations, reason about physical constraints and
safety, and generate valid next actions in natural language. We present (i) a
large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation
framework that disentangles perceptual grounding from action reasoning, and
(iii) empirical analysis of several leading LMMs under this setting. Our
benchmark includes over 1.1k samples with detailed step-by-step reasoning
across 10 tasks and 8 embodiments, covering three different robot types. Our
results highlight both the potential and current limitations of LMMs in
embodied reasoning, pointing towards key challenges and opportunities for
future research in robot intelligence. Our data and code will be made publicly
available.
\\ ( https://arxiv.org/abs/2509.15293 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15330
Date: Thu, 18 Sep 2025 18:23:59 GMT   (615kb)

Title: CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution
  Generalization
Authors: Min Zhang, Bo Jiang, Jie Zhou, Yimeng Liu, Xin Lin
Categories: cs.CV
\\
  Recent advances in pre-training vision-language models (VLMs), e.g.,
contrastive language-image pre-training (CLIP) methods, have shown great
potential in learning out-of-distribution (OOD) representations. Despite
showing competitive performance, the prompt-based CLIP methods still suffer
from: i) inaccurate text descriptions, which leads to degraded accuracy and
robustness, and poses a challenge for zero-shot CLIP methods. ii) limited
vision-language embedding alignment, which significantly affects the
generalization performance. To tackle the above issues, this paper proposes a
novel Conditional Domain prompt Learning (CoDoL) method, which utilizes
readily-available domain information to form prompts and improves the
vision-language embedding alignment for improving OOD generalization. To
capture both instance-specific and domain-specific information, we further
propose a lightweight Domain Meta Network (DMN) to generate input-conditional
tokens for images in each domain. Extensive experiments on four OOD benchmarks
(PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed
CoDoL in terms of improving the vision-language embedding alignment as well as
the out-of-distribution generalization performance.
\\ ( https://arxiv.org/abs/2509.15330 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15333
Date: Thu, 18 Sep 2025 18:25:43 GMT   (41836kb)

Title: Emulating Human-like Adaptive Vision for Efficient and Flexible Machine
  Visual Perception
Authors: Yulin Wang, Yang Yue, Yang Yue, Huanqian Wang, Haojun Jiang, Yizeng
  Han, Zanlin Ni, Yifan Pu, Minglei Shi, Rui Lu, Qisen Yang, Andrew Zhao,
  Zhuofan Xia, Shiji Song, Gao Huang
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  Human vision is highly adaptive, efficiently sampling intricate environments
by sequentially fixating on task-relevant regions. In contrast, prevailing
machine vision models passively process entire scenes at once, resulting in
excessive resource demands scaling with spatial-temporal input resolution and
model size, yielding critical limitations impeding both future advancements and
real-world application. Here we introduce AdaptiveNN, a general framework
aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision
models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential
decision-making process, progressively identifying and attending to regions
pertinent to the task, incrementally combining information across fixations,
and actively concluding observation when sufficient. We establish a theory
integrating representation learning with self-rewarding reinforcement learning,
enabling end-to-end training of the non-differentiable AdaptiveNN without
additional supervision on fixation locations. We assess AdaptiveNN on 17
benchmarks spanning 9 tasks, including large-scale visual recognition,
fine-grained discrimination, visual search, processing images from real driving
and medical scenarios, language-driven embodied AI, and side-by-side
comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction
without sacrificing accuracy, flexibly adapts to varying task demands and
resource budgets without retraining, and provides enhanced interpretability via
its fixation patterns, demonstrating a promising avenue toward efficient,
flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits
closely human-like perceptual behaviors in many cases, revealing its potential
as a valuable tool for investigating visual cognition. Code is available at
https://github.com/LeapLabTHU/AdaptiveNN.
\\ ( https://arxiv.org/abs/2509.15333 ,  41836kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15342
Date: Thu, 18 Sep 2025 18:31:56 GMT   (1225kb)

Title: LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition
Authors: Jiuyi Xu, Qing Jin, Meida Chen, Andrew Feng, Yang Sui, Yangming Shi
Categories: cs.CV
\\
  Diffusion models have achieved remarkable success in image generation but
their practical application is often hindered by the slow sampling speed. Prior
efforts of improving efficiency primarily focus on compressing models or
reducing the total number of denoising steps, largely neglecting the
possibility to leverage multiple input resolutions in the generation process.
In this work, we propose LowDiff, a novel and efficient diffusion framework
based on a cascaded approach by generating increasingly higher resolution
outputs. Besides, LowDiff employs a unified model to progressively refine
images from low resolution to the desired resolution. With the proposed
architecture design and generation techniques, we achieve comparable or even
superior performance with much fewer high-resolution sampling steps. LowDiff is
applicable to diffusion models in both pixel space and latent space. Extensive
experiments on both conditional and unconditional generation tasks across
CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our
method. Results show over 50% throughput improvement across all datasets and
settings while maintaining comparable or better quality. On unconditional
CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional
CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an
FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1
produces high-quality samples with a FID of 4.00 and an IS of 195.06, together
with substantial efficiency gains.
\\ ( https://arxiv.org/abs/2509.15342 ,  1225kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15357
Date: Thu, 18 Sep 2025 18:57:47 GMT   (20717kb)

Title: MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation
Authors: Yu Chang, Jiahao Chen, Anzhe Cheng and Paul Bogdan
Categories: cs.CV cs.LG
Comments: Submitted to ICASSP 2026
\\
  Text-to-image diffusion models achieve impressive realism but often suffer
from compositional failures on prompts with multiple objects, attributes, and
spatial relations, resulting in cross-token interference where entities
entangle, attributes mix across objects, and spatial cues are violated. To
address these failures, we propose MaskAttn-SDXL,a region-level gating
mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s
UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each
cross-attention logit map before softmax to sparsify token-to-latent
interactions so that only semantically relevant connections remain active. The
method requires no positional encodings, auxiliary tokens, or external region
masks, and preserves the original inference path with negligible overhead. In
practice, our model improves spatial compliance and attribute binding in
multi-object prompts while preserving overall image quality and diversity.
These findings demonstrate that logit-level maksed cross-attention is an
data-efficient primitve for enforcing compositional control, and our method
thus serves as a practical extension for spatial control in text-to-image
generation.
\\ ( https://arxiv.org/abs/2509.15357 ,  20717kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15391
Date: Thu, 18 Sep 2025 19:55:37 GMT   (9228kb)

Title: RaceGAN: A Framework for Preserving Individuality while Converting
  Racial Information for Image-to-Image Translation
Authors: Mst Tasnim Pervin, George Bebis, Fang Jiang, Alireza Tavakkoli
Categories: cs.CV
Journal-ref: ICMLA 2024
\\
  Generative adversarial networks (GANs) have demonstrated significant progress
in unpaired image-to-image translation in recent years for several
applications. CycleGAN was the first to lead the way, although it was
restricted to a pair of domains. StarGAN overcame this constraint by tackling
image-to-image translation across various domains, although it was not able to
map in-depth low-level style changes for these domains. Style mapping via
reference-guided image synthesis has been made possible by the innovations of
StarGANv2 and StyleGAN. However, these models do not maintain individuality and
need an extra reference image in addition to the input. Our study aims to
translate racial traits by means of multi-domain image-to-image translation. We
present RaceGAN, a novel framework capable of mapping style codes over several
domains during racial attribute translation while maintaining individuality and
high level semantics without relying on a reference image. RaceGAN outperforms
other models in translating racial features (i.e., Asian, White, and Black)
when tested on Chicago Face Dataset. We also give quantitative findings
utilizing InceptionReNetv2-based classification to demonstrate the
effectiveness of our racial translation. Moreover, we investigate how well the
model partitions the latent space into distinct clusters of faces for each
ethnic group.
\\ ( https://arxiv.org/abs/2509.15391 ,  9228kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15393
Date: Thu, 18 Sep 2025 20:00:49 GMT   (3716kb)

Title: Generating Part-Based Global Explanations Via Correspondence
Authors: Kunal Rathore, Prasad Tadepalli
Categories: cs.CV cs.AI
\\
  Deep learning models are notoriously opaque. Existing explanation methods
often focus on localized visual explanations for individual images.
Concept-based explanations, while offering global insights, require extensive
annotations, incurring significant labeling cost. We propose an approach that
leverages user-defined part labels from a limited set of images and efficiently
transfers them to a larger dataset. This enables the generation of global
symbolic explanations by aggregating part-based local explanations, ultimately
providing human-understandable explanations for model decisions on a large
scale.
\\ ( https://arxiv.org/abs/2509.15393 ,  3716kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15406
Date: Thu, 18 Sep 2025 20:33:27 GMT   (475kb)

Title: Causal Fingerprints of AI Generative Models
Authors: Hui Xu, Chi Liu, Congcong Zhu, Minghao Wang, Youyang Qu, Longxiang Gao
Categories: cs.CV
Comments: 5 page. In submission
\\
  AI generative models leave implicit traces in their generated images, which
are commonly referred to as model fingerprints and are exploited for source
attribution. Prior methods rely on model-specific cues or synthesis artifacts,
yielding limited fingerprints that may generalize poorly across different
generative models. We argue that a complete model fingerprint should reflect
the causality between image provenance and model traces, a direction largely
unexplored. To this end, we conceptualize the \emph{causal fingerprint} of
generative models, and propose a causality-decoupling framework that
disentangles it from image-specific content and style in a semantic-invariant
latent space derived from pre-trained diffusion reconstruction residual. We
further enhance fingerprint granularity with diverse feature representations.
We validate causality by assessing attribution performance across
representative GANs and diffusion models and by achieving source anonymization
using counterfactual examples generated from causal fingerprints. Experiments
show our approach outperforms existing methods in model attribution, indicating
strong potential for forgery detection, model copyright tracing, and identity
protection.
\\ ( https://arxiv.org/abs/2509.15406 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15416
Date: Thu, 18 Sep 2025 20:43:08 GMT   (6435kb)

Title: NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally
  Robust Training
Authors: Moinak Bhattacharya, Angelica P. Kurtz, Fabio M. Iwamoto, Prateek
  Prasanna, Gagandeep Singh
Categories: cs.CV
\\
  Neuro-oncology poses unique challenges for machine learning due to
heterogeneous data and tumor complexity, limiting the ability of foundation
models (FMs) to generalize across cohorts. Existing FMs also perform poorly in
predicting uncommon molecular markers, which are essential for treatment
response and risk stratification. To address these gaps, we developed a
neuro-oncology specific FM with a distributionally robust loss function,
enabling accurate estimation of tumor phenotypes while maintaining
cross-institution generalization. We pretrained self-supervised backbones
(BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applied
distributionally robust optimization (DRO) to mitigate site and class
imbalance. Downstream tasks included molecular classification of common markers
(MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT),
continuous markers (Ki-67, TP53), and overall survival prediction in IDH1
wild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecular
prediction and reduced site-specific embedding differences. At CUIMC, mean
balanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, with
the largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to
0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69).
For survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647
to 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoral
regions, confirming interpretability. Overall, coupling FMs with DRO yields
more site-invariant representations, improves prediction of common and uncommon
markers, and enhances survival discrimination, underscoring the need for
prospective validation and integration of longitudinal and interventional
signals to advance precision neuro-oncology.
\\ ( https://arxiv.org/abs/2509.15416 ,  6435kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15435
Date: Thu, 18 Sep 2025 21:17:23 GMT   (2905kb)

Title: ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in
  Vision-Language Models
Authors: Chung-En Johnny Yu, Hsuan-Chih (Neil) Chen, Brian Jalaian, Nathaniel
  D. Bastian
Categories: cs.CV cs.AI cs.MA
\\
  Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities
but remain vulnerable to hallucinations from intrinsic errors and adversarial
attacks from external exploitations, limiting their reliability in real-world
applications. We present ORCA, an agentic reasoning framework that improves the
factual accuracy and adversarial robustness of pretrained LVLMs through
test-time structured inference reasoning with a suite of small vision models
(less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act
loop, querying multiple visual tools with evidential questions, validating
cross-model inconsistencies, and refining predictions iteratively without
access to model internals or retraining. ORCA also stores intermediate
reasoning traces, which supports auditable decision-making. Though designed
primarily to mitigate object-level hallucinations, ORCA also exhibits emergent
adversarial robustness without requiring adversarial training or defense
mechanisms. We evaluate ORCA across three settings: (1) clean images on
hallucination benchmarks, (2) adversarially perturbed images without defense,
and (3) adversarially perturbed images with defense applied. On the POPE
hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\%
to +40.67\% across different subsets. Under adversarial perturbations on POPE,
ORCA achieves an average accuracy gain of +20.11\% across LVLMs. When combined
with defense techniques on adversarially perturbed AMBER images, ORCA further
improves standalone LVLM performance, with gains ranging from +1.20\% to
+48.00\% across evaluation metrics. These results demonstrate that ORCA offers
a promising path toward building more reliable and robust multimodal systems.
\\ ( https://arxiv.org/abs/2509.15435 ,  2905kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15436
Date: Thu, 18 Sep 2025 21:18:36 GMT   (709kb)

Title: Region-Aware Deformable Convolutions
Authors: Abolfazl Saheban Maleki, Maryam Imani
Categories: cs.CV cs.AI
Comments: Work in progress; 9 pages, 2 figures
\\
  We introduce Region-Aware Deformable Convolution (RAD-Conv), a new
convolutional operator that enhances neural networks' ability to adapt to
complex image structures. Unlike traditional deformable convolutions, which are
limited to fixed quadrilateral sampling areas, RAD-Conv uses four boundary
offsets per kernel element to create flexible, rectangular regions that
dynamically adjust their size and shape to match image content. This approach
allows precise control over the receptive field's width and height, enabling
the capture of both local details and long-range dependencies, even with small
1x1 kernels. By decoupling the receptive field's shape from the kernel's
structure, RAD-Conv combines the adaptability of attention mechanisms with the
efficiency of standard convolutions. This innovative design offers a practical
solution for building more expressive and efficient vision models, bridging the
gap between rigid convolutional architectures and computationally costly
attention-based methods.
\\ ( https://arxiv.org/abs/2509.15436 ,  709kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15459
Date: Thu, 18 Sep 2025 22:10:37 GMT   (15449kb)

Title: CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan
  Reconstruction
Authors: Yiyi Liu, Chunyang Liu, Weiqin Jiao, Bojian Wu, Fashuai Li, Biao Xiong
Categories: cs.CV cs.AI
\\
  We present \textbf{CAGE} (\textit{Continuity-Aware edGE}) network, a
\textcolor{red}{robust} framework for reconstructing vector floorplans directly
from point-cloud density maps. Traditional corner-based polygon representations
are highly sensitive to noise and incomplete observations, often resulting in
fragmented or implausible layouts. Recent line grouping methods leverage
structural cues to improve robustness but still struggle to recover fine
geometric details. To address these limitations, we propose a \textit{native}
edge-centric formulation, modeling each wall segment as a directed,
geometrically continuous edge. This representation enables inference of
coherent floorplan structures, ensuring watertight, topologically valid room
boundaries while improving robustness and reducing artifacts. Towards this
design, we develop a dual-query transformer decoder that integrates perturbed
and latent queries within a denoising framework, which not only stabilizes
optimization but also accelerates convergence. Extensive experiments on
Structured3D and SceneCAD show that \textbf{CAGE} achieves state-of-the-art
performance, with F1 scores of 99.1\% (rooms), 91.7\% (corners), and 89.3\%
(angles). The method also demonstrates strong cross-dataset generalization,
underscoring the efficacy of our architectural innovations. Code and pretrained
models will be released upon acceptance.
\\ ( https://arxiv.org/abs/2509.15459 ,  15449kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15470
Date: Thu, 18 Sep 2025 22:35:44 GMT   (1016kb)

Title: Self-supervised learning of imaging and clinical signatures using a
  multimodal joint-embedding predictive architecture
Authors: Thomas Z. Li, Aravind R. Krishnan, Lianrui Zuo, John M. Still, Kim L.
  Sandler, Fabien Maldonado, Thomas A. Lasko, Bennett A. Landman
Categories: cs.CV cs.AI
\\
  The development of multimodal models for pulmonary nodule diagnosis is
limited by the scarcity of labeled data and the tendency for these models to
overfit on the training distribution. In this work, we leverage self-supervised
learning from longitudinal and multimodal archives to address these challenges.
We curate an unlabeled set of patients with CT scans and linked electronic
health records from our home institution to power joint embedding predictive
architecture (JEPA) pretraining. After supervised finetuning, we show that our
approach outperforms an unregularized multimodal model and imaging-only model
in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC),
but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC).
We develop a synthetic environment that characterizes the context in which JEPA
may underperform. This work innovates an approach that leverages unlabeled
multimodal medical archives to improve predictive models and demonstrates its
advantages and limitations in pulmonary nodule diagnosis.
\\ ( https://arxiv.org/abs/2509.15470 ,  1016kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15472
Date: Thu, 18 Sep 2025 22:36:57 GMT   (10918kb)

Title: Efficient Multimodal Dataset Distillation via Generative Models
Authors: Zhenghao Zhao, Haoxuan Wang, Junyi Wu, Yuzhang Shang, Gaowen Liu, Yan
  Yan
Categories: cs.CV
\\
  Dataset distillation aims to synthesize a small dataset from a large dataset,
enabling the model trained on it to perform well on the original dataset. With
the blooming of large language models and multimodal large language models, the
importance of multimodal datasets, particularly image-text datasets, has grown
significantly. However, existing multimodal dataset distillation methods are
constrained by the Matching Training Trajectories algorithm, which
significantly increases the computing resource requirement, and takes days to
process the distillation. In this work, we introduce EDGE, a generative
distillation method for efficient multimodal dataset distillation.
Specifically, we identify two key challenges of distilling multimodal datasets
with generative models: 1) The lack of correlation between generated images and
captions. 2) The lack of diversity among generated samples. To address the
aforementioned issues, we propose a novel generative model training workflow
with a bi-directional contrastive loss and a diversity loss. Furthermore, we
propose a caption synthesis strategy to further improve text-to-image retrieval
performance by introducing more text information. Our method is evaluated on
Flickr30K, COCO, and CC3M datasets, demonstrating superior performance and
efficiency compared to existing approaches. Notably, our method achieves
results 18x faster than the state-of-the-art method.
\\ ( https://arxiv.org/abs/2509.15472 ,  10918kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15479
Date: Thu, 18 Sep 2025 22:54:13 GMT   (11665kb)

Title: OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining
  and Fine-Tuning Open Source Models with Public Data
Authors: Bj\"orn M\"oller, Zhengyang Li, Malte Stelzer, Thomas Graave, Fabian
  Bettels, Muaaz Ataya, Tim Fingscheidt
Categories: cs.CV
\\
  Recent successful video generation systems that predict and create realistic
automotive driving scenes from short video inputs assign tokenization, future
state prediction (world model), and video decoding to dedicated models. These
approaches often utilize large models that require significant training
resources, offer limited insight into design choices, and lack publicly
available code and datasets. In this work, we address these deficiencies and
present OpenViGA, an open video generation system for automotive driving
scenes. Our contributions are: Unlike several earlier works for video
generation, such as GAIA-1, we provide a deep analysis of the three components
of our system by separate quantitative and qualitative evaluation: Image
tokenizer, world model, video decoder. Second, we purely build upon powerful
pre-trained open source models from various domains, which we fine-tune by
publicly available automotive data (BDD100K) on GPU hardware at academic scale.
Third, we build a coherent video generation system by streamlining interfaces
of our components. Fourth, due to public availability of the underlying models
and data, we allow full reproducibility. Finally, we also publish our code and
models on Github. For an image size of 256x256 at 4 fps we are able to predict
realistic driving scene videos frame-by-frame with only one frame of
algorithmic latency.
\\ ( https://arxiv.org/abs/2509.15479 ,  11665kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15482
Date: Thu, 18 Sep 2025 23:01:13 GMT   (14449kb)

Title: Comparing Computational Pathology Foundation Models using
  Representational Similarity Analysis
Authors: Vaibhav Mishra, William Lotter
Categories: cs.CV cs.AI
\\
  Foundation models are increasingly developed in computational pathology
(CPath) given their promise in facilitating many downstream tasks. While recent
studies have evaluated task performance across models, less is known about the
structure and variability of their learned representations. Here, we
systematically analyze the representational spaces of six CPath foundation
models using techniques popularized in computational neuroscience. The models
analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and
self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through
representational similarity analysis using H&E image patches from TCGA, we find
that UNI2 and Virchow2 have the most distinct representational structures,
whereas Prov-Gigapath has the highest average similarity across models. Having
the same training paradigm (vision-only vs. vision-language) did not guarantee
higher representational similarity. The representations of all models showed a
high slide-dependence, but relatively low disease-dependence. Stain
normalization decreased slide-dependence for all models by a range of 5.5%
(CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language
models demonstrated relatively compact representations, compared to the more
distributed representations of vision-only models. These findings highlight
opportunities to improve robustness to slide-specific features, inform model
ensembling strategies, and provide insights into how training paradigms shape
model representations. Our framework is extendable across medical imaging
domains, where probing the internal representations of foundation models can
help ensure effective development and deployment.
\\ ( https://arxiv.org/abs/2509.15482 ,  14449kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15490
Date: Thu, 18 Sep 2025 23:55:51 GMT   (13236kb)

Title: SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with
  600M Parameters
Authors: Abdarahmane Traore, \'Eric Hervet, Andy Couturier
Categories: cs.CV cs.AI
Comments: 9 pages, 3 figures, IEEE/CVF International Conference on Computer
  Vision Workshops (ICCVW)
\\
  Recent advances in vision-language models (VLMs) have enabled powerful
multimodal reasoning, but state-of-the-art approaches typically rely on
extremely large models with prohibitive computational and memory requirements.
This makes their deployment challenging in resource-constrained environments
such as warehouses, robotics, and industrial applications, where both
efficiency and robust spatial understanding are critical. In this work, we
present SmolRGPT, a compact vision-language architecture that explicitly
incorporates region-level spatial reasoning by integrating both RGB and depth
cues. SmolRGPT employs a three-stage curriculum that progressively align visual
and language features, enables spatial relationship understanding, and adapts
to task-specific datasets. We demonstrate that with only 600M parameters,
SmolRGPT achieves competitive results on challenging warehouse spatial
reasoning benchmarks, matching or exceeding the performance of much larger
alternatives. These findings highlight the potential for efficient, deployable
multimodal intelligence in real-world settings without sacrificing core spatial
reasoning capabilities. The code of the experimentation will be available at:
https://github.com/abtraore/SmolRGPT
\\ ( https://arxiv.org/abs/2509.15490 ,  13236kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15496
Date: Fri, 19 Sep 2025 00:31:57 GMT   (39623kb)

Title: Lynx: Towards High-Fidelity Personalized Video Generation
Authors: Shen Sang, Tiancheng Zhi, Tianpei Gu, Jing Liu, Linjie Luo
Categories: cs.CV
Comments: Lynx Technical Report
\\
  We present Lynx, a high-fidelity model for personalized video synthesis from
a single input image. Built on an open-source Diffusion Transformer (DiT)
foundation model, Lynx introduces two lightweight adapters to ensure identity
fidelity. The ID-adapter employs a Perceiver Resampler to convert
ArcFace-derived facial embeddings into compact identity tokens for
conditioning, while the Ref-adapter integrates dense VAE features from a frozen
reference pathway, injecting fine-grained details across all transformer layers
through cross-attention. These modules collectively enable robust identity
preservation while maintaining temporal coherence and visual realism. Through
evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which
yielded 800 test cases, Lynx has demonstrated superior face resemblance,
competitive prompt following, and strong video quality, thereby advancing the
state of personalized video generation.
\\ ( https://arxiv.org/abs/2509.15496 ,  39623kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15497
Date: Fri, 19 Sep 2025 00:32:19 GMT   (3726kb)

Title: Backdoor Mitigation via Invertible Pruning Masks
Authors: Kealan Dunnett, Reza Arablouei, Dimity Miller, Volkan Dedeoglu, Raja
  Jurdak
Categories: cs.CV
\\
  Model pruning has gained traction as a promising defense strategy against
backdoor attacks in deep learning. However, existing pruning-based approaches
often fall short in accurately identifying and removing the specific parameters
responsible for inducing backdoor behaviors. Despite the dominance of
fine-tuning-based defenses in recent literature, largely due to their superior
performance, pruning remains a compelling alternative, offering greater
interpretability and improved robustness in low-data regimes. In this paper, we
propose a novel pruning approach featuring a learned \emph{selection} mechanism
to identify parameters critical to both main and backdoor tasks, along with an
\emph{invertible} pruning mask designed to simultaneously achieve two
complementary goals: eliminating the backdoor task while preserving it through
the inverse mask. We formulate this as a bi-level optimization problem that
jointly learns selection variables, a sparse invertible mask, and
sample-specific backdoor perturbations derived from clean data. The inner
problem synthesizes candidate triggers using the inverse mask, while the outer
problem refines the mask to suppress backdoor behavior without impairing
clean-task accuracy. Extensive experiments demonstrate that our approach
outperforms existing pruning-based backdoor mitigation approaches, maintains
strong performance under limited data conditions, and achieves competitive
results compared to state-of-the-art fine-tuning approaches. Notably, the
proposed approach is particularly effective in restoring correct predictions
for compromised samples after successful backdoor mitigation.
\\ ( https://arxiv.org/abs/2509.15497 ,  3726kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15514
Date: Fri, 19 Sep 2025 01:37:02 GMT   (312kb)

Title: MEC-Quant: Maximum Entropy Coding for Extremely Low Bit
  Quantization-Aware Training
Authors: Junbiao Pang and Tianyang Cai and Baochang Zhang
Categories: cs.CV
Comments: 7pages;on going work
\\
  Quantization-Aware Training (QAT) has driven much attention to produce
efficient neural networks. Current QAT still obtains inferior performances
compared with the Full Precision (FP) counterpart. In this work, we argue that
quantization inevitably introduce biases into the learned representation,
especially under the extremely low-bit setting. To cope with this issue, we
propose Maximum Entropy Coding Quantization (MEC-Quant), a more principled
objective that explicitly optimizes on the structure of the representation, so
that the learned representation is less biased and thus generalizes better to
unseen in-distribution samples. To make the objective end-to-end trainable, we
propose to leverage the minimal coding length in lossy data coding as a
computationally tractable surrogate for the entropy, and further derive a
scalable reformulation of the objective based on Mixture Of Experts (MOE) that
not only allows fast computation but also handles the long-tailed distribution
for weights or activation values. Extensive experiments on various tasks on
computer vision tasks prove its superiority. With MEC-Qaunt, the limit of QAT
is pushed to the x-bit activation for the first time and the accuracy of
MEC-Quant is comparable to or even surpass the FP counterpart. Without bells
and whistles, MEC-Qaunt establishes a new state of the art for QAT.
\\ ( https://arxiv.org/abs/2509.15514 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15532
Date: Fri, 19 Sep 2025 02:34:50 GMT   (970kb)

Title: GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI
  Agents
Authors: Xianhang Ye, Yiqing Li, Wei Dai, Miancan Liu, Ziyuan Chen, Zhangye
  Han, Hongbo Min, Jinkui Ren, Xiantao Zhang, Wen Yang, Zhi Jin
Categories: cs.CV cs.AI
\\
  Existing GUI grounding methods often struggle with fine-grained localization
in high-resolution screenshots. To address this, we propose GUI-ARP, a novel
framework that enables adaptive multi-stage inference. Equipped with the
proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC),
GUI-ARP dynamically exploits visual attention for cropping task-relevant
regions and adapts its inference strategy, performing a single-stage inference
for simple cases and a multi-stage analysis for more complex scenarios. This is
achieved through a two-phase training pipeline that integrates supervised
fine-tuning with reinforcement fine-tuning based on Group Relative Policy
Optimization (GRPO). Extensive experiments demonstrate that the proposed
GUI-ARP achieves state-of-the-art performance on challenging GUI grounding
benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9%
on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness
against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.
\\ ( https://arxiv.org/abs/2509.15532 ,  970kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15536
Date: Fri, 19 Sep 2025 02:41:37 GMT   (6818kb)

Title: SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world
  models
Authors: Sen Wang, Jingyi Tian, Le Wang, Zhimin Liao, Jiayi Li, Huaiyi Dong,
  Kun Xia, Sanping Zhou, Wei Tang, Hua Gang
Categories: cs.CV cs.RO
Comments: 22 pages,15 figures
\\
  World models allow agents to simulate the consequences of actions in imagined
environments for planning, control, and long-horizon decision-making. However,
existing autoregressive world models struggle with visually coherent
predictions due to disrupted spatial structure, inefficient decoding, and
inadequate motion modeling. In response, we propose \textbf{S}cale-wise
\textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt
(\textbf{SAMPO}), a hybrid framework that combines visual autoregressive
modeling for intra-frame generation with causal modeling for next-frame
generation. Specifically, SAMPO integrates temporal causal decoding with
bidirectional spatial attention, which preserves spatial locality and supports
parallel decoding within each scale. This design significantly enhances both
temporal consistency and rollout efficiency. To further improve dynamic scene
understanding, we devise an asymmetric multi-scale tokenizer that preserves
spatial details in observed frames and extracts compact dynamic representations
for future frames, optimizing both memory usage and model performance.
Additionally, we introduce a trajectory-aware motion prompt module that injects
spatiotemporal cues about object and robot trajectories, focusing attention on
dynamic regions and improving temporal consistency and physical realism.
Extensive experiments show that SAMPO achieves competitive performance in
action-conditioned video prediction and model-based control, improving
generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's
zero-shot generalization and scaling behavior, demonstrating its ability to
generalize to unseen tasks and benefit from larger model sizes.
\\ ( https://arxiv.org/abs/2509.15536 ,  6818kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15540
Date: Fri, 19 Sep 2025 02:49:47 GMT   (9185kb)

Title: Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with
  Non-Verbal Cues
Authors: Wei Chen and Tongguan Wang and Feiyue Xue and Junkai Li and Hui Liu
  and Ying Sha
Categories: cs.CV cs.CL
Comments: 13 page, 5 figures, uploaded by Wei Chen
\\
  Desire, as an intention that drives human behavior, is closely related to
both emotion and sentiment. Multimodal learning has advanced sentiment and
emotion recognition, but multimodal approaches specially targeting human desire
understanding remain underexplored. And existing methods in sentiment analysis
predominantly emphasize verbal cues and overlook images as complementary
non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional
Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition,
which enforces mutual guidance between text and image modalities to effectively
capture intention-related representations in the image. Specifically,
low-resolution images are used to obtain global visual representations for
cross-modal alignment, while high resolution images are partitioned into
sub-images and modeled with masked image modeling to enhance the ability to
capture fine-grained local features. A text-guided image decoder and an
image-guided text decoder are introduced to facilitate deep cross-modal
interaction at both local and global representations of image information.
Additionally, to balance perceptual gains with computation cost, a mixed-scale
image strategy is adopted, where high-resolution images are cropped into
sub-images for masked modeling. The proposed approach is evaluated on MSED, a
multimodal dataset that includes a desire understanding benchmark, as well as
emotion and sentiment recognition. Experimental results indicate consistent
improvements over other state-of-the-art methods, validating the effectiveness
of our proposed method. Specifically, our method outperforms existing
approaches, achieving F1-score improvements of 1.1% in desire understanding,
0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is
available at: https://github.com/especiallyW/SyDES.
\\ ( https://arxiv.org/abs/2509.15540 ,  9185kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15546
Date: Fri, 19 Sep 2025 03:01:27 GMT   (1424kb)

Title: Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for
  7th LSVOS RVOS Track
Authors: Ran Hong and Feng Lu and Leilei Cao and An Yan and Youhai Jiang and
  Fengjie Zhu
Categories: cs.CV
Comments: 6 pages, 2 figures
\\
  Referential Video Object Segmentation (RVOS) aims to segment all objects in a
video that match a given natural language description, bridging the gap between
vision and language understanding. Recent work, such as Sa2VA, combines Large
Language Models (LLMs) with SAM~2, leveraging the strong video reasoning
capability of LLMs to guide video segmentation. In this work, we present a
training-free framework that substantially improves Sa2VA's performance on the
RVOS task. Our method introduces two key components: (1) a Video-Language
Checker that explicitly verifies whether the subject and action described in
the query actually appear in the video, thereby reducing false positives; and
(2) a Key-Frame Sampler that adaptively selects informative frames to better
capture both early object appearances and long-range temporal context. Without
any additional training, our approach achieves a J&F score of 64.14% on the
MeViS test set, ranking 2nd place in the RVOS track of the 7th LSVOS Challenge
at ICCV 2025.
\\ ( https://arxiv.org/abs/2509.15546 ,  1424kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15548
Date: Fri, 19 Sep 2025 03:06:49 GMT   (32892kb)

Title: MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild
Authors: Deming Li, Kaiwen Jiang, Yutao Tang, Ravi Ramamoorthi, Rama Chellappa,
  Cheng Peng
Categories: cs.CV
\\
  In-the-wild photo collections often contain limited volumes of imagery and
exhibit multiple appearances, e.g., taken at different times of day or seasons,
posing significant challenges to scene reconstruction and novel view synthesis.
Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian
Splatting (3DGS) have improved in these areas, they tend to oversmooth and are
prone to overfitting. In this paper, we present MS-GS, a novel framework
designed with Multi-appearance capabilities in Sparse-view scenarios using
3DGS. To address the lack of support due to sparse initializations, our
approach is built on the geometric priors elicited from monocular depth
estimations. The key lies in extracting and utilizing local semantic regions
with a Structure-from-Motion (SfM) points anchored algorithm for reliable
alignment and geometry cues. Then, to introduce multi-view constraints, we
propose a series of geometry-guided supervision at virtual views in a
fine-grained and coarse scheme to encourage 3D consistency and reduce
overfitting. We also introduce a dataset and an in-the-wild experiment setting
to set up more realistic benchmarks. We demonstrate that MS-GS achieves
photorealistic renderings under various challenging sparse-view and
multi-appearance conditions and outperforms existing approaches significantly
across different datasets.
\\ ( https://arxiv.org/abs/2509.15548 ,  32892kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15553
Date: Fri, 19 Sep 2025 03:13:58 GMT   (3521kb)

Title: Diffusion-Based Cross-Modal Feature Extraction for Multi-Label
  Classification
Authors: Tian Lan and Yiming Zheng and Jianxin Yin
Categories: cs.CV cs.AI stat.AP
\\
  Multi-label classification has broad applications and depends on powerful
representations capable of capturing multi-label interactions. We introduce
\textit{Diff-Feat}, a simple but powerful framework that extracts intermediate
features from pre-trained diffusion-Transformer models for images and text, and
fuses them for downstream tasks. We observe that for vision tasks, the most
discriminative intermediate feature along the diffusion process occurs at the
middle step and is located in the middle block in Transformer. In contrast, for
language tasks, the best feature occurs at the noise-free step and is located
in the deepest block. In particular, we observe a striking phenomenon across
varying datasets: a mysterious "Layer $12$" consistently yields the best
performance on various downstream classification tasks for images (under
DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that
pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a
few candidates, avoiding an exhaustive grid search. A simple fusion-linear
projection followed by addition-of the selected representations yields
state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on
Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a
wide margin. t-SNE and clustering metrics further reveal that
\textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts.
The code is available at https://github.com/lt-0123/Diff-Feat.
\\ ( https://arxiv.org/abs/2509.15553 ,  3521kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15558
Date: Fri, 19 Sep 2025 03:42:11 GMT   (88kb)

Title: From Development to Deployment of AI-assisted Telehealth and Screening
  for Vision- and Hearing-threatening diseases in resource-constrained
  settings: Field Observations, Challenges and Way Forward
Authors: Mahesh Shakya, Bijay Adhikari, Nirsara Shrestha, Bipin Koirala, Arun
  Adhikari, Prasanta Poudyal, Luna Mathema, Sarbagya Buddhacharya, Bijay
  Khatri, Bishesh Khanal
Categories: cs.CV cs.HC
Comments: Accepted to MIRASOL (Medical Image Computing in Resource Constrained
  Settings Workshop & KI) Workshop, 2025
\\
  Vision- and hearing-threatening diseases cause preventable disability,
especially in resource-constrained settings(RCS) with few specialists and
limited screening setup. Large scale AI-assisted screening and telehealth has
potential to expand early detection, but practical deployment is challenging in
paper-based workflows and limited documented field experience exist to build
upon. We provide insights on challenges and ways forward in development to
adoption of scalable AI-assisted Telehealth and screening in such settings.
Specifically, we find that iterative, interdisciplinary collaboration through
early prototyping, shadow deployment and continuous feedback is important to
build shared understanding as well as reduce usability hurdles when
transitioning from paper-based to AI-ready workflows. We find public datasets
and AI models highly useful despite poor performance due to domain shift. In
addition, we find the need for automated AI-based image quality check to
capture gradable images for robust screening in high-volume camps.
  Our field learning stress the importance of treating AI development and
workflow digitization as an end-to-end, iterative co-design process. By
documenting these practical challenges and lessons learned, we aim to address
the gap in contextual, actionable field knowledge for building real-world
AI-assisted telehealth and mass-screening programs in RCS.
\\ ( https://arxiv.org/abs/2509.15558 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15563
Date: Fri, 19 Sep 2025 03:49:23 GMT   (828kb)

Title: DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement
  for remote sensing change detection
Authors: Min Sun, Fenghui Guo
Categories: cs.CV
\\
  Remote sensing change detection (RSCD) is vital for identifying land-cover
changes, yet existing methods, including state-of-the-art State Space Models
(SSMs), often lack explicit mechanisms to handle geometric misalignments and
struggle to distinguish subtle, true changes from noise.To address this, we
introduce DC-Mamba, an "align-then-enhance" framework built upon the
ChangeMamba backbone. It integrates two lightweight, plug-and-play modules: (1)
Bi-Temporal Deformable Alignment (BTDA), which explicitly introduces geometric
awareness to correct spatial misalignments at the semantic feature level; and
(2) a Scale-Sparse Change Amplifier(SSCA), which uses multi-source cues to
selectively amplify high-confidence change signals while suppressing noise
before the final classification. This synergistic design first establishes
geometric consistency with BTDA to reduce pseudo-changes, then leverages SSCA
to sharpen boundaries and enhance the visibility of small or subtle targets.
Experiments show our method significantly improves performance over the strong
ChangeMamba baseline, increasing the F1-score from 0.5730 to 0.5903 and IoU
from 0.4015 to 0.4187. The results confirm the effectiveness of our
"align-then-enhance" strategy, offering a robust and easily deployable solution
that transparently addresses both geometric and feature-level challenges in
RSCD.
\\ ( https://arxiv.org/abs/2509.15563 ,  828kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15566
Date: Fri, 19 Sep 2025 04:03:44 GMT   (4503kb)

Title: BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent
Authors: Shaojie Zhang, Ruoceng Zhang, Pei Fu, Shaokang Wang, Jiahui Yang, Xin
  Du, Shiqi Cui, Bin Qin, Ying Huang, Zhenbo Luo, Jian Luan
Categories: cs.CV cs.AI
Comments: Accepted at NeurIPS 2025
\\
  In the field of AI-driven human-GUI interaction automation, while rapid
advances in multimodal large language models and reinforcement fine-tuning
techniques have yielded remarkable progress, a fundamental challenge persists:
their interaction logic significantly deviates from natural human-GUI
communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL),
a brain-inspired framework for human-GUI interaction that mimics the human
cognitive process between users and graphical interfaces. The system decomposes
interactions into three biologically plausible phases: (1) Blink - rapid
detection and attention to relevant screen areas, analogous to saccadic eye
movements; (2) Think - higher-level reasoning and decision-making, mirroring
cognitive planning; and (3) Link - generation of executable commands for
precise motor control, emulating human action selection mechanisms.
Additionally, we introduce two key technical innovations for the BTL framework:
(1) Blink Data Generation - an automated annotation pipeline specifically
optimized for blink data, and (2) BTL Reward -- the first rule-based reward
mechanism that enables reinforcement learning driven by both process and
outcome. Building upon this framework, we develop a GUI agent model named
BTL-UI, which demonstrates consistent state-of-the-art performance across both
static GUI understanding and dynamic interaction tasks in comprehensive
benchmarks. These results provide conclusive empirical validation of the
framework's efficacy in developing advanced GUI Agents.
\\ ( https://arxiv.org/abs/2509.15566 ,  4503kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15573
Date: Fri, 19 Sep 2025 04:12:14 GMT   (23613kb)

Title: Towards Size-invariant Salient Object Detection: A Generic Evaluation
  and Optimization Approach
Authors: Shilong Bao, Qianqian Xu, Feiran Li, Boyu Han, Zhiyong Yang, Xiaochun
  Cao, Qingming Huang
Categories: cs.CV cs.AI cs.LG
\\
  This paper investigates a fundamental yet underexplored issue in Salient
Object Detection (SOD): the size-invariant property for evaluation protocols,
particularly in scenarios when multiple salient objects of significantly
different sizes appear within a single image. We first present a novel
perspective to expose the inherent size sensitivity of existing widely used SOD
metrics. Through careful theoretical derivations, we show that the evaluation
outcome of an image under current SOD metrics can be essentially decomposed
into a sum of several separable terms, with the contribution of each term being
directly proportional to its corresponding region size. Consequently, the
prediction errors would be dominated by the larger regions, while smaller yet
potentially more semantically important objects are often overlooked, leading
to biased performance assessments and practical degradation. To address this
challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed.
The core idea is to evaluate each separable component individually and then
aggregate the results, thereby effectively mitigating the impact of size
imbalance across objects. Building upon this, we further develop a dedicated
optimization framework (SIOpt), which adheres to the size-invariant principle
and significantly enhances the detection of salient objects across a broad
range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly
integrated with a wide range of SOD backbones. Theoretically, we also present
generalization analysis of SOD methods and provide evidence supporting the
validity of our new evaluation protocols. Finally, comprehensive experiments
speak to the efficacy of our proposed approach. The code is available at
https://github.com/Ferry-Li/SI-SOD.
\\ ( https://arxiv.org/abs/2509.15573 ,  23613kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15578
Date: Fri, 19 Sep 2025 04:24:57 GMT   (1129kb)

Title: Multimodal Learning for Fake News Detection in Short Videos Using
  Linguistically Verified Data and Heterogeneous Modality Fusion
Authors: Shanghong Li, Chiam Wen Qi Ruth, Hong Xu, Fang Liu
Categories: cs.CV cs.AI
\\
  The rapid proliferation of short video platforms has necessitated advanced
methods for detecting fake news. This need arises from the widespread influence
and ease of sharing misinformation, which can lead to significant societal
harm. Current methods often struggle with the dynamic and multimodal nature of
short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel
multimodal framework that integrates video, audio, and text data to evaluate
the authenticity of short video content. HFN introduces a Decision Network that
dynamically adjusts modality weights during inference and a Weighted
Multi-Modal Feature Fusion module to ensure robust performance even with
incomplete data. Additionally, we contribute a comprehensive dataset VESV
(VEracity on Short Videos) specifically designed for short video fake news
detection. Experiments conducted on the FakeTT and newly collected VESV
datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over
state-of-the-art methods. This work establishes a robust solution capable of
effectively identifying fake news in the complex landscape of short video
platforms, paving the way for more reliable and comprehensive approaches in
combating misinformation.
\\ ( https://arxiv.org/abs/2509.15578 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15596
Date: Fri, 19 Sep 2025 04:55:56 GMT   (11945kb)

Title: EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge
  Comprehension and Clinical Reasoning in Ophthalmic Surgery
Authors: Gui Wang, Yang Wennuo, Xusen Ma, Zehao Zhong, Zhuoru Wu, Ende Wu, Rong
  Qu, Wooi Ping Cheah, Jianfeng Ren, Linlin Shen
Categories: cs.CV
Comments: Strong accept by NeurIPS2025 Reviewers and AC, but reject by PC.
  (Rating: 6,5,4,4)
\\
  MLLMs (Multimodal Large Language Models) have showcased remarkable
capabilities, but their performance in high-stakes, domain-specific scenarios
like surgical settings, remains largely under-explored. To address this gap, we
develop \textbf{EyePCR}, a large-scale benchmark for ophthalmic surgery
analysis, grounded in structured clinical knowledge to evaluate cognition
across \textit{Perception}, \textit{Comprehension} and \textit{Reasoning}.
EyePCR offers a richly annotated corpus with more than 210k VQAs, which cover
1048 fine-grained attributes for multi-view perception, medical knowledge graph
of more than 25k triplets for comprehension, and four clinically grounded
reasoning tasks. The rich annotations facilitate in-depth cognitive analysis,
simulating how surgeons perceive visual cues and combine them with domain
knowledge to make decisions, thus greatly improving models' cognitive ability.
In particular, \textbf{EyePCR-MLLM}, a domain-adapted variant of Qwen2.5-VL-7B,
achieves the highest accuracy on MCQs for \textit{Perception} among compared
models and outperforms open-source models in \textit{Comprehension} and
\textit{Reasoning}, rivalling commercial models like GPT-4.1. EyePCR reveals
the limitations of existing MLLMs in surgical cognition and lays the foundation
for benchmarking and enhancing clinical reliability of surgical video
understanding models.
\\ ( https://arxiv.org/abs/2509.15596 ,  11945kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15602
Date: Fri, 19 Sep 2025 05:08:05 GMT   (956kb)

Title: TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?
Authors: Zhongyuan Bao, Lejun Zhang
Categories: cs.CV
\\
  Multimodal large language models (MLLMs) excel at general video understanding
but struggle with fast, high-frequency sports like tennis, where rally clips
are short yet information-dense. To systematically evaluate MLLMs in this
challenging domain, we present TennisTV, the first and most comprehensive
benchmark for tennis video understanding. TennisTV models each rally as a
temporal-ordered sequence of consecutive stroke events, using automated
pipelines for filtering and question generation. It covers 8 tasks at rally and
stroke levels and includes 2,500 human-verified questions. Evaluating 16
representative MLLMs, we provide the first systematic assessment of tennis
video understanding. Results reveal substantial shortcomings and yield two key
insights: (i) frame-sampling density should be tailored and balanced across
tasks, and (ii) improving temporal grounding is essential for stronger
reasoning.
\\ ( https://arxiv.org/abs/2509.15602 ,  956kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15608
Date: Fri, 19 Sep 2025 05:14:19 GMT   (4616kb)

Title: Enhancing WSI-Based Survival Analysis with Report-Auxiliary
  Self-Distillation
Authors: Zheng Wang, Hong Liu, Zheng Wang, Danyi Li, Min Cen, Baptiste Magnier,
  Li Liang, Liansheng Wang
Categories: cs.CV
\\
  Survival analysis based on Whole Slide Images (WSIs) is crucial for
evaluating cancer prognosis, as they offer detailed microscopic information
essential for predicting patient outcomes. However, traditional WSI-based
survival analysis usually faces noisy features and limited data accessibility,
hindering their ability to capture critical prognostic features effectively.
Although pathology reports provide rich patient-specific information that could
assist analysis, their potential to enhance WSI-based survival analysis remains
largely unexplored. To this end, this paper proposes a novel Report-auxiliary
self-distillation (Rasa) framework for WSI-based survival analysis. First,
advanced large language models (LLMs) are utilized to extract fine-grained,
WSI-relevant textual descriptions from original noisy pathology reports via a
carefully designed task prompt. Next, a self-distillation-based pipeline is
designed to filter out irrelevant or redundant WSI features for the student
model under the guidance of the teacher model's textual knowledge. Finally, a
risk-aware mix-up strategy is incorporated during the training of the student
model to enhance both the quantity and diversity of the training data.
Extensive experiments carried out on our collected data (CRC) and public data
(TCGA-BRCA) demonstrate the superior effectiveness of Rasa against
state-of-the-art methods. Our code is available at
https://github.com/zhengwang9/Rasa.
\\ ( https://arxiv.org/abs/2509.15608 ,  4616kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15623
Date: Fri, 19 Sep 2025 05:41:17 GMT   (1920kb)

Title: PCSR: Pseudo-label Consistency-Guided Sample Refinement for Noisy
  Correspondence Learning
Authors: Zhuoyao Liu, Yang Liu, Wentao Feng, Shudong Huang
Categories: cs.CV
Comments: 7 pages, 3 figures
\\
  Cross-modal retrieval aims to align different modalities via semantic
similarity. However, existing methods often assume that image-text pairs are
perfectly aligned, overlooking Noisy Correspondences in real data. These
misaligned pairs misguide similarity learning and degrade retrieval
performance. Previous methods often rely on coarse-grained categorizations that
simply divide data into clean and noisy samples, overlooking the intrinsic
diversity within noisy instances. Moreover, they typically apply uniform
training strategies regardless of sample characteristics, resulting in
suboptimal sample utilization for model optimization. To address the above
challenges, we introduce a novel framework, called Pseudo-label
Consistency-Guided Sample Refinement (PCSR), which enhances correspondence
reliability by explicitly dividing samples based on pseudo-label consistency.
Specifically, we first employ a confidence-based estimation to distinguish
clean and noisy pairs, then refine the noisy pairs via pseudo-label consistency
to uncover structurally distinct subsets. We further proposed a Pseudo-label
Consistency Score (PCS) to quantify prediction stability, enabling the
separation of ambiguous and refinable samples within noisy pairs. Accordingly,
we adopt Adaptive Pair Optimization (APO), where ambiguous samples are
optimized with robust loss functions and refinable ones are enhanced via text
replacement during training. Extensive experiments on CC152K, MS-COCO and
Flickr30K validate the effectiveness of our method in improving retrieval
robustness under noisy supervision.
\\ ( https://arxiv.org/abs/2509.15623 ,  1920kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15638
Date: Fri, 19 Sep 2025 06:05:35 GMT   (94kb)

Title: pFedSAM: Personalized Federated Learning of Segment Anything Model for
  Medical Image Segmentation
Authors: Tong Wang, Xingyue Zhao, Linghao Zhuang, Haoyu Zhao, Jiayi Yin, Yuyang
  He, Gang Yu and Bo Lin
Categories: cs.CV
Comments: 5 pages
\\
  Medical image segmentation is crucial for computer-aided diagnosis, yet
privacy constraints hinder data sharing across institutions. Federated learning
addresses this limitation, but existing approaches often rely on lightweight
architectures that struggle with complex, heterogeneous data. Recently, the
Segment Anything Model (SAM) has shown outstanding segmentation capabilities;
however, its massive encoder poses significant challenges in federated
settings. In this work, we present the first personalized federated SAM
framework tailored for heterogeneous data scenarios in medical image
segmentation. Our framework integrates two key innovations: (1) a personalized
strategy that aggregates only the global parameters to capture cross-client
commonalities while retaining the designed L-MoE (Localized Mixture-of-Experts)
component to preserve domain-specific features; and (2) a decoupled
global-local fine-tuning mechanism that leverages a teacher-student paradigm
via knowledge distillation to bridge the gap between the global shared model
and the personalized local models, thereby mitigating overgeneralization.
Extensive experiments on two public datasets validate that our approach
significantly improves segmentation performance, achieves robust cross-domain
adaptation, and reduces communication overhead.
\\ ( https://arxiv.org/abs/2509.15638 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15642
Date: Fri, 19 Sep 2025 06:07:53 GMT   (5384kb)

Title: UNIV: Unified Foundation Model for Infrared and Visible Modalities
Authors: Fangyuan Mao, Shuo Wang, Jilin Mei, Chen Min, Shun Lu, Fuyang Liu, Yu
  Hu
Categories: cs.CV
\\
  The demand for joint RGB-visible and infrared perception is growing rapidly,
particularly to achieve robust performance under diverse weather conditions.
Although pre-trained models for RGB-visible and infrared data excel in their
respective domains, they often underperform in multimodal scenarios, such as
autonomous vehicles equipped with both sensors. To address this challenge, we
propose a biologically inspired UNified foundation model for Infrared and
Visible modalities (UNIV), featuring two key innovations. First, we introduce
Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided
distillation framework that mimics retinal horizontal cells' lateral
inhibition, which enables effective cross-modal feature alignment while
remaining compatible with any transformer-based architecture. Second, our
dual-knowledge preservation mechanism emulates the retina's bipolar cell signal
routing - combining LoRA adapters (2% added parameters) with synchronous
distillation to prevent catastrophic forgetting, thereby replicating the
retina's photopic (cone-driven) and scotopic (rod-driven) functionality. To
support cross-modal learning, we introduce the MVIP dataset, the most
comprehensive visible-infrared benchmark to date. It contains 98,992 precisely
aligned image pairs spanning diverse scenarios. Extensive experiments
demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in
semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+
of the baseline performance on visible RGB tasks. Our code is available at
https://github.com/fangyuanmao/UNIV.
\\ ( https://arxiv.org/abs/2509.15642 ,  5384kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15645
Date: Fri, 19 Sep 2025 06:13:28 GMT   (4291kb)

Title: GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host
  Offloading
Authors: Donghyun Lee, Dawoon Jeong, Jae W. Lee, Hongil Yoon
Categories: cs.CV
\\
  The advent of 3D Gaussian Splatting has revolutionized graphics rendering by
delivering high visual quality and fast rendering speeds. However, training
large-scale scenes at high quality remains challenging due to the substantial
memory demands required to store parameters, gradients, and optimizer states,
which can quickly overwhelm GPU memory. To address these limitations, we
propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian
Splatting. GS-Scale stores all Gaussians in host memory, transferring only a
subset to the GPU on demand for each forward and backward pass. While this
dramatically reduces GPU memory usage, it requires frustum culling and
optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's
limited compute and memory bandwidth. To mitigate this, GS-Scale employs three
system-level optimizations: (1) selective offloading of geometric parameters
for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer
updates with GPU computation, and (3) deferred optimizer update to minimize
unnecessary memory accesses for Gaussians with zero gradients. Our extensive
evaluations on large-scale datasets demonstrate that GS-Scale significantly
lowers GPU memory demands by 3.3-5.6x, while achieving training speeds
comparable to GPU without host offloading. This enables large-scale 3D Gaussian
Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the
number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU,
leading to 23-35% LPIPS (learned perceptual image patch similarity)
improvement.
\\ ( https://arxiv.org/abs/2509.15645 ,  4291kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15648
Date: Fri, 19 Sep 2025 06:18:08 GMT   (3722kb)

Title: FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation
  based on 3D Gaussian Splatting
Authors: Yuwei Jia, Yutang Lu, Zhe Cui, Fei Su
Categories: cs.CV
\\
  Researchers have conducted many pioneer researches on contactless
fingerprints, yet the performance of contactless fingerprint recognition still
lags behind contact-based methods primary due to the insufficient contactless
fingerprint data with pose variations and lack of the usage of implicit 3D
fingerprint representations. In this paper, we introduce a novel contactless
fingerprint 3D registration, reconstruction and generation framework by
integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for
contactless fingerprint recognition that integrates 3D fingerprint
reconstruction and generation. To our knowledge, this is the first work to
apply 3D Gaussian Splatting to the field of fingerprint recognition, and the
first to achieve effective 3D registration and complete reconstruction of
contactless fingerprints with sparse input images and without requiring camera
parameters information. Experiments on 3D fingerprint registration,
reconstruction, and generation prove that our method can accurately align and
reconstruct 3D fingerprints from 2D images, and sequentially generates
high-quality contactless fingerprints from 3D model, thus increasing the
performances for contactless fingerprint recognition.
\\ ( https://arxiv.org/abs/2509.15648 ,  3722kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15675
Date: Fri, 19 Sep 2025 06:49:44 GMT   (1817kb)

Title: A PCA Based Model for Surface Reconstruction from Incomplete Point
  Clouds
Authors: Hao Liu
Categories: cs.CV
\\
  Point cloud data represents a crucial category of information for
mathematical modeling, and surface reconstruction from such data is an
important task across various disciplines. However, during the scanning
process, the collected point cloud data may fail to cover the entire surface
due to factors such as high light-absorption rate and occlusions, resulting in
incomplete datasets. Inferring surface structures in data-missing regions and
successfully reconstructing the surface poses a challenge. In this paper, we
present a Principal Component Analysis (PCA) based model for surface
reconstruction from incomplete point cloud data. Initially, we employ PCA to
estimate the normal information of the underlying surface from the available
point cloud data. This estimated normal information serves as a regularizer in
our model, guiding the reconstruction of the surface, particularly in areas
with missing data. Additionally, we introduce an operator-splitting method to
effectively solve the proposed model. Through systematic experimentation, we
demonstrate that our model successfully infers surface structures in
data-missing regions and well reconstructs the underlying surfaces,
outperforming existing methodologies.
\\ ( https://arxiv.org/abs/2509.15675 ,  1817kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15677
Date: Fri, 19 Sep 2025 06:50:37 GMT   (38693kb)

Title: Camera Splatting for Continuous View Optimization
Authors: Gahye Lee, Hyomin Kim, Gwangjin Ju, Jooeun Son, Hyejeong Yoon,
  Seungyong Lee
Categories: cs.CV
\\
  We propose Camera Splatting, a novel view optimization framework for novel
view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a
camera splat, and virtual cameras, termed point cameras, are placed at 3D
points sampled near the surface to observe the distribution of camera splats.
View optimization is achieved by continuously and differentiably refining the
camera splats so that desirable target distributions are observed from the
point cameras, in a manner similar to the original 3D Gaussian splatting.
Compared to the Farthest View Sampling (FVS) approach, our optimized views
demonstrate superior performance in capturing complex view-dependent phenomena,
including intense metallic reflections and intricate textures such as text.
\\ ( https://arxiv.org/abs/2509.15677 ,  38693kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15678
Date: Fri, 19 Sep 2025 06:53:17 GMT   (672kb)

Title: Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation
  for Style Imitation with Diffusion Model
Authors: Sidra Hanif, Longin Jan Latecki
Categories: cs.CV
\\
  Handwriting stroke generation is crucial for improving the performance of
tasks such as handwriting recognition and writers order recovery. In
handwriting stroke generation, it is significantly important to imitate the
sample calligraphic style. The previous studies have suggested utilizing the
calligraphic features of the handwriting. However, they had not considered word
spacing (word layout) as an explicit handwriting feature, which results in
inconsistent word spacing for style imitation. Firstly, this work proposes
multi-scale attention features for calligraphic style imitation. These
multi-scale feature embeddings highlight the local and global style features.
Secondly, we propose to include the words layout, which facilitates word
spacing for handwriting stroke generation. Moreover, we propose a conditional
diffusion model to predict strokes in contrast to previous work, which directly
generated style images. Stroke generation provides additional temporal
coordinate information, which is lacking in image generation. Hence, our
proposed conditional diffusion model for stroke generation is guided by
calligraphic style and word layout for better handwriting imitation and stroke
generation in a calligraphic style. Our experimentation shows that the proposed
diffusion model outperforms the current state-of-the-art stroke generation and
is competitive with recent image generation networks.
\\ ( https://arxiv.org/abs/2509.15678 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15688
Date: Fri, 19 Sep 2025 07:03:37 GMT   (11385kb)

Title: Saccadic Vision for Fine-Grained Visual Classification
Authors: Johann Schmidt, Sebastian Stober, Joachim Denzler, Paul Bodesheim
Categories: cs.CV cs.AI
\\
  Fine-grained visual classification (FGVC) requires distinguishing between
visually similar categories through subtle, localized features - a task that
remains challenging due to high intra-class variability and limited inter-class
differences. Existing part-based methods often rely on complex localization
networks that learn mappings from pixel to sample space, requiring a deep
understanding of image content while limiting feature utility for downstream
tasks. In addition, sampled points frequently suffer from high spatial
redundancy, making it difficult to quantify the optimal number of required
parts. Inspired by human saccadic vision, we propose a two-stage process that
first extracts peripheral features (coarse view) and generates a sample map,
from which fixation patches are sampled and encoded in parallel using a
weight-shared encoder. We employ contextualized selective attention to weigh
the impact of each fixation patch before fusing peripheral and focus
representations. To prevent spatial collapse - a common issue in part-based
methods - we utilize non-maximum suppression during fixation sampling to
eliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks
(CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect
datasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method
achieves comparable performance to state-of-the-art approaches while
consistently outperforming our baseline encoder.
\\ ( https://arxiv.org/abs/2509.15688 ,  11385kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15693
Date: Fri, 19 Sep 2025 07:13:45 GMT   (2558kb)

Title: SCENEFORGE: Enhancing 3D-text alignment with Structured Scene
  Compositions
Authors: Cristian Sbrolli and Matteo Matteucci
Categories: cs.CV cs.MM
Comments: to appear in NeurIPS 2025
\\
  The whole is greater than the sum of its parts-even in 3D-text contrastive
learning. We introduce SceneForge, a novel framework that enhances contrastive
alignment between 3D point clouds and text through structured multi-object
scene compositions. SceneForge leverages individual 3D shapes to construct
multi-object scenes with explicit spatial relations, pairing them with coherent
multi-object descriptions refined by a large language model. By augmenting
contrastive training with these structured, compositional samples, SceneForge
effectively addresses the scarcity of large-scale 3D-text datasets,
significantly enriching data complexity and diversity. We systematically
investigate critical design elements, such as the optimal number of objects per
scene, the proportion of compositional samples in training batches, and scene
construction strategies. Extensive experiments demonstrate that SceneForge
delivers substantial performance gains across multiple tasks, including
zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,
as well as few-shot part segmentation on ShapeNetPart. SceneForge's
compositional augmentations are model-agnostic, consistently improving
performance across multiple encoder architectures. Moreover, SceneForge
improves 3D visual question answering on ScanQA, generalizes robustly to
retrieval scenarios with increasing scene complexity, and showcases spatial
reasoning capabilities by adapting spatial configurations to align precisely
with textual instructions.
\\ ( https://arxiv.org/abs/2509.15693 ,  2558kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15695
Date: Fri, 19 Sep 2025 07:14:29 GMT   (1798kb)

Title: ORIC: Benchmarking Object Recognition in Incongruous Context for Large
  Vision-Language Models
Authors: Zhaoyang Li, Zhan Ling, Yuchen Zhou, Hao Su
Categories: cs.CV cs.LG
\\
  Large Vision-Language Models (LVLMs) have made significant strides in image
caption, visual question answering, and robotics by integrating visual and
textual information. However, they remain prone to errors in incongruous
contexts, where objects appear unexpectedly or are absent when contextually
expected. This leads to two key recognition failures: object misidentification
and hallucination. To systematically examine this issue, we introduce the
Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark
that evaluates LVLMs in scenarios where object-context relationships deviate
from expectations. ORIC employs two key strategies: (1) LLM-guided sampling,
which identifies objects that are present but contextually incongruous, and (2)
CLIP-guided sampling, which detects plausible yet nonexistent objects that are
likely to be hallucinated, thereby creating an incongruous context. Evaluating
18 LVLMs and two open-vocabulary detection models, our results reveal
significant recognition gaps, underscoring the challenges posed by contextual
incongruity. This work provides critical insights into LVLMs' limitations and
encourages further research on context-aware object recognition.
\\ ( https://arxiv.org/abs/2509.15695 ,  1798kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15704
Date: Fri, 19 Sep 2025 07:28:17 GMT   (13239kb)

Title: Training-Free Pyramid Token Pruning for Efficient Large Vision-Language
  Models via Region, Token, and Instruction-Guided Importance
Authors: Yuxuan Liang, Xu Li, Xiaolei Chen, Yi Zheng, Haotian Chen, Bin Li,
  Xiangyang Xue
Categories: cs.CV
\\
  Large Vision-Language Models (LVLMs) have significantly advanced multimodal
understanding but still struggle with efficiently processing high-resolution
images. Recent approaches partition high-resolution images into multiple
sub-images, dramatically increasing the number of visual tokens and causing
exponential computational overhead during inference. To address these
limitations, we propose a training-free token pruning strategy, Pyramid Token
Pruning (PTP), that integrates bottom-up visual saliency at both region and
token levels with top-down instruction-guided importance. Inspired by human
visual attention mechanisms, PTP selectively retains more tokens from visually
salient regions and further leverages textual instructions to pinpoint tokens
most relevant to specific multimodal tasks. Extensive experiments across 13
diverse benchmarks demonstrate that our method substantially reduces
computational overhead and inference latency with minimal performance loss.
\\ ( https://arxiv.org/abs/2509.15704 ,  13239kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15706
Date: Fri, 19 Sep 2025 07:29:23 GMT   (7761kb)

Title: SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction
  on a New Passive Active Satellite Benchmark
Authors: Chi Yang, Fu Wang, Xiaofei Yang, Hao Huang, Weijia Cao, Xiaowen Chu
Categories: cs.CV cs.AI physics.ao-ph
Comments: 9 pages, 4 figures, 2 tables
\\
  Cloud phase profiles are critical for numerical weather prediction (NWP), as
they directly affect radiative transfer and precipitation processes. In this
study, we present a benchmark dataset and a baseline framework for transforming
multimodal satellite observations into detailed 3D cloud phase structures,
aiming toward operational cloud phase profile retrieval and future integration
with NWP systems to improve cloud microphysics parameterization. The multimodal
observations consist of (1) high--spatiotemporal--resolution, multi-band
visible (VIS) and thermal infrared (TIR) imagery from geostationary satellites,
and (2) accurate vertical cloud phase profiles from spaceborne lidar
(CALIOP\slash CALIPSO) and radar (CPR\slash CloudSat). The dataset consists of
synchronized image--profile pairs across diverse cloud regimes, defining a
supervised learning task: given VIS/TIR patches, predict the corresponding 3D
cloud phase structure. We adopt SGMAGNet as the main model and compare it with
several baseline architectures, including UNet variants and SegNet, all
designed to capture multi-scale spatial patterns. Model performance is
evaluated using standard classification metrics, including Precision, Recall,
F1-score, and IoU. The results demonstrate that SGMAGNet achieves superior
performance in cloud phase reconstruction, particularly in complex multi-layer
and boundary transition regions. Quantitatively, SGMAGNet attains a Precision
of 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617,
significantly outperforming all baselines across these key metrics.
\\ ( https://arxiv.org/abs/2509.15706 ,  7761kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15711
Date: Fri, 19 Sep 2025 07:40:08 GMT   (663kb)

Title: Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel
  Method
Authors: Shuaibo Li, Zhaohu Xing, Hongqiu Wang, Pengfei Hao, Xingyu Li, Zekai
  Liu, Lei Zhu
Categories: cs.CV
\\
  The rapid advancement of generative AI in medical imaging has introduced both
significant opportunities and serious challenges, especially the risk that fake
medical images could undermine healthcare systems. These synthetic images pose
serious risks, such as diagnostic deception, financial fraud, and
misinformation. However, research on medical forensics to counter these threats
remains limited, and there is a critical lack of comprehensive datasets
specifically tailored for this field. Additionally, existing media forensic
methods, which are primarily designed for natural or facial images, are
inadequate for capturing the distinct characteristics and subtle artifacts of
AI-generated medical images. To tackle these challenges, we introduce
\textbf{MedForensics}, a large-scale medical forensics dataset encompassing six
medical modalities and twelve state-of-the-art medical generative models. We
also propose \textbf{DSKI}, a novel \textbf{D}ual-\textbf{S}tage
\textbf{K}nowledge \textbf{I}nfusing detector that constructs a vision-language
feature space tailored for the detection of AI-generated medical images. DSKI
comprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for
extracting subtle forgery clues from both spatial and noise domains during
training, and 2) a medical forensic retrieval module (MFRM) that boosts
detection accuracy through few-shot retrieval during testing. Experimental
results demonstrate that DSKI significantly outperforms both existing methods
and human experts, achieving superior accuracy across multiple medical
modalities.
\\ ( https://arxiv.org/abs/2509.15711 ,  663kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15741
Date: Fri, 19 Sep 2025 08:11:34 GMT   (1158kb)

Title: TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic
  Image Detection
Authors: Laixin Zhang, Shuaibo Li, Wei Ma, Hongbin Zha
Categories: cs.CV
\\
  The rapid progress of generative models has made synthetic image detection an
increasingly critical task. Most existing approaches attempt to construct a
single, universal discriminative space to separate real from fake content.
However, such unified spaces tend to be complex and brittle, often struggling
to generalize to unseen generative patterns. In this work, we propose TrueMoE,
a novel dual-routing Mixture-of-Discriminative-Experts framework that
reformulates the detection task as a collaborative inference across multiple
specialized and lightweight discriminative subspaces. At the core of TrueMoE is
a Discriminative Expert Array (DEA) organized along complementary axes of
manifold structure and perceptual granularity, enabling diverse forgery cues to
be captured across subspaces. A dual-routing mechanism, comprising a
granularity-aware sparse router and a manifold-aware dense router, adaptively
assigns input images to the most relevant experts. Extensive experiments across
a wide spectrum of generative models demonstrate that TrueMoE achieves superior
generalization and robustness.
\\ ( https://arxiv.org/abs/2509.15741 ,  1158kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15748
Date: Fri, 19 Sep 2025 08:23:44 GMT   (2024kb)

Title: Hybrid Lie semi-group and cascade structures for the generalized
  Gaussian derivative model for visual receptive fields
Authors: Tony Lindeberg
Categories: cs.CV q-bio.NC
Comments: 25 pages, 9 figures
\\
  Because of the variabilities of real-world image structures under the natural
image transformations that arise when observing similar objects or
spatio-temporal events under different viewing conditions, the receptive field
responses computed in the earliest layers of the visual hierarchy may be
strongly influenced by such geometric image transformations. One way of
handling this variability is by basing the vision system on covariant receptive
field families, which expand the receptive field shapes over the degrees of
freedom in the image transformations.
  This paper addresses the problem of deriving relationships between spatial
and spatio-temporal receptive field responses obtained for different values of
the shape parameters in the resulting multi-parameter families of receptive
fields. For this purpose, we derive both (i) infinitesimal relationships,
roughly corresponding to a combination of notions from semi-groups and Lie
groups, as well as (ii) macroscopic cascade smoothing properties, which
describe how receptive field responses at coarser spatial and temporal scales
can be computed by applying smaller support incremental filters to the output
from corresponding receptive fields at finer spatial and temporal scales,
structurally related to the notion of Lie algebras, although with directional
preferences.
  The presented results provide (i) a deeper understanding of the relationships
between spatial and spatio-temporal receptive field responses for different
values of the filter parameters, which can be used for both (ii) designing more
efficient schemes for computing receptive field responses over populations of
multi-parameter families of receptive fields, as well as (iii)~formulating
idealized theoretical models of the computations of simple cells in biological
vision.
\\ ( https://arxiv.org/abs/2509.15748 ,  2024kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15750
Date: Fri, 19 Sep 2025 08:27:10 GMT   (7847kb)

Title: FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric
  Fusion
Authors: Han Ye, Haofu Wang, Yunchi Zhang, Jiangjian Xiao, Yuqiang Jin, Jinyuan
  Liu, Wen-An Zhang, Uladzislau Sychou, Alexander Tuzikov, Vladislav
  Sobolevskii, Valerii Zakharov, Boris Sokolov, Minglei Fu
Categories: cs.CV cs.AI
Comments: 12 pages, 15 figures,
\\
  Reconstructing building floor plans from point cloud data is key for indoor
navigation, BIM, and precise measurements. Traditional methods like geometric
algorithms and Mask R-CNN-based deep learning often face issues with noise,
limited generalization, and loss of geometric details. We propose FloorSAM, a
framework that integrates point cloud density maps with the Segment Anything
Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using
grid-based filtering, adaptive resolution projection, and image enhancement, we
create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for
precise room segmentation, improving reconstruction across diverse layouts.
Room masks are generated via adaptive prompt points and multistage filtering,
followed by joint mask and point cloud analysis for contour extraction and
regularization. This produces accurate floor plans and recovers room
topological relationships. Tests on Giblayout and ISPRS datasets show better
accuracy, recall, and robustness than traditional methods, especially in noisy
and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.
\\ ( https://arxiv.org/abs/2509.15750 ,  7847kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15751
Date: Fri, 19 Sep 2025 08:28:06 GMT   (8574kb)

Title: Simulated Cortical Magnification Supports Self-Supervised Object
  Learning
Authors: Zhengyang Yu, Arthur Aubret, Chen Yu, Jochen Triesch
Categories: cs.CV
Comments: Accepted at IEEE ICDL 2025. 6 pages, 5 figures
\\
  Recent self-supervised learning models simulate the development of semantic
object representations by training on visual experience similar to that of
toddlers. However, these models ignore the foveated nature of human vision with
high/low resolution in the center/periphery of the visual field. Here, we
investigate the role of this varying resolution in the development of object
representations. We leverage two datasets of egocentric videos that capture the
visual experience of humans during interactions with objects. We apply models
of human foveation and cortical magnification to modify these inputs, such that
the visual content becomes less distinct towards the periphery. The resulting
sequences are used to train two bio-inspired self-supervised learning models
that implement a time-based learning objective. Our results show that modeling
aspects of foveated vision improves the quality of the learned object
representations in this setting. Our analysis suggests that this improvement
comes from making objects appear bigger and inducing a better trade-off between
central and peripheral visual information. Overall, this work takes a step
towards making models of humans' learning of visual representations more
realistic and performant.
\\ ( https://arxiv.org/abs/2509.15751 ,  8574kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15753
Date: Fri, 19 Sep 2025 08:29:33 GMT   (15215kb)

Title: MCOD: The First Challenging Benchmark for Multispectral Camouflaged
  Object Detection
Authors: Yang Li, Tingfa Xu, Shuyan Bai, Peifu Liu, Jianan Li
Categories: cs.CV
\\
  Camouflaged Object Detection (COD) aims to identify objects that blend
seamlessly into natural scenes. Although RGB-based methods have advanced, their
performance remains limited under challenging conditions. Multispectral
imagery, providing rich spectral information, offers a promising alternative
for enhanced foreground-background discrimination. However, existing COD
benchmark datasets are exclusively RGB-based, lacking essential support for
multispectral approaches, which has impeded progress in this area. To address
this gap, we introduce MCOD, the first challenging benchmark dataset
specifically designed for multispectral camouflaged object detection. MCOD
features three key advantages: (i) Comprehensive challenge attributes: It
captures real-world difficulties such as small object sizes and extreme
lighting conditions commonly encountered in COD tasks. (ii) Diverse real-world
scenarios: The dataset spans a wide range of natural environments to better
reflect practical applications. (iii) High-quality pixel-level annotations:
Each image is manually annotated with precise object masks and corresponding
challenge attribute labels. We benchmark eleven representative COD methods on
MCOD, observing a consistent performance drop due to increased task difficulty.
Notably, integrating multispectral modalities substantially alleviates this
degradation, highlighting the value of spectral information in enhancing
detection robustness. We anticipate MCOD will provide a strong foundation for
future research in multispectral camouflaged object detection. The dataset is
publicly accessible at https://github.com/yl2900260-bit/MCOD.
\\ ( https://arxiv.org/abs/2509.15753 ,  15215kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15768
Date: Fri, 19 Sep 2025 08:51:41 GMT   (22735kb)

Title: Overview of PlantCLEF 2024: multi-species plant identification in
  vegetation plot images
Authors: Herve Goeau, Vincent Espitalier, Pierre Bonnet, Alexis Joly
Categories: cs.CV
Comments: 10 pages, 3 figures, CLEF 2024 Conference and Labs of the Evaluation
  Forum, September 09 to 12, 2024, Grenoble, France
\\
  Plot images are essential for ecological studies, enabling standardized
sampling, biodiversity assessment, long-term monitoring and remote, large-scale
surveys. Plot images are typically fifty centimetres or one square meter in
size, and botanists meticulously identify all the species found there. The
integration of AI could significantly improve the efficiency of specialists,
helping them to extend the scope and coverage of ecological studies. To
evaluate advances in this regard, the PlantCLEF 2024 challenge leverages a new
test set of thousands of multi-label images annotated by experts and covering
over 800 species. In addition, it provides a large training set of 1.7 million
individual plant images as well as state-of-the-art vision transformer models
pre-trained on this data. The task is evaluated as a (weakly-labeled)
multi-label classification task where the aim is to predict all the plant
species present on a high-resolution plot image (using the single-label
training data). In this paper, we provide an detailed description of the data,
the evaluation methodology, the methods and models employed by the participants
and the results achieved.
\\ ( https://arxiv.org/abs/2509.15768 ,  22735kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15772
Date: Fri, 19 Sep 2025 08:54:52 GMT   (31450kb)

Title: Vision-Language Models as Differentiable Semantic and Spatial Rewards
  for Text-to-3D Generation
Authors: Weimin Bai, Yubo Li, Weijian Luo, Wenzheng Chen, He Sun
Categories: cs.CV
\\
  Score Distillation Sampling (SDS) enables high-quality text-to-3D generation
by supervising 3D models through the denoising of multi-view 2D renderings,
using a pretrained text-to-image diffusion model to align with the input prompt
and ensure 3D consistency. However, existing SDS-based methods face two
fundamental limitations: (1) their reliance on CLIP-style text encoders leads
to coarse semantic alignment and struggles with fine-grained prompts; and (2)
2D diffusion priors lack explicit 3D spatial constraints, resulting in
geometric inconsistencies and inaccurate object relationships in multi-object
scenes. To address these challenges, we propose VLM3D, a novel text-to-3D
generation framework that integrates large vision-language models (VLMs) into
the SDS pipeline as differentiable semantic and spatial priors. Unlike standard
text-to-image diffusion priors, VLMs leverage rich language-grounded
supervision that enables fine-grained prompt alignment. Moreover, their
inherent vision language modeling provides strong spatial understanding, which
significantly enhances 3D consistency for single-object generation and improves
relational reasoning in multi-object scenes. We instantiate VLM3D based on the
open-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark.
Experiments across diverse objects and complex scenes show that VLM3D
significantly outperforms prior SDS-based methods in semantic fidelity,
geometric coherence, and spatial correctness.
\\ ( https://arxiv.org/abs/2509.15772 ,  31450kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15781
Date: Fri, 19 Sep 2025 09:11:01 GMT   (4330kb)

Title: Enriched Feature Representation and Motion Prediction Module for MOSEv2
  Track of 7th LSVOS Challenge: 3rd Place Solution
Authors: Chang Soo Lim, Joonyoung Moon, Donghyeon Cho
Categories: cs.CV
Comments: 5 pages,2 figures, ICCV Workshop (MOSEv2 Track of 7th LSVOS
  Challenge)
\\
  Video object segmentation (VOS) is a challenging task with wide applications
such as video editing and autonomous driving. While Cutie provides strong
query-based segmentation and SAM2 offers enriched representations via a
pretrained ViT encoder, each has limitations in feature capacity and temporal
modeling. In this report, we propose a framework that integrates their
complementary strengths by replacing the encoder of Cutie with the ViT encoder
of SAM2 and introducing a motion prediction module for temporal stability. We
further adopt an ensemble strategy combining Cutie, SAM2, and our variant,
achieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer to
our final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). This
demonstrates the effectiveness of enriched feature representation and motion
prediction for robust video object segmentation. The code is available at
https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place.
\\ ( https://arxiv.org/abs/2509.15781 ,  4330kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15784
Date: Fri, 19 Sep 2025 09:16:34 GMT   (1532kb)

Title: Ideal Registration? Segmentation is All You Need
Authors: Xiang Chen, Fengting Zhang, Qinghao Liu, Min Liu, Kun Wu, Yaonan Wang,
  Hang Zhang
Categories: cs.CV cs.AI
\\
  Deep learning has revolutionized image registration by its ability to handle
diverse tasks while achieving significant speed advantages over conventional
approaches. Current approaches, however, often employ globally uniform
smoothness constraints that fail to accommodate the complex, regionally varying
deformations characteristic of anatomical motion. To address this limitation,
we propose SegReg, a Segmentation-driven Registration framework that implements
anatomically adaptive regularization by exploiting region-specific deformation
patterns. Our SegReg first decomposes input moving and fixed images into
anatomically coherent subregions through segmentation. These localized domains
are then processed by the same registration backbone to compute optimized
partial deformation fields, which are subsequently integrated into a global
deformation field. SegReg achieves near-perfect structural alignment (98.23%
Dice on critical anatomies) using ground-truth segmentation, and outperforms
existing methods by 2-12% across three clinical registration scenarios
(cardiac, abdominal, and lung images) even with automatic segmentation. Our
SegReg demonstrates a near-linear dependence of registration accuracy on
segmentation quality, transforming the registration challenge into a
segmentation problem. The source code will be released upon manuscript
acceptance.
\\ ( https://arxiv.org/abs/2509.15784 ,  1532kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15785
Date: Fri, 19 Sep 2025 09:16:54 GMT   (222kb)

Title: CBPNet: A Continual Backpropagation Prompt Network for Alleviating
  Plasticity Loss on Edge Devices
Authors: Runjie Shao, Boyu Diao, Zijia An, Ruiqi Liu, Yongjun Xu
Categories: cs.CV cs.AI
\\
  To meet the demands of applications like robotics and autonomous driving that
require real-time responses to dynamic environments, efficient continual
learning methods suitable for edge devices have attracted increasing attention.
In this transition, using frozen pretrained models with prompts has become a
mainstream strategy to combat catastrophic forgetting. However, this approach
introduces a new critical bottleneck: plasticity loss, where the model's
ability to learn new knowledge diminishes due to the frozen backbone and the
limited capacity of prompt parameters. We argue that the reduction in
plasticity stems from a lack of update vitality in underutilized parameters
during the training process. To this end, we propose the Continual
Backpropagation Prompt Network (CBPNet), an effective and parameter efficient
framework designed to restore the model's learning vitality. We innovatively
integrate an Efficient CBP Block that counteracts plasticity decay by
adaptively reinitializing these underutilized parameters. Experimental results
on edge devices demonstrate CBPNet's effectiveness across multiple benchmarks.
On Split CIFAR-100, it improves average accuracy by over 1% against a strong
baseline, and on the more challenging Split ImageNet-R, it achieves a state of
the art accuracy of 69.41%. This is accomplished by training additional
parameters that constitute less than 0.2% of the backbone's size, validating
our approach.
\\ ( https://arxiv.org/abs/2509.15785 ,  222kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15788
Date: Fri, 19 Sep 2025 09:19:57 GMT   (4274kb)

Title: FoBa: A Foreground-Background co-Guided Method and New Benchmark for
  Remote Sensing Semantic Change Detection
Authors: Haotian Zhang, Han Guo, Keyan Chen, Hao Chen, Zhengxia Zou, and
  Zhenwei Shi
Categories: cs.CV
\\
  Despite the remarkable progress achieved in remote sensing semantic change
detection (SCD), two major challenges remain. At the data level, existing SCD
datasets suffer from limited change categories, insufficient change types, and
a lack of fine-grained class definitions, making them inadequate to fully
support practical applications. At the methodological level, most current
approaches underutilize change information, typically treating it as a
post-processing step to enhance spatial consistency, which constrains further
improvements in model performance. To address these issues, we construct a new
benchmark for remote sensing SCD, LevirSCD. Focused on the Beijing area, the
dataset covers 16 change categories and 210 specific change types, with more
fine-grained class definitions (e.g., roads are divided into unpaved and paved
roads). Furthermore, we propose a foreground-background co-guided SCD (FoBa)
method, which leverages foregrounds that focus on regions of interest and
backgrounds enriched with contextual information to guide the model
collaboratively, thereby alleviating semantic ambiguity while enhancing its
ability to detect subtle changes. Considering the requirements of bi-temporal
interaction and spatial consistency in SCD, we introduce a Gated Interaction
Fusion (GIF) module along with a simple consistency loss to further enhance the
model's detection performance. Extensive experiments on three datasets (SECOND,
JL1, and the proposed LevirSCD) demonstrate that FoBa achieves competitive
results compared to current SOTA methods, with improvements of 1.48%, 3.61%,
and 2.81% in the SeK metric, respectively. Our code and dataset are available
at https://github.com/zmoka-zht/FoBa.
\\ ( https://arxiv.org/abs/2509.15788 ,  4274kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15791
Date: Fri, 19 Sep 2025 09:22:19 GMT   (2763kb)

Title: Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization
Authors: Tan Pan, Kaiyu Guo, Dongli Xu, Zhaorui Tan, Chen Jiang, Deshu Chen,
  Xin Guo, Brian C. Lovell, Limei Han, Yuan Cheng, Mahsa Baktashmotlagh
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\
  The generalization ability of deep learning has been extensively studied in
supervised settings, yet it remains less explored in unsupervised scenarios.
Recently, the Unsupervised Domain Generalization (UDG) task has been proposed
to enhance the generalization of models trained with prevalent unsupervised
learning techniques, such as Self-Supervised Learning (SSL). UDG confronts the
challenge of distinguishing semantics from variations without category labels.
Although some recent methods have employed domain labels to tackle this issue,
such domain labels are often unavailable in real-world contexts. In this paper,
we address these limitations by formalizing UDG as the task of learning a
Minimal Sufficient Semantic Representation: a representation that (i) preserves
all semantic information shared across augmented views (sufficiency), and (ii)
maximally removes information irrelevant to semantics (minimality). We
theoretically ground these objectives from the perspective of information
theory, demonstrating that optimizing representations to achieve sufficiency
and minimality directly reduces out-of-distribution risk. Practically, we
implement this optimization through Minimal-Sufficient UDG (MS-UDG), a
learnable model by integrating (a) an InfoNCE-based objective to achieve
sufficiency; (b) two complementary components to promote minimality: a novel
semantic-variation disentanglement loss and a reconstruction-based mechanism
for capturing adequate variation. Empirically, MS-UDG sets a new
state-of-the-art on popular unsupervised domain-generalization benchmarks,
consistently outperforming existing SSL and UDG methods, without category or
domain labels during representation learning.
\\ ( https://arxiv.org/abs/2509.15791 ,  2763kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15795
Date: Fri, 19 Sep 2025 09:24:24 GMT   (1778kb)

Title: TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale
  Remote Sensing Segmentation
Authors: Tianyang Wang, Xi Xiao, Gaofei Chen, Hanzhang Chi, Qi Zhang, Guo
  Cheng, Yingrui Ji
Categories: cs.CV
\\
  Segment Anything Model (SAM) has demonstrated impressive zero-shot
segmentation capabilities across natural image domains, but it struggles to
generalize to the unique challenges of remote sensing data, such as complex
terrain, multi-scale objects, and temporal dynamics. In this paper, we
introduce TASAM, a terrain and temporally-aware extension of SAM designed
specifically for high-resolution remote sensing image segmentation. TASAM
integrates three lightweight yet effective modules: a terrain-aware adapter
that injects elevation priors, a temporal prompt generator that captures
land-cover changes over time, and a multi-scale fusion strategy that enhances
fine-grained object delineation. Without retraining the SAM backbone, our
approach achieves substantial performance gains across three remote sensing
benchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM and
task-specific models with minimal computational overhead. Our results highlight
the value of domain-adaptive augmentation for foundation models and offer a
scalable path toward more robust geospatial segmentation.
\\ ( https://arxiv.org/abs/2509.15795 ,  1778kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15800
Date: Fri, 19 Sep 2025 09:27:24 GMT   (416kb)

Title: ChronoForge-RL: Chronological Forging through Reinforcement Learning for
  Enhanced Video Understanding
Authors: Kehua Chen
Categories: cs.CV cs.AI
Comments: 10 pages, 2 figures
\\
  Current state-of-the-art video understanding methods typically struggle with
two critical challenges: (1) the computational infeasibility of processing
every frame in dense video content and (2) the difficulty in identifying
semantically significant frames through naive uniform sampling strategies. In
this paper, we propose a novel video understanding framework, called
ChronoForge-RL, which combines Temporal Apex Distillation (TAD) and
KeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle these
issues. Concretely, we introduce a differentiable keyframe selection mechanism
that systematically identifies semantic inflection points through a three-stage
process to enhance computational efficiency while preserving temporal
information. Then, two particular modules are proposed to enable effective
temporal reasoning: Firstly, TAD leverages variation scoring, inflection
detection, and prioritized distillation to select the most informative frames.
Secondly, we introduce KF-GRPO which implements a contrastive learning paradigm
with a saliency-enhanced reward mechanism that explicitly incentivizes models
to leverage both frame content and temporal relationships. Finally, our
proposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBench
compared to baseline methods, clearly surpassing previous approaches while
enabling our 7B parameter model to achieve performance comparable to 72B
parameter alternatives.
\\ ( https://arxiv.org/abs/2509.15800 ,  416kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15803
Date: Fri, 19 Sep 2025 09:30:37 GMT   (2524kb)

Title: CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models
Authors: Fangjian Shen, Zifeng Liang, Chao Wang, Wushao Wen
Categories: cs.CV cs.AI
Comments: 5 pages, 7 figures, submitted to ICASSP2026
\\
  Text-to-image (T2I) models exhibit a significant yet under-explored "brand
bias", a tendency to generate contents featuring dominant commercial brands
from generic prompts, posing ethical and legal risks. We propose CIDER, a
novel, model-agnostic framework to mitigate bias at inference-time through
prompt refinement to avoid costly retraining. CIDER uses a lightweight detector
to identify branded content and a Vision-Language Model (VLM) to generate
stylistically divergent alternatives. We introduce the Brand Neutrality Score
(BNS) to quantify this issue and perform extensive experiments on leading T2I
models. Results show CIDER significantly reduces both explicit and implicit
biases while maintaining image quality and aesthetic appeal. Our work offers a
practical solution for more original and equitable content, contributing to the
development of trustworthy generative AI.
\\ ( https://arxiv.org/abs/2509.15803 ,  2524kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15805
Date: Fri, 19 Sep 2025 09:31:59 GMT   (1277kb)

Title: Boosting Active Learning with Knowledge Transfer
Authors: Tianyang Wang, Xi Xiao, Gaofei Chen, Xiaoying Liao, Guo Cheng, Yingrui
  Ji
Categories: cs.CV
\\
  Uncertainty estimation is at the core of Active Learning (AL). Most existing
methods resort to complex auxiliary models and advanced training fashions to
estimate uncertainty for unlabeled data. These models need special design and
hence are difficult to train especially for domain tasks, such as Cryo-Electron
Tomography (cryo-ET) classification in computational biology. To address this
challenge, we propose a novel method using knowledge transfer to boost
uncertainty estimation in AL. Specifically, we exploit the teacher-student mode
where the teacher is the task model in AL and the student is an auxiliary model
that learns from the teacher. We train the two models simultaneously in each AL
cycle and adopt a certain distance between the model outputs to measure
uncertainty for unlabeled data. The student model is task-agnostic and does not
rely on special training fashions (e.g. adversarial), making our method
suitable for various tasks. More importantly, we demonstrate that data
uncertainty is not tied to concrete value of task loss but closely related to
the upper-bound of task loss. We conduct extensive experiments to validate the
proposed method on classical computer vision tasks and cryo-ET challenges. The
results demonstrate its efficacy and efficiency.
\\ ( https://arxiv.org/abs/2509.15805 ,  1277kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15868
Date: Fri, 19 Sep 2025 11:08:24 GMT   (40835kb)

Title: LC-SLab -- An Object-based Deep Learning Framework for Large-scale Land
  Cover Classification from Satellite Imagery and Sparse In-situ Labels
Authors: Johannes Leonhardt, Juergen Gall, Ribana Roscher
Categories: cs.CV
\\
  Large-scale land cover maps generated using deep learning play a critical
role across a wide range of Earth science applications. Open in-situ datasets
from principled land cover surveys offer a scalable alternative to manual
annotation for training such models. However, their sparse spatial coverage
often leads to fragmented and noisy predictions when used with existing deep
learning-based land cover mapping approaches. A promising direction to address
this issue is object-based classification, which assigns labels to semantically
coherent image regions rather than individual pixels, thereby imposing a
minimum mapping unit. Despite this potential, object-based methods remain
underexplored in deep learning-based land cover mapping pipelines, especially
in the context of medium-resolution imagery and sparse supervision. To address
this gap, we propose LC-SLab, the first deep learning framework for
systematically exploring object-based deep learning methods for large-scale
land cover classification under sparse supervision. LC-SLab supports both
input-level aggregation via graph neural networks, and output-level aggregation
by postprocessing results from established semantic segmentation models.
Additionally, we incorporate features from a large pre-trained network to
improve performance on small datasets. We evaluate the framework on annual
Sentinel-2 composites with sparse LUCAS labels, focusing on the tradeoff
between accuracy and fragmentation, as well as sensitivity to dataset size. Our
results show that object-based methods can match or exceed the accuracy of
common pixel-wise models while producing substantially more coherent maps.
Input-level aggregation proves more robust on smaller datasets, whereas
output-level aggregation performs best with more data. Several configurations
of LC-SLab also outperform existing land cover products, highlighting the
framework's practical utility.
\\ ( https://arxiv.org/abs/2509.15868 ,  40835kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15871
Date: Fri, 19 Sep 2025 11:11:36 GMT   (1099kb)

Title: Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval
Authors: Liwei Liao, Xufeng Li, Xiaoyun Zheng, Boning Liu, Feng Gao, Ronggang
  Wang
Categories: cs.CV cs.MM
\\
  3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text
prompts, which is essential for applications such as robotics. However,
existing 3DVG methods encounter two main challenges: first, they struggle to
handle the implicit representation of spatial textures in 3D Gaussian Splatting
(3DGS), making per-scene training indispensable; second, they typically require
larges amounts of labeled data for effective training. To this end, we propose
\underline{G}rounding via \underline{V}iew \underline{R}etrieval (GVR), a novel
zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D
retrieval task that leverages object-level view retrieval to collect grounding
clues from multiple views, which not only avoids the costly process of 3D
annotation, but also eliminates the need for per-scene training. Extensive
experiments demonstrate that our method achieves state-of-the-art visual
grounding performance while avoiding per-scene training, providing a solid
foundation for zero-shot 3DVG research. Video demos can be found in
https://github.com/leviome/GVR_demos.
\\ ( https://arxiv.org/abs/2509.15871 ,  1099kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15874
Date: Fri, 19 Sep 2025 11:20:22 GMT   (9965kb)

Title: ENSAM: an efficient foundation model for interactive segmentation of 3D
  medical images
Authors: Elias Stenhede, Agnar Martin Bj{\o}rnstad, Arian Ranjbar
Categories: cs.CV
\\
  We present ENSAM (Equivariant, Normalized, Segment Anything Model), a
lightweight and promptable model for universal 3D medical image segmentation.
ENSAM combines a SegResNet-based encoder with a prompt encoder and mask decoder
in a U-Net-style architecture, using latent cross-attention, relative
positional encoding, normalized attention, and the Muon optimizer for training.
ENSAM is designed to achieve good performance under limited data and
computational budgets, and is trained from scratch on under 5,000 volumes from
multiple modalities (CT, MRI, PET, ultrasound, microscopy) on a single 32 GB
GPU in 6 hours. As part of the CVPR 2025 Foundation Models for Interactive 3D
Biomedical Image Segmentation Challenge, ENSAM was evaluated on hidden test set
with multimodal 3D medical images, obtaining a DSC AUC of 2.404, NSD AUC of
2.266, final DSC of 0.627, and final NSD of 0.597, outperforming two previously
published baseline models (VISTA3D, SAM-Med3D) and matching the third (SegVol),
surpassing its performance in final DSC but trailing behind in the other three
metrics. In the coreset track of the challenge, ENSAM ranks 5th of 10 overall
and best among the approaches not utilizing pretrained weights. Ablation
studies confirm that our use of relative positional encodings and the Muon
optimizer each substantially speed up convergence and improve segmentation
quality.
\\ ( https://arxiv.org/abs/2509.15874 ,  9965kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15882
Date: Fri, 19 Sep 2025 11:29:22 GMT   (1977kb)

Title: Self-Supervised Cross-Modal Learning for Image-to-Point Cloud
  Registration
Authors: Xingmei Wang, Xiaoyu Hu, Chengkai Huang, Ziyan Zeng, Guohao Nie, Quan
  Z. Sheng, Lina Yao
Categories: cs.CV cs.AI
\\
  Bridging 2D and 3D sensor modalities is critical for robust perception in
autonomous systems. However, image-to-point cloud (I2P) registration remains
challenging due to the semantic-geometric gap between texture-rich but
depth-ambiguous images and sparse yet metrically precise point clouds, as well
as the tendency of existing methods to converge to local optima. To overcome
these limitations, we introduce CrossI2P, a self-supervised framework that
unifies cross-modal learning and two-stage registration in a single end-to-end
pipeline. First, we learn a geometric-semantic fused embedding space via
dual-path contrastive learning, enabling annotation-free, bidirectional
alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine
registration paradigm: a global stage establishes superpoint-superpixel
correspondences through joint intra-modal context and cross-modal interaction
modeling, followed by a geometry-constrained point-level refinement for precise
registration. Third, we employ a dynamic training mechanism with gradient
normalization to balance losses for feature alignment, correspondence
refinement, and pose estimation. Extensive experiments demonstrate that
CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry
benchmark and by 37.9% on nuScenes, significantly improving both accuracy and
robustness.
\\ ( https://arxiv.org/abs/2509.15882 ,  1977kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15883
Date: Fri, 19 Sep 2025 11:29:42 GMT   (1203kb)

Title: RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented
  Image Captioning
Authors: Xiaosheng Long, Hanyu Wang, Zhentao Song, Kun Luo and Hongde Liu
Categories: cs.CV cs.AI
\\
  Recent retrieval-augmented image captioning methods incorporate external
knowledge to compensate for the limitations in comprehending complex scenes.
However, current approaches face challenges in relation modeling: (1) the
representation of semantic prompts is too coarse-grained to capture
fine-grained relationships; (2) these methods lack explicit modeling of image
objects and their semantic relationships. To address these limitations, we
propose RACap, a relation-aware retrieval-augmented model for image captioning,
which not only mines structured relation semantics from retrieval captions, but
also identifies heterogeneous objects from the image. RACap effectively
retrieves structured relation features that contain heterogeneous visual
information to enhance the semantic consistency and relational expressiveness.
Experimental results show that RACap, with only 10.8M trainable parameters,
achieves superior performance compared to previous lightweight captioning
models.
\\ ( https://arxiv.org/abs/2509.15883 ,  1203kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15886
Date: Fri, 19 Sep 2025 11:33:10 GMT   (2830kb)

Title: RangeSAM: Leveraging Visual Foundation Models for Range-View repesented
  LiDAR segmentation
Authors: Paul Julius K\"uhn, Duc Anh Nguyen, Arjan Kuijper, Holger Graf, Dieter
  Fellner, Saptarshi Neil Sinha
Categories: cs.CV
\\
  Point cloud segmentation is central to autonomous driving and 3D scene
understanding. While voxel- and point-based methods dominate recent research
due to their compatibility with deep architectures and ability to capture
fine-grained geometry, they often incur high computational cost, irregular
memory access, and limited real-time efficiency. In contrast, range-view
methods, though relatively underexplored - can leverage mature 2D semantic
segmentation techniques for fast and accurate predictions. Motivated by the
rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot
recognition, and multimodal tasks, we investigate whether SAM2, the current
state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for
LiDAR point cloud segmentation in the range view. We present , to our
knowledge, the first range-view framework that adapts SAM2 to 3D segmentation,
coupling efficient 2D feature extraction with standard
projection/back-projection to operate on point clouds. To optimize SAM2 for
range-view representations, we implement several architectural modifications to
the encoder: (1) a novel module that emphasizes horizontal spatial dependencies
inherent in LiDAR range images, (2) a customized configuration of tailored to
the geometric properties of spherical projections, and (3) an adapted mechanism
in the encoder backbone specifically designed to capture the unique spatial
patterns and discontinuities present in range-view pseudo-images. Our approach
achieves competitive performance on SemanticKITTI while benefiting from the
speed, scalability, and deployment simplicity of 2D-centric pipelines. This
work highlights the viability of VFMs as general-purpose backbones for 3D
perception and opens a path toward unified, foundation-model-driven LiDAR
segmentation. Results lets us conclude that range-view segmentation methods
using VFMs leads to promising results.
\\ ( https://arxiv.org/abs/2509.15886 ,  2830kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15891
Date: Fri, 19 Sep 2025 11:42:07 GMT   (1258kb)

Title: Global Regulation and Excitation via Attention Tuning for Stereo
  Matching
Authors: Jiahao Li, Xinhong Chen, Zhengmin Jiang, Qian Zhou, Yung-Hui Li,
  Jianping Wang
Categories: cs.CV
Comments: International Conference on Computer Vision (ICCV 2025)
\\
  Stereo matching achieves significant progress with iterative algorithms like
RAFT-Stereo and IGEV-Stereo. However, these methods struggle in ill-posed
regions with occlusions, textureless, or repetitive patterns, due to a lack of
global context and geometric information for effective iterative refinement. To
enable the existing iterative approaches to incorporate global context, we
propose the Global Regulation and Excitation via Attention Tuning (GREAT)
framework which encompasses three attention modules. Specifically, Spatial
Attention (SA) captures the global context within the spatial dimension,
Matching Attention (MA) extracts global context along epipolar lines, and
Volume Attention (VA) works in conjunction with SA and MA to construct a more
robust cost-volume excited by global context and geometric details. To verify
the universality and effectiveness of this framework, we integrate it into
several representative iterative stereo-matching methods and validate it
through extensive experiments, collectively denoted as GREAT-Stereo. This
framework demonstrates superior performance in challenging ill-posed regions.
Applied to IGEV-Stereo, among all published methods, our GREAT-IGEV ranks first
on the Scene Flow test set, KITTI 2015, and ETH3D leaderboards, and achieves
second on the Middlebury benchmark. Code is available at
https://github.com/JarvisLee0423/GREAT-Stereo.
\\ ( https://arxiv.org/abs/2509.15891 ,  1258kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15905
Date: Fri, 19 Sep 2025 12:03:18 GMT   (304kb)

Title: Deep Feedback Models
Authors: David Calhas, Arlindo L. Oliveira
Categories: cs.CV
\\
  Deep Feedback Models (DFMs) are a new class of stateful neural networks that
combine bottom up input with high level representations over time. This
feedback mechanism introduces dynamics into otherwise static architectures,
enabling DFMs to iteratively refine their internal state and mimic aspects of
biological decision making. We model this process as a differential equation
solved through a recurrent neural network, stabilized via exponential decay to
ensure convergence. To evaluate their effectiveness, we measure DFMs under two
key conditions: robustness to noise and generalization with limited data. In
both object recognition and segmentation tasks, DFMs consistently outperform
their feedforward counterparts, particularly in low data or high noise regimes.
In addition, DFMs translate to medical imaging settings, while being robust
against various types of noise corruption. These findings highlight the
importance of feedback in achieving stable, robust, and generalizable learning.
Code is available at https://github.com/DCalhas/deep_feedback_models.
\\ ( https://arxiv.org/abs/2509.15905 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15924
Date: Fri, 19 Sep 2025 12:22:24 GMT   (12768kb)

Title: Sparse Multiview Open-Vocabulary 3D Detection
Authors: Olivier Moliner, Viktor Larsson, Kalle {\AA}str\"om
Categories: cs.CV
Comments: ICCV 2025; OpenSUN3D Workshop; Camera ready version
\\
  The ability to interpret and comprehend a 3D scene is essential for many
vision and robotics systems. In numerous applications, this involves 3D object
detection, i.e.~identifying the location and dimensions of objects belonging to
a specific category, typically represented as bounding boxes. This has
traditionally been solved by training to detect a fixed set of categories,
which limits its use. In this work, we investigate open-vocabulary 3D object
detection in the challenging yet practical sparse-view setting, where only a
limited number of posed RGB images are available as input. Our approach is
training-free, relying on pre-trained, off-the-shelf 2D foundation models
instead of employing computationally expensive 3D feature fusion or requiring
3D-specific learning. By lifting 2D detections and directly optimizing 3D
proposals for featuremetric consistency across views, we fully leverage the
extensive training data available in 2D compared to 3D. Through standard
benchmarks, we demonstrate that this simple pipeline establishes a powerful
baseline, performing competitively with state-of-the-art techniques in densely
sampled scenarios while significantly outperforming them in the sparse-view
setting.
\\ ( https://arxiv.org/abs/2509.15924 ,  12768kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15935
Date: Fri, 19 Sep 2025 12:40:49 GMT   (8546kb)

Title: PAN: Pillars-Attention-Based Network for 3D Object Detection
Authors: Ruan Bispo, Dane Mitrev, Letizia Mariotti, Cl\'ement Botty, Denver
  Humphrey, Anthony Scanlan, Ciar\'an Eising
Categories: cs.CV
\\
  Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar
fusion for the 3D object detection task in real-time under adverse weather and
lighting conditions. However, currently, in the literature, it is possible to
find few works focusing on this modality and, most importantly, developing new
architectures to explore the advantages of the radar point cloud, such as
accurate distance estimation and speed information. Therefore, this work
presents a novel and efficient 3D object detection algorithm using cameras and
radars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of
radar before fusing the features into a detection head. A new backbone is
introduced, which maps the radar pillar features into an embedded dimension. A
self-attention mechanism allows the backbone to model the dependencies between
the radar points. We are using a simplified convolutional layer to replace the
FPN-based convolutional layers used in the PointPillars-based architectures
with the main goal of reducing inference time. Our results show that with this
modification, our approach achieves the new state-of-the-art in the 3D object
detection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,
while also setting a new benchmark for inference time on the nuScenes dataset
for the same category.
\\ ( https://arxiv.org/abs/2509.15935 ,  8546kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15966
Date: Fri, 19 Sep 2025 13:24:33 GMT   (7066kb)

Title: A multi-temporal multi-spectral attention-augmented deep convolution
  neural network with contrastive learning for crop yield prediction
Authors: Shalini Dangi, Surya Karthikeya Mullapudi, Chandravardhan Singh
  Raghaw, Shahid Shafi Dar, Mohammad Zia Ur Rehman, Nagendra Kumar
Categories: cs.CV
Comments: Published in Computers and Electronics in Agriculture
DOI: 10.1016/j.compag.2025.110895
\\
  Precise yield prediction is essential for agricultural sustainability and
food security. However, climate change complicates accurate yield prediction by
affecting major factors such as weather conditions, soil fertility, and farm
management systems. Advances in technology have played an essential role in
overcoming these challenges by leveraging satellite monitoring and data
analysis for precise yield estimation. Current methods rely on spatio-temporal
data for predicting crop yield, but they often struggle with multi-spectral
data, which is crucial for evaluating crop health and growth patterns. To
resolve this challenge, we propose a novel Multi-Temporal Multi-Spectral Yield
Prediction Network, MTMS-YieldNet, that integrates spectral data with
spatio-temporal information to effectively capture the correlations and
dependencies between them. While existing methods that rely on pre-trained
models trained on general visual data, MTMS-YieldNet utilizes contrastive
learning for feature discrimination during pre-training, focusing on capturing
spatial-spectral patterns and spatio-temporal dependencies from remote sensing
data. Both quantitative and qualitative assessments highlight the excellence of
the proposed MTMS-YieldNet over seven existing state-of-the-art methods.
MTMS-YieldNet achieves MAPE scores of 0.336 on Sentinel-1, 0.353 on Landsat-8,
and an outstanding 0.331 on Sentinel-2, demonstrating effective yield
prediction performance across diverse climatic and seasonal conditions. The
outstanding performance of MTMS-YieldNet improves yield predictions and
provides valuable insights that can assist farmers in making better decisions,
potentially improving crop yields.
\\ ( https://arxiv.org/abs/2509.15966 ,  7066kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15980
Date: Fri, 19 Sep 2025 13:45:18 GMT   (3076kb)

Title: Shedding Light on Depth: Explainability Assessment in Monocular Depth
  Estimation
Authors: Lorenzo Cirillo, Claudio Schiavella, Lorenzo Papa, Paolo Russo, Irene
  Amerini
Categories: cs.CV cs.AI
Comments: 8 pages, 3 figures, 2 tables. This paper has been accepted at the
  International Joint Conference on Neural Networks (IJCNN) 2025
\\
  Explainable artificial intelligence is increasingly employed to understand
the decision-making process of deep learning models and create trustworthiness
in their adoption. However, the explainability of Monocular Depth Estimation
(MDE) remains largely unexplored despite its wide deployment in real-world
applications. In this work, we study how to analyze MDE networks to map the
input image to the predicted depth map. More in detail, we investigate
well-established feature attribution methods, Saliency Maps, Integrated
Gradients, and Attention Rollout on different computationally complex models
for MDE: METER, a lightweight network, and PixelFormer, a deep network. We
assess the quality of the generated visual explanations by selectively
perturbing the most relevant and irrelevant pixels, as identified by the
explainability methods, and analyzing the impact of these perturbations on the
model's output. Moreover, since existing evaluation metrics can have some
limitations in measuring the validity of visual explanations for MDE, we
additionally introduce the Attribution Fidelity. This metric evaluates the
reliability of the feature attribution by assessing their consistency with the
predicted depth map. Experimental results demonstrate that Saliency Maps and
Integrated Gradients have good performance in highlighting the most important
input features for MDE lightweight and deep models, respectively. Furthermore,
we show that Attribution Fidelity effectively identifies whether an
explainability method fails to produce reliable visual maps, even in scenarios
where conventional metrics might suggest satisfactory results.
\\ ( https://arxiv.org/abs/2509.15980 ,  3076kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15984
Date: Fri, 19 Sep 2025 13:50:49 GMT   (5245kb)

Title: CoPAD : Multi-source Trajectory Fusion and Cooperative Trajectory
  Prediction with Anchor-oriented Decoder in V2X Scenarios
Authors: Kangyu Wu, Jiaqi Qiao, Ya Zhang
Categories: cs.CV cs.MA cs.RO
Comments: 7 pages, 4 pages, IROS2025
\\
  Recently, data-driven trajectory prediction methods have achieved remarkable
results, significantly advancing the development of autonomous driving.
However, the instability of single-vehicle perception introduces certain
limitations to trajectory prediction. In this paper, a novel lightweight
framework for cooperative trajectory prediction, CoPAD, is proposed. This
framework incorporates a fusion module based on the Hungarian algorithm and
Kalman filtering, along with the Past Time Attention (PTA) module, mode
attention module and anchor-oriented decoder (AoD). It effectively performs
early fusion on multi-source trajectory data from vehicles and road
infrastructure, enabling the trajectories with high completeness and accuracy.
The PTA module can efficiently capture potential interaction information among
historical trajectories, and the mode attention module is proposed to enrich
the diversity of predictions. Additionally, the decoder based on sparse anchors
is designed to generate the final complete trajectories. Extensive experiments
show that CoPAD achieves the state-of-the-art performance on the DAIR-V2X-Seq
dataset, validating the effectiveness of the model in cooperative trajectory
prediction in V2X scenarios.
\\ ( https://arxiv.org/abs/2509.15984 ,  5245kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15987
Date: Fri, 19 Sep 2025 13:53:51 GMT   (1544kb)

Title: Towards Sharper Object Boundaries in Self-Supervised Depth Estimation
Authors: Aur\'elien Cecille, Stefan Duffner, Franck Davoine, R\'emi Agier,
  Thibault Neveu
Categories: cs.CV cs.AI cs.RO
Comments: BMVC 2025 Oral, 10 pages, 6 figures
\\
  Accurate monocular depth estimation is crucial for 3D scene understanding,
but existing methods often blur depth at object boundaries, introducing
spurious intermediate 3D points. While achieving sharp edges usually requires
very fine-grained supervision, our method produces crisp depth discontinuities
using only self-supervision. Specifically, we model per-pixel depth as a
mixture distribution, capturing multiple plausible depths and shifting
uncertainty from direct regression to the mixture weights. This formulation
integrates seamlessly into existing pipelines via variance-aware loss functions
and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show
that our method achieves up to 35% higher boundary sharpness and improves point
cloud quality compared to state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2509.15987 ,  1544kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15990
Date: Fri, 19 Sep 2025 13:57:29 GMT   (1134kb)

Title: DAFTED: Decoupled Asymmetric Fusion of Tabular and Echocardiographic
  Data for Cardiac Hypertension Diagnosis
Authors: J\'er\'emie Stym-Popper, Nathan Painchaud, Cl\'ement Rambour,
  Pierre-Yves Courand, Nicolas Thome, Olivier Bernard
Categories: cs.CV
Comments: 9 pages, Accepted at MIDL 2025 (Oral)
\\
  Multimodal data fusion is a key approach for enhancing diagnosis in medical
applications. We propose an asymmetric fusion strategy starting from a primary
modality and integrating secondary modalities by disentangling shared and
modality-specific information. Validated on a dataset of 239 patients with
echocardiographic time series and tabular records, our model outperforms
existing methods, achieving an AUC over 90%. This improvement marks a crucial
benchmark for clinical use.
\\ ( https://arxiv.org/abs/2509.15990 ,  1134kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16011
Date: Fri, 19 Sep 2025 14:24:48 GMT   (414kb)

Title: Towards Robust Visual Continual Learning with Multi-Prototype
  Supervision
Authors: Xiwei Liu, Yulong Li, Yichen Li, Xinlin Zhuang, Haolin Yang, Huifa Li,
  Imran Razzak
Categories: cs.CV
\\
  Language-guided supervision, which utilizes a frozen semantic target from a
Pretrained Language Model (PLM), has emerged as a promising paradigm for visual
Continual Learning (CL). However, relying on a single target introduces two
critical limitations: 1) semantic ambiguity, where a polysemous category name
results in conflicting visual representations, and 2) intra-class visual
diversity, where a single prototype fails to capture the rich variety of visual
appearances within a class. To this end, we propose MuproCL, a novel framework
that replaces the single target with multiple, context-aware prototypes.
Specifically, we employ a lightweight LLM agent to perform category
disambiguation and visual-modal expansion to generate a robust set of semantic
prototypes. A LogSumExp aggregation mechanism allows the vision model to
adaptively align with the most relevant prototype for a given image. Extensive
experiments across various CL baselines demonstrate that MuproCL consistently
enhances performance and robustness, establishing a more effective path for
language-guided continual learning.
\\ ( https://arxiv.org/abs/2509.16011 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16017
Date: Fri, 19 Sep 2025 14:26:25 GMT   (3906kb)

Title: DistillMatch: Leveraging Knowledge Distillation from Vision Foundation
  Model for Multimodal Image Matching
Authors: Meng Yang, Fan Fan, Zizhuo Li, Songchu Deng, Yong Ma, Jiayi Ma
Categories: cs.CV
Comments: 10 pages, 4 figures, 3 tables
ACM-class: I.4.3; I.5.2
\\
  Multimodal image matching seeks pixel-level correspondences between images of
different modalities, crucial for cross-modal perception, fusion and analysis.
However, the significant appearance differences between modalities make this
task challenging. Due to the scarcity of high-quality annotated datasets,
existing deep learning methods that extract modality-common features for
matching perform poorly and lack adaptability to diverse scenarios. Vision
Foundation Model (VFM), trained on large-scale data, yields generalizable and
robust feature representations adapted to data and tasks of various modalities,
including multimodal matching. Thus, we propose DistillMatch, a multimodal
image matching method using knowledge distillation from VFM. DistillMatch
employs knowledge distillation to build a lightweight student model that
extracts high-level semantic features from VFM (including DINOv2 and DINOv3) to
assist matching across modalities. To retain modality-specific information, it
extracts and injects modality category information into the other modality's
features, which enhances the model's understanding of cross-modal correlations.
Furthermore, we design V2I-GAN to boost the model's generalization by
translating visible to pseudo-infrared images for data augmentation.
Experiments show that DistillMatch outperforms existing algorithms on public
datasets.
\\ ( https://arxiv.org/abs/2509.16017 ,  3906kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16022
Date: Fri, 19 Sep 2025 14:31:40 GMT   (8046kb)

Title: Generalized Deep Multi-view Clustering via Causal Learning with
  Partially Aligned Cross-view Correspondence
Authors: Xihong Yang, Siwei Wang, Jiaqi Jin, Fangdi Wang, Tianrui Liu, Yueming
  Jin, Xinwang Liu, En Zhu, Kunlun He
Categories: cs.CV
\\
  Multi-view clustering (MVC) aims to explore the common clustering structure
across multiple views. Many existing MVC methods heavily rely on the assumption
of view consistency, where alignments for corresponding samples across
different views are ordered in advance. However, real-world scenarios often
present a challenge as only partial data is consistently aligned across
different views, restricting the overall clustering performance. In this work,
we consider the model performance decreasing phenomenon caused by data order
shift (i.e., from fully to partially aligned) as a generalized multi-view
clustering problem. To tackle this problem, we design a causal multi-view
clustering network, termed CauMVC. We adopt a causal modeling approach to
understand multi-view clustering procedure. To be specific, we formulate the
partially aligned data as an intervention and multi-view clustering with
partially aligned data as an post-intervention inference. However, obtaining
invariant features directly can be challenging. Thus, we design a Variational
Auto-Encoder for causal learning by incorporating an encoder from existing
information to estimate the invariant features. Moreover, a decoder is designed
to perform the post-intervention inference. Lastly, we design a contrastive
regularizer to capture sample correlations. To the best of our knowledge, this
paper is the first work to deal generalized multi-view clustering via causal
learning. Empirical experiments on both fully and partially aligned data
illustrate the strong generalization and effectiveness of CauMVC.
\\ ( https://arxiv.org/abs/2509.16022 ,  8046kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16031
Date: Fri, 19 Sep 2025 14:36:01 GMT   (10410kb)

Title: GLip: A Global-Local Integrated Progressive Framework for Robust Visual
  Speech Recognition
Authors: Tianyue Wang, Shuang Yang, Shiguang Shan and Xilin Chen
Categories: cs.CV
\\
  Visual speech recognition (VSR), also known as lip reading, is the task of
recognizing speech from silent video. Despite significant advancements in VSR
over recent decades, most existing methods pay limited attention to real-world
visual challenges such as illumination variations, occlusions, blurring, and
pose changes. To address these challenges, we propose GLip, a Global-Local
Integrated Progressive framework designed for robust VSR. GLip is built upon
two key insights: (i) learning an initial \textit{coarse} alignment between
visual features across varying conditions and corresponding speech content
facilitates the subsequent learning of \textit{precise} visual-to-speech
mappings in challenging environments; (ii) under adverse conditions, certain
local regions (e.g., non-occluded areas) often exhibit more discriminative cues
for lip reading than global features. To this end, GLip introduces a dual-path
feature extraction architecture that integrates both global and local features
within a two-stage progressive learning framework. In the first stage, the
model learns to align both global and local visual features with corresponding
acoustic speech units using easily accessible audio-visual data, establishing a
coarse yet semantically robust foundation. In the second stage, we introduce a
Contextual Enhancement Module (CEM) to dynamically integrate local features
with relevant global context across both spatial and temporal dimensions,
refining the coarse representations into precise visual-speech mappings. Our
framework uniquely exploits discriminative local regions through a progressive
learning strategy, demonstrating enhanced robustness against various visual
challenges and consistently outperforming existing methods on the LRS2 and LRS3
benchmarks. We further validate its effectiveness on a newly introduced
challenging Mandarin dataset.
\\ ( https://arxiv.org/abs/2509.16031 ,  10410kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16050
Date: Fri, 19 Sep 2025 14:59:51 GMT   (762kb)

Title: Graph-based Point Cloud Surface Reconstruction using B-Splines
Authors: Stuti Pathak, Rhys G. Evans, Gunther Steenackers, and Rudi Penne
Categories: cs.CV
\\
  Generating continuous surfaces from discrete point cloud data is a
fundamental task in several 3D vision applications. Real-world point clouds are
inherently noisy due to various technical and environmental factors. Existing
data-driven surface reconstruction algorithms rely heavily on ground truth
normals or compute approximate normals as an intermediate step. This dependency
makes them extremely unreliable for noisy point cloud datasets, even if the
availability of ground truth training data is ensured, which is not always the
case. B-spline reconstruction techniques provide compact surface
representations of point clouds and are especially known for their smoothening
properties. However, the complexity of the surfaces approximated using
B-splines is directly influenced by the number and location of the spline
control points. Existing spline-based modeling methods predict the locations of
a fixed number of control points for a given point cloud, which makes it very
difficult to match the complexity of its underlying surface. In this work, we
develop a Dictionary-Guided Graph Convolutional Network-based surface
reconstruction strategy where we simultaneously predict both the location and
the number of control points for noisy point cloud data to generate smooth
surfaces without the use of any point normals. We compare our reconstruction
method with several well-known as well as recent baselines by employing
widely-used evaluation metrics, and demonstrate that our method outperforms all
of them both qualitatively and quantitatively.
\\ ( https://arxiv.org/abs/2509.16050 ,  762kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16054
Date: Fri, 19 Sep 2025 15:05:45 GMT   (1773kb)

Title: Language-Instructed Reasoning for Group Activity Detection via
  Multimodal Large Language Model
Authors: Jihua Peng, Qianxiong Xu, Yichen Liu, Chenxi Liu, Cheng Long, Rui
  Zhao, Ziyue Li
Categories: cs.CV
Comments: 9 pages, 5 figures
\\
  Group activity detection (GAD) aims to simultaneously identify group members
and categorize their collective activities within video sequences. Existing
deep learning-based methods develop specialized architectures (e.g.,
transformer networks) to model the dynamics of individual roles and semantic
dependencies between individuals and groups. However, they rely solely on
implicit pattern recognition from visual features and struggle with contextual
reasoning and explainability. In this work, we propose LIR-GAD, a novel
framework of language-instructed reasoning for GAD via Multimodal Large
Language Model (MLLM). Our approach expand the original vocabulary of MLLM by
introducing an activity-level <ACT> token and multiple cluster-specific <GROUP>
tokens. We process video frames alongside two specially designed tokens and
language instructions, which are then integrated into the MLLM. The pretrained
commonsense knowledge embedded in the MLLM enables the <ACT> token and <GROUP>
tokens to effectively capture the semantic information of collective activities
and learn distinct representational features of different groups, respectively.
Also, we introduce a multi-label classification loss to further enhance the
<ACT> token's ability to learn discriminative semantic representations. Then,
we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates
MLLM's hidden embeddings corresponding to the designed tokens with visual
features, significantly enhancing the performance of GAD. Both quantitative and
qualitative experiments demonstrate the superior performance of our proposed
method in GAD taks.
\\ ( https://arxiv.org/abs/2509.16054 ,  1773kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16087
Date: Fri, 19 Sep 2025 15:30:26 GMT   (5139kb)

Title: See&Trek: Training-Free Spatial Prompting for Multimodal Large Language
  Model
Authors: Pengteng Li and Pinhao Song and Wuyang Li and Weiyu Guo and Huizai Yao
  and Yijie Xu and Dugang Liu and Hui Xiong
Categories: cs.CV cs.AI
Comments: Accepted by NeurIPS 2025
\\
  We introduce SEE&TREK, the first training-free prompting framework tailored
to enhance the spatial understanding of Multimodal Large Language Models
(MLLMS) under vision-only constraints. While prior efforts have incorporated
modalities like depth or point clouds to improve spatial reasoning, purely
visualspatial understanding remains underexplored. SEE&TREK addresses this gap
by focusing on two core principles: increasing visual diversity and motion
reconstruction. For visual diversity, we conduct Maximum Semantic Richness
Sampling, which employs an off-the-shell perception model to extract
semantically rich keyframes that capture scene structure. For motion
reconstruction, we simulate visual trajectories and encode relative spatial
positions into keyframes to preserve both spatial relations and temporal
coherence. Our method is training&GPU-free, requiring only a single forward
pass, and can be seamlessly integrated into existing MLLM'S. Extensive
experiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently
boosts various MLLM S performance across diverse spatial reasoning tasks with
the most +3.5% improvement, offering a promising path toward stronger spatial
intelligence.
\\ ( https://arxiv.org/abs/2509.16087 ,  5139kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16091
Date: Fri, 19 Sep 2025 15:35:07 GMT   (15466kb)

Title: Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising
Authors: Shen Cheng, Haipeng Li, Haibin Huang, Xiaohong Liu, Shuaicheng Liu
Categories: cs.CV
\\
  In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised
framework for real-world image denoising. Our approach addresses two major
challenges: the limitations of blind-spot networks (BSNs), which often
sacrifice local detail and introduce pixel discontinuities due to spatial
independence assumptions, and the difficulty of adapting diffusion models to
self-supervised denoising. We propose a dual-branch diffusion framework that
combines a BSN-based diffusion branch, generating semi-clean images, with a
conventional diffusion branch that captures underlying noise distributions. To
enable effective training without paired data, we use the BSN-based branch to
guide the sampling process, capturing noise structure while preserving local
details. Extensive experiments on the SIDD and DND datasets demonstrate
state-of-the-art performance, establishing our method as a highly effective
self-supervised solution for real-world denoising. Code and pre-trained models
are released at: https://github.com/Sumching/BSGD.
\\ ( https://arxiv.org/abs/2509.16091 ,  15466kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16095
Date: Fri, 19 Sep 2025 15:38:27 GMT   (4819kb)

Title: AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent
  Trajectory Modeling in Sports
Authors: Yi Xu, Yun Fu
Categories: cs.CV
Comments: Accepted by ICDM 2025
\\
  Trajectory prediction in multi-agent sports scenarios is inherently
challenging due to the structural heterogeneity across agent roles (e.g.,
players vs. ball) and dynamic distribution gaps across different sports
domains. Existing unified frameworks often fail to capture these structured
distributional shifts, resulting in suboptimal generalization across roles and
domains. We propose AdaSports-Traj, an adaptive trajectory modeling framework
that explicitly addresses both intra-domain and inter-domain distribution
discrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and
Domain-Aware Adapter to conditionally adjust latent representations based on
agent identity and domain context. Additionally, we introduce a Hierarchical
Contrastive Learning objective, which separately supervises role-sensitive and
domain-aware representations to encourage disentangled latent structures
without introducing optimization conflict. Experiments on three diverse sports
datasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness
of our adaptive design, achieving strong performance in both unified and
cross-domain trajectory prediction settings.
\\ ( https://arxiv.org/abs/2509.16095 ,  4819kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16098
Date: Fri, 19 Sep 2025 15:41:10 GMT   (15602kb)

Title: SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and
  Object-Level 2D Features
Authors: Jinyuan Qu, Hongyang Li, Xingyu Chen, Shilong Liu, Yukai Shi, Tianhe
  Ren, Ruitao Jing, Lei Zhang
Categories: cs.CV
\\
  In this paper, we present SegDINO3D, a novel Transformer encoder-decoder
framework for 3D instance segmentation. As 3D training data is generally not as
sufficient as 2D training images, SegDINO3D is designed to fully leverage 2D
representation from a pre-trained 2D detection model, including both
image-level and object-level features, for improving 3D representation.
SegDINO3D takes both a point cloud and its associated 2D images as input. In
the encoder stage, it first enriches each 3D point by retrieving 2D image
features from its corresponding image views and then leverages a 3D encoder for
3D context fusion. In the decoder stage, it formulates 3D object queries as 3D
anchor boxes and performs cross-attention from 3D queries to 2D object queries
obtained from 2D images using the 2D detection model. These 2D object queries
serve as a compact object-level representation of 2D images, effectively
avoiding the challenge of keeping thousands of image feature maps in the memory
while faithfully preserving the knowledge of the pre-trained 2D model. The
introducing of 3D box queries also enables the model to modulate
cross-attention using the predicted boxes for more precise querying. SegDINO3D
achieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D
instance segmentation benchmarks. Notably, on the challenging ScanNet200
dataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAP
on the validation and hidden test sets, respectively, demonstrating its
superiority.
\\ ( https://arxiv.org/abs/2509.16098 ,  15602kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16119
Date: Fri, 19 Sep 2025 16:13:09 GMT   (3847kb)

Title: RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D
  Detector with 4D Automotive Radars
Authors: Weiyi Xiong, Bing Zhu, Tao Huang, Zewei Zheng
Categories: cs.CV
\\
  4D automotive radars have gained increasing attention for autonomous driving
due to their low cost, robustness, and inherent velocity measurement
capability. However, existing 4D radar-based 3D detectors rely heavily on
pillar encoders for BEV feature extraction, where each point contributes to
only a single BEV grid, resulting in sparse feature maps and degraded
representation quality. In addition, they also optimize bounding box attributes
independently, leading to sub-optimal detection accuracy. Moreover, their
inference speed, while sufficient for high-end GPUs, may fail to meet the
real-time requirement on vehicle-mounted embedded devices. To overcome these
limitations, an efficient and effective Gaussian-based 3D detector, namely
RadarGaussianDet3D is introduced, leveraging Gaussian primitives and
distributions as intermediate representations for radar points and bounding
boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed
to transform each point into a Gaussian primitive after feature aggregation and
employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,
yielding denser feature maps. PGE exhibits exceptionally low latency, owing to
the optimized algorithm for point feature aggregation and fast rendering of
3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts
bounding boxes into 3D Gaussian distributions and measures their distance to
enable more comprehensive and consistent optimization. Extensive experiments on
TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves
state-of-the-art detection accuracy while delivering substantially faster
inference, highlighting its potential for real-time deployment in autonomous
driving.
\\ ( https://arxiv.org/abs/2509.16119 ,  3847kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16127
Date: Fri, 19 Sep 2025 16:25:26 GMT   (1565kb)

Title: BaseReward: A Strong Baseline for Multimodal Reward Model
Authors: Yi-Fan Zhang, Haihua Yang, Huanyu Zhang, Yang Shi, Zezhou Chen,
  Haochen Tian, Chaoyou Fu, Haotian Wang, Kai Wu, Bo Cui, Xu Wang, Jianfei Pan,
  Haotian Wang, Zhang Zhang, Liang Wang
Categories: cs.CV
\\
  The rapid advancement of Multimodal Large Language Models (MLLMs) has made
aligning them with human preferences a critical challenge. Reward Models (RMs)
are a core technology for achieving this goal, but a systematic guide for
building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking
in both academia and industry. Through exhaustive experimental analysis, this
paper aims to provide a clear ``recipe'' for constructing high-performance
MRMs. We systematically investigate every crucial component in the MRM
development pipeline, including \textit{reward modeling paradigms} (e.g.,
Naive-RM, Critic-based RM, and Generative RM), \textit{reward head
architecture}, \textit{training strategies}, \textit{data curation} (covering
over ten multimodal and text-only preference datasets), \textit{backbone model}
and \textit{model scale}, and \textit{ensemble methods}.
  Based on these experimental insights, we introduce \textbf{BaseReward}, a
powerful and efficient baseline for multimodal reward modeling. BaseReward
adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone,
featuring an optimized two-layer reward head, and is trained on a carefully
curated mixture of high-quality multimodal and text-only preference data. Our
results show that BaseReward establishes a new SOTA on major benchmarks such as
MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench,
outperforming previous models. Furthermore, to validate its practical utility
beyond static benchmarks, we integrate BaseReward into a real-world
reinforcement learning pipeline, successfully enhancing an MLLM's performance
across various perception, reasoning, and conversational tasks. This work not
only delivers a top-tier MRM but, more importantly, provides the community with
a clear, empirically-backed guide for developing robust reward models for the
next generation of MLLMs.
\\ ( https://arxiv.org/abs/2509.16127 ,  1565kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16132
Date: Fri, 19 Sep 2025 16:31:05 GMT   (28605kb)

Title: Recovering Parametric Scenes from Very Few Time-of-Flight Pixels
Authors: Carter Sifferman, Yiquan Li, Yiming Li, Fangzhou Mu, Michael Gleicher,
  Mohit Gupta, Yin Li
Categories: cs.CV
Comments: ICCV 2025
\\
  We aim to recover the geometry of 3D parametric scenes using very few depth
measurements from low-cost, commercially available time-of-flight sensors.
These sensors offer very low spatial resolution (i.e., a single pixel), but
image a wide field-of-view per pixel and capture detailed time-of-flight data
in the form of time-resolved photon counts. This time-of-flight data encodes
rich scene information and thus enables recovery of simple scenes from sparse
measurements. We investigate the feasibility of using a distributed set of few
measurements (e.g., as few as 15 pixels) to recover the geometry of simple
parametric scenes with a strong prior, such as estimating the 6D pose of a
known object. To achieve this, we design a method that utilizes both
feed-forward prediction to infer scene parameters, and differentiable rendering
within an analysis-by-synthesis framework to refine the scene parameter
estimate. We develop hardware prototypes and demonstrate that our method
effectively recovers object pose given an untextured 3D model in both
simulations and controlled real-world captures, and show promising initial
results for other parametric scenes. We additionally conduct experiments to
explore the limits and capabilities of our imaging solution.
\\ ( https://arxiv.org/abs/2509.16132 ,  28605kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16141
Date: Fri, 19 Sep 2025 16:41:39 GMT   (6948kb)

Title: AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models
Authors: Vatsal Malaviya, Agneet Chatterjee, Maitreya Patel, Yezhou Yang,
  Chitta Baral
Categories: cs.CV
Comments: Project Page : https://vatsal-malaviya.github.io/AcT2I/
\\
  Text-to-Image (T2I) models have recently achieved remarkable success in
generating images from textual descriptions. However, challenges still persist
in accurately rendering complex scenes where actions and interactions form the
primary semantic focus. Our key observation in this work is that T2I models
frequently struggle to capture nuanced and often implicit attributes inherent
in action depiction, leading to generating images that lack key contextual
details. To enable systematic evaluation, we introduce AcT2I, a benchmark
designed to evaluate the performance of T2I models in generating images from
action-centric prompts. We experimentally validate that leading T2I models do
not fare well on AcT2I. We further hypothesize that this shortcoming arises
from the incomplete representation of the inherent attributes and contextual
dependencies in the training corpora of existing T2I models. We build upon this
by developing a training-free, knowledge distillation technique utilizing Large
Language Models to address this limitation. Specifically, we enhance prompts by
incorporating dense information across three dimensions, observing that
injecting prompts with temporal details significantly improves image generation
accuracy, with our best model achieving an increase of 72%. Our findings
highlight the limitations of current T2I methods in generating images that
require complex reasoning and demonstrate that integrating linguistic knowledge
in a systematic way can notably advance the generation of nuanced and
contextually accurate images.
\\ ( https://arxiv.org/abs/2509.16141 ,  6948kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16149
Date: Fri, 19 Sep 2025 16:57:20 GMT   (20925kb)

Title: Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal
  Large Language Models
Authors: Renjie Pi, Kehao Miao, Li Peihang, Runtao Liu, Jiahui Gao, Jipeng
  Zhang, Xiaofang Zhou
Categories: cs.CV
\\
  Multimodal large language models (MLLMs) have demonstrated extraordinary
capabilities in conducting conversations based on image inputs. However, we
observe that MLLMs exhibit a pronounced form of visual sycophantic behavior.
While similar behavior has also been noted in text-based large language models
(LLMs), it becomes significantly more prominent when MLLMs process image
inputs. We refer to this phenomenon as the "sycophantic modality gap." To
better understand this issue, we further analyze the factors that contribute to
the exacerbation of this gap. To mitigate the visual sycophantic behavior, we
first experiment with naive supervised fine-tuning to help the MLLM resist
misleading instructions from the user. However, we find that this approach also
makes the MLLM overly resistant to corrective instructions (i.e., stubborn even
if it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective
Tuning (SRT), which enables the MLLM to engage in reflective reasoning,
allowing it to determine whether a user's instruction is misleading or
corrective before drawing a conclusion. After applying SRT, we observe a
significant reduction in sycophantic behavior toward misleading instructions,
without resulting in excessive stubbornness when receiving corrective
instructions.
\\ ( https://arxiv.org/abs/2509.16149 ,  20925kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16163
Date: Fri, 19 Sep 2025 17:16:32 GMT   (1556kb)

Title: Robust Vision-Language Models via Tensor Decomposition: A Defense
  Against Adversarial Attacks
Authors: Het Patel, Muzammil Allie, Qian Zhang, Jia Chen, and Evangelos E.
  Papalexakis
Categories: cs.CV cs.AI cs.CL
Comments: To be presented as a poster at the Workshop on Safe and Trustworthy
  Multimodal AI Systems (SafeMM-AI), 2025
\\
  Vision language models (VLMs) excel in multimodal understanding but are prone
to adversarial attacks. Existing defenses often demand costly retraining or
significant architecture changes. We introduce a lightweight defense using
tensor decomposition suitable for any pre-trained VLM, requiring no retraining.
By decomposing and reconstructing vision encoder representations, it filters
adversarial noise while preserving meaning. Experiments with CLIP on COCO and
Flickr30K show improved robustness. On Flickr30K, it restores 12.3\%
performance lost to attacks, raising Recall@1 accuracy from 7.5\% to 19.8\%. On
COCO, it recovers 8.1\% performance, improving accuracy from 3.8\% to 11.9\%.
Analysis shows Tensor Train decomposition with low rank (8-32) and low residual
strength ($\alpha=0.1-0.2$) is optimal. This method is a practical,
plug-and-play solution with minimal overhead for existing VLMs.
\\ ( https://arxiv.org/abs/2509.16163 ,  1556kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16170
Date: Fri, 19 Sep 2025 17:29:25 GMT   (1940kb)

Title: UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical
  Self-Supervised Compensation
Authors: Xiaoqi Zhao, Youwei Pang, Chenyang Yu, Lihe Zhang, Huchuan Lu, Shijian
  Lu, Georges El Fakhri, Xiaofeng Liu
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\
  Multi-modal image segmentation faces real-world deployment challenges from
incomplete/corrupted modalities degrading performance. While existing methods
address training-inference modality gaps via specialized per-combination
models, they introduce high deployment costs by requiring exhaustive model
subsets and model-modality matching. In this work, we propose a unified
modality-relax segmentation network (UniMRSeg) through hierarchical
self-supervised compensation (HSSC). Our approach hierarchically bridges
representation gaps between complete and incomplete modalities across input,
feature and output levels. % First, we adopt modality reconstruction with the
hybrid shuffled-masking augmentation, encouraging the model to learn the
intrinsic modality characteristics and generate meaningful representations for
missing modalities through cross-modal fusion. % Next, modality-invariant
contrastive learning implicitly compensates the feature space distance among
incomplete-complete modality pairs. Furthermore, the proposed lightweight
reverse attention adapter explicitly compensates for the weak perceptual
semantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid
consistency constraint to ensure stable prediction under all modality
combinations without large performance fluctuations. Without bells and
whistles, UniMRSeg significantly outperforms the state-of-the-art methods under
diverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D
semantic segmentation, RGB-D/T salient object segmentation. The code will be
released at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.
\\ ( https://arxiv.org/abs/2509.16170 ,  1940kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16179
Date: Fri, 19 Sep 2025 17:40:42 GMT   (716kb)

Title: Fast OTSU Thresholding Using Bisection Method
Authors: Sai Varun Kodathala
Categories: cs.CV cs.AI cs.NA math.NA
Comments: 12 pages, 7 tables
\\
  The Otsu thresholding algorithm represents a fundamental technique in image
segmentation, yet its computational efficiency is severely limited by
exhaustive search requirements across all possible threshold values. This work
presents an optimized implementation that leverages the bisection method to
exploit the unimodal characteristics of the between-class variance function.
Our approach reduces the computational complexity from O(L) to O(log L)
evaluations while preserving segmentation accuracy. Experimental validation on
48 standard test images demonstrates a 91.63% reduction in variance
computations and 97.21% reduction in algorithmic iterations compared to
conventional exhaustive search. The bisection method achieves exact threshold
matches in 66.67% of test cases, with 95.83% exhibiting deviations within 5
gray levels. The algorithm maintains universal convergence within theoretical
logarithmic bounds while providing deterministic performance guarantees
suitable for real-time applications. This optimization addresses critical
computational bottlenecks in large-scale image processing systems without
compromising the theoretical foundations or segmentation quality of the
original Otsu method.
\\ ( https://arxiv.org/abs/2509.16179 ,  716kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16197
Date: Fri, 19 Sep 2025 17:58:00 GMT   (17924kb)

Title: MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid
  Vision Tokenizer
Authors: Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen
  Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, Hyunjik Kim, Chao Jia,
  Zhenbang Wang, Yinfei Yang, Mingfei Gao, Zi-Yi Dou, Wenze Hu, Chang Gao,
  Dongxu Li, Philipp Dufter, Zirui Wang, Guoli Yin, Zhengdong Zhang, Chen Chen,
  Yang Zhao, Ruoming Pang, Zhifeng Chen
Categories: cs.CV cs.CL cs.LG
\\
  Unified multimodal Large Language Models (LLMs) that can both understand and
generate visual content hold immense potential. However, existing open-source
models often suffer from a performance trade-off between these capabilities. We
present Manzano, a simple and scalable unified framework that substantially
reduces this tension by coupling a hybrid image tokenizer with a well-curated
training recipe. A single shared vision encoder feeds two lightweight adapters
that produce continuous embeddings for image-to-text understanding and discrete
tokens for text-to-image generation within a common semantic space. A unified
autoregressive LLM predicts high-level semantics in the form of text and image
tokens, with an auxiliary diffusion decoder subsequently translating the image
tokens into pixels. The architecture, together with a unified training recipe
over understanding and generation data, enables scalable joint learning of both
capabilities. Manzano achieves state-of-the-art results among unified models,
and is competitive with specialist models, particularly on text-rich
evaluation. Our studies show minimal task conflicts and consistent gains from
scaling model size, validating our design choice of a hybrid tokenizer.
\\ ( https://arxiv.org/abs/2509.16197 ,  17924kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15450
Date: Thu, 18 Sep 2025 21:45:28 GMT   (754kb)

Title: PCCL: Photonic circuit-switched collective communication for distributed
  ML
Authors: Abhishek Vijaya Kumar, Arjun Devraj, Rachee Singh
Categories: cs.DC
\\
  Modern distributed ML suffers from a fundamental gap between the theoretical
and realized performance of collective communication algorithms due to
congestion and hop-count induced dilation in practical GPU clusters. We present
PCCL, a Photonic Collective Communication Library that reconfigures the network
topology to match the communication patterns of collective algorithms, thereby
eliminating congestion and dilation by creating direct, contention-free
circuits between communicating GPUs. Unlike prior approaches that synthesize
algorithms for specific network topologies and collectives, PCCL generalizes to
any collective primitive and any topology by adapting the network to match each
algorithm's communication pattern. PCCL's key innovation lies in its
hardware-agnostic optimization framework that intelligently decides when to
reconfigure based on the trade-off between network reconfiguration delay and
congestion/dilation costs, making it practical across different optical
hardware with varying switching speeds. Our evaluation demonstrates that PCCL
achieves up to 3X speedup over state-of-the-art algorithms on 128 GPUs across
various workloads, buffer sizes, and topologies, translating to a 1.3X speedup
in end-to-end training throughput.
\\ ( https://arxiv.org/abs/2509.15450 ,  754kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15847
Date: Fri, 19 Sep 2025 10:30:25 GMT   (294kb)

Title: Angelfish: Consensus with Optimal Throughput and Latency Across the
  Leader-DAG Spectrum
Authors: Qianyu Yu, Giuliano Losa, Nibesh Shrestha, Xuechao Wang
Categories: cs.DC cs.CR
\\
  To maximize performance, many modern blockchain systems rely on
eventually-synchronous, Byzantine fault-tolerant (BFT) consensus protocols. Two
protocol designs have emerged in this space: protocols that minimize latency
using a leader that drives both data dissemination and consensus, and protocols
that maximize throughput using a separate, asynchronous data dissemination
layer. Recent protocols such as Partially-Synchronous Bullshark and Sailfish
combine elements of both approaches by using a DAG to enable parallel data
dissemination and a leader that paces DAG formation. This improves latency
while achieving state-of-the-art throughput. Yet the latency of leader-based
protocols is still better under moderate loads.
  We present Angelfish, a hybrid protocol that adapts smoothly across this
design space, from leader-based to Sailfish-like DAG-based consensus. Angelfish
lets a dynamically-adjusted subset of parties use best-effort broadcast to
issue lightweight votes instead of reliably broadcasting costlier DAG vertices.
This reduces communication, helps lagging nodes catch up, and lowers latency in
practice compared to prior DAG-based protocols. Our empirical evaluation shows
that Angelfish attains state-of-the-art peak throughput while matching the
latency of leader-based protocols under moderate throughput, delivering the
best of both worlds.
\\ ( https://arxiv.org/abs/2509.15847 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15940
Date: Fri, 19 Sep 2025 12:52:32 GMT   (900kb)

Title: Efficient Pre-Training of LLMs via Topology-Aware Communication
  Alignment on More Than 9600 GPUs
Authors: Guoliang He, Youhe Jiang, Wencong Xiao, Kaihua Jiang, Shuguang Wang,
  Jun Wang, Zixian Du, Zhuo Jiang, Xinlei Zhang, Binhang Yuan, Eiko Yoneki
Categories: cs.DC
Comments: NeurIPS 2025
\\
  The scaling law for large language models (LLMs) depicts that the path
towards machine intelligence necessitates training at large scale. Thus,
companies continuously build large-scale GPU clusters, and launch training jobs
that span over thousands of computing nodes. However, LLM pre-training presents
unique challenges due to its complex communication patterns, where GPUs
exchange data in sparse yet high-volume bursts within specific groups.
Inefficient resource scheduling exacerbates bandwidth contention, leading to
suboptimal training performance. This paper presents Arnold, a scheduling
system summarizing our experience to effectively align LLM communication
patterns with data center topology at scale. An in-depth characteristic study
is performed to identify the impact of physical network topology to LLM
pre-training jobs. Based on the insights, we develop a scheduling algorithm to
effectively align communication patterns with the physical network topology in
modern data centers. Through simulation experiments, we show the effectiveness
of our algorithm in reducing the maximum spread of communication groups by up
to $1.67$x. In production training, our scheduling system improves the
end-to-end performance by $10.6\%$ when training with more than $9600$ GPUs, a
significant improvement for our training pipeline.
\\ ( https://arxiv.org/abs/2509.15940 ,  900kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15238
Date: Wed, 17 Sep 2025 15:34:02 GMT   (526kb,D)

Title: Generating Plans for Belief-Desire-Intention (BDI) Agents Using
  Alternating-Time Temporal Logic (ATL)
Authors: Dylan L\'eveill\'e (Carleton University)
Categories: cs.MA cs.AI
Comments: In Proceedings GandALF 2025, arXiv:2509.13258
Journal-ref: EPTCS 428, 2025, pp. 127-143
DOI: 10.4204/EPTCS.428.10
\\
  Belief-Desire-Intention (BDI) is a framework for modelling agents based on
their beliefs, desires, and intentions. Plans are a central component of BDI
agents, and define sequences of actions that an agent must undertake to achieve
a certain goal. Existing approaches to plan generation often require
significant manual effort, and are mainly focused on single-agent systems. As a
result, in this work, we have developed a tool that automatically generates BDI
plans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans
generated accommodate for possible competition or cooperation between the
agents in the system. We demonstrate the effectiveness of the tool by
generating plans for an illustrative game that requires agent collaboration to
achieve a shared goal. We show that the generated plans allow the agents to
successfully attain this goal.
\\ ( https://arxiv.org/abs/2509.15238 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15381
Date: Thu, 18 Sep 2025 19:35:54 GMT   (3043kb)

Title: Dynamic Agent Grouping ECBS: Scaling Windowed Multi-Agent Path Finding
  with Completeness Guarantees
Authors: Tiannan Zhang, Rishi Veerapaneni, Shao-Hung Chan, Jiaoyang Li, Maxim
  Likhachev
Categories: cs.MA
\\
  Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths for a team of agents. Although several MAPF methods which
solve full-horizon MAPF have completeness guarantees, very few MAPF methods
that plan partial paths have completeness guarantees. Recent work introduced
the Windowed Complete MAPF (WinC-MAPF) framework, which shows how windowed
optimal MAPF solvers (e.g., SS-CBS) can use heuristic updates and disjoint
agent groups to maintain completeness even when planning partial paths
(Veerapaneni et al. 2024). A core limitation of WinC-MAPF is that they required
optimal MAPF solvers. Our main contribution is to extend WinC-MAPF by showing
how we can use a bounded suboptimal solver while maintaining completeness. In
particular, we design Dynamic Agent Grouping ECBS (DAG-ECBS) which dynamically
creates and plans agent groups while maintaining that each agent group solution
is bounded suboptimal. We prove how DAG-ECBS can maintain completeness in the
WinC-MAPF framework. DAG-ECBS shows improved scalability compared to SS-CBS and
can outperform windowed ECBS without completeness guarantees. More broadly, our
work serves as a blueprint for designing more MAPF methods that can use the
WinC-MAPF framework.
\\ ( https://arxiv.org/abs/2509.15381 ,  3043kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2509.15230 (*cross-listing*)
Date: Fri, 5 Sep 2025 13:28:04 GMT   (1278kb)

Title: Pre-Forgettable Models: Prompt Learning as a Native Mechanism for
  Unlearning
Authors: Rutger Hendrix, Giovanni Patan\`e, Leonardo G. Russo, Simone
  Carnemolla, Giovanni Bellitto, Federica Proietto Salanitri, Concetto
  Spampinato and Matteo Pennisi
Categories: cs.LG cs.AI
Comments: Accepted at ACM multimedia 2025 BNI track
DOI: 10.1145/3746027.3758171
\\
  Foundation models have transformed multimedia analysis by enabling robust and
transferable representations across diverse modalities and tasks. However,
their static deployment conflicts with growing societal and regulatory demands
-- particularly the need to unlearn specific data upon request, as mandated by
privacy frameworks such as the GDPR. Traditional unlearning approaches,
including retraining, activation editing, or distillation, are often
computationally expensive, fragile, and ill-suited for real-time or
continuously evolving systems. In this paper, we propose a paradigm shift:
rethinking unlearning not as a retroactive intervention but as a built-in
capability. We introduce a prompt-based learning framework that unifies
knowledge acquisition and removal within a single training phase. Rather than
encoding information in model weights, our approach binds class-level semantics
to dedicated prompt tokens. This design enables instant unlearning simply by
removing the corresponding prompt -- without retraining, model modification, or
access to original data. Experiments demonstrate that our framework preserves
predictive performance on retained classes while effectively erasing forgotten
ones. Beyond utility, our method exhibits strong privacy and security
guarantees: it is resistant to membership inference attacks, and prompt removal
prevents any residual knowledge extraction, even under adversarial conditions.
This ensures compliance with data protection principles and safeguards against
unauthorized access to forgotten information, making the framework suitable for
deployment in sensitive and regulated environments. Overall, by embedding
removability into the architecture itself, this work establishes a new
foundation for designing modular, scalable and ethically responsive AI models.
\\ ( https://arxiv.org/abs/2509.15230 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15236 (*cross-listing*)
Date: Wed, 17 Sep 2025 13:18:05 GMT   (1026kb)

Title: ChannelFlow-Tools: A Standardized Dataset Creation Pipeline for 3D
  Obstructed Channel Flows
Authors: Shubham Kavane, Kajol Kulkarni, Harald Koestler
Categories: cs.GR cs.AI
\\
  We present ChannelFlow-Tools, a configuration-driven framework that
standardizes the end-to-end path from programmatic CAD solid generation to
ML-ready inputs and targets for 3D obstructed channel flows. The toolchain
integrates geometry synthesis with feasibility checks, signed distance field
(SDF) voxelization, automated solver orchestration on HPC (waLBerla LBM), and
Cartesian resampling to co-registered multi-resolution tensors. A single
Hydra/OmegaConf configuration governs all stages, enabling deterministic
reproduction and controlled ablations. As a case study, we generate 10k+ scenes
spanning Re=100-15000 with diverse shapes and poses. An end-to-end evaluation
of storage trade-offs directly from the emitted artifacts, a minimal 3D U-Net
at 128x32x32, and example surrogate models with dataset size illustrate that
the standardized representations support reproducible ML training.
ChannelFlow-Tools turns one-off dataset creation into a reproducible,
configurable pipeline for CFD surrogate modeling.
\\ ( https://arxiv.org/abs/2509.15236 ,  1026kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15246 (*cross-listing*)
Date: Wed, 17 Sep 2025 19:10:44 GMT   (6655kb)

Title: GenCAD-3D: CAD Program Generation using Multimodal Latent Space
  Alignment and Synthetic Dataset Balancing
Authors: Nomi Yu (1), Md Ferdous Alam (1), A. John Hart (1), and Faez Ahmed (1)
  ((1) Massachusetts Institute of Technology)
Categories: cs.GR cs.AI
Comments: 9 figures, 15 pages. Accepted and soon published in the ASME Journal
  of Mechanical Design
DOI: 10.1115/1.4069276
\\
  CAD programs, structured as parametric sequences of commands that compile
into precise 3D geometries, are fundamental to accurate and efficient
engineering design processes. Generating these programs from nonparametric data
such as point clouds and meshes remains a crucial yet challenging task,
typically requiring extensive manual intervention. Current deep generative
models aimed at automating CAD generation are significantly limited by
imbalanced and insufficiently large datasets, particularly those lacking
representation for complex CAD programs. To address this, we introduce
GenCAD-3D, a multimodal generative framework utilizing contrastive learning for
aligning latent embeddings between CAD and geometric encoders, combined with
latent diffusion models for CAD sequence generation and retrieval.
Additionally, we present SynthBal, a synthetic data augmentation strategy
specifically designed to balance and expand datasets, notably enhancing
representation of complex CAD geometries. Our experiments show that SynthBal
significantly boosts reconstruction accuracy, reduces the generation of invalid
CAD models, and markedly improves performance on high-complexity geometries,
surpassing existing benchmarks. These advancements hold substantial
implications for streamlining reverse engineering and enhancing automation in
engineering design. We will publicly release our datasets and code, including a
set of 51 3D-printed and laser-scanned parts on our project site.
\\ ( https://arxiv.org/abs/2509.15246 ,  6655kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15249 (*cross-listing*)
Date: Thu, 18 Sep 2025 01:03:21 GMT   (2394kb)

Title: Causal Reasoning Elicits Controllable 3D Scene Generation
Authors: Shen Chen, Ruiyu Zhao, Jiale Zhou, Zongkai Wu, Jenq-Neng Hwang, Lei Li
Categories: cs.GR cs.AI
\\
  Existing 3D scene generation methods often struggle to model the complex
logical dependencies and physical constraints between objects, limiting their
ability to adapt to dynamic and realistic environments. We propose
CausalStruct, a novel framework that embeds causal reasoning into 3D scene
generation. Utilizing large language models (LLMs), We construct causal graphs
where nodes represent objects and attributes, while edges encode causal
dependencies and physical constraints. CausalStruct iteratively refines the
scene layout by enforcing causal order to determine the placement order of
objects and applies causal intervention to adjust the spatial configuration
according to physics-driven constraints, ensuring consistency with textual
descriptions and real-world dynamics. The refined scene causal graph informs
subsequent optimization steps, employing a
Proportional-Integral-Derivative(PID) controller to iteratively tune object
scales and positions. Our method uses text or images to guide object placement
and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation
Sampling improving shape accuracy and rendering stability. Extensive
experiments show that CausalStruct generates 3D scenes with enhanced logical
coherence, realistic spatial interactions, and robust adaptability.
\\ ( https://arxiv.org/abs/2509.15249 ,  2394kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15253 (*cross-listing*)
Date: Thu, 18 Sep 2025 05:49:57 GMT   (329kb)

Title: Emotion-Aware Speech Generation with Character-Specific Voices for
  Comics
Authors: Zhiwen Qian, Jinhua Liang, Huan Zhang
Categories: cs.SD cs.AI cs.MM eess.AS
\\
  This paper presents an end-to-end pipeline for generating character-specific,
emotion-aware speech from comics. The proposed system takes full comic volumes
as input and produces speech aligned with each character's dialogue and
emotional state. An image processing module performs character detection, text
recognition, and emotion intensity recognition. A large language model performs
dialogue attribution and emotion analysis by integrating visual information
with the evolving plot context. Speech is synthesized through a text-to-speech
model with distinct voice profiles tailored to each character and emotion. This
work enables automated voiceover generation for comics, offering a step toward
interactive and immersive comic reading experience.
\\ ( https://arxiv.org/abs/2509.15253 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15256 (*cross-listing*)
Date: Thu, 18 Sep 2025 07:48:10 GMT   (853kb)

Title: A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for
  Drug-Drug Interactions Prediction
Authors: Zimo Yan and Jie Zhang and Zheng Xie and Yiping Song and Hao Li
Categories: cs.LG cs.AI
\\
  Accurate prediction of drug-drug interactions (DDI) is crucial for medication
safety and effective drug development. However, existing methods often struggle
to capture structural information across different scales, from local
functional groups to global molecular topology, and typically lack mechanisms
to quantify prediction confidence. To address these limitations, we propose
MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of
MPNP-DDI is a unique message-passing scheme that, by being iteratively applied,
learns a hierarchy of graph representations at multiple scales. Crucially, a
cross-drug co-attention mechanism then dynamically fuses these multi-scale
representations to generate context-aware embeddings for interacting drug
pairs, while an integrated neural process module provides principled
uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI
significantly outperforms state-of-the-art baselines on benchmark datasets. By
providing accurate, generalizable, and uncertainty-aware predictions built upon
multi-scale structural features, MPNP-DDI represents a powerful computational
tool for pharmacovigilance, polypharmacy risk assessment, and precision
medicine.
\\ ( https://arxiv.org/abs/2509.15256 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15258 (*cross-listing*)
Date: Thu, 18 Sep 2025 07:51:25 GMT   (1650kb)

Title: Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model
Authors: Zheng Yang, Guoxuan Chi, Chenshu Wu, Hanyu Liu, Yuchong Gao, Yunhao
  Liu, Jie Xu, Tony Xiao Han
Categories: cs.LG cs.AI eess.SP
\\
  Generative Artificial Intelligence (GenAI) has made significant advancements
in fields such as computer vision (CV) and natural language processing (NLP),
demonstrating its capability to synthesize high-fidelity data and improve
generalization. Recently, there has been growing interest in integrating GenAI
into wireless sensing systems. By leveraging generative techniques such as data
augmentation, domain adaptation, and denoising, wireless sensing applications,
including device localization, human activity recognition, and environmental
monitoring, can be significantly improved. This survey investigates the
convergence of GenAI and wireless sensing from two complementary perspectives.
First, we explore how GenAI can be integrated into wireless sensing pipelines,
focusing on two modes of integration: as a plugin to augment task-specific
models and as a solver to directly address sensing tasks. Second, we analyze
the characteristics of mainstream generative models, such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion
models, and discuss their applicability and unique advantages across various
wireless sensing tasks. We further identify key challenges in applying GenAI to
wireless sensing and outline a future direction toward a wireless foundation
model: a unified, pre-trained design capable of scalable, adaptable, and
efficient signal understanding across diverse sensing tasks.
\\ ( https://arxiv.org/abs/2509.15258 ,  1650kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15259 (*cross-listing*)
Date: Thu, 18 Sep 2025 08:14:17 GMT   (1571kb)

Title: IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on
  Information Entropy for EEG Classification of Neurological Disorders
Authors: Liang Zhang and Hanyang Dong and Jia-Hong Gao and Yi Sun and Kuntao
  Xiao and Wanli Yang and Zhao Lv and Shurong Sheng
Categories: cs.LG cs.AI
\\
  Deep learning-based EEG classification is crucial for the automated detection
of neurological disorders, improving diagnostic accuracy and enabling early
intervention. However, the low signal-to-noise ratio of EEG signals limits
model performance, making feature selection (FS) vital for optimizing
representations learned by neural network encoders. Existing FS methods are
seldom designed specifically for EEG diagnosis; many are architecture-dependent
and lack interpretability, limiting their applicability. Moreover, most rely on
single-iteration data, resulting in limited robustness to variability. To
address these issues, we propose IEFS-GMB, an Information Entropy-based Feature
Selection method guided by a Gradient Memory Bank. This approach constructs a
dynamic memory bank storing historical gradients, computes feature importance
via information entropy, and applies entropy-based weighting to select
informative EEG features. Experiments on four public neurological disease
datasets show that encoders enhanced with IEFS-GMB achieve accuracy
improvements of 0.64% to 6.45% over baseline models. The method also
outperforms four competing FS techniques and improves model interpretability,
supporting its practical use in clinical settings.
\\ ( https://arxiv.org/abs/2509.15259 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15269 (*cross-listing*)
Date: Thu, 18 Sep 2025 10:20:26 GMT   (2749kb)

Title: Modeling Transformers as complex networks to analyze learning dynamics
Authors: Elisabetta Rocchetti
Categories: cs.LG cs.AI
\\
  The process by which Large Language Models (LLMs) acquire complex
capabilities during training remains a key open question in mechanistic
interpretability. This project investigates whether these learning dynamics can
be characterized through the lens of Complex Network Theory (CNT). I introduce
a novel methodology to represent a Transformer-based LLM as a directed,
weighted graph where nodes are the model's computational components (attention
heads and MLPs) and edges represent causal influence, measured via an
intervention-based ablation technique. By tracking the evolution of this
component-graph across 143 training checkpoints of the Pythia-14M model on a
canonical induction task, I analyze a suite of graph-theoretic metrics. The
results reveal that the network's structure evolves through distinct phases of
exploration, consolidation, and refinement. Specifically, I identify the
emergence of a stable hierarchy of information spreader components and a
dynamic set of information gatherer components, whose roles reconfigure at key
learning junctures. This work demonstrates that a component-level network
perspective offers a powerful macroscopic lens for visualizing and
understanding the self-organizing principles that drive the formation of
functional circuits in LLMs.
\\ ( https://arxiv.org/abs/2509.15269 ,  2749kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15275 (*cross-listing*)
Date: Thu, 18 Sep 2025 12:19:22 GMT   (42kb)

Title: Partial Column Generation with Graph Neural Networks for Team Formation
  and Routing
Authors: Giacomo Dall'Olio, Rainer Kolisch, Yaoxin Wu
Categories: cs.LG cs.AI
Comments: 30 pages, 4 figures
\\
  The team formation and routing problem is a challenging optimization problem
with several real-world applications in fields such as airport, healthcare, and
maintenance operations. To solve this problem, exact solution methods based on
column generation have been proposed in the literature. In this paper, we
propose a novel partial column generation strategy for settings with multiple
pricing problems, based on predicting which ones are likely to yield columns
with a negative reduced cost. We develop a machine learning model tailored to
the team formation and routing problem that leverages graph neural networks for
these predictions. Computational experiments demonstrate that applying our
strategy enhances the solution method and outperforms traditional partial
column generation approaches from the literature, particularly on hard
instances solved under a tight time limit.
\\ ( https://arxiv.org/abs/2509.15275 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15283 (*cross-listing*)
Date: Thu, 18 Sep 2025 14:13:30 GMT   (811kb)

Title: Evaluating the Limitations of Local LLMs in Solving Complex Programming
  Challenges
Authors: Kadin Matotek, Heather Cassel, Md Amiruzzaman, Linh B. Ngo
Categories: cs.SE cs.AI cs.LG cs.PL
Comments: Comments: 16 pages, 3 figures, 8 tables, accepted to CCSC Eastern
  2025
ACM-class: I.2.7; F.2.2; I.2.2
\\
  This study examines the performance of today's open-source, locally hosted
large-language models (LLMs) in handling complex competitive programming tasks
with extended problem descriptions and contexts. Building on the original
Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit
the pipeline to work entirely offline through the Ollama runtime, collapsing
FACE's sprawling per-problem directory tree into a handful of consolidated JSON
files, and adding robust checkpointing so multi-day runs can resume after
failures. The enhanced framework generates, submits, and records solutions for
the full Kattis corpus of 3,589 problems across eight code-oriented models
ranging from 6.7-9 billion parameters. The submission results show that the
overall pass@1 accuracy is modest for the local models, with the best models
performing at approximately half the acceptance rate of the proprietary models,
Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between
private, cost-controlled LLM deployments and state-of-the-art proprietary
services, yet also highlight the rapid progress of open models and the
practical benefits of an evaluation workflow that organizations can replicate
on in-house hardware.
\\ ( https://arxiv.org/abs/2509.15283 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15289 (*cross-listing*)
Date: Thu, 18 Sep 2025 16:38:58 GMT   (1773kb)

Title: Collective Voice: Recovered-Peer Support Mediated by An LLM-Based
  Chatbot for Eating Disorder Recovery
Authors: Ryuhaerang Choi, Taehan Kim, Subin Park, Seohyeon Yoo, Jennifer G.
  Kim, Sung-Ju Lee
Categories: cs.HC cs.AI
\\
  Peer recovery narratives provide unique benefits beyond professional or lay
mentoring by fostering hope and sustained recovery in eating disorder (ED)
contexts. Yet, such support is limited by the scarcity of peer-involved
programs and potential drawbacks on recovered peers, including relapse risk. To
address this, we designed RecoveryTeller, a chatbot adopting a recovered-peer
persona that portrays itself as someone recovered from an ED. We examined
whether such a persona can reproduce the support affordances of peer recovery
narratives. We compared RecoveryTeller with a lay-mentor persona chatbot
offering similar guidance but without a recovery background. We conducted a
20-day cross-over deployment study with 26 ED participants, each using both
chatbots for 10 days. RecoveryTeller elicited stronger emotional resonance than
a lay-mentor chatbot, yet tensions between emotional and epistemic trust led
participants to view the two personas as complementary rather than substitutes.
We provide design implications for mental health chatbot persona design.
\\ ( https://arxiv.org/abs/2509.15289 ,  1773kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15363 (*cross-listing*)
Date: Thu, 18 Sep 2025 19:03:41 GMT   (1282kb)

Title: Recent Advancements in Microscopy Image Enhancement using Deep Learning:
  A Survey
Authors: Debasish Dutta, Neeharika Sonowal, Risheraj Barauh, Deepjyoti Chetia
  and Sanjib Kr Kalita
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 7 pages, 3 figures and 1 table. 2024 IEEE International Conference on
  Computer Vision and Machine Intelligence (CVMI). IEEE, 2024
DOI: 10.1109/CVMI61877.2024.10782829
\\
  Microscopy image enhancement plays a pivotal role in understanding the
details of biological cells and materials at microscopic scales. In recent
years, there has been a significant rise in the advancement of microscopy image
enhancement, specifically with the help of deep learning methods. This survey
paper aims to provide a snapshot of this rapidly growing state-of-the-art
method, focusing on its evolution, applications, challenges, and future
directions. The core discussions take place around the key domains of
microscopy image enhancement of super-resolution, reconstruction, and
denoising, with each domain explored in terms of its current trends and their
practical utility of deep learning.
\\ ( https://arxiv.org/abs/2509.15363 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15380 (*cross-listing*)
Date: Thu, 18 Sep 2025 19:32:07 GMT   (10817kb)

Title: Efficient and Versatile Model for Multilingual Information Retrieval of
  Islamic Text: Development and Deployment in Real-World Scenarios
Authors: Vera Pavlova and Mohammed Makhlouf
Categories: cs.IR cs.AI cs.CL
\\
  Despite recent advancements in Multilingual Information Retrieval (MLIR), a
significant gap remains between research and practical deployment. Many studies
assess MLIR performance in isolated settings, limiting their applicability to
real-world scenarios. In this work, we leverage the unique characteristics of
the Quranic multilingual corpus to examine the optimal strategies to develop an
ad-hoc IR system for the Islamic domain that is designed to satisfy users'
information needs in multiple languages. We prepared eleven retrieval models
employing four training approaches: monolingual, cross-lingual,
translate-train-all, and a novel mixed method combining cross-lingual and
monolingual techniques. Evaluation on an in-domain dataset demonstrates that
the mixed approach achieves promising results across diverse retrieval
scenarios. Furthermore, we provide a detailed analysis of how different
training configurations affect the embedding space and their implications for
multilingual retrieval effectiveness. Finally, we discuss deployment
considerations, emphasizing the cost-efficiency of deploying a single
versatile, lightweight model for real-world MLIR applications.
\\ ( https://arxiv.org/abs/2509.15380 ,  10817kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15400 (*cross-listing*)
Date: Thu, 18 Sep 2025 20:17:29 GMT   (2553kb)

Title: Exploring multimodal implicit behavior learning for vehicle navigation
  in simulated cities
Authors: Eric Aislan Antonelo, Gustavo Claudio Karl Couto, Christian M\"oller
Categories: cs.LG cs.AI cs.RO
Comments: ENIAC conference
\\
  Standard Behavior Cloning (BC) fails to learn multimodal driving decisions,
where multiple valid actions exist for the same scenario. We explore Implicit
Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this
multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning
by perturbing expert actions to form the counterexamples of IBC training and
using better initialization for derivative-free inference. Experiments in the
CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC outperforms
standard IBC in urban driving tasks designed to evaluate multimodal behavior
learning in a test environment. The learned energy landscapes are able to
represent multimodal action distributions, which BC fails to achieve.
\\ ( https://arxiv.org/abs/2509.15400 ,  2553kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15437 (*cross-listing*)
Date: Thu, 18 Sep 2025 21:19:53 GMT   (220kb)

Title: Impact of Phonetics on Speaker Identity in Adversarial Voice Attack
Authors: Daniyal Kabir Dar, Qiben Yan, Li Xiao, Arun Ross
Categories: cs.SD cs.AI cs.CR eess.AS
Comments: Additional figures for extended visualization:
  https://daniyalkabir.github.io/icassp-2025-results/
ACM-class: I.2.0; I.2.7; I.5.4; K.6.5
\\
  Adversarial perturbations in speech pose a serious threat to automatic speech
recognition (ASR) and speaker verification by introducing subtle waveform
modifications that remain imperceptible to humans but can significantly alter
system outputs. While targeted attacks on end-to-end ASR models have been
widely studied, the phonetic basis of these perturbations and their effect on
speaker identity remain underexplored. In this work, we analyze adversarial
audio at the phonetic level and show that perturbations exploit systematic
confusions such as vowel centralization and consonant substitutions. These
distortions not only mislead transcription but also degrade phonetic cues
critical for speaker verification, leading to identity drift. Using DeepSpeech
as our ASR target, we generate targeted adversarial examples and evaluate their
impact on speaker embeddings across genuine and impostor samples. Results
across 16 phonetically diverse target phrases demonstrate that adversarial
audio induces both transcription errors and identity drift, highlighting the
need for phonetic-aware defenses to ensure the robustness of ASR and speaker
recognition systems.
\\ ( https://arxiv.org/abs/2509.15437 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15439 (*cross-listing*)
Date: Thu, 18 Sep 2025 21:25:18 GMT   (1500kb)

Title: Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP
  and P300 Responses
Authors: Ekgari Kasawala and Surej Mouli
Categories: cs.IR cs.AI
Comments: 15 Pages
DOI: 10.3390/s25061802
\\
  In brain-computer interface (BCI) systems, steady-state visual evoked
potentials (SSVEP) and P300 responses have achieved widespread implementation
owing to their superior information transfer rates (ITR) and minimal training
requirements. These neurophysiological signals have exhibited robust efficacy
and versatility in external device control, demonstrating enhanced precision
and scalability. However, conventional implementations predominantly utilise
liquid crystal display (LCD)-based visual stimulation paradigms, which present
limitations in practical deployment scenarios. This investigation presents the
development and evaluation of a novel light-emitting diode (LED)-based dual
stimulation apparatus designed to enhance SSVEP classification accuracy through
the integration of both SSVEP and P300 paradigms. The system employs four
distinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward,
backward, right, and left directional controls, respectively. Oscilloscopic
verification confirmed the precision of these stimulation frequencies.
Real-time feature extraction was accomplished through the concurrent analysis
of maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to
ascertain user intent. Directional control was determined by the frequency
exhibiting maximal amplitude characteristics. The visual stimulation hardware
demonstrated minimal frequency deviation, with error differentials ranging from
0.15%to 0.20%across all frequencies. The implemented signal processing
algorithm successfully discriminated all four stimulus frequencies whilst
correlating them with their respective P300 event markers. Classification
accuracy was evaluated based on correct task intention recognition. The
proposed hybrid system achieved a mean classification accuracy of 86.25%,
coupled with an average ITR of 42.08 bits per minute (bpm).
\\ ( https://arxiv.org/abs/2509.15439 ,  1500kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15440 (*cross-listing*)
Date: Thu, 18 Sep 2025 21:27:12 GMT   (1407kb)

Title: Where Do I 'Add the Egg'?: Exploring Agency and Ownership in AI Creative
  Co-Writing Systems
Authors: Dashiel Carrera, Jeb Thomas-Mitchell, Daniel Wigdor
Categories: cs.HC cs.AI
Comments: 17 pages, 3 figures, 3 tables
\\
  AI co-writing systems challenge long held ideals about agency and ownership
in the creative process, thereby hindering widespread adoption. In order to
address this, we investigate conceptions of agency and ownership in AI creative
co-writing. Drawing on insights from a review of commercial systems, we
developed three co-writing systems with identical functionality but distinct
interface metaphors: agentic, tool-like, and magical. Through interviews with
professional and non-professional writers (n = 18), we explored how these
metaphors influenced participants' sense of control and authorship. Our
analysis resulted in a taxonomy of agency and ownership subtypes and underscore
how tool-like metaphors shift writers' expected points of control while agentic
metaphors foreground conceptual contributions. We argue that interface
metaphors not only guide expectations of control but also frame conceptions of
authorship. We conclude with recommendations for the design of AI co-writing
systems, emphasizing how metaphor shapes user experience and creative practice.
\\ ( https://arxiv.org/abs/2509.15440 ,  1407kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15443 (*cross-listing*)
Date: Thu, 18 Sep 2025 21:34:02 GMT   (1756kb)

Title: Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation
  Learning
Authors: Xingyu Chen, Hanyu Wu, Sikai Wu, Mingliang Zhou, Diyun Xiang, Haodong
  Zhang
Categories: cs.RO cs.AI
\\
  Human-to-humanoid imitation learning aims to learn a humanoid whole-body
controller from human motion. Motion retargeting is a crucial step in enabling
robots to acquire reference trajectories when exploring locomotion skills.
However, current methods focus on motion retargeting frame by frame, which
lacks scalability. Could we directly convert large-scale human motion into
robot-executable motion through a more efficient approach? To address this
issue, we propose Implicit Kinodynamic Motion Retargeting (IKMR), a novel
efficient and scalable retargeting framework that considers both kinematics and
dynamics. In kinematics, IKMR pretrains motion topology feature representation
and a dual encoder-decoder architecture to learn a motion domain mapping. In
dynamics, IKMR integrates imitation learning with the motion retargeting
network to refine motion into physically feasible trajectories. After
fine-tuning using the tracking results, IKMR can achieve large-scale physically
feasible motion retargeting in real time, and a whole-body controller could be
directly trained and deployed for tracking its retargeted trajectories. We
conduct our experiments both in the simulator and the real robot on a full-size
humanoid robot. Extensive experiments and evaluation results verify the
effectiveness of our proposed framework.
\\ ( https://arxiv.org/abs/2509.15443 ,  1756kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15448 (*cross-listing*)
Date: Thu, 18 Sep 2025 21:44:07 GMT   (1137kb)

Title: Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to
  Multi-Scale Problems
Authors: Saeed Amizadeh, Sara Abdali, Yinheng Li, Kazuhito Koishida
Categories: cs.LG cs.AI cs.NE stat.ML
Comments: In The Thirty-Ninth Annual Conference on Neural Information
  Processing Systems (NeurIPS 2025)
\\
  Transformers and their attention mechanism have been revolutionary in the
field of Machine Learning. While originally proposed for the language data,
they quickly found their way to the image, video, graph, etc. data modalities
with various signal geometries. Despite this versatility, generalizing the
attention mechanism to scenarios where data is presented at different scales
from potentially different modalities is not straightforward. The attempts to
incorporate hierarchy and multi-modality within transformers are largely based
on ad hoc heuristics, which are not seamlessly generalizable to similar
problems with potentially different structures. To address this problem, in
this paper, we take a fundamentally different approach: we first propose a
mathematical construct to represent multi-modal, multi-scale data. We then
mathematically derive the neural attention mechanics for the proposed construct
from the first principle of entropy minimization. We show that the derived
formulation is optimal in the sense of being the closest to the standard
Softmax attention while incorporating the inductive biases originating from the
hierarchical/geometric information of the problem. We further propose an
efficient algorithm based on dynamic programming to compute our derived
attention mechanism. By incorporating it within transformers, we show that the
proposed hierarchical attention mechanism not only can be employed to train
transformer models in hierarchical/multi-modal settings from scratch, but it
can also be used to inject hierarchical information into classical, pre-trained
transformer models post training, resulting in more efficient models in
zero-shot manner.
\\ ( https://arxiv.org/abs/2509.15448 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15460 (*cross-listing*)
Date: Thu, 18 Sep 2025 22:13:48 GMT   (4462kb)

Title: Incorporating Visual Cortical Lateral Connection Properties into CNN:
  Recurrent Activation and Excitatory-Inhibitory Separation
Authors: Jin Hyun Park, Cheng Zhang, Yoonsuck Choe
Categories: q-bio.NC cs.AI cs.CV
\\
  The original Convolutional Neural Networks (CNNs) and their modern updates
such as the ResNet are heavily inspired by the mammalian visual system. These
models include afferent connections (retina and LGN to the visual cortex) and
long-range projections (connections across different visual cortical areas).
However, in the mammalian visual system, there are connections within each
visual cortical area, known as lateral (or horizontal) connections. These would
roughly correspond to connections within CNN feature maps, and this important
architectural feature is missing in current CNN models. In this paper, we
present how such lateral connections can be modeled within the standard CNN
framework, and test its benefits and analyze its emergent properties in
relation to the biological visual system. We will focus on two main
architectural features of lateral connections: (1) recurrent activation and (2)
separation of excitatory and inhibitory connections. We show that recurrent CNN
using weight sharing is equivalent to lateral connections, and propose a custom
loss function to separate excitatory and inhibitory weights. The addition of
these two leads to increased classification accuracy, and importantly, the
activation properties and connection properties of the resulting model show
properties similar to those observed in the biological visual system. We expect
our approach to help align CNN closer to its biological counterpart and better
understand the principles of visual cortical computation.
\\ ( https://arxiv.org/abs/2509.15460 ,  4462kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15491 (*cross-listing*)
Date: Thu, 18 Sep 2025 23:59:13 GMT   (689kb)

Title: Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent
  Robotic Systems
Authors: Reza Pirayeshshirazinezhad, Nima Fathi
Categories: cs.RO cs.AI cs.SY eess.SY
\\
  We present an explainable AI-enhanced supervisory control framework for
multi-agent robotics that combines (i) a timed-automata supervisor for safe,
auditable mode switching, (ii) robust continuous control (Lyapunov-based
controller for large-angle maneuver; sliding-mode controller (SMC) with
boundary layers for precision and disturbance rejection), and (iii) an
explainable predictor that maps mission context to gains and expected
performance (energy, error). Monte Carlo-driven optimization provides the
training data, enabling transparent real-time trade-offs.
  We validated the approach in two contrasting domains, spacecraft formation
flying and autonomous underwater vehicles (AUVs). Despite different
environments (gravity/actuator bias vs. hydrodynamic drag/currents), both share
uncertain six degrees of freedom (6-DOF) rigid-body dynamics, relative motion,
and tight tracking needs, making them representative of general robotic
systems. In the space mission, the supervisory logic selects parameters that
meet mission criteria. In AUV leader-follower tests, the same SMC structure
maintains a fixed offset under stochastic currents with bounded steady error.
In spacecraft validation, the SMC controller achieved submillimeter alignment
with 21.7% lower tracking error and 81.4% lower energy consumption compared to
Proportional-Derivative PD controller baselines. At the same time, in AUV
tests, SMC maintained bounded errors under stochastic currents. These results
highlight both the portability and the interpretability of the approach for
safety-critical, resource-constrained multi-agent robotics.
\\ ( https://arxiv.org/abs/2509.15491 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15510 (*cross-listing*)
Date: Fri, 19 Sep 2025 01:20:28 GMT   (6054kb)

Title: The (Short-Term) Effects of Large Language Models on Unemployment and
  Earnings
Authors: Danqing Chen, Carina Kane, Austin Kozlowski, Nadav Kunievsky, and
  James A. Evans
Categories: econ.GN cs.AI cs.CY q-fin.EC
\\
  Large Language Models have spread rapidly since the release of ChatGPT in
late 2022, accompanied by claims of major productivity gains but also concerns
about job displacement. This paper examines the short-run labor market effects
of LLM adoption by comparing earnings and unemployment across occupations with
differing levels of exposure to these technologies. Using a Synthetic
Difference in Differences approach, we estimate the impact of LLM exposure on
earnings and unemployment. Our findings show that workers in highly exposed
occupations experienced earnings increases following ChatGPT's introduction,
while unemployment rates remained unchanged. These results suggest that initial
labor market adjustments to LLMs operate primarily through earnings rather than
worker reallocation.
\\ ( https://arxiv.org/abs/2509.15510 ,  6054kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15557 (*cross-listing*)
Date: Fri, 19 Sep 2025 03:40:27 GMT   (174kb)

Title: Reward Hacking Mitigation using Verifiable Composite Rewards
Authors: Mirza Farhan Bin Tarek, Rahmatollah Beheshti
Categories: cs.LG cs.AI
Comments: Accepted at the 16th ACM Conference on Bioinformatics, Computational
  Biology, and Health Informatics (ACM-BCB 2025)
\\
  Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that
large language models (LLMs) can develop their own reasoning without direct
supervision. However, applications in the medical domain, specifically for
question answering, are susceptible to significant reward hacking during the
reasoning phase. Our work addresses two primary forms of this behavior: i)
providing a final answer without preceding reasoning, and ii) employing
non-standard reasoning formats to exploit the reward mechanism. To mitigate
these, we introduce a composite reward function with specific penalties for
these behaviors. Our experiments show that extending RLVR with our proposed
reward model leads to better-formatted reasoning with less reward hacking and
good accuracy compared to the baselines. This approach marks a step toward
reducing reward hacking and enhancing the reliability of models utilizing RLVR.
\\ ( https://arxiv.org/abs/2509.15557 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15570 (*cross-listing*)
Date: Fri, 19 Sep 2025 04:10:31 GMT   (822kb)

Title: Contrastive Learning with Spectrum Information Augmentation in Abnormal
  Sound Detection
Authors: Xinxin Meng, Jiangtao Guo, Yunxiang Zhang, Shun Huang
Categories: cs.SD cs.AI eess.AS
Comments: Accepted CVIPPR 2024 April Xiamen China
\\
  The outlier exposure method is an effective approach to address the
unsupervised anomaly sound detection problem. The key focus of this method is
how to make the model learn the distribution space of normal data. Based on
biological perception and data analysis, it is found that anomalous audio and
noise often have higher frequencies. Therefore, we propose a data augmentation
method for high-frequency information in contrastive learning. This enables the
model to pay more attention to the low-frequency information of the audio,
which represents the normal operational mode of the machine. We evaluated the
proposed method on the DCASE 2020 Task 2. The results showed that our method
outperformed other contrastive learning methods used on this dataset. We also
evaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.
\\ ( https://arxiv.org/abs/2509.15570 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15582 (*cross-listing*)
Date: Fri, 19 Sep 2025 04:33:39 GMT   (2295kb)

Title: Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework
  with Residual-enhanced DRL for Visually Impaired Scenarios
Authors: Yuting Zeng, Zhiwen Zheng, You Zhou, JiaLing Xiao, Yongbin Yu, Manping
  Fan, Bo Gong, and Liyong Ren
Categories: cs.RO cs.AI
Comments: 20 pages, 16 figures
\\
  This paper proposes a momentum-constrained hybrid heuristic trajectory
optimization framework (MHHTOF) tailored for assistive navigation in visually
impaired scenarios, integrating trajectory sampling generation, optimization
and evaluation with residual-enhanced deep reinforcement learning (DRL). In the
first stage, heuristic trajectory sampling cluster (HTSC) is generated in the
Frenet coordinate system using third-order interpolation with fifth-order
polynomials and momentum-constrained trajectory optimization (MTO) constraints
to ensure smoothness and feasibility. After first stage cost evaluation, the
second stage leverages a residual-enhanced actor-critic network with LSTM-based
temporal feature modeling to adaptively refine trajectory selection in the
Cartesian coordinate system. A dual-stage cost modeling mechanism (DCMM) with
weight transfer aligns semantic priorities across stages, supporting
human-centered optimization. Experimental results demonstrate that the proposed
LSTM-ResB-PPO achieves significantly faster convergence, attaining stable
policy performance in approximately half the training iterations required by
the PPO baseline, while simultaneously enhancing both reward outcomes and
training stability. Compared to baseline method, the selected model reduces
average cost and cost variance by 30.3% and 53.3%, and lowers ego and obstacle
risks by over 77%. These findings validate the framework's effectiveness in
enhancing robustness, safety, and real-time feasibility in complex assistive
planning tasks.
\\ ( https://arxiv.org/abs/2509.15582 ,  2295kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15588 (*cross-listing*)
Date: Fri, 19 Sep 2025 04:42:31 GMT   (552kb)

Title: CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational
  Search via Query Reformulation and Rank Fusion
Authors: Yu-Cheng Chang, Guan-Wei Yeo, Quah Eugene, Fan-Jie Shih, Yuan-Ching
  Kuo, Tsung-En Yu, Hung-Chun Hsu, Ming-Feng Tsai, Chuan-Ju Wang
Categories: cs.IR cs.AI
\\
  The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both
interactive and offline submission tasks. The former requires systems to
operate under real-time constraints, making robustness and efficiency as
important as accuracy, while the latter enables controlled evaluation of
passage ranking and response generation with pre-defined datasets. To address
this, we explored query rewriting and retrieval fusion as core strategies. We
built our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion
(RRF) strategies to handle different submission tasks. Results show that
reranking and fusion improve robustness while revealing trade-offs between
effectiveness and efficiency across both tasks.
\\ ( https://arxiv.org/abs/2509.15588 ,  552kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15591 (*cross-listing*)
Date: Fri, 19 Sep 2025 04:47:16 GMT   (8682kb)

Title: Latent Zoning Network: A Unified Principle for Generative Modeling,
  Representation Learning, and Classification
Authors: Zinan Lin, Enshu Liu, Xuefei Ning, Junyi Zhu, Wenyu Wang, Sergey
  Yekhanin
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: Published in NeurIPS 2025
\\
  Generative modeling, representation learning, and classification are three
core problems in machine learning (ML), yet their state-of-the-art (SoTA)
solutions remain largely disjoint. In this paper, we ask: Can a unified
principle address all three? Such unification could simplify ML pipelines and
foster greater synergy across tasks. We introduce Latent Zoning Network (LZN)
as a step toward this goal. At its core, LZN creates a shared Gaussian latent
space that encodes information across all tasks. Each data type (e.g., images,
text, labels) is equipped with an encoder that maps samples to disjoint latent
zones, and a decoder that maps latents back to data. ML tasks are expressed as
compositions of these encoders and decoders: for example, label-conditional
image generation uses a label encoder and image decoder; image embedding uses
an image encoder; classification uses an image encoder and label decoder. We
demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN
can enhance existing models (image generation): When combined with the SoTA
Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without
modifying the training objective. (2) LZN can solve tasks independently
(representation learning): LZN can implement unsupervised representation
learning without auxiliary loss functions, outperforming the seminal MoCo and
SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear
classification on ImageNet. (3) LZN can solve multiple tasks simultaneously
(joint generation and classification): With image and label encoders/decoders,
LZN performs both tasks jointly by design, improving FID and achieving SoTA
classification accuracy on CIFAR10. The code and trained models are available
at https://github.com/microsoft/latent-zoning-networks. The project website is
at https://zinanlin.me/blogs/latent_zoning_networks.html.
\\ ( https://arxiv.org/abs/2509.15591 ,  8682kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15641 (*cross-listing*)
Date: Fri, 19 Sep 2025 06:07:38 GMT   (234kb)

Title: Information Geometry of Variational Bayes
Authors: Mohammad Emtiyaz Khan
Categories: cs.LG cs.AI stat.ML
\\
  We highlight a fundamental connection between information geometry and
variational Bayes (VB) and discuss its consequences for machine learning. Under
certain conditions, a VB solution always requires estimation or computation of
natural gradients. We show several consequences of this fact by using the
natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian
Learning Rule (BLR). These include (i) a simplification of Bayes' rule as
addition of natural gradients, (ii) a generalization of quadratic surrogates
used in gradient-based methods, and (iii) a large-scale implementation of VB
algorithms for large language models. Neither the connection nor its
consequences are new but we further emphasize the common origins of the two
fields of information geometry and Bayes with a hope to facilitate more work at
the intersection of the two fields.
\\ ( https://arxiv.org/abs/2509.15641 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15651 (*cross-listing*)
Date: Fri, 19 Sep 2025 06:20:54 GMT   (341kb)

Title: Toward Efficient Influence Function: Dropout as a Compression Tool
Authors: Yuchen Zhang, Mohammad Mohammadi Amiri
Categories: cs.LG cs.AI
\\
  Assessing the impact the training data on machine learning models is crucial
for understanding the behavior of the model, enhancing the transparency, and
selecting training data. Influence function provides a theoretical framework
for quantifying the effect of training data points on model's performance given
a specific test data. However, the computational and memory costs of influence
function presents significant challenges, especially for large-scale models,
even when using approximation methods, since the gradients involved in
computation are as large as the model itself. In this work, we introduce a
novel approach that leverages dropout as a gradient compression mechanism to
compute the influence function more efficiently. Our method significantly
reduces computational and memory overhead, not only during the influence
function computation but also in gradient compression process. Through
theoretical analysis and empirical validation, we demonstrate that our method
could preserves critical components of the data influence and enables its
application to modern large-scale models.
\\ ( https://arxiv.org/abs/2509.15651 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15658 (*cross-listing*)
Date: Fri, 19 Sep 2025 06:32:30 GMT   (47kb)

Title: Chunk Knowledge Generation Model for Enhanced Information Retrieval: A
  Multi-task Learning Approach
Authors: Jisu Kim, Jinhee Park, Changhyun Jeon, Jungwoo Choi, Keonwoo Kim,
  Minji Hong, Sehyun Kim
Categories: cs.IR cs.AI
\\
  Traditional query expansion techniques for addressing vocabulary mismatch
problems in information retrieval are context-sensitive and may lead to
performance degradation. As an alternative, document expansion research has
gained attention, but existing methods such as Doc2Query have limitations
including excessive preprocessing costs, increased index size, and reliability
issues with generated content. To mitigate these problems and seek more
structured and efficient alternatives, this study proposes a method that
divides documents into chunk units and generates textual data for each chunk to
simultaneously improve retrieval efficiency and accuracy. The proposed "Chunk
Knowledge Generation Model" adopts a T5-based multi-task learning structure
that simultaneously generates titles and candidate questions from each document
chunk while extracting keywords from user queries. This approach maximizes
computational efficiency by generating and extracting three types of semantic
information in parallel through a single encoding and two decoding processes.
The generated data is utilized as additional information in the retrieval
system. GPT-based evaluation on 305 query-document pairs showed that retrieval
using the proposed model achieved 95.41% accuracy at Top@10, demonstrating
superior performance compared to document chunk-level retrieval. This study
contributes by proposing an approach that simultaneously generates titles and
candidate questions from document chunks for application in retrieval
pipelines, and provides empirical evidence applicable to large-scale
information retrieval systems by demonstrating improved retrieval accuracy
through qualitative evaluation.
\\ ( https://arxiv.org/abs/2509.15658 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15661 (*cross-listing*)
Date: Fri, 19 Sep 2025 06:39:39 GMT   (9440kb)

Title: SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio
  Language Models
Authors: Qiaolin Wang, Xilin Jiang, Linyang He, Junkai Wu, Nima Mesgarani
Categories: cs.SD cs.AI cs.CL eess.AS
\\
  While large audio-language models (LALMs) have demonstrated state-of-the-art
audio understanding, their reasoning capability in complex soundscapes still
falls behind large vision-language models (LVLMs). Compared to the visual
domain, one bottleneck is the lack of large-scale chain-of-thought audio data
to teach LALM stepwise reasoning. To circumvent this data and modality gap, we
present SightSound-R1, a cross-modal distillation framework that transfers
advanced reasoning from a stronger LVLM teacher to a weaker LALM student on the
same audio-visual question answering (AVQA) dataset. SightSound-R1 consists of
three core steps: (i) test-time scaling to generate audio-focused chains of
thought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter
hallucinations, and (iii) a distillation pipeline with supervised fine-tuning
(SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM
student. Results show that SightSound-R1 improves LALM reasoning performance
both in the in-domain AVQA test set as well as in unseen auditory scenes and
questions, outperforming both pretrained and label-only distilled baselines.
Thus, we conclude that vision reasoning can be effectively transferred to audio
models and scaled with abundant audio-visual data.
\\ ( https://arxiv.org/abs/2509.15661 ,  9440kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15666 (*cross-listing*)
Date: Fri, 19 Sep 2025 06:42:27 GMT   (235kb)

Title: TISDiSS: A Training-Time and Inference-Time Scalable Framework for
  Discriminative Source Separation
Authors: Yongsheng Feng, Yuetonghui Xu, Jiehui Luo, Hongjia Liu, Xiaobing Li,
  Feng Yu, Wei Li
Categories: cs.SD cs.AI eess.AS
Comments: submitted to ICASSP 2026
\\
  Source separation is a fundamental task in speech, music, and audio
processing, and it also provides cleaner and larger data for training
generative models. However, improving separation performance in practice often
depends on increasingly large networks, inflating training and deployment
costs. Motivated by recent advances in inference-time scaling for generative
modeling, we propose Training-Time and Inference-Time Scalable Discriminative
Source Separation (TISDiSS), a unified framework that integrates early-split
multi-loss supervision, shared-parameter design, and dynamic inference
repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting
inference depth without retraining additional models. We further provide
systematic analyses of architectural and training choices and show that
training with more inference repetitions improves shallow-inference
performance, benefiting low-latency applications. Experiments on standard
speech separation benchmarks demonstrate state-of-the-art performance with a
reduced parameter count, establishing TISDiSS as a scalable and practical
framework for adaptive source separation.
\\ ( https://arxiv.org/abs/2509.15666 ,  235kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15674 (*cross-listing*)
Date: Fri, 19 Sep 2025 06:49:40 GMT   (2744kb)

Title: Inference Offloading for Cost-Sensitive Binary Classification at the
  Edge
Authors: Vishnu Narayanan Moothedath, Umang Agarwal, Umeshraja N, James Richard
  Gross, Jaya Prakash Champati, Sharayu Moharir
Categories: cs.LG cs.AI cs.DC
\\
  We focus on a binary classification problem in an edge intelligence system
where false negatives are more costly than false positives. The system has a
compact, locally deployed model, which is supplemented by a larger, remote
model, which is accessible via the network by incurring an offloading cost. For
each sample, our system first uses the locally deployed model for inference.
Based on the output of the local model, the sample may be offloaded to the
remote model. This work aims to understand the fundamental trade-off between
classification accuracy and these offloading costs within such a hierarchical
inference (HI) system. To optimize this system, we propose an online learning
framework that continuously adapts a pair of thresholds on the local model's
confidence scores. These thresholds determine the prediction of the local model
and whether a sample is classified locally or offloaded to the remote model. We
present a closed-form solution for the setting where the local model is
calibrated. For the more general case of uncalibrated models, we introduce
H2T2, an online two-threshold hierarchical inference policy, and prove it
achieves sublinear regret. H2T2 is model-agnostic, requires no training, and
learns in the inference phase using limited feedback. Simulations on real-world
datasets show that H2T2 consistently outperforms naive and single-threshold HI
policies, sometimes even surpassing offline optima. The policy also
demonstrates robustness to distribution shifts and adapts effectively to
mismatched classifiers.
\\ ( https://arxiv.org/abs/2509.15674 ,  2744kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15676 (*cross-listing*)
Date: Fri, 19 Sep 2025 06:50:03 GMT   (741kb)

Title: KITE: Kernelized and Information Theoretic Exemplars for In-Context
  Learning
Authors: Vaibhav Singh, Soumya Suvra Ghosal, Kapu Nirmal Joshua, Soumyabrata
  Pal, Sayak Ray Chowdhury
Categories: cs.LG cs.AI cs.CL
\\
  In-context learning (ICL) has emerged as a powerful paradigm for adapting
large language models (LLMs) to new and data-scarce tasks using only a few
carefully selected task-specific examples presented in the prompt. However,
given the limited context size of LLMs, a fundamental question arises: Which
examples should be selected to maximize performance on a given user query?
While nearest-neighbor-based methods like KATE have been widely adopted for
this purpose, they suffer from well-known drawbacks in high-dimensional
embedding spaces, including poor generalization and a lack of diversity. In
this work, we study this problem of example selection in ICL from a principled,
information theory-driven perspective. We first model an LLM as a linear
function over input embeddings and frame the example selection task as a
query-specific optimization problem: selecting a subset of exemplars from a
larger example bank that minimizes the prediction error on a specific query.
This formulation departs from traditional generalization-focused learning
theoretic approaches by targeting accurate prediction for a specific query
instance. We derive a principled surrogate objective that is approximately
submodular, enabling the use of a greedy algorithm with an approximation
guarantee. We further enhance our method by (i) incorporating the kernel trick
to operate in high-dimensional feature spaces without explicit mappings, and
(ii) introducing an optimal design-based regularizer to encourage diversity in
the selected examples. Empirically, we demonstrate significant improvements
over standard retrieval methods across a suite of classification tasks,
highlighting the benefits of structure-aware, diverse example selection for ICL
in real-world, label-scarce scenarios.
\\ ( https://arxiv.org/abs/2509.15676 ,  741kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15733 (*cross-listing*)
Date: Fri, 19 Sep 2025 08:04:50 GMT   (21033kb)

Title: GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic
  Manipulation
Authors: Quanhao Qian, Guoyang Zhao, Gongjie Zhang, Jiuniu Wang, Ran Xu,
  Junlong Gao, Deli Zhao
Categories: cs.RO cs.AI
\\
  Effective robotic manipulation relies on a precise understanding of 3D scene
geometry, and one of the most straightforward ways to acquire such geometry is
through multi-view observations. Motivated by this, we present GP3 -- a 3D
geometry-aware robotic manipulation policy that leverages multi-view input. GP3
employs a spatial encoder to infer dense spatial features from RGB
observations, which enable the estimation of depth and camera parameters,
leading to a compact yet expressive 3D scene representation tailored for
manipulation. This representation is fused with language instructions and
translated into continuous actions via a lightweight policy head. Comprehensive
experiments demonstrate that GP3 consistently outperforms state-of-the-art
methods on simulated benchmarks. Furthermore, GP3 transfers effectively to
real-world robots without depth sensors or pre-mapped environments, requiring
only minimal fine-tuning. These results highlight GP3 as a practical,
sensor-agnostic solution for geometry-aware robotic manipulation.
\\ ( https://arxiv.org/abs/2509.15733 ,  21033kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15759 (*cross-listing*)
Date: Fri, 19 Sep 2025 08:37:51 GMT   (423kb)

Title: On Optimal Steering to Achieve Exact Fairness
Authors: Mohit Sharma, Amit Jayant Deshpande, Chiranjib Bhattacharyya, Rajiv
  Ratn Shah
Categories: cs.LG cs.AI
Comments: Accepted for Presentation at Neurips 2025
\\
  To fix the 'bias in, bias out' problem in fair machine learning, it is
important to steer feature distributions of data or internal representations of
Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes.
Previous work on fair generative models and representation steering could
greatly benefit from provable fairness guarantees on the model output. We
define a distribution as ideal if the minimizer of any cost-sensitive risk on
it is guaranteed to have exact group-fair outcomes (e.g., demographic parity,
equal opportunity)-in other words, it has no fairness-utility trade-off. We
formulate an optimization program for optimal steering by finding the nearest
ideal distribution in KL-divergence, and provide efficient algorithms for it
when the underlying distributions come from well-known parametric families
(e.g., normal, log-normal). Empirically, our optimal steering techniques on
both synthetic and real-world datasets improve fairness without diminishing
utility (and sometimes even improve utility). We demonstrate affine steering of
LLM representations to reduce bias in multi-class classification, e.g.,
occupation prediction from a short biography in Bios dataset (De-Arteaga et
al.). Furthermore, we steer internal representations of LLMs towards desired
outputs so that it works equally well across different groups.
\\ ( https://arxiv.org/abs/2509.15759 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15796 (*cross-listing*)
Date: Fri, 19 Sep 2025 09:24:42 GMT   (1055kb)

Title: Monte Carlo Tree Diffusion with Multiple Experts for Protein Design
Authors: Xuefeng Liu, Mingxuan Cao, Songhao Jiang, Xiao Luo, Xiaotian Duan,
  Mengdi Wang, Tobin R. Sosnick, Jinbo Xu, Rick Stevens
Categories: cs.LG cs.AI q-bio.BM
\\
  The goal of protein design is to generate amino acid sequences that fold into
functional structures with desired properties. Prior methods combining
autoregressive language models with Monte Carlo Tree Search (MCTS) struggle
with long-range dependencies and suffer from an impractically large search
space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts,
which integrates masked diffusion models with tree search to enable multi-token
planning and efficient exploration. Unlike autoregressive planners, MCTD-ME
uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine,
jointly revising multiple positions and scaling to large sequence spaces. It
further leverages experts of varying capacities to enrich exploration, guided
by a pLDDT-based masking schedule that targets low-confidence regions while
preserving reliable residues. We propose a novel multi-expert selection rule
(PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse
folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and
unguided baselines in both sequence recovery (AAR) and structural similarity
(scTM), with gains increasing for longer proteins and benefiting from
multi-expert guidance. More generally, the framework is model-agnostic and
applicable beyond inverse folding, including de novo protein engineering and
multi-objective molecular generation.
\\ ( https://arxiv.org/abs/2509.15796 ,  1055kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15799 (*cross-listing*)
Date: Fri, 19 Sep 2025 09:27:15 GMT   (3023kb)

Title: Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent
  Control
Authors: Max Studt and Georg Schildbach
Categories: eess.SY cs.AI cs.RO cs.SY math.OC
\\
  Achieving safe and coordinated behavior in dynamic, constraint-rich
environments remains a major challenge for learning-based control. Pure
end-to-end learning often suffers from poor sample efficiency and limited
reliability, while model-based methods depend on predefined references and
struggle to generalize. We propose a hierarchical framework that combines
tactical decision-making via reinforcement learning (RL) with low-level
execution through Model Predictive Control (MPC). For the case of multi-agent
systems this means that high-level policies select abstract targets from
structured regions of interest (ROIs), while MPC ensures dynamically feasible
and safe motion. Tested on a predator-prey benchmark, our approach outperforms
end-to-end and shielding-based RL baselines in terms of reward, safety, and
consistency, underscoring the benefits of combining structured learning with
model-based control.
\\ ( https://arxiv.org/abs/2509.15799 ,  3023kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15810 (*cross-listing*)
Date: Fri, 19 Sep 2025 09:37:48 GMT   (2582kb)

Title: Instance Generation for Meta-Black-Box Optimization through Latent Space
  Reverse Engineering
Authors: Chen Wang, Zeyuan Ma, Zhiguang Cao, Yue-Jiao Gong
Categories: cs.LG cs.AI cs.NE
\\
  To relieve intensive human-expertise required to design optimization
algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage
generalization strength of meta-learning to train neural network-based
algorithm design policies over a predefined training problem set, which
automates the adaptability of the low-level optimizers on unseen problem
instances. Currently, a common training problem set choice in existing MetaBBOs
is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the
MetaBBO's development, problem instances in CoCo-BBOB are more or less limited
in diversity, raising the risk of overfitting of MetaBBOs, which might further
results in poor generalization. In this paper, we propose an instance
generation approach, termed as \textbf{LSRE}, which could generate diverse
training problem instances for MetaBBOs to learn more generalizable policies.
LSRE first trains an autoencoder which maps high-dimensional problem features
into a 2-dimensional latent space. Uniform-grid sampling in this latent space
leads to hidden representations of problem instances with sufficient diversity.
By leveraging a genetic-programming approach to search function formulas with
minimal L2-distance to these hidden representations, LSRE reverse engineers a
diversified problem set, termed as \textbf{Diverse-BBO}. We validate the
effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe
their generalization performances on either synthetic or realistic scenarios.
Extensive experimental results underscore the superiority of Diverse-BBO to
existing training set choices in MetaBBOs. Further ablation studies not only
demonstrate the effectiveness of design choices in LSRE, but also reveal
interesting insights on instance diversity and MetaBBO's generalization.
\\ ( https://arxiv.org/abs/2509.15810 ,  2582kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15812 (*cross-listing*)
Date: Fri, 19 Sep 2025 09:40:39 GMT   (3823kb)

Title: Diversity of Structured Domains via k-Kemeny Scores
Authors: Piotr Faliszewski, Krzysztof Sornat, Stanis{\l}aw Szufa, Tomasz
  W\k{a}s
Categories: cs.GT cs.AI cs.MA
\\
  In the k-Kemeny problem, we are given an ordinal election, i.e., a collection
of votes ranking the candidates from best to worst, and we seek the smallest
number of swaps of adjacent candidates that ensure that the election has at
most k different rankings. We study this problem for a number of structured
domains, including the single-peaked, single-crossing, group-separable, and
Euclidean ones. We obtain two kinds of results: (1) We show that k-Kemeny
remains intractable under most of these domains, even for k=2, and (2) we use
k-Kemeny to rank these domains in terms of their diversity.
\\ ( https://arxiv.org/abs/2509.15812 ,  3823kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15857 (*cross-listing*)
Date: Fri, 19 Sep 2025 10:47:34 GMT   (3370kb)

Title: EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving
  Brain Network
Authors: Rikuto Kotoge, Zheng Chen, Tasuku Kimura, Yasuko Matsubara, Takufumi
  Yanagisawa, Haruhiko Kishima, Yasushi Sakurai
Categories: cs.LG cs.AI
Comments: Accepted by NeurIPS 2025 (spotlight)
\\
  Dynamic GNNs, which integrate temporal and spatial features in
Electroencephalography (EEG) data, have shown great potential in automating
seizure detection. However, fully capturing the underlying dynamics necessary
to represent brain states, such as seizure and non-seizure, remains a
non-trivial task and presents two fundamental challenges. First, most existing
dynamic GNN methods are built on temporally fixed static graphs, which fail to
reflect the evolving nature of brain connectivity during seizure progression.
Second, current efforts to jointly model temporal signals and graph structures
and, more importantly, their interactions remain nascent, often resulting in
inconsistent performance. To address these challenges, we present the first
theoretical analysis of these two problems, demonstrating the effectiveness and
necessity of explicit dynamic modeling and time-then-graph dynamic GNN method.
Building on these insights, we propose EvoBrain, a novel seizure detection
model that integrates a two-stream Mamba architecture with a GCN enhanced by
Laplacian Positional Encoding, following neurological insights. Moreover,
EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes
and edges to evolve over time. Our contributions include (a) a theoretical
analysis proving the expressivity advantage of explicit dynamic modeling and
time-then-graph over other approaches, (b) a novel and efficient model that
significantly improves AUROC by 23% and F1 score by 30%, compared with the
dynamic GNN baseline, and (c) broad evaluations of our method on the
challenging early seizure prediction tasks.
\\ ( https://arxiv.org/abs/2509.15857 ,  3370kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15872 (*cross-listing*)
Date: Fri, 19 Sep 2025 11:14:46 GMT   (2104kb)

Title: DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism
  Prediction
Authors: Manajit Das, Ajnabiul Hoque, Mayank Baranwal, Raghavan B. Sunoj
Categories: physics.chem-ph cs.AI cs.LG
Comments: 37 pages, 8 figures
\\
  Prediction of complete step-by-step chemical reaction mechanisms (CRMs)
remains a major challenge. Whereas the traditional approaches in CRM tasks rely
on expert-driven experiments or costly quantum chemical computations,
contemporary deep learning (DL) alternatives ignore key intermediates and
mechanistic steps and often suffer from hallucinations. We present DeepMech, an
interpretable graph-based DL framework employing atom- and bond-level
attention, guided by generalized templates of mechanistic operations (TMOps),
to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K
atom-mapped and mass-balanced elementary steps), DeepMech achieves
98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in
complete CRM tasks, besides maintaining high fidelity even in
out-of-distribution scenarios as well as in predicting side and/or byproducts.
Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the
ability of DeepMech in effectively reconstructing pathways from simple
primordial substrates to complex biomolecules such as serine and aldopentose.
Attention analysis identifies reactive atoms/bonds in line with chemical
intuition, rendering our model interpretable and suitable for reaction design.
\\ ( https://arxiv.org/abs/2509.15872 ,  2104kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15892 (*cross-listing*)
Date: Fri, 19 Sep 2025 11:43:01 GMT   (19565kb)

Title: MoAngelo: Motion-Aware Neural Surface Reconstruction for Dynamic Scenes
Authors: Mohamed Ebbed, Zorah L\"ahner
Categories: cs.GR cs.AI cs.CV
\\
  Dynamic scene reconstruction from multi-view videos remains a fundamental
challenge in computer vision. While recent neural surface reconstruction
methods have achieved remarkable results in static 3D reconstruction, extending
these approaches with comparable quality for dynamic scenes introduces
significant computational and representational challenges. Existing dynamic
methods focus on novel-view synthesis, therefore, their extracted meshes tend
to be noisy. Even approaches aiming for geometric fidelity often result in too
smooth meshes due to the ill-posedness of the problem. We present a novel
framework for highly detailed dynamic reconstruction that extends the static 3D
reconstruction method NeuralAngelo to work in dynamic settings. To that end, we
start with a high-quality template scene reconstruction from the initial frame
using NeuralAngelo, and then jointly optimize deformation fields that track the
template and refine it based on the temporal sequence. This flexible template
allows updating the geometry to include changes that cannot be modeled with the
deformation field, for instance occluded parts or the changes in the topology.
We show superior reconstruction accuracy in comparison to previous
state-of-the-art methods on the ActorsHQ dataset.
\\ ( https://arxiv.org/abs/2509.15892 ,  19565kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15895 (*cross-listing*)
Date: Fri, 19 Sep 2025 11:48:48 GMT   (3550kb)

Title: From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and
  AI Methods for Childhood Leukemia Prediction
Authors: Henning H\"ofener (1), Farina Kock (1), Martina Pontones (2), Tabita
  Ghete (2 and 3), David Pfrang (1), Nicholas Dickel (4), Meik Kunz (4),
  Daniela P. Schacherer (1), David A. Clunie (5), Andrey Fedorov (6), Max
  Westphal (1), Markus Metzler (2 and 3 and 7) ((1) Fraunhofer Institute for
  Digital Medicine MEVIS, Bremen, Germany, (2) Department of Pediatrics and
  Adolescent Medicine, University Hospital Erlangen, Erlangen, Germany, (3)
  Bavarian Cancer Research Center (BZKF), Erlangen, Germany, (4) Medical
  Informatics, Friedrich-Alexander University of Erlangen-N\"urnberg, Erlangen,
  Germany, (5) PixelMed Publishing LLC, Bangor, PA, USA, (6) Department of
  Radiology, Brigham and Women's Hospital and Harvard Medical School, Boston,
  MA, USA, (7) Comprehensive Cancer Center Erlangen-EMN, Erlangen, Germany)
Categories: cs.LG cs.AI cs.CV
\\
  Leukemia diagnosis primarily relies on manual microscopic analysis of bone
marrow morphology supported by additional laboratory parameters, making it
complex and time consuming. While artificial intelligence (AI) solutions have
been proposed, most utilize private datasets and only cover parts of the
diagnostic pipeline. Therefore, we present a large, high-quality, publicly
available leukemia bone marrow dataset spanning the entire diagnostic process,
from cell detection to diagnosis. Using this dataset, we further propose
methods for cell detection, cell classification, and diagnosis prediction. The
dataset comprises 246 pediatric patients with diagnostic, clinical and
laboratory information, over 40 000 cells with bounding box annotations and
more than 28 000 of these with high-quality class labels, making it the most
comprehensive dataset publicly available. Evaluation of the AI models yielded
an average precision of 0.96 for the cell detection, an area under the curve of
0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean
F1-score of 0.90 for the diagnosis prediction using predicted cell counts.
While the proposed approaches demonstrate their usefulness for AI-assisted
diagnostics, the dataset will foster further research and development in the
field, ultimately contributing to more precise diagnoses and improved patient
outcomes.
\\ ( https://arxiv.org/abs/2509.15895 ,  3550kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15908 (*cross-listing*)
Date: Fri, 19 Sep 2025 12:04:26 GMT   (5492kb)

Title: An Equivariant Graph Network for Interpretable Nanoporous Materials
  Design
Authors: Zhenhao Zhou, Salman Bin Kashif, Dawei Feng, Jin-Hu Dou, Kaihang Shi,
  Tao Deng, Zhenpeng Yao
Categories: cond-mat.mtrl-sci cs.AI
\\
  Nanoporous materials hold promise for diverse sustainable applications, yet
their vast chemical space poses challenges for efficient design. Machine
learning offers a compelling pathway to accelerate the exploration, but
existing models lack either interpretability or fidelity for elucidating the
correlation between crystal geometry and property. Here, we report a
three-dimensional periodic space sampling method that decomposes large
nanoporous structures into local geometrical sites for combined property
prediction and site-wise contribution quantification. Trained with a
constructed database and retrieved datasets, our model achieves
state-of-the-art accuracy and data efficiency for property prediction on gas
storage, separation, and electrical conduction. Meanwhile, this approach
enables the interpretation of the prediction and allows for accurate
identification of significant local sites for targeted properties. Through
identifying transferable high-performance sites across diverse nanoporous
frameworks, our model paves the way for interpretable, symmetry-aware
nanoporous materials design, which is extensible to other materials, like
molecular crystals and beyond.
\\ ( https://arxiv.org/abs/2509.15908 ,  5492kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15915 (*cross-listing*)
Date: Fri, 19 Sep 2025 12:10:28 GMT   (877kb)

Title: Foundation Models as World Models: A Foundational Study in Text-Based
  GridWorlds
Authors: Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber
Categories: cs.LG cs.AI
Comments: 20 pages, 9 figures. Accepted for presentation at the 39th Conference
  on Neural Information Processing Systems (NeurIPS 2025) Workshop on Embodied
  World Models for Decision Making
MSC-class: 68T05
ACM-class: I.2.6; I.2.8
\\
  While reinforcement learning from scratch has shown impressive results in
solving sequential decision-making tasks with efficient simulators, real-world
applications with expensive interactions require more sample-efficient agents.
Foundation models (FMs) are natural candidates to improve sample efficiency as
they possess broad knowledge and reasoning capabilities, but it is yet unclear
how to effectively integrate them into the reinforcement learning framework. In
this paper, we anticipate and, most importantly, evaluate two promising
strategies. First, we consider the use of foundation world models (FWMs) that
exploit the prior knowledge of FMs to enable training and evaluating agents
with simulated interactions. Second, we consider the use of foundation agents
(FAs) that exploit the reasoning capabilities of FMs for decision-making. We
evaluate both approaches empirically in a family of grid-world environments
that are suitable for the current generation of large language models (LLMs).
Our results suggest that improvements in LLMs already translate into better
FWMs and FAs; that FAs based on current LLMs can already provide excellent
policies for sufficiently simple environments; and that the coupling of FWMs
and reinforcement learning agents is highly promising for more complex settings
with partial observability and stochastic elements.
\\ ( https://arxiv.org/abs/2509.15915 ,  877kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15927 (*cross-listing*)
Date: Fri, 19 Sep 2025 12:30:26 GMT   (5184kb)

Title: Enhancing Generative Auto-bidding with Offline Reward Evaluation and
  Policy Search
Authors: Zhiyu Mou, Yiqin Lv, Miao Xu, Cheems Wang, Yixiu Mao, Qichen Ye, Chao
  Li, Rongquan Bai, Chuan Yu, Jian Xu, Bo Zheng
Categories: cs.LG cs.AI
\\
  Auto-bidding is an essential tool for advertisers to enhance their
advertising performance. Recent progress has shown that AI-Generated Bidding
(AIGB), which formulates the auto-bidding as a trajectory generation task and
trains a conditional diffusion-based planner on offline data, achieves superior
and stable performance compared to typical offline reinforcement learning
(RL)-based auto-bidding methods. However, existing AIGB methods still encounter
a performance bottleneck due to their neglect of fine-grained generation
quality evaluation and inability to explore beyond static datasets. To address
this, we propose AIGB-Pearl (\emph{Planning with EvAluator via RL}), a novel
method that integrates generative planning and policy optimization. The key to
AIGB-Pearl is to construct a non-bootstrapped \emph{trajectory evaluator} to
assign rewards and guide policy search, enabling the planner to optimize its
generation quality iteratively through interaction. Furthermore, to enhance
trajectory evaluator accuracy in offline settings, we incorporate three key
techniques: (i) a Large Language Model (LLM)-based architecture for better
representational capacity, (ii) hybrid point-wise and pair-wise losses for
better score learning, and (iii) adaptive integration of expert feedback for
better generalization ability. Extensive experiments on both simulated and
real-world advertising systems demonstrate the state-of-the-art performance of
our approach.
\\ ( https://arxiv.org/abs/2509.15927 ,  5184kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15932 (*cross-listing*)
Date: Fri, 19 Sep 2025 12:38:30 GMT   (102kb)

Title: The Alignment Bottleneck
Authors: Wenjun Cao
Categories: cs.LG cs.AI cs.IT math.IT stat.ML
\\
  Large language models improve with scale, yet feedback-based alignment still
exhibits systematic deviations from intended behavior. Motivated by bounded
rationality in economics and cognitive science, we view judgment as
resource-limited and feedback as a constrained channel. On this basis, we model
the loop as a two-stage cascade $U \to H \to Y$ given $S$, with cognitive
capacity $C_{\text{cog}|S}$ and average total capacity
$\bar{C}_{\text{tot}|S}$. Our main result is a capacity-coupled Alignment
Performance Interval. It pairs a data size-independent Fano lower bound proved
on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is
controlled by the same channel via $m \, \bar{C}_{\text{tot}|S}$. The PAC-Bayes
bound becomes an upper bound on the same true risk when the canonical
observable loss is used and the dataset is drawn from the same mixture. Under
these matched conditions, both limits are governed by a single capacity.
Consequences include that, with value complexity and capacity fixed, adding
labels alone cannot cross the bound; attaining lower risk on more complex
targets requires capacity that grows with $\log M$; and once useful signal
saturates capacity, further optimization tends to fit channel regularities,
consistent with reports of sycophancy and reward hacking. The analysis views
alignment as interface engineering: measure and allocate limited capacity,
manage task complexity, and decide where information is spent.
\\ ( https://arxiv.org/abs/2509.15932 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15937 (*cross-listing*)
Date: Fri, 19 Sep 2025 12:44:29 GMT   (14367kb)

Title: A Vision-Language-Action-Critic Model for Robotic Real-World
  Reinforcement Learning
Authors: Shaopeng Zhai, Qi Zhang, Tianyi Zhang, Fuxian Huang, Haoran Zhang,
  Ming Zhou, Shengzhe Zhang, Litao Liu, Sixu Lin, Jiangmiao Pang
Categories: cs.RO cs.AI
Comments: 26 pages,10 figures
\\
  Robotic real-world reinforcement learning (RL) with vision-language-action
(VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient
exploration. We introduce VLAC, a general process reward model built upon
InternVL and trained on large scale heterogeneous datasets. Given pairwise
observations and a language goal, it outputs dense progress delta and done
signal, eliminating task-specific reward engineering, and supports one-shot
in-context transfer to unseen tasks and environments. VLAC is trained on
vision-language datasets to strengthen perception, dialogic and reasoning
capabilities, together with robot and human trajectories data that ground
action generation and progress estimation, and additionally strengthened to
reject irrelevant prompts as well as detect regression or stagnation by
constructing large numbers of negative and semantically mismatched samples.
With prompt control, a single VLAC model alternately generating reward and
action tokens, unifying critic and policy. Deployed inside an asynchronous
real-world RL loop, we layer a graded human-in-the-loop protocol (offline
demonstration replay, return and explore, human guided explore) that
accelerates exploration and stabilizes early learning. Across four distinct
real-world manipulation tasks, VLAC lifts success rates from about 30\% to
about 90\% within 200 real-world interaction episodes; incorporating
human-in-the-loop interventions yields a further 50% improvement in sample
efficiency and achieves up to 100% final success.
\\ ( https://arxiv.org/abs/2509.15937 ,  14367kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15942 (*cross-listing*)
Date: Fri, 19 Sep 2025 12:53:24 GMT   (9132kb)

Title: ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow
  Matching
Authors: Graham Clyne, Guillaume Couairon, Guillaume Gastineau, Claire
  Monteleoni, Anastase Charantonis
Categories: physics.ao-ph cs.AI
\\
  Climate projections have uncertainties related to components of the climate
system and their interactions. A typical approach to quantifying these
uncertainties is to use climate models to create ensembles of repeated
simulations under different initial conditions. Due to the complexity of these
simulations, generating such ensembles of projections is computationally
expensive. In this work, we present ArchesClimate, a deep learning-based
climate model emulator that aims to reduce this cost. ArchesClimate is trained
on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution
of approximately 2.5x1.25 degrees. We train a flow matching model following
ArchesWeatherGen, which we adapt to predict near-term climate. Once trained,
the model generates states at a one-month lead time and can be used to
auto-regressively emulate climate model simulations of any length. We show that
for up to 10 years, these generations are stable and physically consistent. We
also show that for several important climate variables, ArchesClimate generates
simulations that are interchangeable with the IPSL model. This work suggests
that climate model emulators could significantly reduce the cost of climate
model simulations.
\\ ( https://arxiv.org/abs/2509.15942 ,  9132kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15952 (*cross-listing*)
Date: Fri, 19 Sep 2025 13:07:39 GMT   (3361kb)

Title: Compose Yourself: Average-Velocity Flow Matching for One-Step Speech
  Enhancement
Authors: Gang Yang, Yue Lei, Wenxin Tai, Jin Wu, Jia Chen, Ting Zhong, Fan Zhou
Categories: cs.SD cs.AI cs.LG
Comments: 5 pages, 2 figures, submitted to ICASSP 2026
\\
  Diffusion and flow matching (FM) models have achieved remarkable progress in
speech enhancement (SE), yet their dependence on multi-step generation is
computationally expensive and vulnerable to discretization errors. Recent
advances in one-step generative modeling, particularly MeanFlow, provide a
promising alternative by reformulating dynamics through average velocity
fields. In this work, we present COSE, a one-step FM framework tailored for SE.
To address the high training overhead of Jacobian-vector product (JVP)
computations in MeanFlow, we introduce a velocity composition identity to
compute average velocity efficiently, eliminating expensive computation while
preserving theoretical consistency and achieving competitive enhancement
quality. Extensive experiments on standard benchmarks show that COSE delivers
up to 5x faster sampling and reduces training cost by 40%, all without
compromising speech quality. Code is available at
https://github.com/ICDM-UESTC/COSE.
\\ ( https://arxiv.org/abs/2509.15952 ,  3361kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15959 (*cross-listing*)
Date: Fri, 19 Sep 2025 13:18:54 GMT   (3386kb)

Title: Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive
  Interfaces and Trustworthy Human-AI Collaboration
Authors: Zhuoyue Zhang, Haitong Xu
Categories: cs.HC cs.AI cs.CY
\\
  Autonomous navigation in maritime domains is accelerating alongside advances
in artificial intelligence, sensing, and connectivity. Opaque decision-making
and poorly calibrated human-automation interaction remain key barriers to safe
adoption. This article synthesizes 100 studies on automation transparency for
Maritime Autonomous Surface Ships (MASS) spanning situation awareness (SA),
human factors, interface design, and regulation. We (i) map the
Guidance-Navigation-Control stack to shore-based operational modes -- remote
supervision (RSM) and remote control (RCM) -- and identify where human unsafe
control actions (Human-UCAs) concentrate in handover and emergency loops; (ii)
summarize evidence that transparency features (decision rationales,
alternatives, confidence/uncertainty, and rule-compliance indicators) improve
understanding and support trust calibration, though reliability and
predictability often dominate trust; (iii) distill design strategies for
transparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI
presentation (textual/graphical overlays, color coding, conversational and
immersive UIs), and engineer-facing processes (resilient interaction design,
validation, and standardization). We integrate methods for Human-UCA
identification (STPA-Cog + IDAC), quantitative trust/SA assessment, and
operator workload monitoring, and outline regulatory and rule-based
implications including COLREGs formalization and route exchange. We conclude
with an adaptive transparency framework that couples operator state estimation
with explainable decision support to reduce cognitive overload and improve
takeover timeliness. The review highlights actionable figure-of-merit displays
(e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs
(rule traceability, confidence), and training pipelines (HIL/MIL, simulation)
as near-term levers for safer MASS operations.
\\ ( https://arxiv.org/abs/2509.15959 ,  3386kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15964 (*cross-listing*)
Date: Fri, 19 Sep 2025 13:23:08 GMT   (1513kb)

Title: MoE-CE: Enhancing Generalization for Deep Learning based Channel
  Estimation via a Mixture-of-Experts Framework
Authors: Tianyu Li and Yan Xin and Jianzhong (Charlie) Zhang
Categories: eess.SP cs.AI
\\
  Reliable channel estimation (CE) is fundamental for robust communication in
dynamic wireless environments, where models must generalize across varying
conditions such as signal-to-noise ratios (SNRs), the number of resource blocks
(RBs), and channel profiles. Traditional deep learning (DL)-based methods
struggle to generalize effectively across such diverse settings, particularly
under multitask and zero-shot scenarios. In this work, we propose MoE-CE, a
flexible mixture-of-experts (MoE) framework designed to enhance the
generalization capability of DL-based CE methods. MoE-CE provides an
appropriate inductive bias by leveraging multiple expert subnetworks, each
specialized in distinct channel characteristics, and a learned router that
dynamically selects the most relevant experts per input. This architecture
enhances model capacity and adaptability without a proportional rise in
computational cost while being agnostic to the choice of the backbone model and
the learning algorithm. Through extensive experiments on synthetic datasets
generated under diverse SNRs, RB numbers, and channel profiles, including
multitask and zero-shot evaluations, we demonstrate that MoE-CE consistently
outperforms conventional DL approaches, achieving significant performance gains
while maintaining efficiency.
\\ ( https://arxiv.org/abs/2509.15964 ,  1513kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15965 (*cross-listing*)
Date: Fri, 19 Sep 2025 13:24:17 GMT   (400kb)

Title: RLinf: Flexible and Efficient Large-scale Reinforcement Learning via
  Macro-to-Micro Flow Transformation
Authors: Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu
  Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing
  Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang
  Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu
  Wang
Categories: cs.LG cs.AI cs.DC
Comments: GitHub Repo: https://github.com/RLinf/RLinf
\\
  Reinforcement learning (RL) has demonstrated immense potential in advancing
artificial general intelligence, agentic intelligence, and embodied
intelligence. However, the inherent heterogeneity and dynamicity of RL
workflows often lead to low hardware utilization and slow training on existing
systems. In this paper, we present RLinf, a high-performance RL training system
based on our key observation that the major roadblock to efficient RL training
lies in system flexibility. To maximize flexibility and efficiency, RLinf is
built atop a novel RL system design paradigm called macro-to-micro flow
transformation (M2Flow), which automatically breaks down high-level,
easy-to-compose RL workflows at both the temporal and spatial dimensions, and
recomposes them into optimized execution flows. Supported by RLinf worker's
adaptive communication capability, we devise context switching and elastic
pipelining to realize M2Flow transformation, and a profiling-guided scheduling
policy to generate optimal execution plans. Extensive evaluations on both
reasoning RL and embodied RL tasks demonstrate that RLinf consistently
outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in
end-to-end training throughput.
\\ ( https://arxiv.org/abs/2509.15965 ,  400kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15981 (*cross-listing*)
Date: Fri, 19 Sep 2025 13:47:20 GMT   (4515kb)

Title: Uncertainty-Based Smooth Policy Regularisation for Reinforcement
  Learning with Few Demonstrations
Authors: Yujie Zhu, Charles A. Hepburn, Matthew Thorpe, Giovanni Montana
Categories: cs.LG cs.AI cs.RO stat.ML
\\
  In reinforcement learning with sparse rewards, demonstrations can accelerate
learning, but determining when to imitate them remains challenging. We propose
Smooth Policy Regularisation from Demonstrations (SPReD), a framework that
addresses the fundamental question: when should an agent imitate a
demonstration versus follow its own policy? SPReD uses ensemble methods to
explicitly model Q-value distributions for both demonstration and policy
actions, quantifying uncertainty for comparisons. We develop two complementary
uncertainty-aware methods: a probabilistic approach estimating the likelihood
of demonstration superiority, and an advantage-based approach scaling imitation
by statistical significance. Unlike prevailing methods (e.g. Q-filter) that
make binary imitation decisions, SPReD applies continuous,
uncertainty-proportional regularisation weights, reducing gradient variance
during training. Despite its computational simplicity, SPReD achieves
remarkable gains in experiments across eight robotics tasks, outperforming
existing approaches by up to a factor of 14 in complex tasks while maintaining
robustness to demonstration quality and quantity. Our code is available at
https://github.com/YujieZhu7/SPReD.
\\ ( https://arxiv.org/abs/2509.15981 ,  4515kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15986 (*cross-listing*)
Date: Fri, 19 Sep 2025 13:52:22 GMT   (982kb)

Title: EmoHeal: An End-to-End System for Personalized Therapeutic Music
  Retrieval from Fine-grained Emotions
Authors: Xinchen Wan, Jinhua Liang, Huan Zhang
Categories: cs.LG cs.AI cs.CL cs.HC cs.SD
Comments: 5 pages, 5 figures. Submitted to the 2026 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP 2026)
\\
  Existing digital mental wellness tools often overlook the nuanced emotional
states underlying everyday challenges. For example, pre-sleep anxiety affects
more than 1.5 billion people worldwide, yet current approaches remain largely
static and "one-size-fits-all", failing to adapt to individual needs. In this
work, we present EmoHeal, an end-to-end system that delivers personalized,
three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions
from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical
parameters via a knowledge graph grounded in music therapy principles (GEMS,
iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to
guide users from their current state toward a calmer one
("match-guide-target"). A within-subjects study (N=40) demonstrated significant
supportive effects, with participants reporting substantial mood improvement
(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,
p<0.001). A strong correlation between perceived accuracy and therapeutic
outcome (r=0.72, p<0.001) validates our fine-grained approach. These findings
establish the viability of theory-driven, emotion-aware digital wellness tools
and provides a scalable AI blueprint for operationalizing music therapy
principles.
\\ ( https://arxiv.org/abs/2509.15986 ,  982kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16010 (*cross-listing*)
Date: Fri, 19 Sep 2025 14:24:45 GMT   (367kb)

Title: Fed-PISA: Federated Voice Cloning via Personalized Identity-Style
  Adaptation
Authors: Qi Wang, Shituo Ma, Guoxin Yu, Hanyang Peng, Yue Yu
Categories: cs.SD cs.AI eess.AS
\\
  Voice cloning for Text-to-Speech (TTS) aims to generate expressive and
personalized speech from text using limited data from a target speaker.
Federated Learning (FL) offers a collaborative and privacy-preserving framework
for this task, but existing approaches suffer from high communication costs and
tend to suppress stylistic heterogeneity, resulting in insufficient
personalization. To address these issues, we propose Fed-PISA, which stands for
Federated Personalized Identity-Style Adaptation. To minimize communication
costs, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism:
the speaker's timbre is retained locally through a private ID-LoRA, while only
a lightweight style-LoRA is transmitted to the server, thereby minimizing
parameter exchange. To harness heterogeneity, our aggregation method, inspired
by collaborative filtering, is introduced to create custom models for each
client by learning from stylistically similar peers. Experiments show that
Fed-PISA improves style expressivity, naturalness, and speaker similarity,
outperforming standard federated baselines with minimal communication costs.
\\ ( https://arxiv.org/abs/2509.16010 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16020 (*cross-listing*)
Date: Fri, 19 Sep 2025 14:28:22 GMT   (330kb)

Title: AI Methods for Permutation Circuit Synthesis Across Generic Topologies
Authors: Victor Villar, Juan Cruz-Benito, Ismael Faro, David Kremer
Categories: quant-ph cs.AI cs.LG
Comments: This paper has been accepted by First AAAI Symposium on Quantum
  Information & Machine Learning (QIML): Bridging Quantum Computing and
  Artificial Intelligence at AAAI 2025 Fall Symposium
\\
  This paper investigates artificial intelligence (AI) methodologies for the
synthesis and transpilation of permutation circuits across generic topologies.
Our approach uses Reinforcement Learning (RL) techniques to achieve
near-optimal synthesis of permutation circuits up to 25 qubits. Rather than
developing specialized models for individual topologies, we train a
foundational model on a generic rectangular lattice, and employ masking
mechanisms to dynamically select subsets of topologies during the synthesis.
This enables the synthesis of permutation circuits on any topology that can be
embedded within the rectangular lattice, without the need to re-train the
model. In this paper we show results for 5x5 lattice and compare them to
previous AI topology-oriented models and classical methods, showing that they
outperform classical heuristics, and match previous specialized AI models, and
performs synthesis even for topologies that were not seen during training. We
further show that the model can be fine tuned to strengthen the performance for
selected topologies of interest. This methodology allows a single trained model
to efficiently synthesize circuits across diverse topologies, allowing its
practical integration into transpilation workflows.
\\ ( https://arxiv.org/abs/2509.16020 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16053 (*cross-listing*)
Date: Fri, 19 Sep 2025 15:03:18 GMT   (4614kb)

Title: Compose by Focus: Scene Graph-based Atomic Skills
Authors: Han Qi, Changhe Chen, Heng Yang
Categories: cs.RO cs.AI
\\
  A key requirement for generalist robots is compositional generalization - the
ability to combine atomic skills to solve complex, long-horizon tasks. While
prior work has primarily focused on synthesizing a planner that sequences
pre-learned skills, robust execution of the individual skills themselves
remains challenging, as visuomotor policies often fail under distribution
shifts induced by scene composition. To address this, we introduce a scene
graph-based representation that focuses on task-relevant objects and relations,
thereby mitigating sensitivity to irrelevant variation. Building on this idea,
we develop a scene-graph skill learning framework that integrates graph neural
networks with diffusion-based imitation learning, and further combine "focused"
scene-graph skills with a vision-language model (VLM) based task planner.
Experiments in both simulation and real-world manipulation tasks demonstrate
substantially higher success rates than state-of-the-art baselines,
highlighting improved robustness and compositional generalization in
long-horizon tasks.
\\ ( https://arxiv.org/abs/2509.16053 ,  4614kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16068 (*cross-listing*)
Date: Fri, 19 Sep 2025 15:17:08 GMT   (1566kb)

Title: Communications to Circulations: 3D Wind Field Retrieval and Real-Time
  Prediction Using 5G GNSS Signals and Deep Learning
Authors: Yuchen Ye, Hong Liang, Chaoxia Yuan, Mingyu Li, Aoqi Zhou, Chunqing
  Shang, Hua Cai, Peixi Liu, Kezuan Wang, Yifeng Zheng
Categories: cs.LG cs.AI
Comments: 31 pages,11 figures,1 table
MSC-class: 68T07
ACM-class: I.2.1
\\
  Accurate atmospheric wind field information is crucial for various
applications, including weather forecasting, aviation safety, and disaster risk
reduction. However, obtaining high spatiotemporal resolution wind data remains
challenging due to limitations in traditional in-situ observations and remote
sensing techniques, as well as the computational expense and biases of
numerical weather prediction (NWP) models. This paper introduces G-WindCast, a
novel deep learning framework that leverages signal strength variations from 5G
Global Navigation Satellite System (GNSS) signals to retrieve and forecast
three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward
Neural Networks (FNN) and Transformer networks to capture complex, nonlinear,
and spatiotemporal relationships between GNSS-derived features and wind
dynamics. Our preliminary results demonstrate promising accuracy in both wind
retrieval and short-term wind forecasting (up to 30 minutes lead time), with
skill scores comparable to high-resolution NWP outputs in certain scenarios.
The model exhibits robustness across different forecast horizons and pressure
levels, and its predictions for wind speed and direction show superior
agreement with observations compared to concurrent ERA5 reanalysis data.
Furthermore, we show that the system can maintain excellent performance for
localized forecasting even with a significantly reduced number of GNSS stations
(e.g., around 100), highlighting its cost-effectiveness and scalability. This
interdisciplinary approach underscores the transformative potential of
exploiting non-traditional data sources and deep learning for advanced
environmental monitoring and real-time atmospheric applications.
\\ ( https://arxiv.org/abs/2509.16068 ,  1566kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16117 (*cross-listing*)
Date: Fri, 19 Sep 2025 16:09:33 GMT   (1535kb)

Title: DiffusionNFT: Online Diffusion Reinforcement with Forward Process
Authors: Kaiwen Zheng, Huayu Chen, Haotian Ye, Haoxiang Wang, Qinsheng Zhang,
  Kai Jiang, Hang Su, Stefano Ermon, Jun Zhu, Ming-Yu Liu
Categories: cs.LG cs.AI cs.CV
\\
  Online reinforcement learning (RL) has been central to post-training language
models, but its extension to diffusion models remains challenging due to
intractable likelihoods. Recent works discretize the reverse sampling process
to enable GRPO-style training, yet they inherit fundamental drawbacks,
including solver restrictions, forward-reverse inconsistency, and complicated
integration with classifier-free guidance (CFG). We introduce Diffusion
Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that
optimizes diffusion models directly on the forward process via flow matching.
DiffusionNFT contrasts positive and negative generations to define an implicit
policy improvement direction, naturally incorporating reinforcement signals
into the supervised learning objective. This formulation enables training with
arbitrary black-box solvers, eliminates the need for likelihood estimation, and
requires only clean images rather than sampling trajectories for policy
optimization. DiffusionNFT is up to $25\times$ more efficient than FlowGRPO in
head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT
improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO
achieves 0.95 with over 5k steps and additional CFG employment. By leveraging
multiple reward models, DiffusionNFT significantly boosts the performance of
SD3.5-Medium in every benchmark tested.
\\ ( https://arxiv.org/abs/2509.16117 ,  1535kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16126 (*cross-listing*)
Date: Fri, 19 Sep 2025 16:24:48 GMT   (7656kb)

Title: Network-Based Detection of Autism Spectrum Disorder Using Sustainable
  and Non-invasive Salivary Biomarkers
Authors: Janayna M. Fernandes and Robinson Sabino-Silva and Murillo G. Carneiro
Categories: cs.LG cs.AI
\\
  Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying
early diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy,
we developed GANet, a genetic algorithm-based network optimization framework
leveraging PageRank and Degree for importance-based feature characterization.
GANet systematically optimizes network structure to extract meaningful patterns
from high-dimensional spectral data. It achieved superior performance compared
to linear discriminant analysis, support vector machines, and deep learning
models, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74
harmonic mean. These results demonstrate GANet's potential as a robust,
bio-inspired, non-invasive tool for precise ASD detection and broader
spectral-based health applications.
\\ ( https://arxiv.org/abs/2509.16126 ,  7656kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16184 (*cross-listing*)
Date: Fri, 19 Sep 2025 17:44:03 GMT   (338kb)

Title: Accelerating Atomic Fine Structure Determination with Graph
  Reinforcement Learning
Authors: M. Ding, V.-A. Darvariu, A. N. Ryabtsev, N. Hawes, J. C. Pickering
Categories: physics.atom-ph cs.AI cs.LG
\\
  Atomic data determined by analysis of observed atomic spectra are essential
for plasma diagnostics. For each low-ionisation open d- and f-subshell atomic
species, around $10^3$ fine structure level energies can be determined through
years of analysis of $10^4$ observable spectral lines. We propose the
automation of this task by casting the analysis procedure as a Markov decision
process and solving it by graph reinforcement learning using reward functions
learned on historical human decisions. In our evaluations on existing spectral
line lists and theoretical calculations for Co II and Nd II-III, hundreds of
level energies were computed within hours, agreeing with published values in
95% of cases for Co II and 54-87% for Nd II-III. As the current efficiency in
atomic fine structure determination struggles to meet growing atomic data
demands from astronomy and fusion science, our new artificial intelligence
approach sets the stage for closing this gap.
\\ ( https://arxiv.org/abs/2509.16184 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16195 (*cross-listing*)
Date: Fri, 19 Sep 2025 17:57:13 GMT   (263kb)

Title: FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal
  Distillation
Authors: Luca Della Libera, Cem Subakan, Mirco Ravanelli
Categories: cs.SD cs.AI cs.LG
Comments: 5 pages, 1 figure
\\
  Neural audio codecs are a fundamental component of modern generative audio
pipelines. Although recent codecs achieve strong low-bitrate reconstruction and
provide powerful representations for downstream tasks, most are non-streamable,
limiting their use in real-time applications. We present FocalCodec-Stream, a
hybrid codec based on focal modulation that compresses speech into a single
binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our
approach combines multi-stage causal distillation of WavLM with targeted
architectural improvements, including a lightweight refiner module that
enhances quality under latency constraints. Experiments show that
FocalCodec-Stream outperforms existing streamable codecs at comparable
bitrates, while preserving both semantic and acoustic information. The result
is a favorable trade-off between reconstruction quality, downstream task
performance, latency, and efficiency. Code and checkpoints will be released at
https://github.com/lucadellalib/focalcodec.
\\ ( https://arxiv.org/abs/2509.16195 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12399 (*cross-listing*)
Date: Sun, 29 Jan 2023 09:36:41 GMT   (6658kb,D)

Title: Learning Analytics from Spoken Discussion Dialogs in Flipped Classroom
Authors: Hang Su, Borislav Dzodzo, Changlun Li, Danyang Zhao, Hao Geng,
  Yunxiang Li, Sidharth Jaggi, and Helen Meng
Categories: cs.CY cs.CL
\\
  The flipped classroom is a new pedagogical strategy that has been gaining
increasing importance recently. Spoken discussion dialog commonly occurs in
flipped classroom, which embeds rich information indicating processes and
progression of students' learning. This study focuses on learning analytics
from spoken discussion dialog in the flipped classroom, which aims to collect
and analyze the discussion dialogs in flipped classroom in order to get to know
group learning processes and outcomes. We have recently transformed a course
using the flipped classroom strategy, where students watched video-recorded
lectures at home prior to group-based problem-solving discussions in class. The
in-class group discussions were recorded throughout the semester and then
transcribed manually. After features are extracted from the dialogs by multiple
tools and customized processing techniques, we performed statistical analyses
to explore the indicators that are related to the group learning outcomes from
face-to-face discussion dialogs in the flipped classroom. Then, machine
learning algorithms are applied to the indicators in order to predict the group
learning outcome as High, Mid or Low. The best prediction accuracy reaches
78.9%, which demonstrates the feasibility of achieving automatic learning
outcome prediction from group discussion dialog in flipped classroom.
\\ ( https://arxiv.org/abs/2301.12399 ,  6658kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15233 (*cross-listing*)
Date: Wed, 17 Sep 2025 02:50:54 GMT   (30815kb)

Title: Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided
  Role-playing Agents
Authors: Xueqiao Zhang, Chao Zhang, Jingtao Xu, Yifan Zhu, Xin Shi, Yi Yang,
  Yawei Luo
Categories: cs.MM cs.CL cs.CV
Comments: Accepted at EMNLP2025 Main
\\
  Role-playing agents (RPAs) have attracted growing interest for their ability
to simulate immersive and interactive characters. However, existing approaches
primarily focus on static role profiles, overlooking the dynamic perceptual
abilities inherent to humans. To bridge this gap, we introduce the concept of
dynamic role profiles by incorporating video modality into RPAs. To support
this, we construct Role-playing-Video60k, a large-scale, high-quality dataset
comprising 60k videos and 700k corresponding dialogues. Based on this dataset,
we develop a comprehensive RPA framework that combines adaptive temporal
sampling with both dynamic and static role profile representations.
Specifically, the dynamic profile is created by adaptively sampling video
frames and feeding them to the LLM in temporal order, while the static profile
consists of (1) character dialogues from training videos during fine-tuning,
and (2) a summary context from the input video during inference. This joint
integration enables RPAs to generate greater responses. Furthermore, we propose
a robust evaluation method covering eight metrics. Experimental results
demonstrate the effectiveness of our framework, highlighting the importance of
dynamic role profiles in developing RPAs.
\\ ( https://arxiv.org/abs/2509.15233 ,  30815kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15279 (*cross-listing*)
Date: Thu, 18 Sep 2025 13:35:14 GMT   (276kb)

Title: Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement
  Learning
Authors: Chi Liu, Derek Li, Yan Shu, Robin Chen, Derek Duan, Teng Fang, Bryan
  Dai
Categories: cs.LG cs.CL
\\
  While large language models show promise in medical applications, achieving
expert-level clinical reasoning remains challenging due to the need for both
accurate answers and transparent reasoning processes. To address this
challenge, we introduce Fleming-R1, a model designed for verifiable medical
reasoning through three complementary innovations. First, our
Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets
with knowledge-graph-guided synthesis to improve coverage of underrepresented
diseases, drugs, and multi-hop reasoning chains. Second, we employ
Chain-of-Thought (CoT) cold start to distill high-quality reasoning
trajectories from teacher models, establishing robust inference priors. Third,
we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR)
framework using Group Relative Policy Optimization, which consolidates core
reasoning skills while targeting persistent failure modes through adaptive
hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers
substantial parameter-efficient improvements: the 7B variant surpasses much
larger baselines, while the 32B model achieves near-parity with GPT-4o and
consistently outperforms strong open-source alternatives. These results
demonstrate that structured data design, reasoning-oriented initialization, and
verifiable reinforcement learning can advance clinical reasoning beyond simple
accuracy optimization. We release Fleming-R1 publicly to promote transparent,
reproducible, and auditable progress in medical AI, enabling safer deployment
in high-stakes clinical environments.
\\ ( https://arxiv.org/abs/2509.15279 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15561 (*cross-listing*)
Date: Fri, 19 Sep 2025 03:46:42 GMT   (543kb)

Title: Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning
Authors: Om Naphade, Saksham Bansal, Parikshit Pareek
Categories: cs.LG cs.CL
\\
  Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)
pipelines but becomes computationally expensive and opaque with larger models.
Recently, Large Language Models (LLMs) have been explored for HPT, yet most
rely on models exceeding 100 billion parameters. We propose an Expert Block
Framework for HPT using Small LLMs. At its core is the Trajectory Context
Summarizer (TCS), a deterministic block that transforms raw training
trajectories into structured context, enabling small LLMs to analyze
optimization progress with reliability comparable to larger models. Using two
locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial
budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9
percentage points of GPT-4 across six diverse tasks.
\\ ( https://arxiv.org/abs/2509.15561 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15692 (*cross-listing*)
Date: Fri, 19 Sep 2025 07:12:18 GMT   (340kb)

Title: Direct Simultaneous Translation Activation for Large Audio-Language
  Models
Authors: Pei Zhang, Yiming Wang, Jialong Tang, Baosong Yang, Rui Wang, Derek F.
  Wong and Fei Huang
Categories: cs.SD cs.CL eess.AS
\\
  Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech
into target text in real time, outputting translations while receiving source
speech input, rather than waiting for the entire utterance to be spoken.
Simul-S2TT research often modifies model architectures to implement read-write
strategies. However, with the rise of large audio-language models (LALMs), a
key challenge is how to directly activate Simul-S2TT capabilities in base
models without additional architectural changes. In this paper, we introduce
{\bf Simul}taneous {\bf S}elf-{\bf A}ugmentation ({\bf SimulSA}), a strategy
that utilizes LALMs' inherent capabilities to obtain simultaneous data by
randomly truncating speech and constructing partially aligned translation. By
incorporating them into offline SFT data, SimulSA effectively bridges the
distribution gap between offline translation during pretraining and
simultaneous translation during inference. Experimental results demonstrate
that augmenting only about {\bf 1\%} of the simultaneous data, compared to the
full offline SFT data, can significantly activate LALMs' Simul-S2TT
capabilities without modifications to model architecture or decoding strategy.
\\ ( https://arxiv.org/abs/2509.15692 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15969 (*cross-listing*)
Date: Fri, 19 Sep 2025 13:26:46 GMT   (1750kb)

Title: VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency
Authors: Nikita Torgashov, Gustav Eje Henter, Gabriel Skantze
Categories: eess.AS cs.CL cs.HC cs.LG cs.SD
Comments: 5 pages, 1 figure, submitted to IEEE ICASSP 2026
\\
  We present VoXtream, a fully autoregressive, zero-shot streaming
text-to-speech (TTS) system for real-time use that begins speaking from the
first word. VoXtream directly maps incoming phonemes to audio tokens using a
monotonic alignment scheme and a dynamic look-ahead that does not delay onset.
Built around an incremental phoneme transformer, a temporal transformer
predicting semantic and duration tokens, and a depth transformer producing
acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay
among publicly available streaming TTS: 102 ms on GPU. Despite being trained on
a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several
metrics, while delivering competitive quality in both output- and
full-streaming settings. Demo and code are available at
https://herimor.github.io/voxtream.
\\ ( https://arxiv.org/abs/2509.15969 ,  1750kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16060 (*cross-listing*)
Date: Fri, 19 Sep 2025 15:10:19 GMT   (260kb)

Title: SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer
  Residual Connection
Authors: Maithili Joshi, Palash Nandi, Tanmoy Chakraborty
Categories: cs.LG cs.CL
Comments: Accepted in EMNLP'25 Main
\\
  Large Language Models (LLMs) with safe-alignment training are powerful
instruments with robust language comprehension capabilities. These models
typically undergo meticulous alignment procedures involving human feedback to
ensure the acceptance of safe inputs while rejecting harmful or unsafe ones.
However, despite their massive scale and alignment efforts, LLMs remain
vulnerable to jailbreak attacks, where malicious users manipulate the model to
produce harmful outputs that it was explicitly trained to avoid. In this study,
we find that the safety mechanisms in LLMs are predominantly embedded in the
middle-to-late layers. Building on this insight, we introduce a novel white-box
jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which
connects two intermediate layers $s$ and $e$ such that $s < e$, through a
residual connection. Our approach achieves a 51% improvement over the
best-performing baseline on the HarmBench test set. Furthermore, SABER induces
only a marginal shift in perplexity when evaluated on the HarmBench validation
set. The source code is publicly available at
https://github.com/PalGitts/SABER.
\\ ( https://arxiv.org/abs/2509.16060 ,  260kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16189 (*cross-listing*)
Date: Fri, 19 Sep 2025 17:49:25 GMT   (7734kb)

Title: Latent learning: episodic memory complements parametric learning by
  enabling flexible reuse of experiences
Authors: Andrew Kyle Lampinen and Martin Engelcke and Yuxuan Li and Arslan
  Chaudhry and James L. McClelland
Categories: cs.LG cs.CL
\\
  When do machine learning systems fail to generalize, and what mechanisms
could improve their generalization? Here, we draw inspiration from cognitive
science to argue that one weakness of machine learning systems is their failure
to exhibit latent learning -- learning information that is not relevant to the
task at hand, but that might be useful in a future task. We show how this
perspective links failures ranging from the reversal curse in language modeling
to new findings on agent-based navigation. We then highlight how cognitive
science points to episodic memory as a potential part of the solution to these
issues. Correspondingly, we show that a system with an oracle retrieval
mechanism can use learning experiences more flexibly to generalize better
across many of these challenges. We also identify some of the essential
components for effectively using retrieval, including the importance of
within-example in-context learning for acquiring the ability to use information
across retrieved examples. In summary, our results illustrate one possible
contributor to the relative data inefficiency of current machine learning
systems compared to natural intelligence, and help to understand how retrieval
methods can complement parametric learning to improve generalization.
\\ ( https://arxiv.org/abs/2509.16189 ,  7734kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15328 (*cross-listing*)
Date: Thu, 18 Sep 2025 18:18:49 GMT   (7989kb)

Title: Kuramoto Orientation Diffusion Models
Authors: Yue Song, T. Anderson Keller, Sevan Brodjian, Takeru Miyato, Yisong
  Yue, Pietro Perona, Max Welling
Categories: cs.LG cs.CV q-bio.NC
Comments: NeurIPS 2025
\\
  Orientation-rich images, such as fingerprints and textures, often exhibit
coherent angular directional patterns that are challenging to model using
standard generative approaches based on isotropic Euclidean diffusion.
Motivated by the role of phase synchronization in biological systems, we
propose a score-based generative model built on periodic domains by leveraging
stochastic Kuramoto dynamics in the diffusion process. In neural and physical
systems, Kuramoto models capture synchronization phenomena across coupled
oscillators -- a behavior that we re-purpose here as an inductive bias for
structured image generation. In our framework, the forward process performs
\textit{synchronization} among phase variables through globally or locally
coupled oscillator interactions and attraction to a global reference phase,
gradually collapsing the data into a low-entropy von Mises distribution. The
reverse process then performs \textit{desynchronization}, generating diverse
patterns by reversing the dynamics with a learned score function. This approach
enables structured destruction during forward diffusion and a hierarchical
generation process that progressively refines global coherence into fine-scale
details. We implement wrapped Gaussian transition kernels and periodicity-aware
networks to account for the circular geometry. Our method achieves competitive
results on general image benchmarks and significantly improves generation
quality on orientation-dense datasets like fingerprints and textures.
Ultimately, this work demonstrates the promise of biologically inspired
synchronization dynamics as structured priors in generative modeling.
\\ ( https://arxiv.org/abs/2509.15328 ,  7989kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15347 (*cross-listing*)
Date: Thu, 18 Sep 2025 18:41:06 GMT   (6394kb)

Title: Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive
  Strategy for Continual Learning
Authors: Jia Tang, Xinrui Wang and Songcan Chen
Categories: cs.LG cs.CV
Comments: The article has been accepted by Frontiers of Computer Science (FCS),
  with the DOI: {10.1007/s11704-025-50623-6}
\\
  Continual learning (CL) involves acquiring and accumulating knowledge from
evolving tasks while alleviating catastrophic forgetting. Recently, leveraging
contrastive loss to construct more transferable and less forgetful
representations has been a promising direction in CL. Despite advancements,
their performance is still limited due to confusion arising from both
inter-task and intra-task features. To address the problem, we propose a simple
yet effective contrastive strategy named \textbf{G}lobal \textbf{P}re-fixing,
\textbf{L}ocal \textbf{A}djusting for \textbf{S}upervised \textbf{C}ontrastive
learning (GPLASC). Specifically, to avoid task-level confusion, we divide the
entire unit hypersphere of representations into non-overlapping regions, with
the centers of the regions forming an inter-task pre-fixed \textbf{E}quiangular
\textbf{T}ight \textbf{F}rame (ETF). Meanwhile, for individual tasks, our
method helps regulate the feature structure and form intra-task adjustable ETFs
within their respective allocated regions. As a result, our method
\textit{simultaneously} ensures discriminative feature structures both between
tasks and within tasks and can be seamlessly integrated into any existing
contrastive continual learning framework. Extensive experiments validate its
effectiveness.
\\ ( https://arxiv.org/abs/2509.15347 ,  6394kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15422 (*cross-listing*)
Date: Thu, 18 Sep 2025 21:01:44 GMT   (1136kb)

Title: Analysis Plug-and-Play Methods for Imaging Inverse Problems
Authors: Edward P. Chandler, Shirin Shoushtari, Brendt Wohlberg, Ulugbek S.
  Kamilov
Categories: eess.IV cs.CV
\\
  Plug-and-Play Priors (PnP) is a popular framework for solving imaging inverse
problems by integrating learned priors in the form of denoisers trained to
remove Gaussian noise from images. In standard PnP methods, the denoiser is
applied directly in the image domain, serving as an implicit prior on natural
images. This paper considers an alternative analysis formulation of PnP, in
which the prior is imposed on a transformed representation of the image, such
as its gradient. Specifically, we train a Gaussian denoiser to operate in the
gradient domain, rather than on the image itself. Conceptually, this is an
extension of total variation (TV) regularization to learned TV regularization.
To incorporate this gradient-domain prior in image reconstruction algorithms,
we develop two analysis PnP algorithms based on half-quadratic splitting
(APnP-HQS) and the alternating direction method of multipliers (APnP-ADMM). We
evaluate our approach on image deblurring and super-resolution, demonstrating
that the analysis formulation achieves performance comparable to image-domain
PnP algorithms.
\\ ( https://arxiv.org/abs/2509.15422 ,  1136kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15595 (*cross-listing*)
Date: Fri, 19 Sep 2025 04:55:03 GMT   (8392kb)

Title: Prostate Capsule Segmentation from Micro-Ultrasound Images using
  Adaptive Focal Loss
Authors: Kaniz Fatema, Vaibhav Thakur, Emad A. Mohammed
Categories: eess.IV cs.CV
\\
  Micro-ultrasound (micro-US) is a promising imaging technique for cancer
detection and computer-assisted visualization. This study investigates prostate
capsule segmentation using deep learning techniques from micro-US images,
addressing the challenges posed by the ambiguous boundaries of the prostate
capsule. Existing methods often struggle in such cases, motivating the
development of a tailored approach. This study introduces an adaptive focal
loss function that dynamically emphasizes both hard and easy regions, taking
into account their respective difficulty levels and annotation variability. The
proposed methodology has two primary strategies: integrating a standard focal
loss function as a baseline to design an adaptive focal loss function for
proper prostate capsule segmentation. The focal loss baseline provides a robust
foundation, incorporating class balancing and focusing on examples that are
difficult to classify. The adaptive focal loss offers additional flexibility,
addressing the fuzzy region of the prostate capsule and annotation variability
by dilating the hard regions identified through discrepancies between expert
and non-expert annotations. The proposed method dynamically adjusts the
segmentation model's weights better to identify the fuzzy regions of the
prostate capsule. The proposed adaptive focal loss function demonstrates
superior performance, achieving a mean dice coefficient (DSC) of 0.940 and a
mean Hausdorff distance (HD) of 1.949 mm in the testing dataset. These results
highlight the effectiveness of integrating advanced loss functions and adaptive
techniques into deep learning models. This enhances the accuracy of prostate
capsule segmentation in micro-US images, offering the potential to improve
clinical decision-making in prostate cancer diagnosis and treatment planning.
\\ ( https://arxiv.org/abs/2509.15595 ,  8392kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15758 (*cross-listing*)
Date: Fri, 19 Sep 2025 08:37:33 GMT   (837kb)

Title: Uncertainty-Gated Deformable Network for Breast Tumor Segmentation in MR
  Images
Authors: Yue Zhang, Jiahua Dong, Chengtao Peng, Qiuli Wang, Dan Song, Guiduo
  Duan
Categories: eess.IV cs.CV
Comments: 5 pages, 2 figures
\\
  Accurate segmentation of breast tumors in magnetic resonance images (MRI) is
essential for breast cancer diagnosis, yet existing methods face challenges in
capturing irregular tumor shapes and effectively integrating local and global
features. To address these limitations, we propose an uncertainty-gated
deformable network to leverage the complementary information from CNN and
Transformers. Specifically, we incorporates deformable feature modeling into
both convolution and attention modules, enabling adaptive receptive fields for
irregular tumor contours. We also design an Uncertainty-Gated Enhancing Module
(U-GEM) to selectively exchange complementary features between CNN and
Transformer based on pixel-wise uncertainty, enhancing both local and global
representations. Additionally, a Boundary-sensitive Deep Supervision Loss is
introduced to further improve tumor boundary delineation. Comprehensive
experiments on two clinical breast MRI datasets demonstrate that our method
achieves superior segmentation performance compared with state-of-the-art
methods, highlighting its clinical potential for accurate breast tumor
delineation.
\\ ( https://arxiv.org/abs/2509.15758 ,  837kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15802 (*cross-listing*)
Date: Fri, 19 Sep 2025 09:30:13 GMT   (1450kb)

Title: DPC-QA Net: A No-Reference Dual-Stream Perceptual and Cellular Quality
  Assessment Network for Histopathology Images
Authors: Qijun Yang, Boyang Wang, Hujun Yin
Categories: eess.IV cs.CV
\\
  Reliable whole slide imaging (WSI) hinges on image quality,yet staining
artefacts, defocus, and cellular degradations are common. We present DPC-QA
Net, a no-reference dual-stream network that couples wavelet-based global
difference perception with cellular quality assessment from nuclear and
membrane embeddings via an Aggr-RWKV module. Cross-attention fusion and
multi-term losses align perceptual and cellular cues. Across different
datasets, our model detects staining, membrane, and nuclear issues with >92%
accuracy and aligns well with usability scores; on LIVEC and KonIQ it
outperforms state-of-the-art NR-IQA. A downstream study further shows strong
positive correlations between predicted quality and cell recognition accuracy
(e.g., nuclei PQ/Dice, membrane boundary F-score), enabling practical
pre-screening of WSI regions for computational pathology.
\\ ( https://arxiv.org/abs/2509.15802 ,  1450kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15814 (*cross-listing*)
Date: Fri, 19 Sep 2025 09:41:48 GMT   (20710kb)

Title: QWD-GAN: Quality-aware Wavelet-driven GAN for Unsupervised Medical
  Microscopy Images Denoising
Authors: Qijun Yang, Yating Huang, Lintao Xiang, Hujun Yin
Categories: eess.IV cs.CV
\\
  Image denoising plays a critical role in biomedical and microscopy imaging,
especially when acquiring wide-field fluorescence-stained images. This task
faces challenges in multiple fronts, including limitations in image acquisition
conditions, complex noise types, algorithm adaptability, and clinical
application demands. Although many deep learning-based denoising techniques
have demonstrated promising results, further improvements are needed in
preserving image details, enhancing algorithmic efficiency, and increasing
clinical interpretability. We propose an unsupervised image denoising method
based on a Generative Adversarial Network (GAN) architecture. The approach
introduces a multi-scale adaptive generator based on the Wavelet Transform and
a dual-branch discriminator that integrates difference perception feature maps
with original features. Experimental results on multiple biomedical microscopy
image datasets show that the proposed model achieves state-of-the-art denoising
performance, particularly excelling in the preservation of high-frequency
information. Furthermore, the dual-branch discriminator is seamlessly
compatible with various GAN frameworks. The proposed quality-aware,
wavelet-driven GAN denoising model is termed as QWD-GAN.
\\ ( https://arxiv.org/abs/2509.15814 ,  20710kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15844 (*cross-listing*)
Date: Fri, 19 Sep 2025 10:27:02 GMT   (1248kb)

Title: FedHK-MVFC: Federated Heat Kernel Multi-View Clustering
Authors: Kristina P. Sinaga
Categories: cs.LG cs.CV cs.DC math.AG
Comments: 41 pages, 9 figures, and 3 tables
\\
  In the realm of distributed AI and privacy-focused medical applications, we
propose a framework for multi-view clustering that links quantum field theory
with federated healthcare analytics. Our method uses heat-kernel coefficients
from spectral analysis to convert Euclidean distances into geometry-aware
similarity measures, capturing the structure of diverse medical data. We lay
this out through the Heat Kernel Distance (HKD) transformation with convergence
guarantees. Two algorithms are developed: Heat Kernel-Enhanced Multi-View Fuzzy
Clustering (HK-MVFC) for central analysis, and Federated Heat Kernel Multi-View
Fuzzy Clustering (FedHK-MVFC) for secure, privacy-preserving learning across
hospitals using differential privacy and secure aggregation to facilitate
HIPAA-compliant collaboration. Tests on synthetic datasets of cardiovascular
patients show an $8-12 \%$ increase in clustering accuracy, $70 \%$ reduced
communication, and $98.2 \%$ efficiency retention over centralized methods.
Validated on 10,000 patient records across two hospitals, it proves useful for
collaborative phenotyping involving ECG, cardiac imaging, and behavioral data.
Our theoretical contributions include update rules with proven convergence,
adaptive view weighting, and privacy-preserving protocols. This presents a new
standard for geometry-aware federated learning in healthcare, turning advanced
math into workable solutions for analyzing sensitive medical data while
ensuring both rigor and clinical relevance.
\\ ( https://arxiv.org/abs/2509.15844 ,  1248kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15859 (*cross-listing*)
Date: Fri, 19 Sep 2025 10:52:31 GMT   (37kb)

Title: Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data
Authors: Nakul Sharma
Categories: cs.LG cs.CV
Comments: Accepted to Curated Data for Efficient Learning Workshop at ICCV 2025
\\
  Imbalanced classification datasets pose significant challenges in machine
learning, often leading to biased models that perform poorly on
underrepresented classes. With the rise of foundation models, recent research
has focused on the full, partial, and parameter-efficient fine-tuning of these
models to deal with long-tail classification. Despite the impressive
performance of these works on the benchmark datasets, they still fail to close
the gap with the networks trained using the balanced datasets and still require
substantial computational resources, even for relatively smaller datasets.
Underscoring the importance of computational efficiency and simplicity, in this
work we propose a novel framework that leverages the rich semantic latent space
of Vision Foundation Models to generate synthetic data and train a simple
linear classifier using a mixture of real and synthetic data for long-tail
classification. The computational efficiency gain arises from the number of
trainable parameters that are reduced to just the number of parameters in the
linear model. Our method sets a new state-of-the-art for the CIFAR-100-LT
benchmark and demonstrates strong performance on the Places-LT benchmark,
highlighting the effectiveness and adaptability of our simple and effective
approach.
\\ ( https://arxiv.org/abs/2509.15859 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15947 (*cross-listing*)
Date: Fri, 19 Sep 2025 12:55:51 GMT   (38772kb)

Title: The Missing Piece: A Case for Pre-Training in 3D Medical Object
  Detection
Authors: Katharina Eckstein, Constantin Ulrich, Michael Baumgartner, Jessica
  K\"achele, Dimitrios Bounias, Tassilo Wald, Ralf Floca and Klaus H.
  Maier-Hein
Categories: eess.IV cs.CV cs.LG
Comments: MICCAI 2025
Journal-ref: Medical Image Computing and Computer Assisted Intervention -
  MICCAI 2025. MICCAI 2025. Lecture Notes in Computer Science, vol 15963.
  Springer, Cham
DOI: 10.1007/978-3-032-04965-0_58
\\
  Large-scale pre-training holds the promise to advance 3D medical object
detection, a crucial component of accurate computer-aided diagnosis. Yet, it
remains underexplored compared to segmentation, where pre-training has already
demonstrated significant benefits. Existing pre-training approaches for 3D
object detection rely on 2D medical data or natural image pre-training, failing
to fully leverage 3D volumetric information. In this work, we present the first
systematic study of how existing pre-training methods can be integrated into
state-of-the-art detection architectures, covering both CNNs and Transformers.
Our results show that pre-training consistently improves detection performance
across various tasks and datasets. Notably, reconstruction-based
self-supervised pre-training outperforms supervised pre-training, while
contrastive pre-training provides no clear benefit for 3D medical object
detection. Our code is publicly available at:
https://github.com/MIC-DKFZ/nnDetection-finetuning.
\\ ( https://arxiv.org/abs/2509.15947 ,  38772kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15968 (*cross-listing*)
Date: Fri, 19 Sep 2025 13:25:56 GMT   (4584kb)

Title: CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for
  Long-Tail Scenarios via Collect-and-Refine
Authors: Shiyu Fang, Yiming Cui, Haoyang Liang, Chen Lv, Peng Hang, Jian Sun
Categories: cs.RO cs.CV
\\
  Autonomous Driving (AD) systems have made notable progress, but their
performance in long-tail, safety-critical scenarios remains limited. These rare
cases contribute a disproportionate number of accidents. Vision-Language Action
(VLA) models have strong reasoning abilities and offer a potential solution,
but their effectiveness is limited by the lack of high-quality data and
inefficient learning in such conditions. To address these challenges, we
propose CoReVLA, a continual learning end-to-end autonomous driving framework
that improves the performance in long-tail scenarios through a dual-stage
process of data Collection and behavior Refinement. First, the model is jointly
fine-tuned on a mixture of open-source driving QA datasets, allowing it to
acquire a foundational understanding of driving scenarios. Next, CoReVLA is
deployed within the Cave Automatic Virtual Environment (CAVE) simulation
platform, where driver takeover data is collected from real-time interactions.
Each takeover indicates a long-tail scenario that CoReVLA fails to handle
reliably. Finally, the model is refined via Direct Preference Optimization
(DPO), allowing it to learn directly from human preferences and thereby avoid
reward hacking caused by manually designed rewards. Extensive open-loop and
closed-loop experiments demonstrate that the proposed CoReVLA model can
accurately perceive driving scenarios and make appropriate decisions. On the
Bench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a
Success Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and
15% SR under long-tail, safety-critical scenarios. Furthermore, case studies
demonstrate the model's ability to continually improve its performance in
similar failure-prone scenarios by leveraging past takeover experiences. All
codea and preprocessed datasets are available at:
https://github.com/FanGShiYuu/CoReVLA
\\ ( https://arxiv.org/abs/2509.15968 ,  4584kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16019 (*cross-listing*)
Date: Fri, 19 Sep 2025 14:27:35 GMT   (994kb)

Title: SLaM-DiMM: Shared Latent Modeling for Diffusion Based Missing Modality
  Synthesis in MRI
Authors: Bhavesh Sandbhor, Bheeshm Sharma, Balamurugan Palaniappan
Categories: eess.IV cs.CV
\\
  Brain MRI scans are often found in four modalities, consisting of T1-weighted
with and without contrast enhancement (T1ce and T1w), T2-weighted imaging
(T2w), and Flair. Leveraging complementary information from these different
modalities enables models to learn richer, more discriminative features for
understanding brain anatomy, which could be used in downstream tasks such as
anomaly detection. However, in clinical practice, not all MRI modalities are
always available due to various reasons. This makes missing modality generation
a critical challenge in medical image analysis. In this paper, we propose
SLaM-DiMM, a novel missing modality generation framework that harnesses the
power of diffusion models to synthesize any of the four target MRI modalities
from other available modalities. Our approach not only generates high-fidelity
images but also ensures structural coherence across the depth of the volume
through a dedicated coherence enhancement mechanism. Qualitative and
quantitative evaluations on the BraTS-Lighthouse-2025 Challenge dataset
demonstrate the effectiveness of the proposed approach in synthesizing
anatomically plausible and structurally consistent results. Code is available
at https://github.com/BheeshmSharma/SLaM-DiMM-MICCAI-BraTS-Challenge-2025.
\\ ( https://arxiv.org/abs/2509.16019 ,  994kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16044 (*cross-listing*)
Date: Fri, 19 Sep 2025 14:53:55 GMT   (1497kb)

Title: FMD-TransUNet: Abdominal Multi-Organ Segmentation Based on Frequency
  Domain Multi-Axis Representation Learning and Dual Attention Mechanisms
Authors: Fang Lu, Jingyu Xu, Qinxiu Sun, Qiong Lou
Categories: eess.IV cs.CV
\\
  Accurate abdominal multi-organ segmentation is critical for clinical
applications. Although numerous deep learning-based automatic segmentation
methods have been developed, they still struggle to segment small, irregular,
or anatomically complex organs. Moreover, most current methods focus on
spatial-domain analysis, often overlooking the synergistic potential of
frequency-domain representations. To address these limitations, we propose a
novel framework named FMD-TransUNet for precise abdominal multi-organ
segmentation. It innovatively integrates the Multi-axis External Weight Block
(MEWB) and the improved dual attention module (DA+) into the TransUNet
framework. The MEWB extracts multi-axis frequency-domain features to capture
both global anatomical structures and local boundary details, providing
complementary information to spatial-domain representations. The DA+ block
utilizes depthwise separable convolutions and incorporates spatial and channel
attention mechanisms to enhance feature fusion, reduce redundant information,
and narrow the semantic gap between the encoder and decoder. Experimental
validation on the Synapse dataset shows that FMD-TransUNet outperforms other
recent state-of-the-art methods, achieving an average DSC of 81.32\% and a HD
of 16.35 mm across eight abdominal organs. Compared to the baseline model, the
average DSC increased by 3.84\%, and the average HD decreased by 15.34 mm.
These results demonstrate the effectiveness of FMD-TransUNet in improving the
accuracy of abdominal multi-organ segmentation.
\\ ( https://arxiv.org/abs/2509.16044 ,  1497kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16078 (*cross-listing*)
Date: Fri, 19 Sep 2025 15:25:43 GMT   (1125kb)

Title: MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time
  Series Representation Learning
Authors: Yi Xu, Yitian Zhang, Yun Fu
Categories: cs.LG cs.CV
Comments: Accepted by ICDM 2025
\\
  Unsupervised multivariate time series (MTS) representation learning aims to
extract compact and informative representations from raw sequences without
relying on labels, enabling efficient transfer to diverse downstream tasks. In
this paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked
time-series modeling framework for unsupervised MTS representation learning.
DMAE formulates two complementary pretext tasks: (1) reconstructing masked
values based on visible attributes, and (2) estimating latent representations
of masked features, guided by a teacher encoder. To further improve
representation quality, we introduce a feature-level alignment constraint that
encourages the predicted latent representations to align with the teacher's
outputs. By jointly optimizing these objectives, DMAE learns temporally
coherent and semantically rich representations. Comprehensive evaluations
across classification, regression, and forecasting tasks demonstrate that our
approach achieves consistent and superior performance over competitive
baselines.
\\ ( https://arxiv.org/abs/2509.16078 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16106 (*cross-listing*)
Date: Fri, 19 Sep 2025 15:49:03 GMT   (4887kb)

Title: PRISM: Probabilistic and Robust Inverse Solver with
  Measurement-Conditioned Diffusion Prior for Blind Inverse Problems
Authors: Yuanyun Hu, Evan Bell, Guijin Wang, Yu Sun
Categories: eess.IV cs.CV cs.LG
\\
  Diffusion models are now commonly used to solve inverse problems in
computational imaging. However, most diffusion-based inverse solvers require
complete knowledge of the forward operator to be used. In this work, we
introduce a novel probabilistic and robust inverse solver with
measurement-conditioned diffusion prior (PRISM) to effectively address blind
inverse problems. PRISM offers a technical advancement over current methods by
incorporating a powerful measurement-conditioned diffusion model into a
theoretically principled posterior sampling scheme. Experiments on blind image
deblurring validate the effectiveness of the proposed method, demonstrating the
superior performance of PRISM over state-of-the-art baselines in both image and
blur kernel recovery.
\\ ( https://arxiv.org/abs/2509.16106 ,  4887kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16131 (*cross-listing*)
Date: Fri, 19 Sep 2025 16:27:19 GMT   (29158kb)

Title: Dynamic Classifier-Free Diffusion Guidance via Online Feedback
Authors: Pinelopi Papalampidi, Olivia Wiles, Ira Ktena, Aleksandar Shtedritski,
  Emanuele Bugliarello, Ivana Kajic, Isabela Albuquerque, Aida Nematzadeh
Categories: cs.LG cs.CV
\\
  Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion
models, yet its effectiveness is limited by the use of static guidance scales.
This "one-size-fits-all" approach fails to adapt to the diverse requirements of
different prompts; moreover, prior solutions like gradient-based correction or
fixed heuristic schedules introduce additional complexities and fail to
generalize. In this work, we challeng this static paradigm by introducing a
framework for dynamic CFG scheduling. Our method leverages online feedback from
a suite of general-purpose and specialized small-scale latent-space
evaluations, such as CLIP for alignment, a discriminator for fidelity and a
human preference reward model, to assess generation quality at each step of the
reverse diffusion process. Based on this feedback, we perform a greedy search
to select the optimal CFG scale for each timestep, creating a unique guidance
schedule tailored to every prompt and sample. We demonstrate the effectiveness
of our approach on both small-scale models and the state-of-the-art Imagen 3,
showing significant improvements in text alignment, visual quality, text
rendering and numerical reasoning. Notably, when compared against the default
Imagen 3 baseline, our method achieves up to 53.8% human preference win-rate
for overall preference, a figure that increases up to to 55.5% on prompts
targeting specific capabilities like text rendering. Our work establishes that
the optimal guidance schedule is inherently dynamic and prompt-dependent, and
provides an efficient and generalizable framework to achieve it.
\\ ( https://arxiv.org/abs/2509.16131 ,  29158kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15861 (*cross-listing*)
Date: Fri, 19 Sep 2025 10:54:25 GMT   (1476kb)

Title: ToFU: Transforming How Federated Learning Systems Forget User Data
Authors: Van-Tuan Tran, Hong-Hanh Nguyen-Le, Quoc-Viet Pham
Categories: cs.LG cs.DC
Comments: ECAI-2025
\\
  Neural networks unintentionally memorize training data, creating privacy
risks in federated learning (FL) systems, such as inference and reconstruction
attacks on sensitive data. To mitigate these risks and to comply with privacy
regulations, Federated Unlearning (FU) has been introduced to enable
participants in FL systems to remove their data's influence from the global
model. However, current FU methods primarily act post-hoc, struggling to
efficiently erase information deeply memorized by neural networks. We argue
that effective unlearning necessitates a paradigm shift: designing FL systems
inherently amenable to forgetting. To this end, we propose a
learning-to-unlearn Transformation-guided Federated Unlearning (ToFU) framework
that incorporates transformations during the learning process to reduce
memorization of specific instances. Our theoretical analysis reveals how
transformation composition provably bounds instance-specific information,
directly simplifying subsequent unlearning. Crucially, ToFU can work as a
plug-and-play framework that improves the performance of existing FU methods.
Experiments on CIFAR-10, CIFAR-100, and the MUFAC benchmark show that ToFU
outperforms existing FU baselines, enhances performance when integrated with
current methods, and reduces unlearning time.
\\ ( https://arxiv.org/abs/2509.15861 ,  1476kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16052 (*cross-listing*)
Date: Fri, 19 Sep 2025 15:02:47 GMT   (431kb)

Title: How Exclusive are Ethereum Transactions? Evidence from non-winning
  blocks
Authors: Vabuk Pahari and Andrea Canidio
Categories: cs.CR cs.DC econ.GN q-fin.EC
Comments: arXiv admin note: text overlap with arXiv:2506.04940
\\
  We analyze 15,097 blocks proposed for inclusion in Ethereum's blockchain over
an 8-minute window on December 3, 2024, during which 38 blocks were added to
the chain. We classify transactions as exclusive -- present only in blocks from
a single builder -- or private -- absent from the public mempool but included
in blocks from multiple builders. We find that exclusive transactions account
for 84% of the total fees paid by transactions in winning blocks. Furthermore,
we show that exclusivity cannot be fully explained by exclusive relationships
between senders and builders: about 7% of all exclusive transactions included
on-chain, by value, come from senders who route exclusively to a single
builder. Analyzing transaction logs shows that some exclusive transactions are
duplicates or variations of the same strategy, but even accounting for that,
the share of the total fees paid by transactions in winning blocks is at least
77.2%. Taken together, our findings highlight that exclusive transactions are
the dominant source of builder revenues.
\\ ( https://arxiv.org/abs/2509.16052 ,  431kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16101 (*cross-listing*)
Date: Fri, 19 Sep 2025 15:45:02 GMT   (128kb)

Title: Personalized Federated Learning with Heat-Kernel Enhanced Tensorized
  Multi-View Clustering
Authors: Kristina P. Sinaga
Categories: cs.LG cs.DC
Comments: 26 pages, 3 algorithms, and 3 figures
\\
  We present a robust personalized federated learning framework that leverages
heat-kernel enhanced tensorized multi-view fuzzy c-means clustering with
advanced tensor decomposition techniques. Our approach integrates heat-kernel
coefficients adapted from quantum field theory with Tucker decomposition and
canonical polyadic decomposition (CANDECOMP/PARAFAC) to transform conventional
distance metrics and efficiently represent high-dimensional multi-view
structures. The framework employs matriculation and vectorization techniques to
facilitate the discovery of hidden structures and multilinear relationships via
N-way generalized tensors. The proposed method introduces a dual-level
optimization scheme: local heat-kernel enhanced fuzzy clustering with tensor
decomposition operating on order-N input tensors, and federated aggregation of
tensor factors with privacy-preserving personalization mechanisms. The local
stage employs tensorized kernel Euclidean distance transformations and Tucker
decomposition to discover client-specific patterns in multi-view tensor data,
while the global aggregation process coordinates tensor factors (core tensors
and factor matrices) across clients through differential privacy-preserving
protocols. This tensorized approach enables efficient handling of
high-dimensional multi-view data with significant communication savings through
low-rank tensor approximations.
\\ ( https://arxiv.org/abs/2509.16101 ,  128kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2409.04793
replaced with revised version Fri, 19 Sep 2025 06:57:31 GMT   (630kb)

Title: Action is the primary key: a categorical framework for episodic memories
  and logical reasoning
Authors: Yoshiki Fukada
Categories: cs.AI
Comments: 9 pages, 9 figures, 3 tables
\\ ( https://arxiv.org/abs/2409.04793 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2409.17411
replaced with revised version Thu, 18 Sep 2025 22:08:45 GMT   (11996kb)

Title: Enhancing Interpretability in Deep Reinforcement Learning through
  Semantic Clustering
Authors: Liang Zhang, Justin Lieffers, Adarsh Pyarelal
Categories: cs.AI
Comments: Accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2409.17411 ,  11996kb)
------------------------------------------------------------------------------
\\
arXiv:2409.20016
replaced with revised version Fri, 19 Sep 2025 09:26:03 GMT   (1782kb)

Title: Dynamic Policy Fusion for User Alignment Without Re-Interaction
Authors: Ajsal Shereef Palattuparambil, Thommen George Karimpanal, Santu Rana
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2409.20016 ,  1782kb)
------------------------------------------------------------------------------
\\
arXiv:2410.11900
replaced with revised version Fri, 19 Sep 2025 13:12:33 GMT   (1182kb)

Title: FLARE: Faithful Logic-Aided Reasoning and Exploration
Authors: Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle
  Augenstein
Categories: cs.AI cs.CL cs.LG cs.LO
Comments: Published at EMNLP 2025
\\ ( https://arxiv.org/abs/2410.11900 ,  1182kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03455
replaced with revised version Fri, 19 Sep 2025 13:47:46 GMT   (829kb)

Title: Watson: A Cognitive Observability Framework for the Reasoning of
  LLM-Powered Agents
Authors: Benjamin Rombaut, Sogol Masoumzadeh, Kirill Vasilevski, Dayi Lin,
  Ahmed E. Hassan
Categories: cs.AI cs.SE
\\ ( https://arxiv.org/abs/2411.03455 ,  829kb)
------------------------------------------------------------------------------
\\
arXiv:2501.11447
replaced with revised version Fri, 19 Sep 2025 10:54:57 GMT   (960kb)

Title: Decomposing Interventional Causality into Synergistic, Redundant, and
  Unique Components
Authors: Abel Jansma
Categories: cs.AI cs.IT math.IT physics.data-an
Comments: 10 pages, 6 figures
MSC-class: 68T01 (Primary) 06A11, 62D20 (Secondary)
ACM-class: I.2.4; F.2.1; G.2.1
\\ ( https://arxiv.org/abs/2501.11447 ,  960kb)
------------------------------------------------------------------------------
\\
arXiv:2502.08177
replaced with revised version Fri, 19 Sep 2025 14:30:28 GMT   (1209kb)

Title: SycEval: Evaluating LLM Sycophancy
Authors: Aaron Fanous and Jacob Goldberg (1), Ank A. Agarwal (1), Joanna Lin
  (1), Anson Zhou (1), Roxana Daneshjou (1), Sanmi Koyejo (1) ((1) Stanford
  University)
Categories: cs.AI
Comments: AIES 2025
\\ ( https://arxiv.org/abs/2502.08177 ,  1209kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12566
replaced with revised version Thu, 18 Sep 2025 01:59:34 GMT   (16381kb)

Title: Exploring the Impact of Personality Traits on LLM Bias and Toxicity
Authors: Shuo Wang and Renhao Li and Xi Chen and Yulin Yuan and Derek F. Wong
  and Min Yang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2502.12566 ,  16381kb)
------------------------------------------------------------------------------
\\
arXiv:2503.04429
replaced with revised version Fri, 19 Sep 2025 11:03:59 GMT   (1387kb)

Title: Activation Space Interventions Can Be Transferred Between Large Language
  Models
Authors: Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash, Michael Lan, Abir
  Harrasse, Amirali Abdullah
Categories: cs.AI
Comments: 75 pages. Accepted to ICML 2025
\\ ( https://arxiv.org/abs/2503.04429 ,  1387kb)
------------------------------------------------------------------------------
\\
arXiv:2503.23781
replaced with revised version Fri, 19 Sep 2025 03:03:58 GMT   (7431kb)

Title: DebFlow: Automating Agent Creation via Agent Debate
Authors: Jinwei Su, Yinghui Xia, Yiqun Duan, Jun Du, Jianuo Huang, Tianyu Shi,
  Lewei He
Categories: cs.AI
\\ ( https://arxiv.org/abs/2503.23781 ,  7431kb)
------------------------------------------------------------------------------
\\
arXiv:2504.03603
replaced with revised version Fri, 19 Sep 2025 15:48:11 GMT   (906kb)

Title: Towards deployment-centric multimodal AI beyond vision and language
Authors: Xianyuan Liu, Jiayang Zhang, Shuo Zhou, Thijs L. van der Plas, Avish
  Vijayaraghavan, Anastasiia Grishina, Mengdie Zhuang, Daniel Schofield,
  Christopher Tomlinson, Yuhan Wang, Ruizhe Li, Louisa van Zeeland, Sina
  Tabakhi, Cyndie Demeocq, Xiang Li, Arunav Das, Orlando Timmerman, Thomas
  Baldwin-McDonald, Jinge Wu, Peizhen Bai, Zahraa Al Sahili, Omnia Alwazzan,
  Thao N. Do, Mohammod N.I. Suvon, Angeline Wang, Lucia Cipolina-Kun, Luigi A.
  Moretti, Lucas Farndale, Nitisha Jain, Natalia Efremova, Yan Ge, Marta
  Varela, Hak-Keung Lam, Oya Celiktutan, Ben R. Evans, Alejandro Coca-Castro,
  Honghan Wu, Zahraa S. Abdallah, Chen Chen, Valentin Danchev, Nataliya
  Tkachenko, Lei Lu, Tingting Zhu, Gregory G. Slabaugh, Roger K. Moore, William
  K. Cheung, Peter H. Charlton, Haiping Lu
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2504.03603 ,  906kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16686
replaced with revised version Fri, 19 Sep 2025 09:36:34 GMT   (1865kb)

Title: SPaRC: A Spatial Pathfinding Reasoning Challenge
Authors: Lars Benedikt Kaesberg, Jan Philip Wahle, Terry Ruas, Bela Gipp
Categories: cs.AI cs.CL
Comments: Accepted at EMNLP 2025 (Main)
Journal-ref: EMNLP 2025
\\ ( https://arxiv.org/abs/2505.16686 ,  1865kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18931
replaced with revised version Fri, 19 Sep 2025 15:55:46 GMT   (2015kb)

Title: Can Large Language Models Infer Causal Relationships from Real-World
  Text?
Authors: Ryan Saklad, Aman Chadha, Oleg Pavlov, Raha Moraffah
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2505.18931 ,  2015kb)
------------------------------------------------------------------------------
\\
arXiv:2506.02918
replaced with revised version Fri, 19 Sep 2025 03:54:30 GMT   (7799kb)

Title: World Modelling Improves Language Model Agents
Authors: Shangmin Guo, Omar Darwiche Domingues, Rapha\"el Avalos, Aaron
  Courville, Florian Strub
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2506.02918 ,  7799kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20130
replaced with revised version Fri, 19 Sep 2025 17:22:03 GMT   (216kb)

Title: AI Copilots for Reproducibility in Science: A Case Study
Authors: Adrien Bibal and Steven N. Minton and Deborah Khider and Yolanda Gil
Categories: cs.AI
\\ ( https://arxiv.org/abs/2506.20130 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2507.19733
replaced with revised version Thu, 18 Sep 2025 23:28:04 GMT   (818kb)

Title: Integrating Activity Predictions in Knowledge Graphs
Authors: Forrest Hare, Alec Sculley, and Cameron Stockton
Categories: cs.AI cs.DB
Comments: 21 pages. 18 figures. Conference: Semantic Technology for
  Intelligence, Defense, and Security (STIDS 2024)
\\ ( https://arxiv.org/abs/2507.19733 ,  818kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15432
replaced with revised version Fri, 19 Sep 2025 12:53:25 GMT   (4231kb)

Title: SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality
  Tagging, and Management of Synthetic Data
Authors: Bidyapati Pradhan, Surajit Dasgupta, Amit Kumar Saha, Omkar Anustoop,
  Sriram Puttagunta, Vipul Mittal, Gopal Sarda
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2508.15432 ,  4231kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16051
replaced with revised version Fri, 19 Sep 2025 06:41:50 GMT   (15644kb)

Title: MMAPG: A Training-Free Framework for Multimodal Multi-hop Question
  Answering via Adaptive Planning Graphs
Authors: Yiheng Hu, Xiaoyang Wang, Qing Liu, Xiwei Xu, Qian Fu, Wenjie Zhang,
  Liming Zhu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2508.16051 ,  15644kb)
------------------------------------------------------------------------------
\\
arXiv:2508.20148
replaced with revised version Thu, 18 Sep 2025 17:43:25 GMT   (91545kb)

Title: The Anatomy of a Personal Health Agent
Authors: A. Ali Heydari, Ken Gu, Vidya Srinivas, Hong Yu, Zhihan Zhang, Yuwei
  Zhang, Akshay Paruchuri, Qian He, Hamid Palangi, Nova Hammerquist, Ahmed A.
  Metwally, Brent Winslow, Yubin Kim, Kumar Ayush, Yuzhe Yang, Girish
  Narayanswamy, Maxwell A. Xu, Jake Garrison, Amy Armento Lee, Jenny Vafeiadou,
  Ben Graef, Isaac R. Galatzer-Levy, Erik Schenck, Andrew Barakat, Javier
  Perez, Jacqueline Shreibati, John Hernandez, Anthony Z. Faranesh, Javier L.
  Prieto, Connor Heneghan, Yun Liu, Jiening Zhan, Mark Malhotra, Shwetak Patel,
  Tim Althoff, Xin Liu, Daniel McDuff, Xuhai "Orson" Xu
Categories: cs.AI cs.HC cs.MA
Comments: Minor updates to the manuscript (V2)
\\ ( https://arxiv.org/abs/2508.20148 ,  91545kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07894
replaced with revised version Fri, 19 Sep 2025 16:18:35 GMT   (6323kb)

Title: HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics
  Olympiad Benchmark?
Authors: Fangchen Yu, Haiyuan Wan, Qianjia Cheng, Yuchen Zhang, Jiacheng Chen,
  Fujun Han, Yulun Wu, Junchi Yao, Ruilizhen Hu, Ning Ding, Yu Cheng, Tao Chen,
  Lei Bai, Dongzhan Zhou, Yun Luo, Ganqu Cui, Peng Ye
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.07894 ,  6323kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10162
replaced with revised version Fri, 19 Sep 2025 11:43:08 GMT   (1243kb)

Title: Online Robust Planning under Model Uncertainty: A Sample-Based Approach
Authors: Tamir Shazman, Idan Lev-Yehudi, Ron Benchetit, Vadim Indelman
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.10162 ,  1243kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10707
replaced with revised version Fri, 19 Sep 2025 14:57:35 GMT   (4351kb)

Title: Understanding AI Evaluation Patterns: How Different GPT Models Assess
  Vision-Language Descriptions
Authors: Sajjad Abdoli, Rudi Cilibrasi, Rima Al-Shikh
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2509.10707 ,  4351kb)
------------------------------------------------------------------------------
\\
arXiv:2201.05878
replaced with revised version Fri, 19 Sep 2025 11:48:12 GMT   (0kb,I)

Title: Automatic Lexical Simplification for Turkish
Authors: Ahmet Yavuz Uluslu
Categories: cs.CL
Comments: Incomplete work. Due to inconsistencies and unclear guidelines in the
  data annotation process
\\ ( https://arxiv.org/abs/2201.05878 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2405.17764
replaced with revised version Fri, 19 Sep 2025 04:35:15 GMT   (1728kb)

Title: BBScoreV2: Learning Time-Evolution and Latent Alignment from Stochastic
  Representation
Authors: Tianhao Zhang, Zhecheng Sheng, Zhexiao Lin, Chen Jiang, Dongyeop Kang
Categories: cs.CL cs.AI math.ST stat.TH
Journal-ref: The 2025 Conference on Empirical Methods in Natural Language
  Processing
\\ ( https://arxiv.org/abs/2405.17764 ,  1728kb)
------------------------------------------------------------------------------
\\
arXiv:2406.16013
replaced with revised version Thu, 18 Sep 2025 21:11:49 GMT   (2069kb)

Title: Database-Augmented Query Representation for Information Retrieval
Authors: Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park
Categories: cs.CL cs.AI cs.IR
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2406.16013 ,  2069kb)
------------------------------------------------------------------------------
\\
arXiv:2407.12015
replaced with revised version Thu, 18 Sep 2025 13:58:43 GMT   (4464kb)

Title: The Great AI Witch Hunt: Reviewers Perception and (Mis)Conception of
  Generative AI in Research Writing
Authors: Hilda Hadan, Derrick Wang, Reza Hadi Mogavi, Joseph Tu, Leah
  Zhang-Kennedy, Lennart E. Nacke
Categories: cs.CL cs.AI cs.HC
Journal-ref: Computers in Human Behavior: Artificial Humans 2, no. 2 (2024):
  100095
DOI: 10.1016/j.chbah.2024.100095
\\ ( https://arxiv.org/abs/2407.12015 ,  4464kb)
------------------------------------------------------------------------------
\\
arXiv:2408.04675
replaced with revised version Fri, 19 Sep 2025 16:49:40 GMT   (12805kb)

Title: ConfReady: A RAG based Assistant and Dataset for Conference Checklist
  Responses
Authors: Michael Galarnyk, Rutwik Routu, Vidhyakshaya Kannan, Kosha Bheda,
  Prasun Banerjee, Agam Shah, Sudheer Chava
Categories: cs.CL cs.AI cs.IR
Comments: Accepted at EMNLP 2025 Demo
\\ ( https://arxiv.org/abs/2408.04675 ,  12805kb)
------------------------------------------------------------------------------
\\
arXiv:2409.11022
replaced with revised version Fri, 19 Sep 2025 01:02:13 GMT   (7445kb)

Title: DynamicNER: A Dynamic, Multilingual, and Fine-Grained Dataset for
  LLM-based Named Entity Recognition
Authors: Hanjun Luo, Yingbin Jin, Xinfeng Li, Xuecheng Liu, Ruizhe Chen, Tong
  Shang, Kun Wang, Qingsong Wen, Zuozhu Liu
Categories: cs.CL cs.AI
Comments: This paper is accepted by EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2409.11022 ,  7445kb)
------------------------------------------------------------------------------
\\
arXiv:2410.01508
replaced with revised version Thu, 18 Sep 2025 21:29:48 GMT   (92kb)

Title: Disentangling Latent Shifts of In-Context Learning with Weak Supervision
Authors: Josip Juki\'c, Jan \v{S}najder
Categories: cs.CL cs.LG
Comments: Accepted at NeurIPS 2025
\\ ( https://arxiv.org/abs/2410.01508 ,  92kb)
------------------------------------------------------------------------------
\\
arXiv:2411.07820
replaced with revised version Fri, 19 Sep 2025 17:38:31 GMT   (289kb)

Title: Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented Large Language Models
Authors: Youan Cong, Pritom Saha Akash, Cheng Wang, Kevin Chen-Chuan Chang
Categories: cs.CL cs.IR
\\ ( https://arxiv.org/abs/2411.07820 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2501.07824
replaced with revised version Fri, 19 Sep 2025 11:59:49 GMT   (247kb)

Title: Efficient Real-time Refinement of Language Model Text Generation
Authors: Joonho Ko, Jinheon Baek, Sung Ju Hwang
Categories: cs.CL cs.AI cs.LG
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2501.07824 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2501.13951
replaced with revised version Fri, 19 Sep 2025 17:50:58 GMT   (547kb)

Title: A Layered Multi-Expert Framework for Long-Context Mental Health
  Assessments
Authors: Jinwen Tang, Qiming Guo, Wenbo Sun and Yi Shang
Categories: cs.CL cs.AI
Journal-ref: Proc. 2025 IEEE Conference on Artificial Intelligence (CAI), Santa
  Clara, CA, USA, 2025, pp. 435-440
DOI: 10.1109/CAI64502.2025.00080
\\ ( https://arxiv.org/abs/2501.13951 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01349
replaced with revised version Fri, 19 Sep 2025 13:06:51 GMT   (723kb)

Title: Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product
  Recommendations
Authors: Giorgos Filandrianos, Angeliki Dimitriou, Maria Lymperaiou,
  Konstantinos Thomas, Giorgos Stamou
Categories: cs.CL
Comments: Accepted at EMNLP 2025
\\ ( https://arxiv.org/abs/2502.01349 ,  723kb)
------------------------------------------------------------------------------
\\
arXiv:2502.02534
replaced with revised version Fri, 19 Sep 2025 04:28:06 GMT   (1636kb)

Title: Adaptive Self-improvement LLM Agentic System for ML Library Development
Authors: Genghan Zhang, Weixin Liang, Olivia Hsu, and Kunle Olukotun
Categories: cs.CL
\\ ( https://arxiv.org/abs/2502.02534 ,  1636kb)
------------------------------------------------------------------------------
\\
arXiv:2502.05849
replaced with revised version Fri, 19 Sep 2025 02:54:17 GMT   (705kb)

Title: Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation
  through Cognitive Biases
Authors: Jen-tse Huang, Yuhang Yan, Linqi Liu, Yixin Wan, Wenxuan Wang, Kai-Wei
  Chang, Michael R. Lyu
Categories: cs.CL
Comments: Accepted to EMNLP 2025 (Fingings)
\\ ( https://arxiv.org/abs/2502.05849 ,  705kb)
------------------------------------------------------------------------------
\\
arXiv:2502.08415
replaced with revised version Fri, 19 Sep 2025 17:45:35 GMT   (464kb)

Title: FSLI: An Interpretable Formal Semantic System for One-Dimensional
  Ordering Inference
Authors: Maha Alkhairy, Vincent Homer, Brendan O'Connor
Categories: cs.CL cs.LO
Comments: 3 figures, 9 pages main paper and 8 pages references and appendix
\\ ( https://arxiv.org/abs/2502.08415 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2502.15975
replaced with revised version Fri, 19 Sep 2025 00:27:26 GMT   (182kb)

Title: Sparsity May Be All You Need: Sparse Random Parameter Adaptation
Authors: Jesus Rios and Pierre Dognin and Ronny Luss and Karthikeyan N.
  Ramamurthy
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2502.15975 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16781
replaced with revised version Fri, 19 Sep 2025 11:13:18 GMT   (428kb)

Title: Evaluating Robustness of LLMs in Question Answering on Multilingual
  Noisy OCR Data
Authors: Bhawna Piryani, Jamshid Mozafari, Abdelrahman Abdallah, Antoine
  Doucet, Adam Jatowt
Categories: cs.CL
Comments: Accepted at CIKM 2025
\\ ( https://arxiv.org/abs/2502.16781 ,  428kb)
------------------------------------------------------------------------------
\\
arXiv:2502.18036
replaced with revised version Thu, 18 Sep 2025 20:23:29 GMT   (547kb)

Title: Harnessing Multiple Large Language Models: A Survey on LLM Ensemble
Authors: Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai
  Luo, Qianren Mao, Ming Li, Likang Xiao, Dingqi Yang, Yikun Ban, Hailong Sun,
  Philip S. Yu
Categories: cs.CL
Comments: 10 pages, 2 figures, codebase:
  https://github.com/junchenzhi/Awesome-LLM-Ensemble
\\ ( https://arxiv.org/abs/2502.18036 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2503.00032
replaced with revised version Fri, 19 Sep 2025 04:46:10 GMT   (3342kb)

Title: KatFishNet: Detecting LLM-Generated Korean Text through Linguistic
  Feature Analysis
Authors: Shinwoo Park, Shubin Kim, Do-Kyung Kim, Yo-Sub Han
Categories: cs.CL cs.AI
Comments: ACL 2025
\\ ( https://arxiv.org/abs/2503.00032 ,  3342kb)
------------------------------------------------------------------------------
\\
arXiv:2503.04990
replaced with revised version Fri, 19 Sep 2025 04:53:46 GMT   (296kb)

Title: DP-GTR: Differentially Private Prompt Protection via Group Text
  Rewriting
Authors: Mingchen Li, Heng Fan, Song Fu, Junhua Ding, Yunhe Feng
Categories: cs.CL
Comments: 9 pages, 3 figures, 5 tables
\\ ( https://arxiv.org/abs/2503.04990 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2503.05362
replaced with revised version Fri, 19 Sep 2025 02:55:36 GMT   (697kb)

Title: Chain of Strategy Optimization Makes Large Language Models Better
  Emotional Supporter
Authors: Weixiang Zhao, Xingyu Sui, Xinyang Han, Yang Deng, Yulin Hu, Jiahe
  Guo, Libo Qin, Qianyun Du, Shijin Wang, Yanyan Zhao, Bing Qin, Ting Liu
Categories: cs.CL
Comments: 21 pages, 9 figures, 17 tables
\\ ( https://arxiv.org/abs/2503.05362 ,  697kb)
------------------------------------------------------------------------------
\\
arXiv:2503.11751
replaced with revised version Thu, 18 Sep 2025 17:58:20 GMT   (1276kb)

Title: reWordBench: Benchmarking and Improving the Robustness of Reward Models
  with Transformed Inputs
Authors: Zhaofeng Wu, Michihiro Yasunaga, Andrew Cohen, Yoon Kim, Asli
  Celikyilmaz, Marjan Ghazvininejad
Categories: cs.CL
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2503.11751 ,  1276kb)
------------------------------------------------------------------------------
\\
arXiv:2503.12123
replaced with revised version Fri, 19 Sep 2025 08:49:13 GMT   (513kb)

Title: MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine
  Translation via Reward Modeling
Authors: Zhaopeng Feng, Jiahan Ren, Jiayuan Su, Jiamei Zheng, Hongwei Wang,
  Zuozhu Liu
Categories: cs.CL cs.AI
Comments: EMNLP 2025 Findings. Project
  page:https://sabijun.github.io/MT_RewardTreePage
\\ ( https://arxiv.org/abs/2503.12123 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2503.18008
replaced with revised version Fri, 19 Sep 2025 07:12:43 GMT   (369kb)

Title: Personalized Language Models via Privacy-Preserving Evolutionary Model
  Merging
Authors: Kyuyoung Kim, Jinwoo Shin, Jaehyung Kim
Categories: cs.CL cs.NE
Comments: EMNLP 2025 Oral
\\ ( https://arxiv.org/abs/2503.18008 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2504.04083
replaced with revised version Thu, 18 Sep 2025 23:51:48 GMT   (221kb)

Title: A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with
  LLMs: Experiments with OpenAI Models
Authors: Aviv Brokman and Xuguang Ai and Yuhang Jiang and Shashank Gupta and
  Ramakanth Kavuluru
Categories: cs.CL
Comments: New experiments added with the GPT-OSS-120B model
\\ ( https://arxiv.org/abs/2504.04083 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2504.09407
replaced with revised version Fri, 19 Sep 2025 17:52:50 GMT   (6004kb)

Title: UXAgent: A System for Simulating Usability Testing of Web Design with
  LLM Agents
Authors: Yuxuan Lu, Bingsheng Yao, Hansu Gu, Jing Huang, Jessie Wang, Yang Li,
  Jiri Gesi, Qi He, Toby Jia-Jun Li, Dakuo Wang
Categories: cs.CL cs.HC
\\ ( https://arxiv.org/abs/2504.09407 ,  6004kb)
------------------------------------------------------------------------------
\\
arXiv:2504.14871
replaced with revised version Fri, 19 Sep 2025 03:48:32 GMT   (334kb)

Title: Natural Fingerprints of Large Language Models
Authors: Teppei Suzuki, Ryokan Ri, Sho Takase
Categories: cs.CL
\\ ( https://arxiv.org/abs/2504.14871 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2504.20484
replaced with revised version Fri, 19 Sep 2025 04:09:02 GMT   (2580kb)

Title: Enhancing LLM Language Adaption through Cross-lingual In-Context
  Pre-training
Authors: Linjuan Wu, Haoran Wei, Huan Lin, Tianhao Li, Baosong Yang, Fei Huang,
  Weiming Lu
Categories: cs.CL
Comments: 12 pages, 6 figures, EMNLP 2025
\\ ( https://arxiv.org/abs/2504.20484 ,  2580kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11277
replaced with revised version Fri, 19 Sep 2025 12:21:03 GMT   (632kb)

Title: Search and Refine During Think: Facilitating Knowledge Refinement for
  Improved Retrieval-Augmented Reasoning
Authors: Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing
  Cai, An Zhang, Xiang Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.11277 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13090
replaced with revised version Fri, 19 Sep 2025 13:54:04 GMT   (776kb)

Title: The Effect of Language Diversity When Fine-Tuning Large Language Models
  for Translation
Authors: David Stap, Christof Monz
Categories: cs.CL
Comments: EMNLP 2025 Camera Ready
\\ ( https://arxiv.org/abs/2505.13090 ,  776kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13252
replaced with revised version Fri, 19 Sep 2025 15:19:24 GMT   (1847kb)

Title: Are LLMs Better Formalizers than Solvers on Complex Problems?
Authors: Rikhil Amonkar, May Lai, Ronan Le Bras, Li Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.13252 ,  1847kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14395
replaced with revised version Fri, 19 Sep 2025 14:26:02 GMT   (2094kb)

Title: MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation
  Capabilities in Any Language
Authors: Seyoung Song, Seogyeong Jeong, Eunsu Kim, Jiho Jin, Dongkwan Kim, Jay
  Shin, Alice Oh
Categories: cs.CL cs.AI
Comments: To appear in Findings of EMNLP 2025
\\ ( https://arxiv.org/abs/2505.14395 ,  2094kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14442
replaced with revised version Fri, 19 Sep 2025 11:33:34 GMT   (177kb)

Title: Creative Preference Optimization
Authors: Mete Ismayilzada, Antonio Laverghetta Jr., Simone A. Luchini, Reet
  Patel, Antoine Bosselut, Lonneke van der Plas, Roger Beaty
Categories: cs.CL cs.AI
Comments: Accepted to EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2505.14442 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14815
replaced with revised version Fri, 19 Sep 2025 06:57:55 GMT   (3671kb)

Title: Language Mixing in Reasoning Language Models: Patterns, Impact, and
  Internal Causes
Authors: Mingyang Wang, Lukas Lange, Heike Adel, Yunpu Ma, Jannik Str\"otgen,
  Hinrich Sch\"utze
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.14815 ,  3671kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15389
replaced with revised version Fri, 19 Sep 2025 13:54:09 GMT   (2182kb)

Title: Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark
  Study
Authors: DongGeon Lee, Joonwon Jang, Jihae Jeong, Hwanjo Yu
Categories: cs.CL cs.CR cs.CV
Comments: Accepted to EMNLP 2025
\\ ( https://arxiv.org/abs/2505.15389 ,  2182kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16232
replaced with revised version Fri, 19 Sep 2025 17:11:25 GMT   (1586kb)

Title: MuseScorer: Idea Originality Scoring At Scale
Authors: Ali Sarosh Bangash, Krish Veera, Ishfat Abrar Islam, Raiyan Abdul
  Baten
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.16232 ,  1586kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16325
replaced with revised version Fri, 19 Sep 2025 05:32:03 GMT   (942kb)

Title: CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report
  Evaluation
Authors: Yuyang Jiang, Chacha Chen, Shengyuan Wang, Feng Li, Zecong Tang,
  Benjamin M. Mervak, Lydia Chelala, Christopher M Straus, Reve Chahine, Samuel
  G. Armato III, Chenhao Tan
Categories: cs.CL cs.AI cs.CY
Comments: Accepted to Findings of EMNLP 2025; 20 pages, 5 figures
\\ ( https://arxiv.org/abs/2505.16325 ,  942kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17390
replaced with revised version Fri, 19 Sep 2025 01:31:58 GMT   (345kb)

Title: Measuring Lexical Diversity of Synthetic Data Generated through
  Fine-Grained Persona Prompting
Authors: Gauri Kambhatla, Chantal Shaib, Venkata Govindarajan
Categories: cs.CL
Comments: Accepted to EMNLP Findings 2025
\\ ( https://arxiv.org/abs/2505.17390 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17464
replaced with revised version Fri, 19 Sep 2025 05:34:25 GMT   (2319kb)

Title: HydraRAG: Structured Cross-Source Enhanced Large Language Model
  Reasoning
Authors: Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu,
  Wenjie Zhang
Categories: cs.CL
Comments: Accepted by EMNLP2025 (Main Conference)
\\ ( https://arxiv.org/abs/2505.17464 ,  2319kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19528
replaced with revised version Fri, 19 Sep 2025 06:12:42 GMT   (1210kb)

Title: AmpleHate: Amplifying the Attention for Versatile Implicit Hate
  Detection
Authors: Yejin Lee, Joonghyuk Hahn, Hyeseon Ahn, Yo-Sub Han
Categories: cs.CL cs.AI cs.CY
Comments: 13 pages, 4 figures, EMNLP 2025
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2505.19528 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20422
replaced with revised version Fri, 19 Sep 2025 09:31:17 GMT   (1661kb)

Title: SEMMA: A Semantic Aware Knowledge Graph Foundation Model
Authors: Arvindh Arun, Sumit Kumar, Mojtaba Nayyeri, Bo Xiong, Ponnurangam
  Kumaraguru, Antonio Vergari, Steffen Staab
Categories: cs.CL cs.AI
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2505.20422 ,  1661kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21772
replaced with revised version Thu, 18 Sep 2025 20:45:36 GMT   (13217kb)

Title: Calibrating LLM Confidence by Probing Perturbed Representation Stability
Authors: Reza Khanmohammadi, Erfan Miahi, Mehrsa Mardikoraem, Simerjot Kaur,
  Ivan Brugere, Charese H. Smiley, Kundan Thind, and Mohammad M. Ghassemi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.21772 ,  13217kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22777
replaced with revised version Fri, 19 Sep 2025 13:18:19 GMT   (9800kb)

Title: MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain
  Dialogue Evaluators
Authors: John Mendon\c{c}a, Alon Lavie, Isabel Trancoso
Categories: cs.CL
Comments: October ARR
\\ ( https://arxiv.org/abs/2505.22777 ,  9800kb)
------------------------------------------------------------------------------
\\
arXiv:2505.24544
replaced with revised version Fri, 19 Sep 2025 17:11:56 GMT   (932kb)

Title: Cross-Attention Speculative Decoding
Authors: Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji,
  Chul Lee
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.24544 ,  932kb)
------------------------------------------------------------------------------
\\
arXiv:2506.00288
replaced with revised version Fri, 19 Sep 2025 11:27:30 GMT   (481kb)

Title: Emergent Abilities of Large Language Models under Continued Pretraining
  for Language Adaptation
Authors: Ahmed Elhady, Eneko Agirre, Mikel Artetxe
Categories: cs.CL cs.AI
Comments: Published as a Conference Paper at the main track of ACL 2025
\\ ( https://arxiv.org/abs/2506.00288 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2506.04586
replaced with revised version Fri, 19 Sep 2025 09:40:33 GMT   (200kb)

Title: LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech
  Foundational Models Using in-the-wild Data
Authors: Wen Ding, Fan Qian
Categories: cs.CL cs.SD eess.AS
Comments: Submitted to ICASSP 2026
\\ ( https://arxiv.org/abs/2506.04586 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09627
replaced with revised version Fri, 19 Sep 2025 15:02:54 GMT   (280kb)

Title: Benchmarking Debiasing Methods for LLM-based Parameter Estimates
Authors: Nicolas Audinet de Pieuchon, Adel Daoud, Connor T. Jerzak, Moa
  Johansson, Richard Johansson
Categories: cs.CL
Comments: To appear as: Nicolas Audinet de Pieuchon, Adel Daoud, Connor T.
  Jerzak, Moa Johansson, Richard Johansson. Benchmarking Debiasing Methods for
  LLM-based Parameter Estimates. In: Proceedings of the 2025 Conference on
  Empirical Methods in Natural Language Processing (EMNLP), 2025
\\ ( https://arxiv.org/abs/2506.09627 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09996
replaced with revised version Fri, 19 Sep 2025 10:07:39 GMT   (4373kb)

Title: From Judgment to Interference: Early Stopping LLM Harmful Outputs via
  Streaming Content Monitoring
Authors: Yang Li, Qiang Sheng, Yehan Yang, Xueyao Zhang, Juan Cao
Categories: cs.CL cs.CY
Comments: NeurIPS 2025 Accepted Paper
\\ ( https://arxiv.org/abs/2506.09996 ,  4373kb)
------------------------------------------------------------------------------
\\
arXiv:2506.12158
replaced with revised version Fri, 19 Sep 2025 17:07:44 GMT   (2050kb)

Title: A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource
  Languages
Authors: Tatiana Anikina, Jan Cegin, Jakub Simko, Simon Ostermann
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Main
\\ ( https://arxiv.org/abs/2506.12158 ,  2050kb)
------------------------------------------------------------------------------
\\
arXiv:2506.13229
replaced with revised version Fri, 19 Sep 2025 08:04:33 GMT   (18766kb)

Title: IGD: Token Decisiveness Modeling via Information Gain in LLMs for
  Personalized Recommendation
Authors: Zijie Lin, Yang Zhang, Xiaoyan Zhao, Fengbin Zhu, Fuli Feng, Tat-Seng
  Chua
Categories: cs.CL
\\ ( https://arxiv.org/abs/2506.13229 ,  18766kb)
------------------------------------------------------------------------------
\\
arXiv:2507.08660
replaced with revised version Fri, 19 Sep 2025 15:43:31 GMT   (317kb)

Title: The Impact of Automatic Speech Transcription on Speaker Attribution
Authors: Cristina Aggazzotti, Matthew Wiesner, Elizabeth Allyn Smith, Nicholas
  Andrews
Categories: cs.CL cs.LG
Comments: Accepted to Transactions of the Association for Computational
  Linguistics (TACL)
\\ ( https://arxiv.org/abs/2507.08660 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2507.12260
replaced with revised version Fri, 19 Sep 2025 15:29:20 GMT   (420kb)

Title: Translationese-index: Using Likelihood Ratios for Graded and
  Generalizable Measurement of Translationese
Authors: Yikang Liu, Wanyang Zhang, Yiming Wang, Jialong Tang, Pei Zhang,
  Baosong Yang, Fei Huang, Rui Wang, Hai Hu
Categories: cs.CL
Comments: EMNLP 2025 camera-ready
\\ ( https://arxiv.org/abs/2507.12260 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2507.23386
replaced with revised version Fri, 19 Sep 2025 13:35:35 GMT   (186kb)

Title: Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models
Authors: Ailiang Lin, Zhuoyun Li, Kotaro Funakoshi, Manabu Okumura
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2507.23386 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2508.00924
replaced with revised version Fri, 19 Sep 2025 14:51:43 GMT   (320kb)

Title: XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and
  AutoML
Authors: Ernesto L. Estevanell-Valladares, Suilan Estevez-Velarde, Yoan
  Guti\'errez, Andr\'es Montoyo and Ruslan Mitkov
Categories: cs.CL
Comments: 18 pages, 10 figures, 7 tables. Preprint. Accepted at EMNLP 2025
MSC-class: 68T05, 68T50
ACM-class: I.2.6; I.2.7; I.2.8
\\ ( https://arxiv.org/abs/2508.00924 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15239
replaced with revised version Fri, 19 Sep 2025 10:08:52 GMT   (907kb)

Title: WangchanThaiInstruct: An instruction-following Dataset for
  Culture-Aware, Multitask, and Multi-domain Evaluation in Thai
Authors: Peerat Limkonchotiwat, Pume Tuchinda, Lalita Lowphansirikul, Surapon
  Nonesung, Panuthep Tasawong, Alham Fikri Aji, Can Udomcharoenchaikit, Sarana
  Nutanong
Categories: cs.CL
Comments: Accepted to EMNLP 2025 (Main). Model and Dataset:
  https://huggingface.co/collections/airesearch/wangchan-thai-instruction-6835722a30b98e01598984fd
\\ ( https://arxiv.org/abs/2508.15239 ,  907kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15474
replaced with revised version Thu, 18 Sep 2025 19:00:28 GMT   (353kb)

Title: Subjective Behaviors and Preferences in LLM: Language of Browsing
Authors: Sai Sundaresan, Harshita Chopra, Atanu R. Sinha, Koustava Goswami,
  Nagasai Saketh Naidu, Raghav Karan, N Anushka
Categories: cs.CL cs.AI
Comments: Accepted at EMNLP 2025
\\ ( https://arxiv.org/abs/2508.15474 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16048
replaced with revised version Fri, 19 Sep 2025 03:20:15 GMT   (169kb)

Title: OpenWHO: A Document-Level Parallel Corpus for Health Translation in
  Low-Resource Languages
Authors: Rapha\"el Merx, Hanna Suominen, Trevor Cohn, Ekaterina Vylomova
Categories: cs.CL cs.AI
Comments: Accepted at WMT 2025
\\ ( https://arxiv.org/abs/2508.16048 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2508.19282
replaced with revised version Fri, 19 Sep 2025 09:16:41 GMT   (1304kb)

Title: CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via
  Reinforcement Learning
Authors: Ziqiang Cui, Yunpeng Weng, Xing Tang, Peiyang Liu, Shiwei Li, Bowei
  He, Jiamin Chen, Yansen Zhang, Xiuqiang He, Chen Ma
Categories: cs.CL cs.AI
Comments: This paper is under continuous improvement
\\ ( https://arxiv.org/abs/2508.19282 ,  1304kb)
------------------------------------------------------------------------------
\\
arXiv:2508.21436
replaced with revised version Fri, 19 Sep 2025 07:46:25 GMT   (30109kb)

Title: Discovering Semantic Subdimensions through Disentangled Conceptual
  Representations
Authors: Yunhao Zhang, Shaonan Wang, Nan Lin, Xinyi Dong, Chong Li, Chengqing
  Zong
Categories: cs.CL
\\ ( https://arxiv.org/abs/2508.21436 ,  30109kb)
------------------------------------------------------------------------------
\\
arXiv:2508.21589
replaced with revised version Fri, 19 Sep 2025 13:25:52 GMT   (1058kb)

Title: Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM
  Fine-Tuning via Closed-Loop Learning
Authors: Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu,
  Conghui He and Lijun Wu
Categories: cs.CL cs.AI
Comments: Accepted by EMNLP 2025 (Main)
\\ ( https://arxiv.org/abs/2508.21589 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2508.21741
replaced with revised version Fri, 19 Sep 2025 14:53:19 GMT   (3224kb)

Title: Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning
  Performance
Authors: Yao Wang, Di Liang, Minlong Peng
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2508.21741 ,  3224kb)
------------------------------------------------------------------------------
\\
arXiv:2509.00934
replaced with revised version Fri, 19 Sep 2025 02:16:48 GMT   (229kb)

Title: MedCOD: Enhancing English-to-Spanish Medical Translation of Large
  Language Models Using Enriched Chain-of-Dictionary Framework
Authors: Md Shahidul Salim, Lian Fu, Arav Adikesh Ramakrishnan, Zonghai Yao,
  Hong Yu
Categories: cs.CL cs.AI
Comments: To appear in Findings of the Association for Computational
  Linguistics: EMNLP 2025
\\ ( https://arxiv.org/abs/2509.00934 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01322
replaced with revised version Fri, 19 Sep 2025 13:34:47 GMT   (7127kb)

Title: LongCat-Flash Technical Report
Authors: Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong,
  Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han,
  Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen,
  Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong,
  Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu,
  Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan,
  Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang,
  Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang,
  Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu,
  Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui
  Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, et
  al. (114 additional authors not shown)
Categories: cs.CL cs.AI cs.DC cs.LG
\\ ( https://arxiv.org/abs/2509.01322 ,  7127kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01476
replaced with revised version Fri, 19 Sep 2025 12:43:33 GMT   (902kb)

Title: Do Retrieval Augmented Language Models Know When They Don't Know?
Authors: Youchao Zhou, Heyan Huang, Yicheng Liu, Rui Dai, Xinglin Wang,
  Xingchen Zhang, Shumin Shi, Yang Deng
Categories: cs.CL cs.AI
Comments: under review
\\ ( https://arxiv.org/abs/2509.01476 ,  902kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07188
replaced with revised version Fri, 19 Sep 2025 02:20:08 GMT   (1392kb)

Title: DischargeSim: A Simulation Benchmark for Educational Doctor-Patient
  Communication at Discharge
Authors: Zonghai Yao, Michael Sun, Won Seok Jang, Sunjae Kwon, Soie Kwon, Hong
  Yu
Categories: cs.CL cs.AI
Comments: Equal contribution for the first two authors. To appear in the
  proceedings of the Main Conference on Empirical Methods in Natural Language
  Processing (EMNLP) 2025
\\ ( https://arxiv.org/abs/2509.07188 ,  1392kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10179
replaced with revised version Thu, 18 Sep 2025 23:31:43 GMT   (777kb)

Title: Benchmark of stylistic variation in LLM-generated texts
Authors: Ji\v{r}\'i Mili\v{c}ka, Anna Marklov\'a, V\'aclav Cvr\v{c}ek
Categories: cs.CL cs.AI
Comments: Data and scripts: https://osf.io/hs7xt/. Interactive charts:
  https://www.korpus.cz/stylisticbenchmark/
\\ ( https://arxiv.org/abs/2509.10179 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12385
replaced with revised version Fri, 19 Sep 2025 15:48:12 GMT   (272kb)

Title: SENTRA: Selected-Next-Token Transformer for LLM Text Detection
Authors: Mitchell Plyler, Yilun Zhang, Alexander Tuzhilin, Saoud Khalifah, Sen
  Tian
Categories: cs.CL cs.LG
Comments: EMNLP Findings 2025
\\ ( https://arxiv.org/abs/2509.12385 ,  272kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14834
replaced with revised version Fri, 19 Sep 2025 03:11:00 GMT   (8194kb)

Title: LLM Agents at the Roundtable: A Multi-Perspective and Dialectical
  Reasoning Framework for Essay Scoring
Authors: Jinhee Jang, Ayoung Moon, Minkyoung Jung, YoungBin Kim, Seung Jin Lee
Categories: cs.CL
\\ ( https://arxiv.org/abs/2509.14834 ,  8194kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14851
replaced with revised version Fri, 19 Sep 2025 07:24:59 GMT   (3791kb)

Title: Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for
  Long-Form Mental Health Support
Authors: Xianrong Yao, Dong She, Chenxu Zhang, Yimeng Zhang, Yueru Sun, Noman
  Ahmed, Yang Gao, Zhanpeng Jin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2509.14851 ,  3791kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07733
replaced with revised version Fri, 19 Sep 2025 08:44:50 GMT   (16603kb)

Title: Beyond Pixels: Enhancing LIME with Hierarchical Features and
  Segmentation Foundation Models
Authors: Patrick Knab, Sascha Marton, Christian Bartelt
Categories: cs.CV cs.AI
Comments: ECAI 2025 - Main Track
\\ ( https://arxiv.org/abs/2403.07733 ,  16603kb)
------------------------------------------------------------------------------
\\
arXiv:2405.05587
replaced with revised version Fri, 19 Sep 2025 05:33:16 GMT   (12355kb)

Title: Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural
  Collapse
Authors: Yining Wang, Junjie Sun, Chenyue Wang, Mi Zhang, Min Yang
Categories: cs.CV cs.LG
Comments: CVPR 2024 Highlight
\\ ( https://arxiv.org/abs/2405.05587 ,  12355kb)
------------------------------------------------------------------------------
\\
arXiv:2405.16848
replaced with revised version Fri, 19 Sep 2025 13:54:47 GMT   (4862kb)

Title: A re-calibration method for object detection with multi-modal alignment
  bias in autonomous driving
Authors: Zhihang Song, Dingyi Yao, Ruibo Ming, Lihui Peng, Danya Yao, Yi Zhang
Categories: cs.CV
Comments: Accepted for publication in IST 2025. Official IEEE Xplore entry will
  be available once published
Journal-ref: 2025 IEEE International Conference on Imaging Systems and
  Techniques (IST 2025)
\\ ( https://arxiv.org/abs/2405.16848 ,  4862kb)
------------------------------------------------------------------------------
\\
arXiv:2407.17927
replaced with revised version Fri, 19 Sep 2025 12:24:24 GMT   (55825kb)

Title: Assessing invariance to affine transformations in image quality metrics
Authors: Nuria Alabau-Bosque and Paula Daud\'en-Oliver and Jorge Vila-Tom\'as
  and Valero Laparra and Jes\'us Malo
Categories: cs.CV cs.AI
Comments: 25 pages 41 figures
\\ ( https://arxiv.org/abs/2407.17927 ,  55825kb)
------------------------------------------------------------------------------
\\
arXiv:2408.01437
replaced with revised version Fri, 19 Sep 2025 02:38:11 GMT   (24762kb)

Title: Img2CAD: Reverse Engineering 3D CAD Models from Images through
  VLM-Assisted Conditional Factorization
Authors: Yang You, Mikaela Angelina Uy, Jiaqi Han, Rahul Thomas, Haotong Zhang,
  Yi Du, Hansheng Chen, Francis Engelmann, Suya You, Leonidas Guibas
Categories: cs.CV cs.GR
Comments: Accepted to SIGGRAPH Asia 2025
\\ ( https://arxiv.org/abs/2408.01437 ,  24762kb)
------------------------------------------------------------------------------
\\
arXiv:2408.03591
replaced with revised version Fri, 19 Sep 2025 14:02:24 GMT   (371kb)

Title: FOVAL: Calibration-Free and Subject-Invariant Fixation Depth Estimation
  Across Diverse Eye-Tracking Datasets
Authors: Benedikt W. Hosp
Categories: cs.CV cs.AI cs.HC cs.LG eess.SP
\\ ( https://arxiv.org/abs/2408.03591 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2408.09397
replaced with revised version Thu, 18 Sep 2025 12:14:26 GMT   (12702kb)

Title: Combo: Co-speech holistic 3D human motion generation and efficient
  customizable adaptation in harmony
Authors: Chao Xu, Mingze Sun, Zhi-Qi Cheng, Fei Wang, Yang Liu, Baigui Sun,
  Ruqi Huang, Alexander Hauptmann
Categories: cs.CV
Comments: Accepted to TPAMI
\\ ( https://arxiv.org/abs/2408.09397 ,  12702kb)
------------------------------------------------------------------------------
\\
arXiv:2408.12815
replaced with revised version Fri, 19 Sep 2025 14:31:51 GMT   (4697kb)

Title: CrackSCF: Lightweight Cascaded Fusion Network for Robust and Efficient
  Structural Crack Segmentation
Authors: Hui Liu, Chen Jia, Fan Shi, Xu Cheng, Mianzhao Wang, and Shengyong
  Chen
Categories: cs.CV cs.AI
Comments: This paper is currently under review
\\ ( https://arxiv.org/abs/2408.12815 ,  4697kb)
------------------------------------------------------------------------------
\\
arXiv:2409.19972
replaced with revised version Fri, 19 Sep 2025 01:53:33 GMT   (9496kb)

Title: DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy
  Prediction
Authors: Zhen Yang, Yanpeng Dong, Jiayu Wang, Heng Wang, Lichao Ma, Zijian Cui,
  Qi Liu, Haoran Pei, Kexin Zhang, Chao Zhang
Categories: cs.CV
Comments: TCSVT Accepted version (not the final published version)
\\ ( https://arxiv.org/abs/2409.19972 ,  9496kb)
------------------------------------------------------------------------------
\\
arXiv:2410.08567
replaced with revised version Fri, 19 Sep 2025 11:30:04 GMT   (2794kb)

Title: Diffusion-Based Depth Inpainting for Transparent and Reflective Objects
Authors: Tianyu Sun, Dingchang Hu, Yixiang Dai, Guijin Wang
Categories: cs.CV
DOI: 10.1109/TCSVT.2024.3434740
\\ ( https://arxiv.org/abs/2410.08567 ,  2794kb)
------------------------------------------------------------------------------
\\
arXiv:2410.14710
replaced with revised version Fri, 19 Sep 2025 06:52:49 GMT   (7008kb)

Title: G2D2: Gradient-Guided Discrete Diffusion for Inverse Problem Solving
Authors: Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Bac
  Nguyen, Stefano Ermon, Yuki Mitsufuji
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2410.14710 ,  7008kb)
------------------------------------------------------------------------------
\\
arXiv:2411.11098
replaced with revised version Fri, 19 Sep 2025 05:32:00 GMT   (2170kb)

Title: MolParser: End-to-end Visual Recognition of Molecule Structures in the
  Wild
Authors: Xi Fang, Jiankun Wang, Xiaochen Cai, Shangqian Chen, Shuwen Yang,
  Haoyi Tao, Nan Wang, Lin Yao, Linfeng Zhang, Guolin Ke
Categories: cs.CV
\\ ( https://arxiv.org/abs/2411.11098 ,  2170kb)
------------------------------------------------------------------------------
\\
arXiv:2412.01064
replaced with revised version Fri, 19 Sep 2025 11:22:20 GMT   (15231kb)

Title: FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking
  Portrait
Authors: Taekyung Ki, Dongchan Min, Gyeongsu Chae
Categories: cs.CV cs.AI cs.LG cs.MM eess.IV
Comments: ICCV 2025. Project page:
  https://deepbrainai-research.github.io/float/
\\ ( https://arxiv.org/abs/2412.01064 ,  15231kb)
------------------------------------------------------------------------------
\\
arXiv:2412.07377
replaced with revised version Fri, 19 Sep 2025 02:31:36 GMT   (18230kb)

Title: CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings
Authors: Fuyi Yang, Jiazuo Mu, Yanshun Zhang, Mingqian Zhang, Junxiong Zhang,
  Yongjian Luo, Lan Xu, Jingyi Yu, Yujiao Shi and Yingliang Zhang
Categories: cs.CV
Comments: 16pages, 14 figures, Project web-page:
  https://dgeneai.github.io/cadspotting-pages/
\\ ( https://arxiv.org/abs/2412.07377 ,  18230kb)
------------------------------------------------------------------------------
\\
arXiv:2412.13176
replaced with revised version Thu, 18 Sep 2025 21:33:03 GMT   (20676kb)

Title: NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting
Authors: Andrea Dunn Beltran, Daniel Rho, Marc Niethammer, Roni Sengupta
Categories: cs.CV
\\ ( https://arxiv.org/abs/2412.13176 ,  20676kb)
------------------------------------------------------------------------------
\\
arXiv:2501.01642
replaced with revised version Fri, 19 Sep 2025 07:16:58 GMT   (270kb)

Title: iCBIR-Sli: Interpretable Content-Based Image Retrieval with 2D Slice
  Embeddings
Authors: Shuhei Tomoshige, Hayato Muraki, Kenichi Oishi, and Hitoshi Iyatomi
Categories: cs.CV cs.LG eess.IV
Comments: 8 pages, 2 figures. Accepted at the SPIE Medical Imaging
Journal-ref: Proceedings of the SPIE Medical Imaging, 16-20 February, 2025, San
  Diego, California, US
DOI: 10.1117/12.3047136
\\ ( https://arxiv.org/abs/2501.01642 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2501.16870
replaced with revised version Fri, 19 Sep 2025 10:46:40 GMT   (2141kb)

Title: Experimenting with Affective Computing Models in Video Interviews with
  Spanish-speaking Older Adults
Authors: Josep Lopez Camunas, Cristina Bustos, Yanjun Zhu, Raquel Ros, Agata
  Lapedriza
Categories: cs.CV
Journal-ref: IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV 2025)
\\ ( https://arxiv.org/abs/2501.16870 ,  2141kb)
------------------------------------------------------------------------------
\\
arXiv:2501.18592
replaced with revised version Fri, 19 Sep 2025 11:58:01 GMT   (9236kb)

Title: Advances in Multimodal Adaptation and Generalization: From Traditional
  Approaches to Foundation Models
Authors: Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill
  Stachniss, Olga Fink
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: Project page:
  https://github.com/donghao51/Awesome-Multimodal-Adaptation
\\ ( https://arxiv.org/abs/2501.18592 ,  9236kb)
------------------------------------------------------------------------------
\\
arXiv:2502.08321
replaced with revised version Thu, 18 Sep 2025 21:50:33 GMT   (13449kb)

Title: Screener: Self-supervised Pathology Segmentation in Medical CT Images
Authors: Mikhail Goncharov, Eugenia Soboleva, Mariia Donskova, Daniil Ignatyev,
  Mikhail Belyaev, Ivan Oseledets, Marina Munkhoeva and Maxim Panov
Categories: cs.CV
\\ ( https://arxiv.org/abs/2502.08321 ,  13449kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09657
replaced with revised version Fri, 19 Sep 2025 15:52:12 GMT   (5440kb)

Title: Integrating Spatiotemporal Vision Transformer into Digital Twins for
  High-Resolution Heat Stress Forecasting in Campus Environments
Authors: Wenjing Gong, Xinyue Ye, Keshu Wu, Suphanut Jamonnak, Wenyu Zhang,
  Yifan Yang, Xiao Huang
Categories: cs.CV
Comments: Revised version after peer review (minor revision)
\\ ( https://arxiv.org/abs/2502.09657 ,  5440kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16972
replaced with revised version Thu, 18 Sep 2025 23:02:07 GMT   (2314kb)

Title: SCoT: Straight Consistent Trajectory for Pre-Trained Diffusion Model
  Distillations
Authors: Zhangkai Wu, Xuhui Fan, Hongyu Wu, Longbing Cao
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2502.16972 ,  2314kb)
------------------------------------------------------------------------------
\\
arXiv:2503.00743
replaced with revised version Fri, 19 Sep 2025 07:38:49 GMT   (9399kb)

Title: Quality-Driven Curation of Remote Sensing Vision-Language Data via
  Learned Scoring Models
Authors: Dilxat Muhtar, Enzhuo Zhang, Zhenshi Li, Feng Gu, Yanglangxing He,
  Pengfeng Xiao, and Xueliang Zhang
Categories: cs.CV
Comments: 39 pages, 13 figures. Accept for NeruIPS2025
\\ ( https://arxiv.org/abs/2503.00743 ,  9399kb)
------------------------------------------------------------------------------
\\
arXiv:2503.04997
replaced with revised version Fri, 19 Sep 2025 16:07:12 GMT   (511kb)

Title: ISP-AD: A Large-Scale Real-World Dataset for Advancing Industrial
  Anomaly Detection with Synthetic and Real Defects
Authors: Paul J. Krassnig and Dieter P. Gruber
Categories: cs.CV
Comments: 32 pages, 6 figures, Revised version submitted to the Journal of
  Intelligent Manufacturing, the dataset is available at
  https://doi.org/10.5281/zenodo.14911042
\\ ( https://arxiv.org/abs/2503.04997 ,  511kb)
------------------------------------------------------------------------------
\\
arXiv:2503.11103
replaced with revised version Fri, 19 Sep 2025 05:51:02 GMT   (1208kb)

Title: Pruning the Paradox: How CLIP's Most Informative Heads Enhance
  Performance While Amplifying Bias
Authors: Avinash Madasu, Vasudev Lal, Phillip Howard
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2503.11103 ,  1208kb)
------------------------------------------------------------------------------
\\
arXiv:2503.16188
replaced with revised version Fri, 19 Sep 2025 03:56:41 GMT   (36432kb)

Title: Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual
  Reinforcement Fine-Tuning
Authors: Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Haoquan Zhang, Wang
  Bill Zhu, Kaipeng Zhang
Categories: cs.CV
Comments: Neurips 2025 Spotlight
\\ ( https://arxiv.org/abs/2503.16188 ,  36432kb)
------------------------------------------------------------------------------
\\
arXiv:2503.18177
replaced with revised version Fri, 19 Sep 2025 04:47:24 GMT   (3276kb)

Title: Training A Neural Network For Partially Occluded Road Sign
  Identification In The Context Of Autonomous Vehicles
Authors: Gulnaz Gimaletdinova, Dim Shaiakhmetov, Madina Akpaeva, Mukhammadmuso
  Abduzhabbarov, Kadyrmamat Momunov
Categories: cs.CV
Journal-ref: International Conference on Computer Systems and Technologies
  (CompSysTech), IEEE Xplore, 2025
DOI: 10.1109/CompSysTech65493.2025.11137116
\\ ( https://arxiv.org/abs/2503.18177 ,  3276kb)
------------------------------------------------------------------------------
\\
arXiv:2503.22983
replaced with revised version Thu, 18 Sep 2025 21:23:00 GMT   (35699kb)

Title: scSplit: Bringing Severity Cognizance to Image Decomposition in
  Fluorescence Microscopy
Authors: Ashesh Ashesh, Florian Jug
Categories: cs.CV
Comments: Selected at NeurIPS'25 (poster)
\\ ( https://arxiv.org/abs/2503.22983 ,  35699kb)
------------------------------------------------------------------------------
\\
arXiv:2504.12088
replaced with revised version Fri, 19 Sep 2025 11:47:37 GMT   (536kb)

Title: AttentionDrop: A Novel Regularization Method for Transformer Models
Authors: Mirza Samad Ahmed Baig, Syeda Anshrah Gillani, Abdul Akbar Khan,
  Shahid Munir Shah, Muhammad Omer Khan
Categories: cs.CV cs.AI cs.LG
Comments: 25 pages
\\ ( https://arxiv.org/abs/2504.12088 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15756
replaced with revised version Fri, 19 Sep 2025 13:45:43 GMT   (1042kb)

Title: DSDNet: Raw Domain Demoir\'eing via Dual Color-Space Synergy
Authors: Qirui Yang, Fangpu Zhang, Yeying Jin, Qihua Cheng, Peng-Tao Jiang,
  Huanjing Yue, Jingyu Yang
Categories: cs.CV eess.IV
\\ ( https://arxiv.org/abs/2504.15756 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05467
replaced with revised version Thu, 18 Sep 2025 21:56:09 GMT   (1529kb)

Title: StreamBridge: Turning Your Offline Video Large Language Model into a
  Proactive Streaming Assistant
Authors: Haibo Wang, Bo Feng, Zhengfeng Lai, Mingze Xu, Shiyu Li, Weifeng Ge,
  Afshin Dehghan, Meng Cao, Ping Huang
Categories: cs.CV cs.AI cs.CL
Comments: Accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2505.05467 ,  1529kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05644
replaced with revised version Fri, 19 Sep 2025 08:50:11 GMT   (41090kb)

Title: The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar
  Reconstruction
Authors: Tom Sander, Moritz Tenthoff, Kay Wohlfarth, Christian W\"ohler
Categories: cs.CV eess.IV
Comments: 48pages
\\ ( https://arxiv.org/abs/2505.05644 ,  41090kb)
------------------------------------------------------------------------------
\\
arXiv:2505.06381
replaced with revised version Fri, 19 Sep 2025 07:03:55 GMT   (23620kb)

Title: Temperature-Driven Robust Disease Detection in Brain and
  Gastrointestinal Disorders via Context-Aware Adaptive Knowledge Distillation
Authors: Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas
  Dengel
Categories: cs.CV
Comments: This version v2 updates the title to match the version accepted for
  publication in biomedical-signal-processing-and-control. The title has been
  changed to 'Temperature-Driven Robust Disease Detection in Brain and
  Gastrointestinal Disorders via Context-Aware Adaptive Knowledge
  Distillation'. The scientific content is unchanged from v1
\\ ( https://arxiv.org/abs/2505.06381 ,  23620kb)
------------------------------------------------------------------------------
\\
arXiv:2505.08437
replaced with revised version Fri, 19 Sep 2025 06:55:28 GMT   (548kb)

Title: TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human
  Body Forgery Detection
Authors: Wenkui Yang, Zhida Zhang, Xiaoqiang Zhou, Junxian Duan and Jie Cao
Categories: cs.CV
Comments: Accepted by PRCV 2024
\\ ( https://arxiv.org/abs/2505.08437 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2505.09380
replaced with revised version Fri, 19 Sep 2025 10:50:53 GMT   (5195kb)

Title: Examining Deployment and Refinement of the VIOLA-AI Intracranial
  Hemorrhage Model Using an Interactive NeoMedSys Platform
Authors: Qinghui Liu, Jon E. Nesvold, Hanna Raaum, Elakkyen Murugesu, Martin
  R{\o}vang, Bradley J Maclntosh, Atle Bj{\o}rnerud, Karoline Skogen
Categories: cs.CV cs.AI cs.LG
Comments: 21 pages, 11 figures, on submission to BMC Methods
\\ ( https://arxiv.org/abs/2505.09380 ,  5195kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13212
replaced with revised version Fri, 19 Sep 2025 03:30:17 GMT   (27438kb)

Title: Semantic Change Detection of Roads and Bridges: A Fine-grained Dataset
  and Multimodal Frequency-driven Detector
Authors: Qingling Shu, Sibao Chen, Xiao Wang, Zhihui You, Wei Lu, Jin Tang, Bin
  Luo
Categories: cs.CV
\\ ( https://arxiv.org/abs/2505.13212 ,  27438kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14319
replaced with revised version Thu, 18 Sep 2025 19:51:19 GMT   (0kb,I)

Title: RETRO: REthinking Tactile Representation Learning with Material PriOrs
Authors: Weihao Xia, Chenliang Zhou, Cengiz Oztireli
Categories: cs.CV cs.MM
Comments: This publication has infringed on the authorship rights of other
  researchers. The authors kindly request that readers refrain from citing
  earlier version of this paper
\\ ( https://arxiv.org/abs/2505.14319 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18700
replaced with revised version Fri, 19 Sep 2025 06:24:54 GMT   (4346kb)

Title: GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language
  Models and Enhanced Reasoning Chains
Authors: Chun Wang, Xiaoran Pan, Zihao Pan, Haofan Wang, Yiren Song
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2505.18700 ,  4346kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22914
replaced with revised version Fri, 19 Sep 2025 09:57:39 GMT   (4762kb)

Title: cadrille: Multi-modal CAD Reconstruction with Online Reinforcement
  Learning
Authors: Maksim Kolodiazhnyi, Denis Tarasov, Dmitrii Zhemchuzhnikov, Alexander
  Nikulin, Ilya Zisman, Anna Vorontsova, Anton Konushin, Vladislav Kurenkov,
  Danila Rukhovich
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2505.22914 ,  4762kb)
------------------------------------------------------------------------------
\\
arXiv:2506.02015
replaced with revised version Fri, 19 Sep 2025 02:44:08 GMT   (14853kb)

Title: OSPO: Object-centric Self-improving Preference Optimization for
  Text-to-Image Generation
Authors: Yoonjin Oh, Yongjin Kim, Hyomin Kim, Donghwan Chi, Sungwoong Kim
Categories: cs.CV
\\ ( https://arxiv.org/abs/2506.02015 ,  14853kb)
------------------------------------------------------------------------------
\\
arXiv:2506.03642
replaced with revised version Fri, 19 Sep 2025 05:48:14 GMT   (1857kb)

Title: Spatial Understanding from Videos: Structured Prompts Meet Simulation
  Data
Authors: Haoyu Zhang, Meng Liu, Zaijing Li, Haokun Wen, Weili Guan, Yaowei
  Wang, Liqiang Nie
Categories: cs.CV cs.AI
Comments: Accepted by NeurIPS 2025 as a Spotlight
\\ ( https://arxiv.org/abs/2506.03642 ,  1857kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05439
replaced with revised version Fri, 19 Sep 2025 15:33:50 GMT   (2145kb)

Title: LLMs Can Compensate for Deficiencies in Visual Representations
Authors: Sho Takishita and Jay Gala and Abdelrahman Mohamed and Kentaro Inui
  and Yova Kementchedjhieva
Categories: cs.CV cs.AI cs.CL
Comments: EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2506.05439 ,  2145kb)
------------------------------------------------------------------------------
\\
arXiv:2506.07570
replaced with revised version Fri, 19 Sep 2025 09:25:25 GMT   (6551kb)

Title: OptiScene: LLM-driven Indoor Scene Layout Generation via Scaled
  Human-aligned Data Synthesis and Multi-Stage Preference Optimization
Authors: Yixuan Yang, Zhen Luo, Tongsheng Ding, Junru Lu, Mingqi Gao, Jinyu
  Yang, Victor Sanchez, Feng Zheng
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2506.07570 ,  6551kb)
------------------------------------------------------------------------------
\\
arXiv:2506.13638
replaced with revised version Thu, 18 Sep 2025 21:24:31 GMT   (430kb)

Title: DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models
Authors: Zhiyi Shi, Binjie Wang, Chongjie Si, Yichen Wu, Junsik Kim, Hanspeter
  Pfister
Categories: cs.CV cs.AI
Comments: COLM 2025
\\ ( https://arxiv.org/abs/2506.13638 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2506.17946
replaced with revised version Fri, 19 Sep 2025 06:49:02 GMT   (3213kb)

Title: Classification of Tents in Street Bazaars Using CNN
Authors: Azamat Ibragimov, Ruslan Isaev, Remudin Reshid Mekuria, Gulnaz
  Gimaletdinova, Dim Shaiakhmetov
Categories: cs.CV
Journal-ref: International Conference on Computer Systems and Technologies
  (CompSysTech), IEEE Xplore, 2025
DOI: 10.1109/CompSysTech65493.2025.11137220
\\ ( https://arxiv.org/abs/2506.17946 ,  3213kb)
------------------------------------------------------------------------------
\\
arXiv:2506.18369
replaced with revised version Fri, 19 Sep 2025 01:03:31 GMT   (13115kb)

Title: RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language
  Models
Authors: Yeongtak Oh, Jisoo Mok, Dohyun Chung, Juhyeon Shin, Sangha Park, Johan
  Barthelemy, and Sungroh Yoon
Categories: cs.CV
Comments: Accepted to NeurIPS 2025
\\ ( https://arxiv.org/abs/2506.18369 ,  13115kb)
------------------------------------------------------------------------------
\\
arXiv:2507.07620
replaced with revised version Fri, 19 Sep 2025 12:00:01 GMT   (16789kb)

Title: ViLU: Learning Vision-Language Uncertainties for Failure Prediction
Authors: Marc Lafon, Yannis Karmim, Julio Silva-Rodr\'iguez, Paul Couairon,
  Cl\'ement Rambour, Rapha\"el Fournier-Sniehotta, Ismail Ben Ayed, Jose Dolz,
  Nicolas Thome
Categories: cs.CV
Journal-ref: International Conference on Computer Vision, ICCV 2025
\\ ( https://arxiv.org/abs/2507.07620 ,  16789kb)
------------------------------------------------------------------------------
\\
arXiv:2507.08290
replaced with revised version Fri, 19 Sep 2025 15:27:00 GMT   (7760kb)

Title: Cross-Resolution SAR Target Detection Using Structural Hierarchy
  Adaptation and Reliable Adjacency Alignment
Authors: Jiang Qin, Bin Zou, Haolin Li, Lamei Zhang
Categories: cs.CV
Comments: Accepted IEEE TGRS
\\ ( https://arxiv.org/abs/2507.08290 ,  7760kb)
------------------------------------------------------------------------------
\\
arXiv:2507.09885
replaced with revised version Fri, 19 Sep 2025 16:12:06 GMT   (6577kb)

Title: MCGA: Mixture of Codebooks Hyperspectral Reconstruction via
  Grayscale-Aware Attention
Authors: Zhanjiang Yang, Lijun Sun, Jiawei Dong, Xiaoxin An, Yang Liu, Meng Li
Categories: cs.CV
\\ ( https://arxiv.org/abs/2507.09885 ,  6577kb)
------------------------------------------------------------------------------
\\
arXiv:2507.11550
replaced with revised version Fri, 19 Sep 2025 16:33:33 GMT   (3831kb)

Title: Deformable Dynamic Convolution for Accurate yet Efficient
  Spatio-Temporal Traffic Prediction
Authors: Hyeonseok Jin, Geonmin Kim, Kyungbaek Kim
Categories: cs.CV cs.AI
Comments: 8 pages, 5 figures
\\ ( https://arxiv.org/abs/2507.11550 ,  3831kb)
------------------------------------------------------------------------------
\\
arXiv:2507.14067
replaced with revised version Fri, 19 Sep 2025 06:54:08 GMT   (398kb)

Title: VLA-Mark: A cross modal watermark for large vision-language alignment
  model
Authors: Shuliang Liu, Qi Zheng, Jesse Jiaxi Xu, Yibo Yan, Junyan Zhang, He
  Geng, Aiwei Liu, Peijie Jiang, Jia Liu, Yik-Cheung Tam, and Xuming Hu
Categories: cs.CV cs.AI
Comments: Accepted by the main conference, EMNLP 2025
\\ ( https://arxiv.org/abs/2507.14067 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2507.14312
replaced with revised version Fri, 19 Sep 2025 12:52:46 GMT   (5192kb)

Title: CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation
Authors: Marc Lafon, Gustavo Adolfo Vargas Hakim, Cl\'ement Rambour, Christian
  Desrosier, Nicolas Thome
Categories: cs.CV
Journal-ref: 39th Conference on Neural Information Processing Systems, NeurIPS
  2025
\\ ( https://arxiv.org/abs/2507.14312 ,  5192kb)
------------------------------------------------------------------------------
\\
arXiv:2507.18743
replaced with revised version Fri, 19 Sep 2025 07:05:22 GMT   (26060kb)

Title: SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator
  and Progressive Transfer Learning
Authors: Yiguo He, Xinjun Cheng, Junjie Zhu, Chunping Qiu, Jun Wang, Xichuan
  Zhang, Qiangjuan Huang, Ke Yang
Categories: cs.CV
Comments: IEEE Submission
\\ ( https://arxiv.org/abs/2507.18743 ,  26060kb)
------------------------------------------------------------------------------
\\
arXiv:2508.05244
replaced with revised version Fri, 19 Sep 2025 09:46:41 GMT   (0kb,I)

Title: RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning
  Pre-trained Model for Medical Image Understanding
Authors: Tianchen Fang, Guiru Liu
Categories: cs.CV cs.AI
Comments: Upon further review, we identified that our dataset requires
  optimization to ensure research reliability and accuracy. Additionally,
  considering the target journal's latest submission policies, we believe
  comprehensive manuscript revisions are necessary
\\ ( https://arxiv.org/abs/2508.05244 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01109
replaced with revised version Fri, 19 Sep 2025 10:05:36 GMT   (9087kb)

Title: GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for
  Image Representation and Generation
Authors: Zhengqiang Zhang, Rongyuan Wu, Lingchen Sun, and Lei Zhang
Categories: cs.CV
Comments: Accepted by NIPS 2025
\\ ( https://arxiv.org/abs/2509.01109 ,  9087kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01907
replaced with revised version Thu, 18 Sep 2025 23:04:27 GMT   (3590kb)

Title: RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster
  Events
Authors: Zhenyuan Chen, Chenxi Wang, Ningyu Zhang, Feng Zhang
Categories: cs.CV cs.CL
Comments: Accepted by NeurIPS 2025 Dataset and Benchmark Track
\\ ( https://arxiv.org/abs/2509.01907 ,  3590kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10651
replaced with revised version Fri, 19 Sep 2025 06:42:51 GMT   (541kb)

Title: USCTNet: A deep unfolding nuclear-norm optimization solver for
  physically consistent HSI reconstruction
Authors: Xiaoyang Ma and Yiyang Chai and Xinran Qu and Hong Sun
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.10651 ,  541kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13414
replaced with revised version Thu, 18 Sep 2025 22:34:03 GMT   (9776kb)

Title: MapAnything: Universal Feed-Forward Metric 3D Reconstruction
Authors: Nikhil Keetha, Norman M\"uller, Johannes Sch\"onberger, Lorenzo Porzi,
  Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber,
  Nelson Antunes, Jonathon Luiten, Manuel Lopez-Antequera, Samuel Rota Bul\`o,
  Christian Richardt, Deva Ramanan, Sebastian Scherer, Peter Kontschieder
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: Project Page: https://map-anything.github.io/
\\ ( https://arxiv.org/abs/2509.13414 ,  9776kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13922
replaced with revised version Fri, 19 Sep 2025 06:52:50 GMT   (20097kb)

Title: Towards Robust Defense against Customization via Protective Perturbation
  Resistant to Diffusion-based Purification
Authors: Wenkui Yang, Jie Cao, Junxian Duan, Ran He
Categories: cs.CV
Comments: Accepted by ICCV 2025
\\ ( https://arxiv.org/abs/2509.13922 ,  20097kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14033
replaced with revised version Thu, 18 Sep 2025 15:10:25 GMT   (3673kb)

Title: SAIL-VL2 Technical Report
Authors: Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan
  Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao
  Liang, Shuicheng Yan, Chao Feng
Categories: cs.CV
Comments: Technical Report
\\ ( https://arxiv.org/abs/2509.14033 ,  3673kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14476
replaced with revised version Fri, 19 Sep 2025 06:15:52 GMT   (17631kb)

Title: AToken: A Unified Tokenizer for Vision
Authors: Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen
  Chen, Afshin Dehghan, Yinfei Yang
Categories: cs.CV cs.AI cs.MM
Comments: 30 pages, 14 figures
\\ ( https://arxiv.org/abs/2509.14476 ,  17631kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14981
replaced with revised version Fri, 19 Sep 2025 01:52:30 GMT   (45573kb)

Title: SPATIALGEN: Layout-guided 3D Indoor Scene Generation
Authors: Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu,
  Rui Tang, Zihan Zhou, Ping Tan
Categories: cs.CV
Comments: 3D scene generation; diffusion model; Scene reconstruction and
  understanding
\\ ( https://arxiv.org/abs/2509.14981 ,  45573kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15011
replaced with revised version Fri, 19 Sep 2025 07:43:40 GMT   (24317kb)

Title: Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for
  Realistic Underwater Image Generation
Authors: Vasiliki Ismiroglou, Malte Pedersen, Stefan H. Bengtson, Andreas
  Aakerberg, Thomas B. Moeslund
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2509.15011 ,  24317kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15123
replaced with revised version Fri, 19 Sep 2025 05:25:06 GMT   (38987kb)

Title: RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes
Authors: Fang Li, Hao Zhang, Narendra Ahuja
Categories: cs.CV
Comments: NeurIPS 2025 Spotlight
\\ ( https://arxiv.org/abs/2509.15123 ,  38987kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15221
replaced with revised version Fri, 19 Sep 2025 05:29:03 GMT   (27825kb)

Title: ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform
  Data
Authors: Zhaoyang Liu, Jingjing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu
  Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li,
  Xuan Dong, Yue Yu, Chenyu Lu, YunXiang Mo, Yao Yan, Zeyue Tian, Xiao Zhang,
  Yuan Huang, Yiqian Liu, Weijie Su, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen,
  Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.15221 ,  27825kb)
------------------------------------------------------------------------------
\\
arXiv:2501.07130
replaced with revised version Thu, 18 Sep 2025 23:49:59 GMT   (4423kb)

Title: KubeDSM: A Kubernetes-based Dynamic Scheduling and Migration Framework
  for Cloud-Assisted Edge Clusters
Authors: Amirhossein Pashaeehir, Sina Shariati, Shayan Shafaghi, Manni Moghimi,
  Mahmoud Momtazpour
Categories: cs.DC
\\ ( https://arxiv.org/abs/2501.07130 ,  4423kb)
------------------------------------------------------------------------------
\\
arXiv:2501.14931
replaced with revised version Fri, 19 Sep 2025 08:06:57 GMT   (6256kb)

Title: Pod: An Optimal-Latency, Censorship-Free, and Accountable Generalized
  Consensus Layer
Authors: Orestis Alpos, Bernardo David, Jakov Mitrovski, Odysseas Sofikitis,
  Dionysis Zindros
Categories: cs.DC cs.CR
\\ ( https://arxiv.org/abs/2501.14931 ,  6256kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10371
replaced with revised version Fri, 19 Sep 2025 14:28:47 GMT   (895kb)

Title: Characterizing the Efficiency of Distributed Training: A Power,
  Performance, and Thermal Perspective
Authors: Seokjin Go, Joongun Park, Spandan More, Hanjiang Wu, Irene Wang, Aaron
  Jezghani, Tushar Krishna, Divya Mahajan
Categories: cs.DC cs.LG
\\ ( https://arxiv.org/abs/2509.10371 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15103
replaced with revised version Fri, 19 Sep 2025 08:02:22 GMT   (582kb)

Title: Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement
  Learning
Authors: Simin Li, Zheng Yuwei, Zihao Mao, Linhao Wang, Ruixiao Xu, Chengdong
  Ma, Xin Yu, Yuqing Ma, Qi Dou, Xin Wang, Jie Luo, Bo An, Yaodong Yang,
  Weifeng Lv, Xianglong Liu
Categories: cs.MA cs.AI
Comments: submitted to NIPS 2025
\\ ( https://arxiv.org/abs/2509.15103 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04190
replaced with revised version Fri, 19 Sep 2025 07:27:04 GMT   (7941kb)

Title: Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality
  Monitoring of the Hadron Calorimeter
Authors: Mulugeta Weldezgina Asres, Christian Walter Omlin, Long Wang, David
  Yu, Pavel Parygin, Jay Dittmann, Georgia Karapostoli, Markus Seidel,
  Rosamaria Venditti, Luka Lambrecht, Emanuele Usai, Muhammad Ahmad, Javier
  Fernandez Menendez, Kaori Maeshima and the CMS-HCAL Collaboration
Categories: cs.LG cs.AI
Comments: 23 pages, 17 figures, 3 tables, and published version
Journal-ref: Sensors 23 (2023) 24
DOI: 10.3390/s23249679
\\ ( https://arxiv.org/abs/2311.04190 ,  7941kb)
------------------------------------------------------------------------------
\\
arXiv:2405.18848
replaced with revised version Fri, 19 Sep 2025 12:44:16 GMT   (17565kb)

Title: Two Is Better Than One: Aligned Representation Pairs for Anomaly
  Detection
Authors: Alain Ryser, Thomas M. Sutter, Alexander Marx, Julia E. Vogt
Categories: cs.LG cs.AI
Comments: Published in Transactions on Machine Learning Research (TMLR)
  https://openreview.net/forum?id=Bt0zdsnWYc
\\ ( https://arxiv.org/abs/2405.18848 ,  17565kb)
------------------------------------------------------------------------------
\\
arXiv:2409.12812
replaced with revised version Fri, 19 Sep 2025 10:23:46 GMT   (6743kb)

Title: Towards Interactive and Learnable Cooperative Driving Automation: a
  Large Language Model-Driven Decision-Making Framework
Authors: Shiyu Fang, Jiaqi Liu, Mingyu Ding, Yiming Cui, Chen Lv, Peng Hang,
  Jian Sun
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2409.12812 ,  6743kb)
------------------------------------------------------------------------------
\\
arXiv:2410.10320
replaced with revised version Fri, 19 Sep 2025 06:13:06 GMT   (3990kb)

Title: DiRW: Path-Aware Digraph Learning for Heterophily
Authors: Daohan Su, Xunkai Li, Zhenjun Li, Yinping Liao, Rong-Hua Li, Guoren
  Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2410.10320 ,  3990kb)
------------------------------------------------------------------------------
\\
arXiv:2410.11096
replaced with revised version Thu, 18 Sep 2025 19:12:07 GMT   (986kb)

Title: SeCodePLT: A Unified Platform for Evaluating the Security of Code GenAI
Authors: Yuzhou Nie, Zhun Wang, Yu Yang, Ruizhe Jiang, Yuheng Tang, Xander
  Davies, Yarin Gal, Bo Li, Wenbo Guo, Dawn Song
Categories: cs.CR cs.AI
Comments: Accepted to NeurIPS D&B track 2025
\\ ( https://arxiv.org/abs/2410.11096 ,  986kb)
------------------------------------------------------------------------------
\\
arXiv:2410.15555
replaced with revised version Thu, 18 Sep 2025 23:29:36 GMT   (1869kb)

Title: Bayesian Concept Bottleneck Models with LLM Priors
Authors: Jean Feng, Avni Kothari, Luke Zier, Chandan Singh, Yan Shuo Tan
Categories: cs.LG cs.AI stat.ML
Comments: 2025 Conference on Neural Information Processing Systems
\\ ( https://arxiv.org/abs/2410.15555 ,  1869kb)
------------------------------------------------------------------------------
\\
arXiv:2412.00152
replaced with revised version Fri, 19 Sep 2025 05:58:23 GMT   (11143kb)

Title: Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous
  Goal Discovery
Authors: Quentin Houbre and Roel Pieters
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2412.00152 ,  11143kb)
------------------------------------------------------------------------------
\\
arXiv:2502.00604
replaced with revised version Fri, 19 Sep 2025 16:27:57 GMT   (34022kb)

Title: Gradient Alignment in Physics-informed Neural Networks: A Second-Order
  Optimization Perspective
Authors: Sifan Wang, Ananyae Kumar Bhartari, Bowen Li, Paris Perdikaris
Categories: cs.LG cs.AI physics.comp-ph
Comments: 39 pages, 22 figures
\\ ( https://arxiv.org/abs/2502.00604 ,  34022kb)
------------------------------------------------------------------------------
\\
arXiv:2502.05115
replaced with revised version Fri, 19 Sep 2025 03:42:14 GMT   (2693kb)

Title: "It Felt Like I Was Left in the Dark": Exploring Information Needs and
  Design Opportunities for Family Caregivers of Older Adult Patients in
  Critical Care Settings
Authors: Shihan Fu, Bingsheng Yao, Smit Desai, Yuqi Hu, Yuling Sun, Samantha
  Stonbraker, Yanjun Gao, Elizabeth M. Goldberg, Dakuo Wang
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2502.05115 ,  2693kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09969
replaced with revised version Fri, 19 Sep 2025 01:55:25 GMT   (358kb)

Title: Neural Networks for Learnable and Scalable Influence Estimation of
  Instruction Fine-Tuning Data
Authors: Ishika Agarwal, Dilek Hakkani-T\"ur
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2502.09969 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2502.19668 (*cross-listing*)
replaced with revised version Fri, 19 Sep 2025 11:58:56 GMT   (3043kb)

Title: SuPreME: A Supervised Pre-training Framework for Multimodal ECG
  Representation Learning
Authors: Mingsheng Cai, Jiuming Jiang, Wenhao Huang, Che Liu, Rossella Arcucci
Categories: eess.SP cs.AI cs.CL cs.LG
Comments: Findings of The 2025 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2025)
\\ ( https://arxiv.org/abs/2502.19668 ,  3043kb)
------------------------------------------------------------------------------
\\
arXiv:2503.12613
replaced with revised version Thu, 18 Sep 2025 03:14:35 GMT   (6389kb)

Title: Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes
  -- Insights from Urban Studies
Authors: Rashid Mushkani, Hugo Berard, Shin Koseki
Categories: cs.HC cs.AI cs.CY cs.MA
Comments: 16 pages, 13 figures
\\ ( https://arxiv.org/abs/2503.12613 ,  6389kb)
------------------------------------------------------------------------------
\\
arXiv:2503.19285
replaced with revised version Fri, 19 Sep 2025 03:20:02 GMT   (1417kb)

Title: No Black Box Anymore: Demystifying Clinical Predictive Modeling with
  Temporal-Feature Cross Attention Mechanism
Authors: Yubo Li, Xinyu Yao, Rema Padman
Categories: cs.LG cs.AI
Comments: 10 pages, 3 figures, submitted to AMIA 2025
\\ ( https://arxiv.org/abs/2503.19285 ,  1417kb)
------------------------------------------------------------------------------
\\
arXiv:2504.01029
replaced with revised version Thu, 18 Sep 2025 13:53:51 GMT   (6357kb)

Title: Who is Responsible When AI Fails? Mapping Causes, Entities, and
  Consequences of AI Privacy and Ethical Incidents
Authors: Hilda Hadan, Reza Hadi Mogavi, Leah Zhang-Kennedy, Lennart E. Nacke
Categories: cs.CY cs.AI cs.DB cs.HC
Comments: 63 pages, 7 tables, 7 figures
Journal-ref: International Journal of Human-Computer Interaction (2025): 1-45
DOI: 10.1080/10447318.2025.2549073
\\ ( https://arxiv.org/abs/2504.01029 ,  6357kb)
------------------------------------------------------------------------------
\\
arXiv:2504.06320
replaced with revised version Fri, 19 Sep 2025 06:15:56 GMT   (3026kb)

Title: Hybrid Temporal Differential Consistency Autoencoder for Efficient and
  Sustainable Anomaly Detection in Cyber-Physical Systems
Authors: Michael Somma
Categories: cs.CR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2504.06320 ,  3026kb)
------------------------------------------------------------------------------
\\
arXiv:2504.09474
replaced with revised version Fri, 19 Sep 2025 03:11:51 GMT   (896kb)

Title: MigGPT: Harnessing Large Language Models for Automated Migration of
  Out-of-Tree Linux Kernel Patches Across Versions
Authors: Pucheng Dang, Di Huang, Dong Li, Kang Chen, Yuanbo Wen, Qi Guo, Xing
  Hu
Categories: cs.SE cs.AI cs.OS
\\ ( https://arxiv.org/abs/2504.09474 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2505.04881
replaced with revised version Fri, 19 Sep 2025 02:10:44 GMT   (3763kb)

Title: ConCISE: Confidence-guided Compression in Step-by-step Efficient
  Reasoning
Authors: Ziqing Qiao, Yongheng Deng, Jiali Zeng, Dong Wang, Lai Wei, Guanbo
  Wang, Fandong Meng, Jie Zhou, Ju Ren, Yaoxue Zhang
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2505.04881 ,  3763kb)
------------------------------------------------------------------------------
\\
arXiv:2505.10392
replaced with revised version Fri, 19 Sep 2025 13:45:24 GMT   (0kb,I)

Title: Schreier-Coset Graph Propagation
Authors: Aryan Mishra, Lizhen Lin
Categories: cs.LG cs.AI
Comments: The paper has been updated and now utilizes a more comprehensive
  methodology, we felt that the name does not do justice to it as their is no
  graph rewiring involved. Our method adds embeddings at the every beginning of
  before the propagation begins which is essentially feature augmentation. We
  have a more comprehensive method including graph rewiring which we will
  release in due course of time
\\ ( https://arxiv.org/abs/2505.10392 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2505.10994 (*cross-listing*)
replaced with revised version Fri, 19 Sep 2025 16:41:09 GMT   (4155kb)

Title: Space Group Equivariant Crystal Diffusion
Authors: Rees Chang, Angela Pak, Alex Guerra, Ni Zhan, Nick Richardson, Elif
  Ertekin, Ryan P. Adams
Categories: cond-mat.mtrl-sci cs.AI
Journal-ref: NeurIPS 2025
\\ ( https://arxiv.org/abs/2505.10994 ,  4155kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18475
replaced with revised version Thu, 18 Sep 2025 05:51:08 GMT   (1158kb)

Title: A Survey of Large Language Models for Data Challenges in Graphs
Authors: Mengran Li, Pengyu Zhang, Wenbin Xing, Yijia Zheng, Klim Zaporojets,
  Junzhou Chen, Ronghui Zhang, Yong Zhang, Siyuan Gong, Jia Hu, Xiaolei Ma,
  Zhiyuan Liu, Paul Groth, Marcel Worring
Categories: cs.LG cs.AI
Comments: Accepted by Expert Systems with Applications
\\ ( https://arxiv.org/abs/2505.18475 ,  1158kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19441
replaced with revised version Thu, 18 Sep 2025 18:37:12 GMT   (181kb)

Title: Fairness-in-the-Workflow: How Machine Learning Practitioners at Big Tech
  Companies Approach Fairness in Recommender Systems
Authors: Jing Nathan Yan, Emma Harvey, Junxiong Wang, Jeffrey M. Rzeszotarski,
  Allison Koenecke
Categories: cs.HC cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2505.19441 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23868
replaced with revised version Fri, 19 Sep 2025 01:59:02 GMT   (263kb)

Title: Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning
  Expert
Authors: Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou,
  Libo Qin, Wenhong Tian
Categories: cs.LG cs.AI
Comments: Accecpted to NIPS 2025
\\ ( https://arxiv.org/abs/2505.23868 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2505.24535
replaced with revised version Fri, 19 Sep 2025 10:58:16 GMT   (558kb)

Title: Beyond Linear Steering: Unified Multi-Attribute Control for Language
  Models
Authors: Narmeen Oozeer, Luke Marks, Fazl Barez, Amirali Abdullah
Categories: cs.LG cs.AI cs.CL
Comments: Accepted to Findings of EMNLP, 2025
\\ ( https://arxiv.org/abs/2505.24535 ,  558kb)
------------------------------------------------------------------------------
\\
arXiv:2506.06566 (*cross-listing*)
replaced with revised version Thu, 18 Sep 2025 21:36:33 GMT   (1371kb)

Title: AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech
  Recognition
Authors: Chen Bao, Chuanbing Huo, Qinyu Chen, Chang Gao
Categories: eess.AS cs.AI
Comments: Accepted to 2025 IEEE Biomedical Circuits and Systems Conference
  (BioCAS)
\\ ( https://arxiv.org/abs/2506.06566 ,  1371kb)
------------------------------------------------------------------------------
\\
arXiv:2506.07218
replaced with revised version Fri, 19 Sep 2025 02:21:53 GMT   (1067kb)

Title: Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via
  Visual Perception Reward
Authors: Tong Xiao, Xin Xu, Zhenya Huang, Hongyu Gao, Quan Liu, Qi Liu, Enhong
  Chen
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2506.07218 ,  1067kb)
------------------------------------------------------------------------------
\\
arXiv:2506.12556
replaced with revised version Thu, 18 Sep 2025 23:49:10 GMT   (2694kb)

Title: Algorithmic Fairness: Not a Purely Technical but Socio-Technical
  Property
Authors: Yijun Bian, Lei You, Yuya Sasaki, Haruka Maeda, Akira Igarashi
Categories: cs.LG cs.AI cs.CY
Comments: 11 pages without appendix
ACM-class: I.2.6; K.4.2; A.1
\\ ( https://arxiv.org/abs/2506.12556 ,  2694kb)
------------------------------------------------------------------------------
\\
arXiv:2506.13759
replaced with revised version Fri, 19 Sep 2025 07:18:31 GMT   (2448kb)

Title: Discrete Diffusion in Large Language and Multimodal Models: A Survey
Authors: Runpeng Yu and Qi Li and Xinchao Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2506.13759 ,  2448kb)
------------------------------------------------------------------------------
\\
arXiv:2506.15538
replaced with revised version Fri, 19 Sep 2025 11:06:48 GMT   (4616kb)

Title: Capturing Polysemanticity with PRISM: A Multi-Concept Feature
  Description Framework
Authors: Laura Kopf, Nils Feldhus, Kirill Bykov, Philine Lou Bommer, Anna
  Hedstr\"om, Marina M.-C. H\"ohne, Oliver Eberle
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2506.15538 ,  4616kb)
------------------------------------------------------------------------------
\\
arXiv:2507.07318
replaced with revised version Fri, 19 Sep 2025 13:59:48 GMT   (978kb)

Title: Generating Moving 3D Soundscapes with Latent Diffusion Models
Authors: Christian Templin and Yanda Zhu and Hao Wang
Categories: cs.SD cs.AI eess.AS
\\ ( https://arxiv.org/abs/2507.07318 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2507.09087
replaced with revised version Thu, 18 Sep 2025 18:17:44 GMT   (626kb)

Title: Deep Reinforcement Learning with Gradient Eligibility Traces
Authors: Esraa Elelimy, Brett Daley, Andrew Patterson, Marlos C. Machado, Adam
  White, Martha White
Categories: cs.LG cs.AI stat.ML
Journal-ref: Reinforcement Learning Journal, 2025
\\ ( https://arxiv.org/abs/2507.09087 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2508.11759
replaced with revised version Fri, 19 Sep 2025 17:12:21 GMT   (1523kb)

Title: Using Natural Language for Human-Robot Collaboration in the Real World
Authors: Peter Lindes and Kaoutar Skiker
Categories: cs.RO cs.AI cs.CL
Comments: 34 pages, 11 figures, 5 tables. Submitted for publication (2026) in
  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),
  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,
  Chapter 6
\\ ( https://arxiv.org/abs/2508.11759 ,  1523kb)
------------------------------------------------------------------------------
\\
arXiv:2508.21104
replaced with revised version Fri, 19 Sep 2025 02:37:05 GMT   (358kb)

Title: PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic
  Reasoning
Authors: Wenfeng Feng, Penghong Zhao, Guochao Jiang, Chuzhan Hao, Yuewei Zhang,
  Guohua Liu, Hao Wang
Categories: cs.LG cs.AI
Comments: 17 pages, 9 figures
\\ ( https://arxiv.org/abs/2508.21104 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2509.02598 (*cross-listing*)
replaced with revised version Thu, 18 Sep 2025 13:21:39 GMT   (473kb)

Title: MIDOG 2025: Mitotic Figure Detection with Attention-Guided False
  Positive Correction
Authors: Andrew Broad, Jason Keighley, Lucy Godson and Alex Wright
Categories: eess.IV cs.AI
\\ ( https://arxiv.org/abs/2509.02598 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2509.05755
replaced with revised version Fri, 19 Sep 2025 17:17:58 GMT   (1190kb)

Title: On the Security of Tool-Invocation Prompts for LLM-Based Agentic
  Systems: An Empirical Risk Assessment
Authors: Yuchong Xie, Mingyu Luo, Zesen Liu, Zhixiang Zhang, Kaikai Zhang, Yu
  Liu, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2509.05755 ,  1190kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07115
replaced with revised version Fri, 19 Sep 2025 17:45:21 GMT   (4023kb)

Title: Riemannian Batch Normalization: A Gyro Approach
Authors: Ziheng Chen and Xiao-Jun Wu and Bernhard Sch\"olkopf and Nicu Sebe
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2509.07115 ,  4023kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09744
replaced with revised version Fri, 19 Sep 2025 02:44:22 GMT   (4311kb)

Title: Structure Matters: Brain Graph Augmentation via Learnable Edge Masking
  for Data-efficient Psychiatric Diagnosis
Authors: Mujie Liu, Chenze Wang, Liping Chen, Nguyen Linh Dan Le, Niharika
  Tewari, Ting Dang, Jiangang Ma, and Feng Xia
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2509.09744 ,  4311kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09853
replaced with revised version Thu, 18 Sep 2025 19:31:55 GMT   (99kb)

Title: SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under
  Resource Constraints
Authors: Zhiyu Fan, Kirill Vasilevski, Dayi Lin, Boyuan Chen, Yihao Chen,
  Zhiqing Zhong, Jie M. Zhang, Pinjia He, Ahmed E. Hassan
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2509.09853 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11000
replaced with revised version Fri, 19 Sep 2025 16:19:28 GMT   (2371kb)

Title: Hardness, Structural Knowledge, and Opportunity: An Analytical Framework
  for Modular Performance Modeling
Authors: Omid Gheibi, Christian K\"astner, Pooyan Jamshidi
Categories: cs.SE cs.AI cs.LG
\\ ( https://arxiv.org/abs/2509.11000 ,  2371kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12845
replaced with revised version Fri, 19 Sep 2025 06:17:41 GMT   (555kb)

Title: Improving Anomalous Sound Detection with Attribute-aware Representation
  from Domain-adaptive Pre-training
Authors: Xin Fang, Guirui Zhong, Qing Wang, Fan Chu, Lei Wang, Mengui Qian,
  Mingqi Cai, Jiangzhao Wu, Jianqing Gao, and Jun Du
Categories: cs.SD cs.AI
Comments: Copyright 2026 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\ ( https://arxiv.org/abs/2509.12845 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14049
replaced with revised version Fri, 19 Sep 2025 10:37:07 GMT   (10213kb)

Title: Comprehensive Evaluation of CNN-Based Audio Tagging Models on
  Resource-Constrained Devices
Authors: Jordi Grau-Haro, Ruben Ribes-Serrano, Javier Naranjo-Alcazar, Marta
  Garcia-Ballesteros and Pedro Zuccarello
Categories: cs.SD cs.AI eess.AS
Comments: Accepted at Computing Conference 2026, London, UK
\\ ( https://arxiv.org/abs/2509.14049 ,  10213kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14172
replaced with revised version Fri, 19 Sep 2025 02:13:09 GMT   (1191kb)

Title: TGPO: Tree-Guided Preference Optimization for Robust Web Agent
  Reinforcement Learning
Authors: Ziyuan Chen, Zhenghui Zhao, Zhangye Han, Miancan Liu, Xianhang Ye,
  Yiqing Li, Hongbo Min, Jinkui Ren, Xiantao Zhang, Guitao Cao
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2509.14172 ,  1191kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14657
replaced with revised version Fri, 19 Sep 2025 09:59:54 GMT   (20494kb)

Title: Threat Modeling for Enhancing Security of IoT Audio Classification
  Devices under a Secure Protocols Framework
Authors: Sergio Benlloch-Lopez, Miquel Viel-Vazquez, Javier Naranjo-Alcazar,
  Jordi Grau-Haro and Pedro Zuccarello
Categories: cs.CR cs.AI
Comments: Accepted at Computing Conference 2026, London, UK
\\ ( https://arxiv.org/abs/2509.14657 ,  20494kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14858
replaced with revised version Fri, 19 Sep 2025 02:25:58 GMT   (353kb)

Title: MeanFlowSE: one-step generative speech enhancement via conditional mean
  flow
Authors: Duojia Li, Shenghui Lu, Hongchen Pan, Zongyi Zhan, Qingyang Hong, Lin
  Li
Categories: cs.SD cs.AI
\\ ( https://arxiv.org/abs/2509.14858 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14868
replaced with revised version Fri, 19 Sep 2025 02:06:33 GMT   (85kb)

Title: DPANet: Dual Pyramid Attention Network for Multivariate Time Series
  Forecasting
Authors: Qianyang Li, Xingjun Zhang, Shaoxun Wang, Jia Wei
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2509.14868 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15170
replaced with revised version Fri, 19 Sep 2025 01:16:33 GMT   (621kb)

Title: Watermarking and Anomaly Detection in Machine Learning Models for LORA
  RF Fingerprinting
Authors: Aarushi Mahajan, Wayne Burleson
Categories: cs.CR cs.AI eess.SP
Comments: IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)
\\ ( https://arxiv.org/abs/2509.15170 ,  621kb)
------------------------------------------------------------------------------
\\
arXiv:2412.11006
replaced with revised version Thu, 18 Sep 2025 22:47:30 GMT   (2326kb)

Title: Entropy-Regularized Process Reward Model
Authors: Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze
  Dong, Dylan Zhang, Pavlo Molchanov, Tong Zhang
Categories: cs.LG cs.CL
Comments: Upate TMLR version
\\ ( https://arxiv.org/abs/2412.11006 ,  2326kb)
------------------------------------------------------------------------------
\\
arXiv:2501.08454
replaced with revised version Fri, 19 Sep 2025 15:01:37 GMT   (287kb)

Title: Tag&Tab: Pretraining Data Detection in Large Language Models Using
  Keyword-Based Membership Inference Attack
Authors: Sagiv Antebi, Edan Habler, Asaf Shabtai, Yuval Elovici
Categories: cs.CR cs.CL
\\ ( https://arxiv.org/abs/2501.08454 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2504.09466
replaced with revised version Fri, 19 Sep 2025 03:09:14 GMT   (603kb)

Title: AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender
Authors: Weixiang Zhao, Jiahe Guo, Yulin Hu, Yang Deng, An Zhang, Xingyu Sui,
  Xinyang Han, Yanyan Zhao, Bing Qin, Tat-Seng Chua, Ting Liu
Categories: cs.CR cs.CL
Comments: 19 pages, 6 figures, 10 tables
\\ ( https://arxiv.org/abs/2504.09466 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2504.09723
replaced with revised version Fri, 19 Sep 2025 17:56:58 GMT   (2645kb)

Title: AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM
  Agents
Authors: Dakuo Wang, Ting-Yao Hsu, Yuxuan Lu, Hansu Gu, Limeng Cui, Yaochen
  Xie, William Headean, Bingsheng Yao, Akash Veeragouni, Jiapeng Liu, Sreyashi
  Nag, Jessie Wang
Categories: cs.HC cs.CL
\\ ( https://arxiv.org/abs/2504.09723 ,  2645kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17093 (*cross-listing*)
replaced with revised version Fri, 19 Sep 2025 07:26:59 GMT   (449kb)

Title: P2VA: Converting Persona Descriptions into Voice Attributes for Fair and
  Controllable Text-to-Speech
Authors: Yejin Lee, Jaehoon Kang, Kyuhong Shim
Categories: eess.AS cs.CL
\\ ( https://arxiv.org/abs/2505.17093 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2507.20474 (*cross-listing*)
replaced with revised version Fri, 19 Sep 2025 02:22:53 GMT   (505kb)

Title: MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and
  Adaptive Financial Trading
Authors: Siyi Wu, Junqiao Wang, Zhaoyang Guan, Leyi Zhao, Xinyuan Song, Xinyu
  Ying, Dexu Yu, Jinhao Wang, Hanlin Zhang, Michele Pak, Yangfan He, Yi Xin,
  Jianhui Wang, and Tianyu Shi
Categories: q-fin.TR cs.CL cs.LG
\\ ( https://arxiv.org/abs/2507.20474 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2508.10239
replaced with revised version Thu, 18 Sep 2025 20:54:38 GMT   (3380kb)

Title: Personalized Real-time Jargon Support for Online Meetings
Authors: Yifan Song, Wing Yee Au, Hon Yung Wong, Brian P. Bailey, Tal August
Categories: cs.HC cs.CL
\\ ( https://arxiv.org/abs/2508.10239 ,  3380kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15411
replaced with revised version Fri, 19 Sep 2025 11:28:06 GMT   (2036kb)

Title: Foundational Design Principles and Patterns for Building Robust and
  Adaptive GenAI-Native Systems
Authors: Frederik Vandeputte
Categories: cs.SE cs.CL cs.LG cs.MA
\\ ( https://arxiv.org/abs/2508.15411 ,  2036kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15157
replaced with revised version Fri, 19 Sep 2025 03:36:52 GMT   (134kb)

Title: Mind the Gap: Data Rewriting for Stable Off-Policy Supervised
  Fine-Tuning
Authors: Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong
  Qin
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2509.15157 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2501.01326
replaced with revised version Fri, 19 Sep 2025 07:08:56 GMT   (473kb)

Title: Domain-invariant feature learning in brain MR imaging for content-based
  image retrieval
Authors: Shuya Tobari, Shuhei Tomoshige, Hayato Muraki, Kenichi Oishi, and
  Hitoshi Iyatomi
Categories: cs.LG cs.CV cs.IR
Comments: 6 pages, 1 figures. Accepted at the SPIE Medical Imaging 2025
Journal-ref: Proceedings of the SPIE Medical Imaging, 16--20 February, 2025,
  San Diego, California, US
DOI: 10.1117/12.3046652
\\ ( https://arxiv.org/abs/2501.01326 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2504.11118
replaced with revised version Fri, 19 Sep 2025 05:42:20 GMT   (2011kb)

Title: Revealing Human Internal Attention Patterns from Gameplay Analysis for
  Reinforcement Learning
Authors: Henrik Krauss, Takehisa Yairi
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2504.11118 ,  2011kb)
------------------------------------------------------------------------------
\\
arXiv:2504.17865
replaced with revised version Thu, 18 Sep 2025 21:34:13 GMT   (3510kb)

Title: Set Phasers to Stun: Beaming Power and Control to Mobile Robots with
  Laser Light
Authors: Charles J. Carver, Hadleigh Schwartz, Toma Itagaki, Zachary Englhardt,
  Kechen Liu, Megan Graciela Nauli Manik, Chun-Cheng Chang, Vikram Iyer, Brian
  Plancher, and Xia Zhou
Categories: cs.RO cs.CV
Comments: 8 pages, 7 figures, accepted to IROS 2025
\\ ( https://arxiv.org/abs/2504.17865 ,  3510kb)
------------------------------------------------------------------------------
\\
arXiv:2505.06793 (*cross-listing*)
replaced with revised version Fri, 19 Sep 2025 09:55:38 GMT   (17364kb)

Title: HistDiST: Histopathological Diffusion-based Stain Transfer
Authors: Erik Gro{\ss}kopf, Valay Bundele, Mehran Hosseinzadeh, Hendrik P.A.
  Lensch
Categories: eess.IV cs.CV
Comments: Accepted to DAGM GCPR 2025
\\ ( https://arxiv.org/abs/2505.06793 ,  17364kb)
------------------------------------------------------------------------------
\\
arXiv:2505.24407 (*cross-listing*)
replaced with revised version Thu, 18 Sep 2025 19:46:36 GMT   (6737kb)

Title: Efficient RAW Image Deblurring with Adaptive Frequency Modulation
Authors: Wenlong Jiao, Binglong Li, Wei Shang, Ping Wang, Dongwei Ren
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2505.24407 ,  6737kb)
------------------------------------------------------------------------------
\\
arXiv:2508.10215 (*cross-listing*)
replaced with revised version Fri, 19 Sep 2025 09:25:37 GMT   (626kb)

Title: Data-Efficient Learning for Generalizable Surgical Video Understanding
Authors: Sahar Nasirihaghighi
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2508.10215 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12728 (*cross-listing*)
replaced with revised version Fri, 19 Sep 2025 04:28:20 GMT   (8084kb)

Title: Generalizable Holographic Reconstruction via Amplitude-Only Diffusion
  Priors
Authors: Jeongsol Kim, Chanseok Lee, Jongin You, Jong Chul Ye, and Mooseok Jang
Categories: physics.optics cs.CV cs.LG
Comments: Keywords: Diffusion model, phase retrieval, inline-holography,
  inverse problem
\\ ( https://arxiv.org/abs/2509.12728 ,  8084kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06787
replaced with revised version Fri, 19 Sep 2025 08:04:17 GMT   (160kb)

Title: ForestColl: Throughput-Optimal Collective Communications on
  Heterogeneous Network Fabrics
Authors: Liangyu Zhao and Saeed Maleki and Yuanhong Wang and Zezhou Wang and
  Ziyue Yang and Hossein Pourreza and Arvind Krishnamurthy
Categories: cs.NI cs.DC cs.LG
Comments: arXiv admin note: text overlap with arXiv:2305.18461
\\ ( https://arxiv.org/abs/2402.06787 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2405.17932
replaced with revised version Fri, 19 Sep 2025 02:48:34 GMT   (1872kb)

Title: Towards Communication-efficient Federated Learning via Sparse and
  Aligned Adaptive Optimization
Authors: Xiumei Deng, Jun Li, Kang Wei, Long Shi, Zehui Xiong, Ming Ding, Wen
  Chen, Shi Jin, and H. Vincent Poor
Categories: cs.LG cs.DC
DOI: 10.1109/TSP.2025.3608715
\\ ( https://arxiv.org/abs/2405.17932 ,  1872kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13933
replaced with revised version Fri, 19 Sep 2025 05:24:50 GMT   (171kb)

Title: Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless
  Federated Learning
Authors: Qiyue Li, Yingxin Liu, Hang Qi, Jieping Luo, Zhizhang Liu, Jingjin Wu
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2509.13933 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14453
replaced with revised version Fri, 19 Sep 2025 01:12:28 GMT   (1156kb)

Title: Online Learning of Deceptive Policies under Intermittent Observation
Authors: Gokul Puthumanaillam, Ram Padmanabhan, Jose Fuentes, Nicole Cruz,
  Paulo Padrao, Ruben Hernandez, Hao Jiang, William Schafer, Leonardo
  Bobadilla, Melkior Ornik
Categories: cs.RO cs.MA cs.SY eess.SY
\\ ( https://arxiv.org/abs/2509.14453 ,  1156kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
