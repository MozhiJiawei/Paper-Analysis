Gmail	李嘉维 <qetwe0000@gmail.com>
cs daily Subj-class mailing 8004a1 1
send mail ONLY to cs <no-reply@arxiv.org>	2025年9月24日 12:06
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Computer Vision and Pattern Recognition
Distributed, Parallel, and Cluster Computing
Multiagent Systems
 received from  Mon 22 Sep 25 18:00:00 GMT  to  Tue 23 Sep 25 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2509.18101
Date: Sat, 30 Aug 2025 06:01:53 GMT   (1059kb)

Title: A Cost-Benefit Analysis of On-Premise Large Language Model Deployment:
  Breaking Even with Commercial LLM Services
Authors: Guanzhong Pan, Haibo Wang
Categories: cs.AI cs.LG
\\
  Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.
\\ ( https://arxiv.org/abs/2509.18101 ,  1059kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18123
Date: Wed, 10 Sep 2025 17:41:03 GMT   (628kb)

Title: SPADE: A Large Language Model Framework for Soil Moisture Pattern
  Recognition and Anomaly Detection in Precision Agriculture
Authors: Yeonju Lee, Rui Qi Chen, Joseph Oboamah, Po Nien Su, Wei-zhen Liang,
  Yeyin Shi, Lu Gan, Yongsheng Chen, Xin Qiao, Jing Li
Categories: cs.AI cs.LG
\\
  Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.
\\ ( https://arxiv.org/abs/2509.18123 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18132
Date: Sun, 14 Sep 2025 02:41:26 GMT   (98kb)

Title: Position Paper: Integrating Explainability and Uncertainty Estimation in
  Medical AI
Authors: Xiuyi Fan
Categories: cs.AI
Comments: Accepted at the International Joint Conference on Neural Networks,
  IJCNN 2025
\\
  Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.
\\ ( https://arxiv.org/abs/2509.18132 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18168
Date: Wed, 17 Sep 2025 10:11:02 GMT   (2318kb)

Title: HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics
Authors: Dong Liu, Yanxuan Yu
Categories: cs.AI
\\
  Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.
\\ ( https://arxiv.org/abs/2509.18168 ,  2318kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18178
Date: Wed, 17 Sep 2025 23:44:18 GMT   (18794kb)

Title: Foam-Agent: An End-to-End Composable Multi-Agent Framework for
  Automating CFD Simulation in OpenFOAM
Authors: Ling Yue, Nithin Somasekharan, Tingwen Zhang, Yadi Cao, Shaowu Pan
Categories: cs.AI cs.CE cs.LG
\\
  Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.
\\ ( https://arxiv.org/abs/2509.18178 ,  18794kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18180
Date: Thu, 18 Sep 2025 01:52:19 GMT   (8588kb)

Title: Large Language Models and Operations Research: A Structured Survey
Authors: Yang Wang, Kai Li
Categories: cs.AI
\\
  Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.
\\ ( https://arxiv.org/abs/2509.18180 ,  8588kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18181
Date: Thu, 18 Sep 2025 01:52:27 GMT   (337kb)

Title: Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral
  Theory-Guided LLMs for Ridesourcing Mode Choice Modeling
Authors: Mustafa Sameen, Xiaojian Zhang, Xilei Zhao
Categories: cs.AI cs.LG
\\
  Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.
\\ ( https://arxiv.org/abs/2509.18181 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18186
Date: Thu, 18 Sep 2025 18:18:03 GMT   (988kb)

Title: An Outcome-Based Educational Recommender System
Authors: Nursultan Askarbekuly, Timur Fayzrakhmanov, Sladjan Babarogi\'c, Ivan
  Lukovi\'c
Categories: cs.AI
\\
  Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.
\\ ( https://arxiv.org/abs/2509.18186 ,  988kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18198
Date: Fri, 19 Sep 2025 23:38:18 GMT   (4180kb)

Title: MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy
  with Knowledge Distillation
Authors: Rui Liu, Zikang Wang, Peng Gao, Yu Shen, Pratap Tokekar, Ming Lin
Categories: cs.AI cs.MA cs.RO
\\
  Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.
\\ ( https://arxiv.org/abs/2509.18198 ,  4180kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18215
Date: Sun, 21 Sep 2025 20:26:47 GMT   (131kb)

Title: Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and
  Counterfactual Explanations
Authors: Timotheus Kampik, Kristijonas \v{C}yras, Jos\'e Ruiz Alarc\'on
Categories: cs.AI cs.LO cs.MA
Comments: The publisher's version contains a notation glitch in Example 3, 5th
  line, first sub-script G should be G'. This has always been G' in authors'
  version. Thanks to J. Lanser for pointing this out
Journal-ref: International Journal of Approximate Reasoning 164 (2024) 109066
DOI: 10.1016/J.IJAR.2023.109066
\\
  This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.
\\ ( https://arxiv.org/abs/2509.18215 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18216
Date: Sun, 21 Sep 2025 20:28:18 GMT   (31475kb)

Title: nDNA -- the Semantic Helix of Artificial Cognition
Authors: Amitava Das
Categories: cs.AI cs.LG
\\
  As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.
\\ ( https://arxiv.org/abs/2509.18216 ,  31475kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18218
Date: Sun, 21 Sep 2025 22:34:00 GMT   (31kb)

Title: Similarity Field Theory: A Mathematical Framework for Intelligence
Authors: Kei-Sing Ng
Categories: cs.AI
\\
  We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.
\\ ( https://arxiv.org/abs/2509.18218 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18221
Date: Mon, 22 Sep 2025 05:26:59 GMT   (502kb)

Title: Multimodal Health Risk Prediction System for Chronic Diseases via
  Vision-Language Fusion and Large Language Models
Authors: Dingxin Lu, Shurui Wu, Xinyi Huang
Categories: cs.AI cs.LG
\\
  With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.
\\ ( https://arxiv.org/abs/2509.18221 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18226
Date: Mon, 22 Sep 2025 11:35:47 GMT   (749kb)

Title: From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration
  for Ambiguous User Intent in Recipe Recommendation
Authors: Yu Fu, Linyue Cai, Ruoyu Wu, Yong Zhao
Categories: cs.AI
Comments: 5 pages, 3 figures, submitted to icassp 2026
\\
  Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.
\\ ( https://arxiv.org/abs/2509.18226 ,  749kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18229
Date: Mon, 22 Sep 2025 12:21:58 GMT   (2023kb)

Title: An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering
  Analysis Problems
Authors: Anthony Patera and Rohan Abeyaratne
Categories: cs.AI
MSC-class: 70, 74, 76, 80
\\
  Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.
\\ ( https://arxiv.org/abs/2509.18229 ,  2023kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18230
Date: Mon, 22 Sep 2025 13:14:47 GMT   (597kb)

Title: Towards General Computer Control with Hierarchical Agents and
  Multi-Level Action Spaces
Authors: Zihan Dong, Xinyu Fan, Zixiang Tang, Yunqing Li
Categories: cs.AI cs.LG
\\
  Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.
\\ ( https://arxiv.org/abs/2509.18230 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18234
Date: Mon, 22 Sep 2025 17:48:05 GMT   (17963kb)

Title: The Illusion of Readiness: Stress Testing Large Frontier Models on
  Multimodal Medical Benchmarks
Authors: Yu Gu, Jingjing Fu, Xiaodong Liu, Jeya Maria Jose Valanarasu, Noel
  Codella, Reuben Tan, Qianchu Liu, Ying Jin, Sheng Zhang, Jinyu Wang, Rui
  Wang, Lei Song, Guanghui Qin, Naoto Usuyama, Cliff Wong, Cheng Hao, Hohin
  Lee, Praneeth Sanapathi, Sarah Hilado, Bian Jiang, Javier Alvarez-Valle, Mu
  Wei, Jianfeng Gao, Eric Horvitz, Matt Lungren, Hoifung Poon, Paul Vozila
Categories: cs.AI cs.CL cs.LG
Comments: 35 pages
\\
  Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.
\\ ( https://arxiv.org/abs/2509.18234 ,  17963kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18382
Date: Mon, 22 Sep 2025 20:09:37 GMT   (337kb)

Title: Evaluating the Safety and Skill Reasoning of Large Reasoning Models
  Under Compute Constraints
Authors: Adarsha Balaji and Le Chen and Rajeev Thakur and Franck Cappello and
  Sandeep Madireddy
Categories: cs.AI
\\
  Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.
\\ ( https://arxiv.org/abs/2509.18382 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18383
Date: Mon, 22 Sep 2025 20:11:40 GMT   (31kb)

Title: G\"odel Test: Can Large Language Models Solve Easy Conjectures?
Authors: Moran Feldman, Amin Karbasi
Categories: cs.AI cs.DM cs.LG
\\
  Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.
\\ ( https://arxiv.org/abs/2509.18383 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18400
Date: Mon, 22 Sep 2025 20:32:24 GMT   (46kb)

Title: ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized
  Tariff Code Classification
Authors: Pritish Yuvraj, Siva Devarakonda
Categories: cs.AI
Journal-ref: Paper in Review For ICLR 2026 (Workshop)
\\
  Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.
\\ ( https://arxiv.org/abs/2509.18400 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18420
Date: Mon, 22 Sep 2025 21:04:39 GMT   (100kb)

Title: Instruction-Following Evaluation in Function Calling for Large Language
  Models
Authors: Nikolai Skripko
Categories: cs.AI
\\
  Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.
\\ ( https://arxiv.org/abs/2509.18420 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18436
Date: Mon, 22 Sep 2025 21:41:35 GMT   (3429kb)

Title: Memory-QA: Answering Recall Questions Based on Multimodal Memories
Authors: Hongda Jiang, Xinyuan Zhang, Siddhant Garg, Rishab Arora, Shiun-Zu
  Kuo, Jiayang Xu, Christopher Brossman, Yue Liu, Aaron Colak, Ahmed Aly, Anuj
  Kumar, Xin Luna Dong
Categories: cs.AI cs.CL cs.DB
\\
  We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).
\\ ( https://arxiv.org/abs/2509.18436 ,  3429kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18527
Date: Tue, 23 Sep 2025 01:47:44 GMT   (6400kb)

Title: FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move
  Recognition and Rule Reasoning
Authors: Ziwen Chen and Zhong Wang
Categories: cs.AI
\\
  The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.
\\ ( https://arxiv.org/abs/2509.18527 ,  6400kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18557
Date: Tue, 23 Sep 2025 02:30:14 GMT   (406kb)

Title: LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs
Authors: Tom Pawelek, Raj Patel, Charlotte Crowell, Noorbakhsh Amiri, Sudip
  Mittal, Shahram Rahimi, Andy Perkins
Categories: cs.AI
Comments: 7 pages, 5 figures, to be published and presented at ICMLA 2025
\\
  Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.
\\ ( https://arxiv.org/abs/2509.18557 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18565
Date: Tue, 23 Sep 2025 02:41:39 GMT   (412kb)

Title: Solving Math Word Problems Using Estimation Verification and Equation
  Generation
Authors: Mitchell Piehl, Dillon Wilson, Ananya Kalita, Jugal Kalita
Categories: cs.AI
Comments: Accepted to IEEE ICMLA 2025
\\
  Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.
\\ ( https://arxiv.org/abs/2509.18565 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18633
Date: Tue, 23 Sep 2025 04:33:58 GMT   (320kb)

Title: Adaptive Learning in Spatial Agent-Based Models for Climate Risk
  Assessment: A Geospatial Framework with Evolutionary Economic Agents
Authors: Yara Mohajerani
Categories: cs.AI q-fin.RM
Comments: Submitted and accepted to Tackling Climate Change with Machine
  Learning workshop at NeurIPS 2025. 5 pages, 1 figure. Source code and
  documentation available at
  https://github.com/yaramohajerani/spatial-climate-ABM
\\
  Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.
\\ ( https://arxiv.org/abs/2509.18633 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18667
Date: Tue, 23 Sep 2025 05:34:34 GMT   (311kb)

Title: TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation
Authors: Qiao Xiao, Hong Ting Tsang and Jiaxin Bai
Categories: cs.AI
Comments: 16 pages, 2 figures, 4 tables. Submitted to the 2026 18th
  International Conference on Machine Learning and Computing (ICMLC 2026),
  under review
\\
  Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.
\\ ( https://arxiv.org/abs/2509.18667 ,  311kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18681
Date: Tue, 23 Sep 2025 06:01:52 GMT   (248kb,D)

Title: Implementation of airborne ML models with semantics preservation
Authors: Nicolas Valot, Louis Fabre, Benjamin Lesage, Ammar Mechouche, Claire
  Pagetti
Categories: cs.AI
Journal-ref: 44th Digital Avionics Systems Conference (DASC), Sep 2025,
  Montreal, Canada
\\
  Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.
\\ ( https://arxiv.org/abs/2509.18681 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18690
Date: Tue, 23 Sep 2025 06:16:39 GMT   (765kb)

Title: Advances in Large Language Models for Medicine
Authors: Zhiyu Kan, Wensheng Gan, Zhenlian Qi, Philip S. Yu
Categories: cs.AI
Comments: Preprint. 5 figures, 4 tables
\\
  Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.
\\ ( https://arxiv.org/abs/2509.18690 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18710
Date: Tue, 23 Sep 2025 06:46:41 GMT   (934kb)

Title: Autonomous Data Agents: A New Opportunity for Smart Data
Authors: Yanjie Fu, Dongjie Wang, Wangyang Ying, Xiangliang Zhang, Huan Liu,
  Jian Pei
Categories: cs.AI
\\
  As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.
\\ ( https://arxiv.org/abs/2509.18710 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18771
Date: Tue, 23 Sep 2025 08:04:58 GMT   (3384kb)

Title: Experience Scaling: Post-Deployment Evolution For Large Language Models
Authors: Xingkun Yin, Kaibin Huang, Dong In Kim, Hongyang Du
Categories: cs.AI
\\
  Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.
\\ ( https://arxiv.org/abs/2509.18771 ,  3384kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18787
Date: Tue, 23 Sep 2025 08:25:33 GMT   (25kb)

Title: The AGNTCY Agent Directory Service: Architecture and Implementation
Authors: Luca Muscariello and Vijoy Pandey and Ramiz Polic
Categories: cs.AI
ACM-class: C.2.4
\\
  The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.
\\ ( https://arxiv.org/abs/2509.18787 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18836
Date: Tue, 23 Sep 2025 09:19:37 GMT   (96kb)

Title: Bounded PCTL Model Checking of Large Language Model Outputs
Authors: Dennis Gross, Helge Spieker, Arnaud Gotlieb
Categories: cs.AI
Comments: ICTAI 2025
\\
  In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.
\\ ( https://arxiv.org/abs/2509.18836 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18846
Date: Tue, 23 Sep 2025 09:35:05 GMT   (872kb)

Title: Model selection meets clinical semantics: Optimizing ICD-10-CM
  prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and
  section-aware fine-tuning
Authors: Hong-Jie Dai, Zheng-Hao Li, An-Tai Lu, Bo-Tsz Shain, Ming-Ta Li,
  Tatheer Hussain Mir, Kuang-Te Wang, Min-I Su, Pei-Kang Liu, Ming-Ju Tsai
Categories: cs.AI
Comments: 28 Pages, 4 Figures, 2 Tables
ACM-class: I.2.6; I.2.7; J.3
\\
  Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.
\\ ( https://arxiv.org/abs/2509.18846 ,  872kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18849
Date: Tue, 23 Sep 2025 09:37:16 GMT   (4106kb)

Title: MAPO: Mixed Advantage Policy Optimization
Authors: Wenke Huang and Quan Zhang and Yiyang Fang and Jian Liang and Xuankun
  Rong and Huanjin Yao and Guancheng Wan and Ke Liang and Wenwen He and Mingjun
  Li and Leszek Rutkowski and Mang Ye and Bo Du and Dacheng Tao
Categories: cs.AI
\\
  Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2509.18849 ,  4106kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18864
Date: Tue, 23 Sep 2025 09:58:37 GMT   (6464kb)

Title: Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User
  Profiling
Authors: Yingxin Li, Jianbo Zhao, Xueyu Ren, Jie Tang, Wangjie You, Xu Chen,
  Kan Zhou, Chao Feng, Jiao Ran, Yuan Meng, Zhi Wang
Categories: cs.AI
\\
  User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.
\\ ( https://arxiv.org/abs/2509.18864 ,  6464kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18868
Date: Tue, 23 Sep 2025 10:06:58 GMT   (229kb)

Title: Memory in Large Language Models: Mechanisms, Evaluation and Evolution
Authors: Dianxing Zhang, Wendong Li, Kani Song, Jiaye Lu, Gang Li, Liuchun Yang
  and Sheng Li
Categories: cs.AI
Comments: 50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM
  memory mechanisms and evaluation
\\
  Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.
\\ ( https://arxiv.org/abs/2509.18868 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18883
Date: Tue, 23 Sep 2025 10:25:48 GMT   (2631kb)

Title: LongCat-Flash-Thinking Technical Report
Authors: Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou,
  Borun Chen, Chao Zhang, Chao Zhang, Chengcheng Han, Chenhui Yang, Chi Zhang,
  Chong Peng, Chuyu Zhang, Cong Chen, Fengcun Li, Gang Xu, Guoyuan Lin, Hao
  Jiang, Hao Liang, Haomin Fu, Haoxiang Ma, Hong Liu, Hongyan Hao, Hongyin
  Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiahao Liu, Jiahuan Li, Jialin Liu,
  Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiaqi Sun, Jiaqi Zhang, Jiarong Shi,
  Jiawei Yang, Jingang Wang, Jinrui Ding, Jun Kuang, Jun Xu, Ke He, Kefeng
  Zhang, Keheng Wang, Keqing He, Li Wei, Liang Shi, Lin Qiu, Lingbin Kong,
  Lingchuan Liu, Linsen Guo, Longfei An, Mai Xia, Meng Zhou, Mengshen Zhu, Peng
  Pei, Pengcheng Jia, Qi Gu, Qi Guo, Qiong Huang, Quan Chen, Quanchi Weng,
  Rongxiang Weng, Ruichen Shao, Rumei Li, Shanglin Lei, Shuai Du, et al. (60
  additional authors not shown)
Categories: cs.AI
\\
  We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.
\\ ( https://arxiv.org/abs/2509.18883 ,  2631kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18905
Date: Tue, 23 Sep 2025 12:00:14 GMT   (15565kb)

Title: How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven
  Perspective
Authors: Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei
  Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang, Zhedong Zheng, Zhipeng
  Zhang, Yifan Wang, Lin Song, Lijun Wang, Yanwei Li, Ying Shan, Huchuan Lu
Categories: cs.AI
Comments: a comprehensive visual spatial reasoning evaluation tool, 25 pages,
  16 figures
\\
  Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.
\\ ( https://arxiv.org/abs/2509.18905 ,  15565kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18942
Date: Tue, 23 Sep 2025 12:55:57 GMT   (185kb)

Title: Data Efficient Adaptation in Large Language Models via Continuous
  Low-Rank Fine-Tuning
Authors: Xiao Han, Zimo Zhao, Wanyu Wang, Maolin Wang, Zitao Liu, Yi Chang,
  Xiangyu Zhao
Categories: cs.AI
\\
  Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.
\\ ( https://arxiv.org/abs/2509.18942 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18970
Date: Tue, 23 Sep 2025 13:24:48 GMT   (1595kb)

Title: LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy,
  Methods, and Directions
Authors: Xixun Lin, Yucheng Ning, Jingwen Zhang, Yan Dong, Yilong Liu, Yongxuan
  Wu, Xiaohua Qi, Nan Sun, Yanmin Shang, Pengfei Cao, Lixin Zou, Xu Chen, Chuan
  Zhou, Jia Wu, Shirui Pan, Bin Wang, Yanan Cao, Kai Chen, Songlin Hu, Li Guo
Categories: cs.AI
\\
  Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.
\\ ( https://arxiv.org/abs/2509.18970 ,  1595kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18980
Date: Tue, 23 Sep 2025 13:30:03 GMT   (93kb)

Title: From latent factors to language: a user study on LLM-generated
  explanations for an inherently interpretable matrix-based recommender system
Authors: Maxime Manderlier, Fabian Lecron, Olivier Vu Thanh and Nicolas Gillis
Categories: cs.AI cs.HC cs.IR
ACM-class: H.3.3; H.5.2; I.2.7
Journal-ref: In Proceedings of the 12th Joint Workshop on Interfaces and Human
  Decision Making for Recommender Systems (IntRS 2025) co-located with 19th ACM
  Conference on Recommender Systems (RecSys 2025)
\\
  We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.
\\ ( https://arxiv.org/abs/2509.18980 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18986
Date: Tue, 23 Sep 2025 13:37:09 GMT   (493kb)

Title: Remaining Time Prediction in Outbound Warehouse Processes: A Case Study
  (Short Paper)
Authors: Erik Penther, Michael Grohs, Jana-Rebecca Rehse
Categories: cs.AI
Comments: Short paper at the ML4PM Workshop 2025, held in conjunction with the
  ICPM 2025 in Montevideo, Uruguay
\\
  Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.
\\ ( https://arxiv.org/abs/2509.18986 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19030
Date: Tue, 23 Sep 2025 14:03:54 GMT   (1305kb)

Title: Landmarks, Monuments, and Beacons: Understanding Generative Calls to
  Action
Authors: Victoire Herv\'e, Henrik Warpefelt, Christoph Salge
Categories: cs.AI
\\
  Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation
\\ ( https://arxiv.org/abs/2509.19030 ,  1305kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19058
Date: Tue, 23 Sep 2025 14:22:39 GMT   (573kb)

Title: Towards Causal Representation Learning with Observable Sources as
  Auxiliaries
Authors: Kwonho Kim, Heejeong Nam, Inwoo Hwang, Sanghack Lee
Categories: cs.AI
\\
  Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.
\\ ( https://arxiv.org/abs/2509.19058 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19077
Date: Tue, 23 Sep 2025 14:36:12 GMT   (706kb)

Title: Code Driven Planning with Domain-Adaptive Critic
Authors: Zikang Tian, Shaohui Peng, Du Huang, Jiaming Guo, Ruizhi Chen, Rui
  Zhang, Xishan Zhang, Yuxuan Guo, Zidong Du, Qi Guo, Ling Li, Yewen Pu, Xing
  Hu, Yunji Chen
Categories: cs.AI
\\
  Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.
\\ ( https://arxiv.org/abs/2509.19077 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19236
Date: Tue, 23 Sep 2025 16:58:54 GMT   (1812kb)

Title: AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and
  Expertise Orchestration for Effective and Efficient Collaboration
Authors: Chunhao Tian, Yutong Wang, Xuebo Liu, Zhexuan Wang, Liang Ding, Miao
  Zhang, Min Zhang
Categories: cs.AI
Comments: EMNLP 2025 Findings
\\
  Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.
\\ ( https://arxiv.org/abs/2509.19236 ,  1812kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19265
Date: Tue, 23 Sep 2025 17:24:14 GMT   (4930kb)

Title: Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from
  the Arab World
Authors: Saeed Almheiri, Rania Hossam, Mena Attia, Chenxi Wang, Preslav Nakov,
  Timothy Baldwin, Fajri Koto
Categories: cs.AI cs.CL
Comments: EMNLP 2025 - Findings
\\
  Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.
\\ ( https://arxiv.org/abs/2509.19265 ,  4930kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18113
Date: Tue, 9 Sep 2025 23:42:16 GMT   (995kb)

Title: Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs
Authors: Xin Hu, Yue Kang, Guanzi Yao, Tianze Kang, Mengjie Wang, Heyao Liu
Categories: cs.CL cs.LG
\\
  This study addresses the generalization limitations commonly observed in
large language models under multi-task and cross-domain settings. Unlike prior
methods such as SPoT, which depends on fixed prompt templates, our study
introduces a unified multi-task learning framework with dynamic prompt
scheduling mechanism. By introducing a prompt pool and a task-aware scheduling
strategy, the method dynamically combines and aligns prompts for different
tasks. This enhances the model's ability to capture semantic differences across
tasks. During prompt fusion, the model uses task embeddings and a gating
mechanism to finely control the prompt signals. This ensures alignment between
prompt content and task-specific demands. At the same time, it builds flexible
sharing pathways across tasks. In addition, the proposed optimization objective
centers on joint multi-task learning. It incorporates an automatic learning
strategy for scheduling weights, which effectively mitigates task interference
and negative transfer. To evaluate the effectiveness of the method, a series of
sensitivity experiments were conducted. These experiments examined the impact
of prompt temperature parameters and task number variation. The results confirm
the advantages of the proposed mechanism in maintaining model stability and
enhancing transferability. Experimental findings show that the prompt
scheduling method significantly improves performance on a range of language
understanding and knowledge reasoning tasks. These results fully demonstrate
its applicability and effectiveness in unified multi-task modeling and
cross-domain adaptation.
\\ ( https://arxiv.org/abs/2509.18113 ,  995kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18122
Date: Wed, 10 Sep 2025 17:35:38 GMT   (597kb)

Title: GAUSS: Benchmarking Structured Mathematical Skills for Large Language
  Models
Authors: Yue Zhang, Jiaxin Zhang, Qiuyu Ren, Tahsin Saffat, Xiaoxuan Liu,
  Zitong Yang, Banghua Zhu, Yi Ma
Categories: cs.CL
Comments: 120 pages (including appendix)
\\
  We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.
\\ ( https://arxiv.org/abs/2509.18122 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18156
Date: Tue, 16 Sep 2025 22:05:52 GMT   (7210kb)

Title: Event Causality Identification with Synthetic Control
Authors: Haoyu Wang and Fengze Liu and Jiayao Zhang and Dan Roth and Kyle
  Richardson
Categories: cs.CL cs.AI
Journal-ref: EMNLP 2024
DOI: 10.18653/v1/2024.emnlp-main.103
\\
  Event causality identification (ECI), a process that extracts causal
relations between events from text, is crucial for distinguishing causation
from correlation. Traditional approaches to ECI have primarily utilized
linguistic patterns and multi-hop relational inference, risking false causality
identification due to informal usage of causality and specious graphical
inference. In this paper, we adopt the Rubin Causal Model to identify event
causality: given two temporally ordered events, we see the first event as the
treatment and the second one as the observed outcome. Determining their
causality involves manipulating the treatment and estimating the resultant
change in the likelihood of the outcome. Given that it is only possible to
implement manipulation conceptually in the text domain, as a work-around, we
try to find a twin for the protagonist from existing corpora. This twin should
have identical life experiences with the protagonist before the treatment but
undergoes an intervention of treatment. However, the practical difficulty of
locating such a match limits its feasibility. Addressing this issue, we use the
synthetic control method to generate such a twin' from relevant historical
data, leveraging text embedding synthesis and inversion techniques. This
approach allows us to identify causal relations more robustly than previous
methods, including GPT-4, which is demonstrated on a causality benchmark,
COPES-hard.
\\ ( https://arxiv.org/abs/2509.18156 ,  7210kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18158
Date: Wed, 17 Sep 2025 01:47:29 GMT   (882kb)

Title: ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero
  Instructions to Structured Prompts via Principle-based Optimization
Authors: Seungyoun Yi, Minsoo Khang, Sungrae Park
Categories: cs.CL cs.LG
Comments: 9 pages, 4 figures. To appear in EMNLP 2025 Main Conference (Oral
  Presentation)
ACM-class: I.2.7
\\
  Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.
\\ ( https://arxiv.org/abs/2509.18158 ,  882kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18163
Date: Wed, 17 Sep 2025 06:45:21 GMT   (165kb)

Title: Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning
Authors: Haodong Zhao, Chenyan Zhao, Yansi Li, Zhuosheng Zhang, Gongshen Liu
Categories: cs.CL
Comments: Work in progress
\\
  The capacity of Large Language Models (LLMs) to reason is fundamental to
their application in complex, knowledge-intensive domains. In real-world
scenarios, LLMs are often augmented with external information that can be
helpful, irrelevant, or even misleading. This paper investigates the causal
impact of such auxiliary information on the reasoning process of LLMs with
explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset
derived from ScienceQA, to systematically test the robustness of the model
against these types of information. Our findings reveal a critical
vulnerability: the model's deliberative "thinking mode" is a double-edged
sword. While helpful context improves accuracy, misleading information causes a
catastrophic drop in performance, which is amplified by the thinking process.
Instead of conferring robustness, thinking reinforces the degree of error when
provided with misinformation. This highlights that the challenge is not merely
to make models "think", but to endow them with the critical faculty to evaluate
the information upon which their reasoning is based. The SciAux dataset is
available at https://huggingface.co/datasets/billhdzhao/SciAux.
\\ ( https://arxiv.org/abs/2509.18163 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18167
Date: Wed, 17 Sep 2025 09:09:28 GMT   (1048kb)

Title: SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised
  Multi-Agent Framework
Authors: Junlin Wang, Zehao Wu, Shaowei Lu, Yanlan Li, Xinghao Huang
Categories: cs.CL
Comments: 5 pages,2 figures, IRAC under review
\\
  Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access external knowledge sources, but the effectiveness of RAG relies on the
coordination between the retriever and the generator. Since these components
are developed independently, their interaction is often suboptimal: the
retriever may return irrelevant or redundant documents, while the generator may
fail to fully leverage retrieved evidence. In this work, we propose a
process-supervised multi-agent framework to bridge the gap between retriever
and generator. The framework introduces two lightweight agents: a Decision
Maker, which determines when to continue retrieval or stop for answer
generation, and a Knowledge Selector, which filters retrieved documents to
retain only the most useful evidence. To provide fine-grained supervision, we
employ an LLM-as-a-Judge that evaluates each intermediate action with
process-level rewards, ensuring more accurate credit assignment than relying
solely on final answer correctness. We further adopt a tree-structured rollout
strategy to explore diverse reasoning paths, and train both agents with
Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on
single-hop and multi-hop question answering benchmarks show that our approach
achieves higher accuracy, more stable convergence, and produces more
interpretable reasoning trajectories compared with standard RAG baselines.
Importantly, the proposed framework is modular and plug-and-play, requiring no
modification to the retriever or generator, making it practical for real-world
RAG applications.
\\ ( https://arxiv.org/abs/2509.18167 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18175
Date: Wed, 17 Sep 2025 16:15:49 GMT   (2503kb)

Title: ERFC: Happy Customers with Emotion Recognition and Forecasting in
  Conversation in Call Centers
Authors: Aditi Debsharma, Bhushan Jagyasi, Surajit Sen, Priyanka Pandey,
  Devicharith Dovari, Yuvaraj V.C, Rosalin Parida, Gopali Contractor
Categories: cs.CL
Comments: 7 pages, 6 Figures, 4 Tables, 18 References
\\
  Emotion Recognition in Conversation has been seen to be widely applicable in
call center analytics, opinion mining, finance, retail, healthcare, and other
industries. In a call center scenario, the role of the call center agent is not
just confined to receiving calls but to also provide good customer experience
by pacifying the frustration or anger of the customers. This can be achieved by
maintaining neutral and positive emotion from the agent. As in any
conversation, the emotion of one speaker is usually dependent on the emotion of
other speaker. Hence the positive emotion of an agent, accompanied with the
right resolution will help in enhancing customer experience. This can change an
unhappy customer to a happy one. Imparting the right resolution at right time
becomes easier if the agent has the insight of the emotion of future
utterances. To predict the emotions of the future utterances we propose a novel
architecture, Emotion Recognition and Forecasting in Conversation. Our proposed
ERFC architecture considers multi modalities, different attributes of emotion,
context and the interdependencies of the utterances of the speakers in the
conversation. Our intensive experiments on the IEMOCAP dataset have shown the
feasibility of the proposed ERFC. This approach can provide a tremendous
business value for the applications like call center, where the happiness of
customer is utmost important.
\\ ( https://arxiv.org/abs/2509.18175 ,  2503kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18293
Date: Mon, 22 Sep 2025 18:23:21 GMT   (33869kb)

Title: Evaluating Large Language Models for Detecting Antisemitism
Authors: Jay Patel, Hrudayangam Mehta, Jeremy Blackburn
Categories: cs.CL cs.AI cs.CY
Comments: Accepted to EMNLP 2025 Main Conference
\\
  Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.
\\ ( https://arxiv.org/abs/2509.18293 ,  33869kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18314
Date: Mon, 22 Sep 2025 18:37:24 GMT   (1942kb)

Title: Exploiting Tree Structure for Credit Assignment in RL Training of LLMs
Authors: Hieu Tran, Zonghai Yao, Hong Yu
Categories: cs.CL
Comments: 15 pages
\\
  Reinforcement learning improves LLM reasoning, yet sparse delayed reward over
long sequences makes token-level credit assignment the key bottleneck. We study
the verifiable-reward setting, where the final answer is checkable and multiple
responses can be drawn per prompt. Reasoning tasks in math and medical QA align
with this setup, where only a few decision tokens significantly impact the
outcome. PPO offers token-level advantages with a learned value model, but it
is complex to train both the actor and critic models simultaneously, and it is
not easily generalizable, as the token-level values from the critic model can
make training prone to overfitting. GRPO is critic-free and supports verifiable
rewards, but spreads a single sequence-level return across tokens and ignores
branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that
converts a group of responses into a prefix tree and computes
\emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes.
Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated
\textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a
critic-free algorithm that augments the group-relative outcome signal of GRPO
with \emph{branch-gated} temporal-difference corrections derived from the tree.
At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO
reduces to GRPO; at branching tokens, it supplies precise token-level credit
without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,
TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and
out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and
reaches higher validation accuracy with roughly the same wall-clock time.
\\ ( https://arxiv.org/abs/2509.18314 ,  1942kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18316
Date: Mon, 22 Sep 2025 18:39:09 GMT   (45kb)

Title: Brittleness and Promise: Knowledge Graph Based Reward Modeling for
  Diagnostic Reasoning
Authors: Saksham Khatwani, He Cheng, Majid Afshar, Dmitriy Dligach, Yanjun Gao
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.
\\ ( https://arxiv.org/abs/2509.18316 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18344
Date: Mon, 22 Sep 2025 19:08:57 GMT   (502kb)

Title: Speculate Deep and Accurate: Lossless and Training-Free Acceleration for
  Offloaded LLMs via Substitute Speculative Decoding
Authors: Pei-Shuo Wang, Jian-Jia Chen, Chun-Che Yang, Chi-Chih Chang, Ning-Chi
  Huang, Mohamed S. Abdelfattah, Kai-Chiang Wu
Categories: cs.CL
Comments: Accepted by NeurIPS 2025
\\
  The immense model sizes of large language models (LLMs) challenge deployment
on memory-limited consumer GPUs. Although model compression and parameter
offloading are common strategies to address memory limitations, compression can
degrade quality, and offloading maintains quality but suffers from slow
inference. Speculative decoding presents a promising avenue to accelerate
parameter offloading, utilizing a fast draft model to propose multiple draft
tokens, which are then verified by the target LLM in parallel with a single
forward pass. This method reduces the time-consuming data transfers in forward
passes that involve offloaded weight transfers. Existing methods often rely on
pretrained weights of the same family, but require additional training to align
with custom-trained models. Moreover, approaches that involve draft model
training usually yield only modest speedups. This limitation arises from
insufficient alignment with the target model, preventing higher token
acceptance lengths. To address these challenges and achieve greater speedups,
we propose SubSpec, a plug-and-play method to accelerate parameter offloading
that is lossless and training-free. SubSpec constructs a highly aligned draft
model by generating low-bit quantized substitute layers from offloaded target
LLM portions. Additionally, our method shares the remaining GPU-resident layers
and the KV-Cache, further reducing memory overhead and enhance alignment.
SubSpec achieves a high average acceptance length, delivering 9.1x speedup for
Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for
Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).
\\ ( https://arxiv.org/abs/2509.18344 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18360
Date: Mon, 22 Sep 2025 19:37:20 GMT   (312kb)

Title: Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech
  Documents
Authors: Chutong Meng, Philipp Koehn
Categories: cs.CL
Comments: Accepted by EMNLP 2025 (main)
\\
  We present Speech Vecalign, a parallel speech document alignment method that
monotonically aligns speech segment embeddings and does not depend on text
transcriptions. Compared to the baseline method Global Mining, a variant of
speech mining, Speech Vecalign produces longer speech-to-speech alignments. It
also demonstrates greater robustness than Local Mining, another speech mining
variant, as it produces less noise. We applied Speech Vecalign to 3,000 hours
of unlabeled parallel English-German (En-De) speech documents from VoxPopuli,
yielding about 1,000 hours of high-quality alignments. We then trained En-De
speech-to-speech translation models on the aligned data. Speech Vecalign
improves the En-to-De and De-to-En performance over Global Mining by 0.37 and
0.18 ASR-BLEU, respectively. Moreover, our models match or outperform
SpeechMatrix model performance, despite using 8 times fewer raw speech
documents.
\\ ( https://arxiv.org/abs/2509.18360 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18377
Date: Mon, 22 Sep 2025 20:01:20 GMT   (227kb)

Title: Interactive Real-Time Speaker Diarization Correction with Human Feedback
Authors: Xinlu He, Yiwen Guan, Badrivishal Paurana, Zilin Dai, Jacob Whitehill
Categories: cs.CL
\\
  Most automatic speech processing systems operate in "open loop" mode without
user feedback about who said what; yet, human-in-the-loop workflows can
potentially enable higher accuracy. We propose an LLM-assisted speaker
diarization correction system that lets users fix speaker attribution errors in
real time. The pipeline performs streaming ASR and diarization, uses an LLM to
deliver concise summaries to the users, and accepts brief verbal feedback that
is immediately incorporated without disrupting interactions. Moreover, we
develop techniques to make the workflow more effective: First, a
split-when-merged (SWM) technique detects and splits multi-speaker segments
that the ASR erroneously attributes to just a single speaker. Second, online
speaker enrollments are collected based on users' diarization corrections, thus
helping to prevent speaker diarization errors from occurring in the future.
LLM-driven simulations on the AMI test set indicate that our system
substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We
further analyze correction efficacy under different settings, including summary
vs full transcript display, the number of online enrollments limitation, and
correction frequency.
\\ ( https://arxiv.org/abs/2509.18377 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18395
Date: Mon, 22 Sep 2025 20:29:25 GMT   (34185kb)

Title: NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided
  Social Norm Modeling and Violation Recovery
Authors: Minki Hong, Jangho Choi, Jihie Kim
Categories: cs.CL
Comments: 39 pages, 17 figures, EMNLP 2025 Main Conference
\\
  Social norms govern culturally appropriate behavior in communication,
enabling dialogue systems to produce responses that are not only coherent but
also socially acceptable. We present NormGenesis, a multicultural framework for
generating and annotating socially grounded dialogues across English, Chinese,
and Korean. To model the dynamics of social interaction beyond static norm
classification, we propose a novel dialogue type, Violation-to-Resolution
(V2R), which models the progression of conversations following norm violations
through recognition and socially appropriate repair. To improve pragmatic
consistency in underrepresented languages, we implement an exemplar-based
iterative refinement early in the dialogue synthesis process. This design
introduces alignment with linguistic, emotional, and sociocultural expectations
before full dialogue generation begins. Using this framework, we construct a
dataset of 10,800 multi-turn dialogues annotated at the turn level for norm
adherence, speaker intent, and emotional response. Human and LLM-based
evaluations demonstrate that NormGenesis significantly outperforms existing
datasets in refinement quality, dialogue naturalness, and generalization
performance. We show that models trained on our V2R-augmented data exhibit
improved pragmatic competence in ethically sensitive contexts. Our work
establishes a new benchmark for culturally adaptive dialogue modeling and
provides a scalable methodology for norm-aware generation across linguistically
and culturally diverse languages.
\\ ( https://arxiv.org/abs/2509.18395 ,  34185kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18401
Date: Mon, 22 Sep 2025 20:32:56 GMT   (2249kb)

Title: Evaluating the Creativity of LLMs in Persian Literary Text Generation
Authors: Armin Tourajmehr, Mohammad Reza Modarres, Yadollah Yaghoobzadeh
Categories: cs.CL
\\
  Large language models (LLMs) have demonstrated notable creative abilities in
generating literary texts, including poetry and short stories. However, prior
research has primarily centered on English, with limited exploration of
non-English literary traditions and without standardized methods for assessing
creativity. In this paper, we evaluate the capacity of LLMs to generate Persian
literary text enriched with culturally relevant expressions. We build a dataset
of user-generated Persian literary spanning 20 diverse topics and assess model
outputs along four creativity dimensions-originality, fluency, flexibility, and
elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce
evaluation costs, we adopt an LLM as a judge for automated scoring and validate
its reliability against human judgments using intraclass correlation
coefficients, observing strong agreement. In addition, we analyze the models'
ability to understand and employ four core literary devices: simile, metaphor,
hyperbole, and antithesis. Our results highlight both the strengths and
limitations of LLMs in Persian literary text generation, underscoring the need
for further refinement.
\\ ( https://arxiv.org/abs/2509.18401 ,  2249kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18439
Date: Mon, 22 Sep 2025 21:50:13 GMT   (1649kb)

Title: Developing an AI framework to automatically detect shared
  decision-making in patient-doctor conversations
Authors: Oscar J. Ponce-Ponte, David Toro-Tobon, Luis F. Figueroa, Michael
  Gionfriddo, Megan Branda, Victor M. Montori, Saturnino Luz, Juan P. Brito
Categories: cs.CL cs.AI
Comments: 53 pages, 1 figure, 4 tables, 5 supplementary figures, 13
  supplementary tables
\\
  Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.
\\ ( https://arxiv.org/abs/2509.18439 ,  1649kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18458
Date: Mon, 22 Sep 2025 22:28:33 GMT   (107kb)

Title: CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable
  Length, Intrinsic Difficulty, and Distractor Density
Authors: Daniel Kaiser, Arnoldo Frigessi, Ali Ramezani-Kebrya, Benjamin Ricaud
Categories: cs.CL cs.AI cs.LG
Comments: 29 pages (main: 12 + supplemental material: 17), 6 figures, 4 tables,
  Code: https://github.com/kaiserdan/cogniload, Data:
  https://huggingface.co/datasets/cogniloadteam/cogniload
MSC-class: 68T50 (Primary) 68T07, 68T05, 68T20, 68T27 (Secondary)
ACM-class: I.2.7; I.2.6; I.2.4; I.2.8
\\
  Current benchmarks for long-context reasoning in Large Language Models (LLMs)
often blur critical factors like intrinsic task complexity, distractor
interference, and task length. To enable more precise failure analysis, we
introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load
Theory (CLT). CogniLoad generates natural-language logic puzzles with
independently tunable parameters that reflect CLT's core dimensions: intrinsic
difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$)
regulates extraneous load; and task length ($N$) serves as an operational proxy
for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,
CogniLoad reveals distinct performance sensitivities, identifying task length
as a dominant constraint and uncovering varied tolerances to intrinsic
complexity and U-shaped responses to distractor ratios. By offering systematic,
factorial control over these cognitive load dimensions, CogniLoad provides a
reproducible, scalable, and diagnostically rich tool for dissecting LLM
reasoning limitations and guiding future model development.
\\ ( https://arxiv.org/abs/2509.18458 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18467
Date: Mon, 22 Sep 2025 22:43:44 GMT   (1831kb)

Title: LAWCAT: Efficient Distillation from Quadratic to Linear Attention with
  Convolution across Tokens for Long Context Modeling
Authors: Zeyu Liu, Souvik Kundu, Lianghao Jiang, Anni Li, Srikanth Ronanki,
  Sravan Bodapati, Gourav Datta, Peter A. Beerel
Categories: cs.CL cs.AI
Comments: 17 pages, 8 figures
\\
  Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.
\\ ( https://arxiv.org/abs/2509.18467 ,  1831kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18487
Date: Tue, 23 Sep 2025 00:46:21 GMT   (2705kb)

Title: Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph
  Inference
Authors: Ben Finkelshtein, Silviu Cucerzan, Sujay Kumar Jauhar, Ryen White
Categories: cs.CL
\\
  Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.
\\ ( https://arxiv.org/abs/2509.18487 ,  2705kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18514
Date: Tue, 23 Sep 2025 01:22:15 GMT   (355kb)

Title: A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition
Authors: Mohamad Elzohbi and Richard Zhao
Categories: cs.CL cs.AI cs.LG
Comments: Accepted for the Third Arabic Natural Language Processing Conference
  (ArabicNLP 2025)
\\
  This paper presents a methodology for inserting phrases in Arabic poems to
conform to a specific rhythm using ByT5, a byte-level multilingual
transformer-based model. Our work discusses a rule-based grapheme-to-beat
transformation tailored for extracting the rhythm from fully diacritized Arabic
script. Our approach employs a conditional denoising objective to fine-tune
ByT5, where the model reconstructs masked words to match a target rhythm. We
adopt a curriculum learning strategy, pre-training on a general Arabic dataset
before fine-tuning on poetic dataset, and explore cross-lingual transfer from
English to Arabic. Experimental results demonstrate that our models achieve
high rhythmic alignment while maintaining semantic coherence. The proposed
model has the potential to be used in co-creative applications in the process
of composing classical Arabic poems.
\\ ( https://arxiv.org/abs/2509.18514 ,  355kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18535
Date: Tue, 23 Sep 2025 02:00:35 GMT   (850kb)

Title: Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text
  Detector
Authors: Mo Mu, Dianqiao Lei, Chang Li
Categories: cs.CL eess.SP
\\
  The widespread adoption of ChatGPT has raised concerns about its misuse,
highlighting the need for robust detection of AI-generated text. Current
word-level detectors are vulnerable to paraphrasing or simple prompts (PSP),
suffer from biases induced by ChatGPT's word-level patterns (CWP) and training
data content, degrade on modified text, and often require large models or
online LLM interaction. To tackle these issues, we introduce a novel task to
detect both original and PSP-modified AI-generated texts, and propose a
lightweight framework that classifies texts based on their internal structure,
which remains invariant under word-level changes. Our approach encodes sentence
embeddings from pre-trained language models and models their relationships via
attention. We employ contrastive learning to mitigate embedding biases from
autoregressive generation and incorporate a causal graph with counterfactual
methods to isolate structural features from topic-related biases. Experiments
on two curated datasets, including abstract comparisons and revised life FAQs,
validate the effectiveness of our method.
\\ ( https://arxiv.org/abs/2509.18535 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18536
Date: Tue, 23 Sep 2025 02:01:03 GMT   (3075kb)

Title: CCQA: Generating Question from Solution Can Improve Inference-Time
  Reasoning in SLMs
Authors: Jin Young Kim, Ji Won Yoon
Categories: cs.CL cs.AI
Comments: Published as a main conference paper at EMNLP 2025
\\
  Recently, inference-time reasoning strategies have further improved the
accuracy of large language models (LLMs), but their effectiveness on smaller
models remains unclear. Based on the observation that conventional approaches
often fail to improve performance in this context, we propose
\textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering
(CCQA), a novel reasoning method that can be effectively applied to SLMs.
Inspired by cycle consistency, CCQA generates a question from each reasoning
path and answer, evaluates each by its similarity to the original question, and
then selects the candidate solution with the highest similarity score as the
final response. Since conventional SLMs struggle to generate accurate questions
from their own reasoning paths and answers, we employ a lightweight Flan-T5
model specialized for question generation to support this process efficiently.
From the experimental results, it is verified that CCQA consistently
outperforms existing state-of-the-art (SOTA) methods across eight models on
mathematical and commonsense reasoning benchmarks. Furthermore, our method
establishes a new practical baseline for efficient reasoning in SLMs. Source
code can be found at https://github.com/scai-research/ccqa_official.
\\ ( https://arxiv.org/abs/2509.18536 ,  3075kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18577
Date: Tue, 23 Sep 2025 02:57:29 GMT   (1052kb)

Title: Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For
  Perplexity
Authors: Yeongbin Seo and Gayoung Kim and Jaehyung Kim and Jinyoung Yeo
Categories: cs.CL
MSC-class: 68T50
ACM-class: I.2.7
\\
  As large language models (LLMs) are pretrained on massive web corpora,
careful selection of data becomes essential to ensure effective and efficient
learning. While perplexity (PPL)-based filtering has shown strong performance,
it suffers from drawbacks: substantial time costs and inherent unreliability of
the model when handling noisy or out-of-distribution samples. In this work, we
propose a simple yet powerful alternative: a prior-based data filtering method
that estimates token priors using corpus-level term frequency statistics,
inspired by linguistic insights on word roles and lexical density. Our approach
filters documents based on the mean and standard deviation of token priors,
serving as a fast proxy to PPL while requiring no model inference. Despite its
simplicity, the prior-based filter achieves the highest average performance
across 20 downstream benchmarks, while reducing time cost by over 1000x
compared to PPL-based filtering. We further demonstrate its applicability to
symbolic languages such as code and math, and its dynamic adaptability to
multilingual corpora without supervision
\\ ( https://arxiv.org/abs/2509.18577 ,  1052kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18585
Date: Tue, 23 Sep 2025 03:10:41 GMT   (560kb)

Title: TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for
  Efficient Fine-Tuning
Authors: Yu Chen, Yifei Han, Long Zhang, Yue Du, Bin Li
Categories: cs.CL cs.AI
Comments: 5 pages, 4 figures, published to ICASSP2026
\\
  Fine-tuning large pre-trained models for downstream tasks has become a
fundamental approach in natural language processing. Fully fine-tuning all
model parameters is computationally expensive and memory-intensive, especially
in resource-constrained environments. Existing parameter-efficient fine-tuning
methods reduce the number of trainable parameters but typically overlook the
varying sensitivity of different model layers and the importance of training
data. In this work, we propose TsqLoRA, a novel method that integrates
data-quality-driven selection with sensitivity-aware low-rank adaptation,
consisted of two main components: a quality-aware sampling mechanism for
selecting the most informative training data, and a dynamic rank allocation
module that adjusts the rank of each layer based on its sensitivity to
parameter updates. The experimental results demonstrate that TsqLoRA improves
fine-tuning efficiency while maintaining or even improving performance on a
variety of NLP tasks. Our code will be available at
https://github.com/Benjamin-Ricky/TsqLoRA.
\\ ( https://arxiv.org/abs/2509.18585 ,  560kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18588
Date: Tue, 23 Sep 2025 03:15:53 GMT   (3309kb)

Title: UniECG: Understanding and Generating ECG in One Unified Model
Authors: Jiarui Jin, Haoyu Wang, Xiang Lan, Jun Li, Gaofeng Cheng, Hongyan Li,
  Shenda Hong
Categories: cs.CL
\\
  Recent unified models such as GPT-5 have achieved encouraging progress on
vision-language tasks. However, these unified models typically fail to
correctly understand ECG signals and provide accurate medical diagnoses, nor
can they correctly generate ECG signals. To address these limitations, we
propose UniECG, the first unified model for ECG capable of concurrently
performing evidence-based ECG interpretation and text-conditioned ECG
generation tasks. Through a decoupled two-stage training approach, the model
first learns evidence-based interpretation skills (ECG-to-Text), and then
injects ECG generation capabilities (Text-to-ECG) via latent space alignment.
UniECG can autonomously choose to interpret or generate an ECG based on user
input, significantly extending the capability boundaries of current ECG models.
Our code and checkpoints will be made publicly available at
https://github.com/PKUDigitalHealth/UniECG upon acceptance.
\\ ( https://arxiv.org/abs/2509.18588 ,  3309kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18632
Date: Tue, 23 Sep 2025 04:33:30 GMT   (1425kb)

Title: A Good Plan is Hard to Find: Aligning Models with Preferences is
  Misaligned with What Helps Users
Authors: Nishant Balepur, Matthew Shu, Yoo Yeon Sung, Seraphina
  Goldfarb-Tarrant, Shi Feng, Fumeng Yang, Rachel Rudinger, Jordan Lee
  Boyd-Graber
Categories: cs.CL
Comments: EMNLP 2025
\\
  To assist users in complex tasks, LLMs generate plans: step-by-step
instructions towards a goal. While alignment methods aim to ensure LLM plans
are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,
assuming this reflects what helps them. We test this with Planorama: an
interface where 126 users answer 300 multi-step questions with LLM plans. We
get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA
success) and user preferences on plans, and recreate the setup in agents and
reward models to see if they simulate or prefer what helps users. We expose: 1)
user/model preferences and agent success do not accurately predict which plans
help users, so common alignment feedback can misalign with helpfulness; 2) this
gap is not due to user-specific preferences, as users are similarly successful
when using plans they prefer/disprefer; 3) surface-level cues like brevity and
question similarity strongly link to preferences, but such biases fail to
predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from
real user interactions, not just preferences of what looks helpful, so we
discuss the plan NLP researchers can execute to solve this problem.
\\ ( https://arxiv.org/abs/2509.18632 ,  1425kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18655
Date: Tue, 23 Sep 2025 05:17:39 GMT   (955kb)

Title: Consistency-Aware Parameter-Preserving Knowledge Editing Framework for
  Multi-Hop Question Answering
Authors: Lingwen Deng, Yifei Han, Long Zhang, Yue Du, Bin Li
Categories: cs.CL
Comments: Submitted to ICASSP 2026
\\
  Parameter-Preserving Knowledge Editing (PPKE) enables updating models with
new or corrected information without retraining or parameter adjustment. Recent
PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)
capabilities to multi-hop question answering (MHQA). However, these methods
often lack consistency, leading to knowledge contamination, unstable updates,
and retrieval behaviors that fail to reflect the intended edits. Such
inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We
present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge
Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures
KG construction, update, and retrieval are always aligned with the requirements
of the MHQA task, maintaining coherent reasoning over both unedited and edited
knowledge. Extensive experiments on the MQuAKE benchmark show accuracy
improvements in PPKE performance for MHQA, demonstrating the effectiveness of
addressing consistency in PPKE.
\\ ( https://arxiv.org/abs/2509.18655 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18658
Date: Tue, 23 Sep 2025 05:26:28 GMT   (2328kb)

Title: Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with
  Conformal Prediction
Authors: Huanxin Sheng, Xinyi Liu, Hangfeng He, Jieyu Zhao and Jian Kang
Categories: cs.CL
Comments: To appear in EMNLP 2025. Our code and data are available at
  \url{https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge
\\
  LLM-as-a-judge has become a promising paradigm for using large language
models (LLMs) to evaluate natural language generation (NLG), but the
uncertainty of its evaluation remains underexplored. This lack of reliability
may limit its deployment in many applications. This work presents the first
framework to analyze the uncertainty by offering a prediction interval of
LLM-based scoring via conformal prediction. Conformal prediction constructs
continuous prediction intervals from a single evaluation run, and we design an
ordinal boundary adjustment for discrete rating tasks. We also suggest a
midpoint-based score within the interval as a low-bias alternative to raw model
score and weighted average. We perform extensive experiments and analysis,
which show that conformal prediction can provide valid prediction interval with
coverage guarantees. We also explore the usefulness of interval midpoint and
judge reprompting for better judgment.
\\ ( https://arxiv.org/abs/2509.18658 ,  2328kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18713
Date: Tue, 23 Sep 2025 06:57:07 GMT   (388kb)

Title: MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce
  Customer Service
Authors: Yizhe Huang, Yang Liu, Ruiyu Zhao, Xiaolong Zhong, Xingming Yue, Ling
  Jiang
Categories: cs.CL cs.AI
\\
  Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.
\\ ( https://arxiv.org/abs/2509.18713 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18722
Date: Tue, 23 Sep 2025 07:11:06 GMT   (556kb)

Title: LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR
Authors: Pattara Tipaksorn, Sumonmas Thatphithakkul, Vataya Chunwijitra,
  Kwanchiva Thangthai
Categories: cs.CL cs.SD
\\
  We present LOTUSDIS, a publicly available Thai meeting corpus designed to
advance far-field conversational ASR. The dataset comprises 114 hours of
spontaneous, unscripted dialogue collected in 15-20 minute sessions with three
participants, where overlapping speech is frequent and natural. Speech was
recorded simultaneously by nine independent single-channel devices spanning six
microphone types at distances from 0.12 m to 10 m, preserving the authentic
effects of reverberation, noise, and device coloration without relying on
microphone arrays. We provide standard train, dev, test splits and release a
reproducible baseline system. We benchmarked several Whisper variants under
zero-shot and fine-tuned conditions. Off-the-shelf models showed strong
degradation with distance, confirming a mismatch between pre-training data and
Thai far-field speech. Fine-tuning on LOTUSDIS dramatically improved
robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and
far-field WER from 81.6 to 49.5, with especially large gains on the most
distant microphones. These results underscore the importance of
distance-diverse training data for robust ASR. The corpus is available under
CC-BY-SA 4.0. We also release training and evaluation scripts as a baseline
system to promote reproducible research in this field.
\\ ( https://arxiv.org/abs/2509.18722 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18742
Date: Tue, 23 Sep 2025 07:35:42 GMT   (827kb)

Title: Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with
  Large Language Models
Authors: Yunan Wang, Jianxin Li, Ziwei Zhang
Categories: cs.CL
\\
  Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph
interactions and associated text attributes, are prevalent in real-world
applications. Existing methods, such as Graph Neural Networks (GNNs) and Large
Language Models (LLMs), mostly focus on static TAGs. Extending these existing
methods to DyTAGs is challenging as they largely neglect the recent-global
temporal semantics: the recent semantic dependencies among interaction texts
and the global semantic evolution of nodes over time. Furthermore, applying
LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To
tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic
Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to
efficiently and effectively reason on DyTAGs. Specifically, we first design a
node-centric implicit reasoning method together with a sliding window mechanism
to efficiently capture recent temporal semantics. In addition, to capture
global semantic dynamics of nodes, we leverage explicit reasoning with tailored
prompts and an RNN-like chain structure to infer long-term semantics. Lastly,
we intricately integrate the recent and global temporal semantics as well as
the dynamic graph structural information using updating and merging layers.
Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,
achieving up to 34% improvement in Hit@10 for destination node retrieval task.
Besides, DyGRASP exhibits strong generalization across different temporal GNNs
and LLMs.
\\ ( https://arxiv.org/abs/2509.18742 ,  827kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18750
Date: Tue, 23 Sep 2025 07:47:54 GMT   (986kb)

Title: False Friends Are Not Foes: Investigating Vocabulary Overlap in
  Multilingual Language Models
Authors: Julie Kallini, Dan Jurafsky, Christopher Potts, Martijn Bartelds
Categories: cs.CL
\\
  Subword tokenizers trained on multilingual corpora naturally produce
overlapping tokens across languages. Does token overlap facilitate
cross-lingual transfer or instead introduce interference between languages?
Prior work offers mixed evidence, partly due to varied setups and confounders,
such as token frequency or subword segmentation granularity. To address this
question, we devise a controlled experiment where we train bilingual
autoregressive models on multiple language pairs under systematically varied
vocabulary overlap settings. Crucially, we explore a new dimension to
understanding how overlap affects transfer: the semantic similarity of tokens
shared across languages. We first analyze our models' hidden representations
and find that overlap of any kind creates embedding spaces that capture
cross-lingual semantic relationships, while this effect is much weaker in
models with disjoint vocabularies. On XNLI and XQuAD, we find that models with
overlap outperform models with disjoint vocabularies, and that transfer
performance generally improves as overlap increases. Overall, our findings
highlight the advantages of token overlap in multilingual models and show that
substantial shared vocabulary remains a beneficial design choice for
multilingual tokenizers.
\\ ( https://arxiv.org/abs/2509.18750 ,  986kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18762
Date: Tue, 23 Sep 2025 07:55:38 GMT   (3581kb)

Title: When Long Helps Short: How Context Length in Supervised Fine-tuning
  Affects Behavior of Large Language Models
Authors: Yingming Zheng, Hanqi Li, Kai Yu and Lu Chen
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have achieved impressive performance across
natural language processing (NLP) tasks. As real-world applications
increasingly demand longer context windows, continued pretraining and
supervised fine-tuning (SFT) on long-context data has become a common approach.
While the effects of data length in continued pretraining have been extensively
studied, their implications for SFT remain unclear. In this work, we
systematically investigate how SFT data length influences LLM behavior on
short-context tasks. Counterintuitively, we find that long-context SFT improves
short-context performance, contrary to the commonly observed degradation from
long-context pretraining. To uncover the underlying mechanisms of this
phenomenon, we first decouple and analyze two key components, Multi-Head
Attention (MHA) and Feed-Forward Network (FFN), and show that both
independently benefit from long-context SFT. We further study their interaction
and reveal a knowledge preference bias: long-context SFT promotes contextual
knowledge, while short-context SFT favors parametric knowledge, making
exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that
hybrid training mitigates this bias, offering explainable guidance for
fine-tuning LLMs.
\\ ( https://arxiv.org/abs/2509.18762 ,  3581kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18775
Date: Tue, 23 Sep 2025 08:09:30 GMT   (213kb)

Title: Financial Risk Relation Identification through Dual-view Adaptation
Authors: Wei-Ning Chiu, Yu-Hsiang Wang, Andy Hsiao, Yu-Shiang Huang, Chuan-Ju
  Wang
Categories: cs.CL cs.AI
Comments: 11 pages, 3 figures, EMNLP 2025 Main Conference
\\
  A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.
\\ ( https://arxiv.org/abs/2509.18775 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18776
Date: Tue, 23 Sep 2025 08:09:58 GMT   (10486kb)

Title: AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large
  Language Models in the AEC Field
Authors: Chen Liang, Zhaoqi Huang, Haofen Wang, Fu Chai, Chunying Yu, Huanhuan
  Wei, Zhengjie Liu, Yanpeng Li, Hongjun Wang, Ruifeng Luo, Xianzhong Zhao
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.
\\ ( https://arxiv.org/abs/2509.18776 ,  10486kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18792
Date: Tue, 23 Sep 2025 08:35:58 GMT   (758kb)

Title: Beyond the Leaderboard: Understanding Performance Disparities in Large
  Language Models via Model Diffing
Authors: Sabri Boughorbel, Fahim Dalvi, Nadir Durrani, Majd Hawasly
Categories: cs.CL
Comments: 12 pages, accepted to the 2025 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2025)
\\
  As fine-tuning becomes the dominant paradigm for improving large language
models (LLMs), understanding what changes during this process is increasingly
important. Traditional benchmarking often fails to explain why one model
outperforms another. In this work, we use model diffing, a mechanistic
interpretability approach, to analyze the specific capability differences
between Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we
identify and categorize latent representations that differentiate the two
models. We find that SimPO acquired latent concepts predominantly enhance
safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and
instruction-following (+151.7%), while its additional training also reduces
emphasis on model self-reference (-44.1%) and hallucination management
(-68.5%). Our analysis shows that model diffing can yield fine-grained insights
beyond leaderboard metrics, attributing performance gaps to concrete
mechanistic capabilities. This approach offers a transparent and targeted
framework for comparing LLMs.
\\ ( https://arxiv.org/abs/2509.18792 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18813
Date: Tue, 23 Sep 2025 09:00:43 GMT   (291kb)

Title: MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction
Authors: Liting Zhang, Shiwan Zhao, Aobo Kong and Qicheng Li
Categories: cs.CL
\\
  Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by
4.01\% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.
\\ ( https://arxiv.org/abs/2509.18813 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18843
Date: Tue, 23 Sep 2025 09:27:57 GMT   (70kb)

Title: Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for
  Biomedical Question Answering?
Authors: Damian Stachura, Joanna Konieczna and Artur Nowak
Categories: cs.CL cs.IR cs.LG
Comments: CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain
\\
  Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.
\\ ( https://arxiv.org/abs/2509.18843 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18862
Date: Tue, 23 Sep 2025 09:55:42 GMT   (9kb)

Title: Multi-Hierarchical Feature Detection for Large Language Model Generated
  Text
Authors: Luyan Zhang and Xinyu Xie
Categories: cs.CL
Comments: 9 pages, 6 tables, empirical study on multi-feature AI text detection
ACM-class: I.2.7; I.2.1
\\
  With the rapid advancement of large language model technology, there is
growing interest in whether multi-feature approaches can significantly improve
AI text detection beyond what single neural models achieve. While intuition
suggests that combining semantic, syntactic, and statistical features should
provide complementary signals, this assumption has not been rigorously tested
with modern LLM-generated text. This paper provides a systematic empirical
investigation of multi-hierarchical feature integration for AI text detection,
specifically testing whether the computational overhead of combining multiple
feature types is justified by performance gains. We implement MHFD
(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic
analysis, syntactic parsing, and statistical probability features through
adaptive fusion. Our investigation reveals important negative results: despite
theoretical expectations, multi-feature integration provides minimal benefits
(0.4-0.5% improvement) while incurring substantial computational costs (4.2x
overhead), suggesting that modern neural language models may already capture
most relevant detection signals efficiently. Experimental results on multiple
benchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in
in-domain detection and maintains 84.2% stable performance in cross-domain
detection, showing modest improvements of 0.4-2.6% over existing methods.
\\ ( https://arxiv.org/abs/2509.18862 ,  9kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18880
Date: Tue, 23 Sep 2025 10:21:22 GMT   (369kb)

Title: Diversity Boosts AI-Generated Text Detection
Authors: Advik Raj Basani, Pin-Yu Chen
Categories: cs.CL cs.AI cs.LG
Comments: Project Webpage: https://diveye.vercel.app/
\\
  Detecting AI-generated text is an increasing necessity to combat misuse of
LLMs in education, business compliance, journalism, and social media, where
synthetic fluency can mask misinformation or deception. While prior detectors
often rely on token-level likelihoods or opaque black-box classifiers, these
approaches struggle against high-quality generations and offer little
interpretability. In this work, we propose DivEye, a novel detection framework
that captures how unpredictability fluctuates across a text using
surprisal-based features. Motivated by the observation that human-authored text
exhibits richer variability in lexical and structural unpredictability than LLM
outputs, DivEye captures this signal through a set of interpretable statistical
features. Our method outperforms existing zero-shot detectors by up to 33.2%
and achieves competitive performance with fine-tuned baselines across multiple
benchmarks. DivEye is robust to paraphrasing and adversarial attacks,
generalizes well across domains and models, and improves the performance of
existing detectors by up to 18.7% when used as an auxiliary signal. Beyond
detection, DivEye provides interpretable insights into why a text is flagged,
pointing to rhythmic unpredictability as a powerful and underexplored signal
for LLM detection.
\\ ( https://arxiv.org/abs/2509.18880 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18901
Date: Tue, 23 Sep 2025 11:30:42 GMT   (580kb)

Title: Extractive Fact Decomposition for Interpretable Natural Language
  Inference in one Forward Pass
Authors: Nicholas Popovi\v{c}, Michael F\"arber
Categories: cs.CL
Comments: EMNLP 2025
\\
  Recent works in Natural Language Inference (NLI) and related tasks, such as
automated fact-checking, employ atomic fact decomposition to enhance
interpretability and robustness. For this, existing methods rely on
resource-intensive generative large language models (LLMs) to perform
decomposition. We propose JEDI, an encoder-only architecture that jointly
performs extractive atomic fact decomposition and interpretable inference
without requiring generative models during inference. To facilitate training,
we produce a large corpus of synthetic rationales covering multiple NLI
benchmarks. Experimental results demonstrate that JEDI achieves competitive
accuracy in distribution and significantly improves robustness out of
distribution and in adversarial settings over models based solely on extractive
rationale supervision. Our findings show that interpretability and robust
generalization in NLI can be realized using encoder-only architectures and
synthetic rationales. Code and data available at https://jedi.nicpopovic.com
\\ ( https://arxiv.org/abs/2509.18901 ,  580kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18987
Date: Tue, 23 Sep 2025 13:37:15 GMT   (891kb)

Title: DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation
  with Dynamic Time Warping Alignment
Authors: Abderrahmane Issam, Yusuf Can Semerci, Jan Scholtes, Gerasimos
  Spanakis
Categories: cs.CL
Comments: Accepted at WMT2025
\\
  End-to-End Speech Translation (E2E-ST) is the task of translating source
speech directly into target text bypassing the intermediate transcription step.
The representation discrepancy between the speech and text modalities has
motivated research on what is known as bridging the modality gap.
State-of-the-art methods addressed this by aligning speech and text
representations on the word or token level. Unfortunately, this requires an
alignment tool that is not available for all languages. Although this issue has
been addressed by aligning speech and text embeddings using nearest-neighbor
similarity search, it does not lead to accurate alignments. In this work, we
adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during
training. Our experiments demonstrate the effectiveness of our method in
bridging the modality gap in E2E-ST. Compared to previous work, our method
produces more accurate alignments and achieves comparable E2E-ST results while
being significantly faster. Furthermore, our method outperforms previous work
in low resource settings on 5 out of 6 language directions.
\\ ( https://arxiv.org/abs/2509.18987 ,  891kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19020
Date: Tue, 23 Sep 2025 13:58:16 GMT   (2316kb)

Title: Investigating Test-Time Scaling with Reranking for Machine Translation
Authors: Shaomu Tan, Ryosuke Mitani, Ritvik Choudhary, Toshiyuki Sekiya
Categories: cs.CL
\\
  Scaling model parameters has become the de facto strategy for improving NLP
systems, but it comes with substantial computational costs. Test-Time Scaling
(TTS) offers an alternative by allocating more computation at inference:
generating multiple candidates and selecting the best. While effective in tasks
such as mathematical reasoning, TTS has not been systematically explored for
machine translation (MT). In this paper, we present the first systematic study
of TTS for MT, investigating a simple but practical best-of-N framework on
WMT24 benchmarks. Our experiments cover six high-resource and one low-resource
language pairs, five model sizes (3B-72B), and various TTS compute budget (N up
to 1024). Our results show that a) For high-resource languages, TTS generally
improves translation quality according to multiple neural MT evaluation
metrics, and our human evaluation confirms these gains; b) Augmenting smaller
models with large $N$ can match or surpass larger models at $N{=}1$ with more
compute cost; c) Under fixed compute budgets, larger models are typically more
efficient, and TTS can degrade quality due to metric blind spots in
low-resource cases.
\\ ( https://arxiv.org/abs/2509.19020 ,  2316kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19033
Date: Tue, 23 Sep 2025 14:06:09 GMT   (1048kb)

Title: Charting a Decade of Computational Linguistics in Italy: The CLiC-it
  Corpus
Authors: Chiara Alzetta, Serena Auriemma, Alessandro Bondielli, Luca Dini,
  Chiara Fazzone, Alessio Miaschi, Martina Miliani, Marta Sartor
Categories: cs.CL
Comments: Submitted to IJCoL
MSC-class: 68T50
ACM-class: I.2.7
\\
  Over the past decade, Computational Linguistics (CL) and Natural Language
Processing (NLP) have evolved rapidly, especially with the advent of
Transformer-based Large Language Models (LLMs). This shift has transformed
research goals and priorities, from Lexical and Semantic Resources to Language
Modelling and Multimodality. In this study, we track the research trends of the
Italian CL and NLP community through an analysis of the contributions to
CLiC-it, arguably the leading Italian conference in the field. We compile the
proceedings from the first 10 editions of the CLiC-it conference (from 2014 to
2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its
metadata, including author provenance, gender, affiliations, and more, as well
as the content of the papers themselves, which address various topics. Our goal
is to provide the Italian and international research communities with valuable
insights into emerging trends and key developments over time, supporting
informed decisions and future directions in the field.
\\ ( https://arxiv.org/abs/2509.19033 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19094
Date: Tue, 23 Sep 2025 14:44:46 GMT   (1105kb)

Title: Pathways of Thoughts: Multi-Directional Thinking for Long-form
  Personalized Question Answering
Authors: Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Zhuowan Li,
  Spurthi Amba Hombaiah, Weize Kong, Tao Chen, Hamed Zamani, Michael Bendersky
Categories: cs.CL cs.AI cs.IR
\\
  Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.
\\ ( https://arxiv.org/abs/2509.19094 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19108
Date: Tue, 23 Sep 2025 14:54:19 GMT   (7kb)

Title: Are most sentences unique? An empirical examination of Chomskyan claims
Authors: Hiram Ring
Categories: cs.CL
\\
  A repeated claim in linguistics is that the majority of linguistic utterances
are unique. For example, Pinker (1994: 10), summarizing an argument by Noam
Chomsky, states that "virtually every sentence that a person utters or
understands is a brand-new combination of words, appearing for the first time
in the history of the universe." With the increased availability of large
corpora, this is a claim that can be empirically investigated. The current
paper addresses the question by using the NLTK Python library to parse corpora
of different genres, providing counts of exact string matches in each. Results
show that while completely unique sentences are often the majority of corpora,
this is highly constrained by genre, and that duplicate sentences are not an
insignificant part of any individual corpus.
\\ ( https://arxiv.org/abs/2509.19108 ,  7kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19109
Date: Tue, 23 Sep 2025 14:56:10 GMT   (300kb)

Title: Human-Annotated NER Dataset for the Kyrgyz Language
Authors: Timur Turatali, Anton Alekseev, Gulira Jumalieva, Gulnara Kabaeva,
  Sergey Nikolenko
Categories: cs.CL
Comments: Accepted to TurkLang-2025 conference, DOI and copyright will be added
  upon confirmation of acceptance to publication in IEEE Xplore
\\
  We introduce KyrgyzNER, the first manually annotated named entity recognition
dataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG
news portal, the dataset contains 10,900 sentences and 39,075 entity mentions
across 27 named entity classes. We show our annotation scheme, discuss the
challenges encountered in the annotation process, and present the descriptive
statistics. We also evaluate several named entity recognition models, including
traditional sequence labeling approaches based on conditional random fields and
state-of-the-art multilingual transformer-based models fine-tuned on our
dataset. While all models show difficulties with rare entity categories, models
such as the multilingual RoBERTa variant pretrained on a large corpus across
many languages achieve a promising balance between precision and recall. These
findings emphasize both the challenges and opportunities of using multilingual
pretrained models for processing languages with limited resources. Although the
multilingual RoBERTa model performed best, other multilingual models yielded
comparable results. This suggests that future work exploring more granular
annotation schemes may offer deeper insights for Kyrgyz language processing
pipelines evaluation.
\\ ( https://arxiv.org/abs/2509.19109 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19125
Date: Tue, 23 Sep 2025 15:12:58 GMT   (1192kb)

Title: Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via
  LLM-Guided Multi-Aspect Clustering
Authors: Kun Zhu, Lizi Liao, Yuxuan Gu, Lei Huang, Xiaocheng Feng, Bing Qin
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Main
\\
  The rapid growth of scientific literature demands efficient methods to
organize and synthesize research findings. Existing taxonomy construction
methods, leveraging unsupervised clustering or direct prompting of large
language models (LLMs), often lack coherence and granularity. We propose a
novel context-aware hierarchical taxonomy generation framework that integrates
LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages
LLMs to identify key aspects of each paper (e.g., methodology, dataset,
evaluation) and generates aspect-specific paper summaries, which are then
encoded and clustered along each aspect to form a coherent hierarchy. In
addition, we introduce a new evaluation benchmark of 156 expert-crafted
taxonomies encompassing 11.6k papers, providing the first naturally annotated
dataset for this task. Experimental results demonstrate that our method
significantly outperforms prior approaches, achieving state-of-the-art
performance in taxonomy coherence, granularity, and interpretability.
\\ ( https://arxiv.org/abs/2509.19125 ,  1192kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19143
Date: Tue, 23 Sep 2025 15:26:13 GMT   (1799kb)

Title: Anecdoctoring: Automated Red-Teaming Across Language and Place
Authors: Alejandro Cuevas, Saloni Dash, Bharat Kumar Nayak, Dan Vann, Madeleine
  I. G. Daepp
Categories: cs.CL cs.AI cs.CY
Comments: To be published in EMNLP 2025
\\
  Disinformation is among the top risks of generative artificial intelligence
(AI) misuse. Global adoption of generative AI necessitates red-teaming
evaluations (i.e., systematic adversarial probing) that are robust across
diverse languages and cultures, but red-teaming datasets are commonly US- and
English-centric. To address this gap, we propose "anecdoctoring", a novel
red-teaming approach that automatically generates adversarial prompts across
languages and cultures. We collect misinformation claims from fact-checking
websites in three languages (English, Spanish, and Hindi) and two geographies
(US and India). We then cluster individual claims into broader narratives and
characterize the resulting clusters with knowledge graphs, with which we
augment an attacker LLM. Our method produces higher attack success rates and
offers interpretability benefits relative to few-shot prompting. Results
underscore the need for disinformation mitigations that scale globally and are
grounded in real-world adversarial misuse.
\\ ( https://arxiv.org/abs/2509.19143 ,  1799kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19163
Date: Tue, 23 Sep 2025 15:41:19 GMT   (882kb)

Title: Measuring AI "Slop" in Text
Authors: Chantal Shaib, Tuhin Chakrabarty, Diego Garcia-Olano, Byron C. Wallace
Categories: cs.CL
\\
  AI "slop" is an increasingly popular term used to describe low-quality
AI-generated text, but there is currently no agreed upon definition of this
term nor a means to measure its occurrence. In this work, we develop a taxonomy
of "slop" through interviews with experts in NLP, writing, and philosophy, and
propose a set of interpretable dimensions for its assessment in text. Through
span-level annotation, we find that binary "slop" judgments are (somewhat)
subjective, but such determinations nonetheless correlate with latent
dimensions such as coherence and relevance. Our framework can be used to
evaluate AI-generated text in both detection and binary preference tasks,
potentially offering new insights into the linguistic and stylistic factors
that contribute to quality judgments.
\\ ( https://arxiv.org/abs/2509.19163 ,  882kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19170
Date: Tue, 23 Sep 2025 15:43:47 GMT   (1843kb)

Title: Soft Tokens, Hard Truths
Authors: Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, Yann
  Ollivier
Categories: cs.CL cs.AI cs.LG
\\
  The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.
\\ ( https://arxiv.org/abs/2509.19170 ,  1843kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19199
Date: Tue, 23 Sep 2025 16:15:42 GMT   (2139kb)

Title: Online Process Reward Leanring for Agentic Reinforcement Learning
Authors: Xiaoqian Liu, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li, Junge Zhang,
  Jianbin Jiao
Categories: cs.CL
Comments: preprint
\\
  Large language models (LLMs) are increasingly trained with reinforcement
learning (RL) as autonomous agents that reason and act over long horizons in
interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit
assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but
suffers from biased annotation, reward hacking, high-variance from overly
fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general
credit-assignment strategy for agentic RL that integrates seamlessly with
standard on-policy algorithms without relying on additional rollouts or
explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with
the agent's policy to transform trajectory preferences into implicit step
rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are
combined with episode-level advantages from outcome rewards for policy update,
creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent
with trajectory preferences and act as potential-based shaping rewards,
providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including
WebShop and VisualSokoban, as well as open-ended social interactions with
unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL
baselines across domains, achieving state-of-the-art results with higher
sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using
fewer actions, underscoring its potential for agentic learning in real-world
scenarios.
\\ ( https://arxiv.org/abs/2509.19199 ,  2139kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19212
Date: Tue, 23 Sep 2025 16:32:25 GMT   (5265kb)

Title: Steering Multimodal Large Language Models Decoding for Context-Aware
  Safety
Authors: Zheyuan Liu, Zhangchen Xu, Guangyao Dou, Xiangchi Yuan, Zhaoxuan Tan,
  Radha Poovendran, Meng Jiang
Categories: cs.CL cs.AI
Comments: A lightweight and model-agnostic decoding framework that dynamically
  adjusts token generation based on multimodal context
\\
  Multimodal Large Language Models (MLLMs) are increasingly deployed in
real-world applications, yet their ability to make context-aware safety
decisions remains limited. Existing methods often fail to balance
oversensitivity (unjustified refusals of benign queries) and undersensitivity
(missed detection of visually grounded risks), leaving a persistent gap in
safety alignment. To address this issue, we introduce Safety-aware Contrastive
Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that
dynamically adjusts token generation based on multimodal context. SafeCoDe
operates in two stages: (1) a contrastive decoding mechanism that highlights
tokens sensitive to visual context by contrasting real and Gaussian-noised
images, and (2) a global-aware token modulation strategy that integrates
scene-level reasoning with token-level adjustment to adapt refusals according
to the predicted safety verdict. Extensive experiments across diverse MLLM
architectures and safety benchmarks, covering undersensitivity,
oversensitivity, and general safety evaluations, show that SafeCoDe
consistently improves context-sensitive refusal behaviors while preserving
model helpfulness.
\\ ( https://arxiv.org/abs/2509.19212 ,  5265kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19224
Date: Tue, 23 Sep 2025 16:48:28 GMT   (125kb)

Title: Systematic Comparative Analysis of Large Pretrained Language Models on
  Contextualized Medication Event Extraction
Authors: Tariq Abdul-Quddoos, Xishuang Dong, and Lijun Qian
Categories: cs.CL cs.AI
\\
  Attention-based models have become the leading approach in modeling medical
language for Natural Language Processing (NLP) in clinical notes. These models
outperform traditional techniques by effectively capturing contextual rep-
resentations of language. In this research a comparative analysis is done
amongst pre- trained attention based models namely Bert Base, BioBert, two
variations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task
related to Electronic Health Record (EHR) information extraction. The tasks
from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges
(n2c2) are considered for this comparison, with the Contextualized Medication
Event Dataset (CMED) given for these task. CMED is a dataset of unstructured
EHRs and annotated notes that contain task relevant information about the EHRs.
The goal of the challenge is to develop effective solutions for extracting
contextual information related to patient medication events from EHRs using
data driven methods. Each pre-trained model is fine-tuned and applied on CMED
to perform medication extraction, medical event detection, and
multi-dimensional medication event context classification. Pro- cessing methods
are also detailed for breaking down EHRs for compatibility with the applied
models. Performance analysis has been carried out using a script based on
constructing medical terms from the evaluation portion of CMED with metrics
including recall, precision, and F1-Score. The results demonstrate that models
pre-trained on clinical data are more effective in detecting medication and
medication events, but Bert Base, pre- trained on general domain data showed to
be the most effective for classifying the context of events related to
medications.
\\ ( https://arxiv.org/abs/2509.19224 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19228
Date: Tue, 23 Sep 2025 16:49:43 GMT   (2101kb)

Title: CompLLM: Compression for Long Context Q&A
Authors: Gabriele Berton, Jayakrishnan Unnikrishnan, Son Tran, Mubarak Shah
Categories: cs.CL
\\
  Large Language Models (LLMs) face significant computational challenges when
processing long contexts due to the quadratic complexity of self-attention.
While soft context compression methods, which map input text to smaller latent
representations, have shown promise, their real-world adoption is limited.
Existing techniques typically compress the context as a single unit, which
leads to quadratic compression complexity and an inability to reuse
computations across queries with overlapping contexts. In this work, we
introduce CompLLM, a soft compression technique designed for practical
deployment. Instead of processing the context holistically, CompLLM divides it
into segments and compresses each one independently. This simple design choice
yields three critical properties: efficiency, as the compression step scales
linearly with the context length; scalability, enabling models trained on short
sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and
reusability, allowing compressed segments to be cached and reused across
different queries. Our experiments show that with a 2x compression rate, at
high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x
and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance
comparable to that obtained with the uncompressed context, and even surpasses
it on very long sequences, demonstrating its effectiveness and practical
utility.
\\ ( https://arxiv.org/abs/2509.19228 ,  2101kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19249
Date: Tue, 23 Sep 2025 17:10:40 GMT   (693kb)

Title: Reinforcement Learning on Pre-Training Data
Authors: Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li,
  Haoyuan Wu, Jiajia Wu, Zihao Zheng, Chenchen Zhang, Kun Shi, Kyrierl Deng, Qi
  Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan Zeng,
  Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu,
  Xiaoxue Li, Zhuoyu Li, Yangyu Tao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai
  Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, Di Wang
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress
\\
  The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.
\\ ( https://arxiv.org/abs/2509.19249 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19269
Date: Tue, 23 Sep 2025 17:33:30 GMT   (8867kb)

Title: Extracting Conceptual Spaces from LLMs Using Prototype Embeddings
Authors: Nitesh Kumar and Usashi Chatterjee and Steven Schockaert
Categories: cs.CL
\\
  Conceptual spaces represent entities and concepts using cognitively
meaningful dimensions, typically referring to perceptual features. Such
representations are widely used in cognitive science and have the potential to
serve as a cornerstone for explainable AI. Unfortunately, they have proven
notoriously difficult to learn, although recent LLMs appear to capture the
required perceptual features to a remarkable extent. Nonetheless, practical
methods for extracting the corresponding conceptual spaces are currently still
lacking. While various methods exist for extracting embeddings from LLMs,
extracting conceptual spaces also requires us to encode the underlying
features. In this paper, we propose a strategy in which features (e.g.
sweetness) are encoded by embedding the description of a corresponding
prototype (e.g. a very sweet food). To improve this strategy, we fine-tune the
LLM to align the prototype embeddings with the corresponding conceptual space
dimensions. Our empirical analysis finds this approach to be highly effective.
\\ ( https://arxiv.org/abs/2509.19269 ,  8867kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19270
Date: Tue, 23 Sep 2025 17:33:57 GMT   (906kb)

Title: SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data
Authors: Erik Bo\v{z}\'ik and Marek \v{S}uppa
Categories: cs.CL cs.AI cs.SD
\\
  Automatic Speech Recognition (ASR) for low-resource languages like Slovak is
hindered by the scarcity of training data. To address this, we introduce
SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of
speech from parliamentary proceedings. We developed a robust processing
pipeline to align and segment long-form recordings into clean, 30-second
audio-transcript pairs suitable for model training. We use this dataset to
fine-tune several OpenAI Whisper models (small, medium, large-v3, and
large-v3-turbo), achieving significant Word Error Rate (WER) reductions on
standard Slovak benchmarks like Common Voice and FLEURS. For instance, the
fine-tuned Whisper-small model's WER dropped by up to 70\%, approaching the
baseline performance of the much larger Whisper-large-v3 model. To foster
future research in low-resource speech recognition, we publicly release the
complete SloPalSpeech dataset, the fully segmented transcripts (60 million
words), and all our fine-tuned models.
\\ ( https://arxiv.org/abs/2509.19270 ,  906kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19271
Date: Tue, 23 Sep 2025 17:34:10 GMT   (1094kb)

Title: WolBanking77: Wolof Banking Speech Intent Classification Dataset
Authors: Abdou Karim Kandji and Fr\'ed\'eric Precioso and Cheikh Ba and Samba
  Ndiaye and Augustin Ndione
Categories: cs.CL cs.AI cs.LG
Comments: 10 pages, 7 figures
\\
  Intent classification models have made a lot of progress in recent years.
However, previous studies primarily focus on high-resource languages datasets,
which results in a gap for low-resource languages and for regions with a high
rate of illiterate people where languages are more spoken than read or written.
This is the case in Senegal, for example, where Wolof is spoken by around 90\%
of the population, with an illiteracy rate of 42\% for the country. Wolof is
actually spoken by more than 10 million people in West African region. To
tackle such limitations, we release a Wolof Intent Classification Dataset
(WolBanking77), for academic research in intent classification. WolBanking77
currently contains 9,791 text sentences in the banking domain and more than 4
hours of spoken sentences. Experiments on various baselines are conducted in
this work, including text and voice state-of-the-art models. The results are
very promising on this current dataset. This paper also provides detailed
analyses of the contents of the data. We report baseline f1-score and word
error rate metrics respectively on NLP and ASR models trained on WolBanking77
dataset and also comparisons between models. We plan to share and conduct
dataset maintenance, updates and to release open-source code.
\\ ( https://arxiv.org/abs/2509.19271 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19274
Date: Tue, 23 Sep 2025 17:40:43 GMT   (27090kb)

Title: DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language
  Models' Understanding on Indian Culture
Authors: Arijit Maji, Raghvendra Kumar, Akash Ghosh, Anushka, Nemil Shah,
  Abhilekh Borah, Vanshika Shah, Nishant Mishra, Sriparna Saha
Categories: cs.CL cs.MM
Comments: EMNLP MAINS 2025
\\
  We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual
benchmark centered exclusively on Indian culture, designed to evaluate the
cultural understanding of generative AI systems. Unlike existing benchmarks
with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage
across India's diverse regions, spanning 15 languages, covering all states and
union territories, and incorporating over 64,000 aligned text-image pairs. The
dataset captures rich cultural themes including festivals, attire, cuisines,
art forms, and historical heritage amongst many more. We evaluate a wide range
of vision-language models (VLMs), including open-source small and large models,
proprietary systems, reasoning-specialized VLMs, and Indic-focused models,
across zero-shot and chain-of-thought settings. Our results expose key
limitations in current models' ability to reason over culturally grounded,
multimodal inputs, particularly for low-resource languages and less-documented
traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a
robust testbed to advance culturally aware, multimodally competent language
technologies.
\\ ( https://arxiv.org/abs/2509.19274 ,  27090kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18159
Date: Wed, 17 Sep 2025 02:57:33 GMT   (2760kb)

Title: PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal
  Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization
  on the Kvasir Dataset
Authors: Akwasi Asare, Ulas Bagci
Categories: cs.CV cs.LG
\\
  Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.
\\ ( https://arxiv.org/abs/2509.18159 ,  2760kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18160
Date: Wed, 17 Sep 2025 03:10:23 GMT   (4524kb)

Title: PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology
  Application for Diabetic Retinopathy Diagnosis
Authors: Akwasi Asare, Isaac Baffour Senkyire, Emmanuel Freeman, Simon Hilary
  Ayinedenaba Aluze-Ele, and Kelvin Kwao
Categories: cs.CV
\\
  Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.
\\ ( https://arxiv.org/abs/2509.18160 ,  4524kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18165
Date: Wed, 17 Sep 2025 07:52:20 GMT   (579kb)

Title: Self Identity Mapping
Authors: Xiuding Cai, Yaoyao Zhu, Linjie Fu, Dong Miao and Yu Yao
Categories: cs.CV cs.LG
Comments: Early accepted by Neural Networks 2025
\\
  Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.
\\ ( https://arxiv.org/abs/2509.18165 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18170
Date: Wed, 17 Sep 2025 11:32:15 GMT   (5412kb)

Title: MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients
  for Label-Inference-Free Gradient Inversion
Authors: Zhanting Zhou and Jinbo Wang and Zeqin Wu and Fengli Zhang
Categories: cs.CV
\\
  We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.
\\ ( https://arxiv.org/abs/2509.18170 ,  5412kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18174
Date: Wed, 17 Sep 2025 15:07:29 GMT   (6728kb)

Title: Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR
Authors: Khalil Hennara, Muhammad Hreden, Mohamed Motasim Hamed, Ahmad Bastati,
  Zeina Aldallal, Sara Chrouf, Safwan AlModhayan
Categories: cs.CV cs.CL
\\
  Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.
\\ ( https://arxiv.org/abs/2509.18174 ,  6728kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18176
Date: Wed, 17 Sep 2025 17:10:18 GMT   (2376kb)

Title: A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground
  Deformation in Eastern Ireland
Authors: Wendong Yao, Saeed Azadnejad, Binhua Huang, Shane Donohue and
  Soumyabrata Dev
Categories: cs.CV cs.LG
Comments: This paper is submitted to IEEE Transactions on Geoscience and Remote
  Sensing
\\
  Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.
\\ ( https://arxiv.org/abs/2509.18176 ,  2376kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18177
Date: Wed, 17 Sep 2025 18:37:24 GMT   (9111kb)

Title: A Framework for Generating Artificial Datasets to Validate Absolute and
  Relative Position Concepts
Authors: George Corr\^ea de Ara\'ujo, Helena de Almeida Maia, Helio Pedrini
Categories: cs.CV cs.AI cs.LG
Comments: WIP
\\
  In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.
\\ ( https://arxiv.org/abs/2509.18177 ,  9111kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18179
Date: Thu, 18 Sep 2025 01:48:51 GMT   (6800kb)

Title: The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image
  Generation Outcomes
Authors: Sai Varun Kodathala and Rakesh Vunnam
Categories: cs.CV cs.AI
Comments: 13 pages, 7 Figures
\\
  With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.
\\ ( https://arxiv.org/abs/2509.18179 ,  6800kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18182
Date: Thu, 18 Sep 2025 02:12:50 GMT   (484kb)

Title: AI-Derived Structural Building Intelligence for Urban Resilience: An
  Application in Saint Vincent and the Grenadines
Authors: Isabelle Tingzon, Yoji Toriumi, Caroline Gevaert
Categories: cs.CV cs.LG eess.IV
Comments: Accepted at the 2nd Workshop on Computer Vision for Developing
  Countries (CV4DC) at ICCV 2025
\\
  Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.
\\ ( https://arxiv.org/abs/2509.18182 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18183
Date: Thu, 18 Sep 2025 05:24:39 GMT   (4465kb)

Title: VLA-LPAF: Lightweight Perspective-Adaptive Fusion for
  Vision-Language-Action to Enable More Unconstrained Robotic Manipulation
Authors: Jinyue Bian, Zhaoxing Zhang, Zhengyu Liang, Shiwei Zheng, Shengtao
  Zhang, Rong Shen, Chen Yang, Anzhou Hou
Categories: cs.CV cs.AI
\\
  The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.
\\ ( https://arxiv.org/abs/2509.18183 ,  4465kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18184
Date: Thu, 18 Sep 2025 07:24:50 GMT   (1848kb)

Title: URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth
  Estimation
Authors: Yifeng Cheng and Alois Knoll and Hu Cao
Categories: cs.CV
Comments: This work is accepted by Visual Intelligence Journal
\\
  Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.
\\ ( https://arxiv.org/abs/2509.18184 ,  1848kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18185
Date: Thu, 18 Sep 2025 11:08:28 GMT   (5927kb)

Title: Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous
  System Recognition Applied to Endometriosis Cases
Authors: Giammarco La Barbera, Enzo Bonnot, Thomas Isla, Juan Pablo de la
  Plata, Joy-Rose Dunoyer de Segonzac, Jennifer Attali, C\'ecile Lozach,
  Alexandre Bellucci, Louis Marcellin, Laure Fournier, Sabine Sarnacki, Pietro
  Gori, Isabelle Bloch
Categories: cs.CV cs.AI
Comments: Computer-Aided Pelvic Imaging for Female Health (CAPI) - Workshop
  MICCAI 2025
\\
  Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.
\\ ( https://arxiv.org/abs/2509.18185 ,  5927kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18187
Date: Thu, 18 Sep 2025 21:55:14 GMT   (2158kb)

Title: V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor
  Fusion Framework for Road Safety & Driver Behaviour Modelling
Authors: Muhammad Naveed, Nazia Perwaiz, Sidra Sultana, Mohaira Ahmad, Muhammad
  Moazam Fraz
Categories: cs.CV cs.AI
\\
  Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.
\\ ( https://arxiv.org/abs/2509.18187 ,  2158kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18189
Date: Fri, 19 Sep 2025 07:05:23 GMT   (5299kb)

Title: Qianfan-VL: Domain-Enhanced Universal Vision-Language Models
Authors: Daxiang Dong, Mingming Zheng, Dong Xu, Bairong Zhuang, Wenyu Zhang,
  Chunhua Luo, Haoran Wang, Zijian Zhao, Jie Li, Yuxuan Li, Hanjun Zhong,
  Mengyue Liu, Jieting Chen, Shupeng Li, Lun Tian, Yaping Feng, Xin Li,
  Donggang Jiang, Yong Chen, Yehua Xu, Duohao Qin, Chen Feng, Dan Wang, Henghua
  Zhang, Jingjing Ha, Jinhui He, Yanfeng Zhai, Chengxin Zheng, Jiayi Mao,
  Jiacheng Chen, Ruchang Yao, Ziye Yuan, Jianmin Wu, Guangjun Xie, Dou Shen
Categories: cs.CV cs.AI
Comments: 12 pages
\\
  We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.
\\ ( https://arxiv.org/abs/2509.18189 ,  5299kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18190
Date: Fri, 19 Sep 2025 07:50:09 GMT   (1988kb)

Title: HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze
  Generation for Real-World Dehazing
Authors: Junseong Shin, Seungwoo Chung, Yunjeong Yang, Tae Hyun Kim
Categories: cs.CV cs.AI
\\
  Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.
\\ ( https://arxiv.org/abs/2509.18190 ,  1988kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18193
Date: Fri, 19 Sep 2025 13:57:12 GMT   (1869kb)

Title: TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed
  Detection
Authors: Omar H. Khater, Abdul Jabbar Siddiqui, Aiman El-Maleh, M. Shamim
  Hossain
Categories: cs.CV cs.AI
\\
  Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.
\\ ( https://arxiv.org/abs/2509.18193 ,  1869kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18284
Date: Mon, 22 Sep 2025 18:12:12 GMT   (145kb)

Title: Learning Contrastive Multimodal Fusion with Improved Modality Dropout
  for Disease Detection and Prediction
Authors: Yi Gu, Kuniaki Saito, Jiaxin Ma
Categories: cs.CV
Comments: MICCAI 2025
\\
  As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.
\\ ( https://arxiv.org/abs/2509.18284 ,  145kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18308
Date: Mon, 22 Sep 2025 18:34:30 GMT   (4503kb)

Title: Rethinking Pulmonary Embolism Segmentation: A Study of Current
  Approaches and Challenges with an Open Weight Model
Authors: Yixin Zhang, Ryan Chamberlain, Lawrance Ngo, Kevin Kramer, Maciej A.
  Mazurowski
Categories: cs.CV
Comments: submitted to WACV 2026 application track, model weights available at:
  https://github.com/mazurowski-lab/PulmonaryEmbolismSegmentation
\\
  In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.
\\ ( https://arxiv.org/abs/2509.18308 ,  4503kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18309
Date: Mon, 22 Sep 2025 18:35:16 GMT   (1102kb)

Title: Improving Handshape Representations for Sign Language Processing: A
  Graph Neural Network Approach
Authors: Alessa Carbo, Eric Nalisnick
Categories: cs.CV cs.LG
ACM-class: I.2.10
\\
  Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).
\\ ( https://arxiv.org/abs/2509.18309 ,  1102kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18326
Date: Mon, 22 Sep 2025 18:49:25 GMT   (72kb)

Title: Influence of Classification Task and Distribution Shift Type on OOD
  Detection in Fetal Ultrasound
Authors: Chun Kit Wong and Anders N. Christensen and Cosmin I. Bercea and Julia
  A. Schnabel and Martin G. Tolsgaard and Aasa Feragen
Categories: cs.CV
Comments: MICCAI 2025
\\
  Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.
\\ ( https://arxiv.org/abs/2509.18326 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18350
Date: Mon, 22 Sep 2025 19:22:32 GMT   (33838kb)

Title: OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic
  Geodata
Authors: Oussema Dhaouadi, Riccardo Marin, Johannes Meier, Jacques Kaiser,
  Daniel Cremers
Categories: cs.CV cs.RO
Comments: Accepted at NeurIPS 2025
\\
  Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.
\\ ( https://arxiv.org/abs/2509.18350 ,  33838kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18354
Date: Mon, 22 Sep 2025 19:29:20 GMT   (9844kb)

Title: A Single Image Is All You Need: Zero-Shot Anomaly Localization Without
  Training Data
Authors: Mehrdad Moradi, Shengzhe Chen, Hao Yan, Kamran Paynabar
Categories: cs.CV cs.AI cs.LG eess.IV
Comments: 12 pages, 10 figures, 1 table. Preprint submitted to a CVF conference
MSC-class: 62H35, 68T07, 62M40, 68T45
ACM-class: I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4
\\
  Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet
\\ ( https://arxiv.org/abs/2509.18354 ,  9844kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18369
Date: Mon, 22 Sep 2025 19:49:35 GMT   (884kb)

Title: Align Where the Words Look: Cross-Attention-Guided Patch Alignment with
  Contrastive and Transport Regularization for Bengali Captioning
Authors: Riad Ahmed Anonto, Sardar Md. Saffat Zabin, M. Saifur Rahman
Categories: cs.CV cs.AI
\\
  Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.
\\ ( https://arxiv.org/abs/2509.18369 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18372
Date: Mon, 22 Sep 2025 19:54:02 GMT   (1908kb)

Title: TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task
  Bird's Eye View Perception and Planning
Authors: Reeshad Khan and John Gauch
Categories: cs.CV
\\
  We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.
\\ ( https://arxiv.org/abs/2509.18372 ,  1908kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18387
Date: Mon, 22 Sep 2025 20:16:50 GMT   (4276kb)

Title: BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball
  Tracking
Authors: Thomas Gossard, Filip Radovic, Andreas Ziegler, Andrea Zell
Categories: cs.CV
\\
  Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.
\\ ( https://arxiv.org/abs/2509.18387 ,  4276kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18388
Date: Mon, 22 Sep 2025 20:18:27 GMT   (2313kb)

Title: MVP: Motion Vector Propagation for Zero-Shot Video Object Detection
Authors: Binhua Huang, Ni Wang, Wendong Yao, Soumyabrata Dev
Categories: cs.CV
Comments: 5 pages, 1 figure
\\
  Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.
\\ ( https://arxiv.org/abs/2509.18388 ,  2313kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18390
Date: Mon, 22 Sep 2025 20:23:33 GMT   (10762kb)

Title: Improving the color accuracy of lighting estimation models
Authors: Zitian Zhang, Joshua Urban Davis, Jeanne Phuong Anh Vu, Jiangtao
  Kuang, Jean-Fran\c{c}ois Lalonde
Categories: cs.CV
Comments: Project page: https://lvsn.github.io/coloraccuracy
\\
  Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.
\\ ( https://arxiv.org/abs/2509.18390 ,  10762kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18405
Date: Mon, 22 Sep 2025 20:43:59 GMT   (5873kb)

Title: Check Field Detection Agent (CFD-Agent) using Multimodal Large Language
  and Vision Language Models
Authors: Sourav Halder, Jinjun Tong, Xinyu Wu
Categories: cs.CV cs.AI
Comments: 12 pages, 5 figures, 2 tables
\\
  Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.
\\ ( https://arxiv.org/abs/2509.18405 ,  5873kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18425
Date: Mon, 22 Sep 2025 21:12:20 GMT   (5974kb)

Title: Losing the Plot: How VLM responses degrade on imperfect charts
Authors: Philip Wootaek Shin, Jack Sampson, Vijaykrishnan Narayanan, Andres
  Marquez, Mahantesh Halappanavar
Categories: cs.CV
\\
  Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.
\\ ( https://arxiv.org/abs/2509.18425 ,  5974kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18427
Date: Mon, 22 Sep 2025 21:18:26 GMT   (4063kb)

Title: CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI
  Reconstruction
Authors: Xinyang Wu, Muheng Li, Xia Li, Orso Pusterla, Sairos Safai, Philippe
  C. Cattin, Antony J. Lomax, Ye Zhang
Categories: cs.CV physics.med-ph
\\
  Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.
\\ ( https://arxiv.org/abs/2509.18427 ,  4063kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18451
Date: Mon, 22 Sep 2025 22:12:48 GMT   (6550kb)

Title: An Analysis of Kalman Filter based Object Tracking Methods for
  Fast-Moving Tiny Objects
Authors: Prithvi Raj Singh, Raju Gottumukkala, Anthony Maida
Categories: cs.CV
\\
  Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.
\\ ( https://arxiv.org/abs/2509.18451 ,  6550kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18473
Date: Mon, 22 Sep 2025 23:23:04 GMT   (2893kb)

Title: MoCrop: Training Free Motion Guided Cropping for Efficient Video Action
  Recognition
Authors: Binhua Huang, Wendong Yao, Shaowu Chen, Guoxin Wang, Qingyuan Wang,
  Soumyabrata Dev
Categories: cs.CV
Comments: 5 pages, 2 figures
\\
  We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.
\\ ( https://arxiv.org/abs/2509.18473 ,  2893kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18481
Date: Tue, 23 Sep 2025 00:34:12 GMT   (149kb)

Title: Codebook-Based Adaptive Feature Compression With Semantic Enhancement
  for Edge-Cloud Systems
Authors: Xinyu Wang, Zikun Zhou, Yingjian Li, Xin An, Hongpeng Wang
Categories: cs.CV
\\
  Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.
\\ ( https://arxiv.org/abs/2509.18481 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18493
Date: Tue, 23 Sep 2025 00:54:40 GMT   (852kb)

Title: MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation
Authors: Md Mostafijur Rahman and Radu Marculescu
Categories: cs.CV
Comments: 11 pages, 3 figures, Accepted at ICCV 2025 Workshop CVAMD
\\
  In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.
\\ ( https://arxiv.org/abs/2509.18493 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18501
Date: Tue, 23 Sep 2025 01:09:36 GMT   (14769kb)

Title: BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting
  for Deformable Intraoperative Surgical Navigation
Authors: Maximilian Fehrentz, Alexander Winkler, Thomas Heiliger, Nazim
  Haouchine, Christian Heiliger, Nassir Navab
Categories: cs.CV
Comments: Accepted at MICCAI 2025
DOI: 10.1007/978-3-032-05141-7_5
\\
  We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .
\\ ( https://arxiv.org/abs/2509.18501 ,  14769kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18502
Date: Tue, 23 Sep 2025 01:10:01 GMT   (8851kb)

Title: Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing
  Images with Diffusion-Guided Label Enrichment
Authors: Wenjie Liu, Hongmin Liu, Lixin Zhang, Bin Fan
Categories: cs.CV
\\
  Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.
\\ ( https://arxiv.org/abs/2509.18502 ,  8851kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18504
Date: Tue, 23 Sep 2025 01:12:21 GMT   (1104kb)

Title: Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning
Authors: Jiaxin Dai, Xiang Xiang
Categories: cs.CV cs.AI cs.LG stat.ML
\\
  In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.
\\ ( https://arxiv.org/abs/2509.18504 ,  1104kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18538
Date: Tue, 23 Sep 2025 02:04:19 GMT   (20086kb)

Title: GeoRemover: Removing Objects and Their Causal Visual Artifacts
Authors: Zixin Zhu, Haoxiang Li, Xuelu Feng, He Wu, Chunming Qiao, Junsong Yuan
Categories: cs.CV
Comments: Accepted as Spotlight at NeurIPS 2025
\\
  Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.
\\ ( https://arxiv.org/abs/2509.18538 ,  20086kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18546
Date: Tue, 23 Sep 2025 02:10:42 GMT   (973kb)

Title: SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against
  No-Reference Image Quality Assessment Models
Authors: Yujia Liu, Dingquan Li, Tiejun Huang
Categories: cs.CV
\\
  No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.
\\ ( https://arxiv.org/abs/2509.18546 ,  973kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18550
Date: Tue, 23 Sep 2025 02:20:43 GMT   (1374kb)

Title: HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features
  for enhancing facial emotion recognition of genuine smiles
Authors: Mohammad Junayed Hasan, Nabeel Mohammed, Shafin Rahman and Philipp
  Koehn
Categories: cs.CV
Comments: Accepted to IEEE International Conference on Data Mining (ICDM) 2025.
  Final version to appear in the conference proceedings
\\
  The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.
\\ ( https://arxiv.org/abs/2509.18550 ,  1374kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18566
Date: Tue, 23 Sep 2025 02:50:56 GMT   (2230kb)

Title: Event-guided 3D Gaussian Splatting for Dynamic Human and Scene
  Reconstruction
Authors: Xiaoting Yin, Hao Shi, Kailun Yang, Jiajun Zhai, Shangwei Guo, Lin
  Wang, Kaiwei Wang
Categories: cs.CV cs.RO eess.IV
\\
  Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.
\\ ( https://arxiv.org/abs/2509.18566 ,  2230kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18571
Date: Tue, 23 Sep 2025 02:53:43 GMT   (2064kb)

Title: Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event
  Reasoning and Chain-of-Thought
Authors: Yuhan Wang, Cheng Liu, Zihan Zhao, Weichao Wu
Categories: cs.CV
\\
  Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.
\\ ( https://arxiv.org/abs/2509.18571 ,  2064kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18582
Date: Tue, 23 Sep 2025 02:59:41 GMT   (12779kb)

Title: The Photographer Eye: Teaching Multimodal Large Language Models to See
  and Critique like Photographers
Authors: Daiqing Qi, Handong Zhao, Jing Shi, Simon Jenni, Yifei Fan, Franck
  Dernoncourt, Scott Cohen, Sheng Li
Categories: cs.CV
Journal-ref: CVPR 2025
\\
  While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.
\\ ( https://arxiv.org/abs/2509.18582 ,  12779kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18591
Date: Tue, 23 Sep 2025 03:22:06 GMT   (24kb)

Title: Enhancing Video Object Segmentation in TrackRAD Using XMem Memory
  Network
Authors: Pengchao Deng and Shengqi Chen
Categories: cs.CV
\\
  This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.
\\ ( https://arxiv.org/abs/2509.18591 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18593
Date: Tue, 23 Sep 2025 03:24:32 GMT   (509kb)

Title: SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI
  Super-Resolution
Authors: Xiaoman Wu, Lubin Gan, Siying Wu, Jing Zhang, Yunwei Ou, Xiaoyan Sun
Categories: cs.CV
\\
  Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.
\\ ( https://arxiv.org/abs/2509.18593 ,  509kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18600
Date: Tue, 23 Sep 2025 03:42:26 GMT   (6849kb)

Title: OraPO: Oracle-educated Reinforcement Learning for Data-efficient and
  Factual Radiology Report Generation
Authors: Zhuoxiao Chen, Hongyang Yu, Ying Xu, Yadan Luo, Long Duong, Yuan-Fang
  Li
Categories: cs.CV cs.AI cs.CL
\\
  Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.
\\ ( https://arxiv.org/abs/2509.18600 ,  6849kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18602
Date: Tue, 23 Sep 2025 03:47:59 GMT   (27542kb)

Title: Training-Free Multi-Style Fusion Through Reference-Based Adaptive
  Modulation
Authors: Xu Liu and Yibo Lu and Xinxian Wang and Xinyu Wu
Categories: cs.CV
Comments: Accepted at ACPR 2025 (oral)
\\
  We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.
\\ ( https://arxiv.org/abs/2509.18602 ,  27542kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18613
Date: Tue, 23 Sep 2025 04:02:28 GMT   (15584kb)

Title: MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object
  Detection in Autonomous Driving
Authors: Yuzhi Wu, Li Xiao, Jun Liu, Guangfeng Jiang, XiangGen Xia
Categories: cs.CV
\\
  The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.
\\ ( https://arxiv.org/abs/2509.18613 ,  15584kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18619
Date: Tue, 23 Sep 2025 04:11:06 GMT   (9885kb)

Title: Prompt-Guided Dual Latent Steering for Inversion Problems
Authors: Yichen Wu and Xu Liu and Chenxuan Zhao and Xinyu Wu
Categories: cs.CV
Comments: Accepted at DICTA 2025 (oral)
\\
  Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.
\\ ( https://arxiv.org/abs/2509.18619 ,  9885kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18638
Date: Tue, 23 Sep 2025 04:49:59 GMT   (20893kb)

Title: Learning neuroimaging models from health system-scale data
Authors: Yiwei Lyu, Samir Harake, Asadur Chowdury, Soumyanil Banerjee, Rachel
  Gologorsky, Shixuan Liu, Anna-Katharina Meissner, Akshay Rao, Chenhui Zhao,
  Akhil Kondepudi, Cheng Jiang, Xinhai Hou, Rushikesh S. Joshi, Volker
  Neuschmelting, Ashok Srinivasan, Dawn Kleindorfer, Brian Athey, Vikas Gulani,
  Aditya Pandey, Honglak Lee, Todd Hollon
Categories: cs.CV cs.AI
\\
  Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.
\\ ( https://arxiv.org/abs/2509.18638 ,  20893kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18639
Date: Tue, 23 Sep 2025 04:52:39 GMT   (8920kb)

Title: Understanding-in-Generation: Reinforcing Generative Capability of
  Unified Model via Infusing Understanding into Generation
Authors: Yuanhuiyi Lyu, Chi Kit Wong, Chenfei Liao, Lutao Jiang, Xu Zheng,
  Zexin Lu, Linfeng Zhang, Xuming Hu
Categories: cs.CV
\\
  Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG
\\ ( https://arxiv.org/abs/2509.18639 ,  8920kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18642
Date: Tue, 23 Sep 2025 04:56:25 GMT   (5337kb)

Title: Zero-shot Monocular Metric Depth for Endoscopic Images
Authors: Nicolas Toussaint, Emanuele Colleoni, Ricardo Sanchez-Matilla, Joshua
  Sutcliffe, Vanessa Thompson, Muhammad Asad, Imanol Luengo, Danail Stoyanov
Categories: cs.CV
Comments: Accepted at MICCAI 2025 DEMI Workshop
\\
  Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.
\\ ( https://arxiv.org/abs/2509.18642 ,  5337kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18683
Date: Tue, 23 Sep 2025 06:08:17 GMT   (15174kb)

Title: LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for
  RGB-D Salient Object Detection
Authors: Lanhu Wu, Zilin Gao, Hao Fei, Mong-Li Lee, Wynne Hsu
Categories: cs.CV cs.AI cs.MM
Comments: Accepted to ACM MM 2025
DOI: 10.1145/3746027.3754863
\\
  RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.
\\ ( https://arxiv.org/abs/2509.18683 ,  15174kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18692
Date: Tue, 23 Sep 2025 06:23:50 GMT   (1314kb)

Title: Lightweight Vision Transformer with Window and Spatial Attention for
  Food Image Classification
Authors: Xinle Gao and Linghui Ye and Zhiyong Xiao
Categories: cs.CV
\\
  With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.
\\ ( https://arxiv.org/abs/2509.18692 ,  1314kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18693
Date: Tue, 23 Sep 2025 06:23:56 GMT   (7617kb)

Title: OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of
  Land-cover in Remote Sensing Imagery
Authors: Siyi Chen, Kai Wang, Weicong Pang, Ruiming Yang, Ziru Chen, Renjun
  Gao, Alexis Kai Hon Lau, Dasa Gu, Chenchen Zhang, Cheng Li
Categories: cs.CV
Comments: Project is available at
  https://anonymous.4open.science/r/openset_remotesensing_tagging-2B5F/README.md
\\
  Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.
\\ ( https://arxiv.org/abs/2509.18693 ,  7617kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18697
Date: Tue, 23 Sep 2025 06:26:24 GMT   (2685kb)

Title: Overview of PlantCLEF 2021: cross-domain plant identification
Authors: Herve Goeau, Pierre Bonnet, Alexis Joly
Categories: cs.CV
Comments: 15 pages, 6 figures, CLEF 2021 Conference and Labs of the Evaluation
  Forum, September 21 to 24, 2021, Bucharest, Romania
\\
  Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.
\\ ( https://arxiv.org/abs/2509.18697 ,  2685kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18699
Date: Tue, 23 Sep 2025 06:32:14 GMT   (2768kb)

Title: AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive
  Group Swapping
Authors: Zedong Zhang, Ying Tai, Jianjun Qian, Jian Yang, Jun Li
Categories: cs.CV
DOI: 10.1145/3757377.3763944
\\
  Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.
\\ ( https://arxiv.org/abs/2509.18699 ,  2768kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18705
Date: Tue, 23 Sep 2025 06:42:30 GMT   (462kb)

Title: Overview of LifeCLEF Plant Identification task 2019: diving into data
  deficient tropical countries
Authors: Herve Goeau, Pierre Bonnet, Alexis Joly
Categories: cs.CV
Comments: 13 pages, 5 figures, CLEF 2019 Conference and Labs of the Evaluation
  Forum, September 09 to 12, 2019, Lugano, Switzerland
\\
  Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.
\\ ( https://arxiv.org/abs/2509.18705 ,  462kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18711
Date: Tue, 23 Sep 2025 06:52:15 GMT   (9600kb)

Title: RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot
  Open-Vocabulary Visual Grounding in Remote Sensing Images
Authors: Ke Li, Di Wang, Ting Wang, Fuyu Dong, Yiming Zhang, Luyao Zhang,
  Xiangyu Wang, Shaofeng Li, Quan Wang
Categories: cs.CV cs.AI
\\
  Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.
\\ ( https://arxiv.org/abs/2509.18711 ,  9600kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18715
Date: Tue, 23 Sep 2025 07:03:08 GMT   (16794kb)

Title: What Makes You Unique? Attribute Prompt Composition for Object
  Re-Identification
Authors: Yingquan Wang and Pingping Zhang and Chong Sun and Dong Wang and
  Huchuan Lu
Categories: cs.CV
Comments: Accepted by TCSVT2025
\\
  Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.
\\ ( https://arxiv.org/abs/2509.18715 ,  16794kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18717
Date: Tue, 23 Sep 2025 07:05:43 GMT   (757kb)

Title: Pre-training CLIP against Data Poisoning with Optimal Transport-based
  Matching and Alignment
Authors: Tong Zhang, Kuofeng Gao, Jiawang Bai, Leo Yu Zhang, Xin Yin, Zonghui
  Wang, Shouling Ji, Wenzhi Chen
Categories: cs.CV cs.MM
\\
  Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.
\\ ( https://arxiv.org/abs/2509.18717 ,  757kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18733
Date: Tue, 23 Sep 2025 07:27:36 GMT   (26792kb)

Title: Knowledge Transfer from Interaction Learning
Authors: Yilin Gao, Kangyi Chen, Zhongxing Peng, Hengjie Lu, Shugong Xu
Categories: cs.CV
Comments: Accepted by ICCV2025
\\
  Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.
\\ ( https://arxiv.org/abs/2509.18733 ,  26792kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18738
Date: Tue, 23 Sep 2025 07:32:11 GMT   (10506kb)

Title: HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal
  Salient Object Detection
Authors: Ruichao Hou, Xingyuan Li, Tongwei Ren, Dongming Zhou, Gangshan Wu and
  Jinde Cao
Categories: cs.CV
\\
  RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.
\\ ( https://arxiv.org/abs/2509.18738 ,  10506kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18743
Date: Tue, 23 Sep 2025 07:37:28 GMT   (18475kb)

Title: TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point
  Cloud Processing
Authors: Susmit Neogi
Categories: cs.CV
Comments: 39th Conference on Neural Information Processing Systems (NeurIPS
  2025) Workshop
\\
  LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.
\\ ( https://arxiv.org/abs/2509.18743 ,  18475kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18754
Date: Tue, 23 Sep 2025 07:49:30 GMT   (23387kb)

Title: COLT: Enhancing Video Large Language Models with Continual Tool Usage
Authors: Yuyang Liu, Xinyuan Shi, Bang Yang, Peilin Zhou, Jiahua Dong, Long
  Chen, Ian Reid, Xiaondan Liang
Categories: cs.CV cs.AI
Comments: 16 pages
\\
  The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.
\\ ( https://arxiv.org/abs/2509.18754 ,  23387kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18759
Date: Tue, 23 Sep 2025 07:53:46 GMT   (13543kb)

Title: FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score
  Distillation
Authors: Zhaorui Wang, Yi Gu, Deming Zhou, Renjing Xu
Categories: cs.CV
\\
  Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.
\\ ( https://arxiv.org/abs/2509.18759 ,  13543kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18763
Date: Tue, 23 Sep 2025 07:55:48 GMT   (1515kb)

Title: Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization
  Boundaries in Vision-Language Models
Authors: Xijun Wang, Junyun Huang, Rayyan Abdalla, Chengyuan Zhang, Ruiqi Xian,
  Dinesh Manocha
Categories: cs.CV
\\
  We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.
\\ ( https://arxiv.org/abs/2509.18763 ,  1515kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18765
Date: Tue, 23 Sep 2025 07:58:21 GMT   (19100kb)

Title: DiSSECT: Structuring Transfer-Ready Medical Image Representations
  through Discrete Self-Supervision
Authors: Azad Singh and Deepak Mishra
Categories: cs.CV cs.AI
\\
  Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.
\\ ( https://arxiv.org/abs/2509.18765 ,  19100kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18779
Date: Tue, 23 Sep 2025 08:16:25 GMT   (1229kb)

Title: Real-time Deer Detection and Warning in Connected Vehicles via Thermal
  Sensing and Deep Learning
Authors: Hemanth Puppala, Wayne Sarasua, Srinivas Biyaguda, Farhad Farzinpour,
  Mashrur Chowdhury
Categories: cs.CV cs.LG
Comments: Preprint under review in TRR, 20 pages, 9 figures, 4 tables
\\
  Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.
\\ ( https://arxiv.org/abs/2509.18779 ,  1229kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18796
Date: Tue, 23 Sep 2025 08:40:40 GMT   (5790kb)

Title: Towards Application Aligned Synthetic Surgical Image Synthesis
Authors: Danush Kumar Venkatesh, Stefanie Speidel
Categories: cs.CV
\\
  The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.
\\ ( https://arxiv.org/abs/2509.18796 ,  5790kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18801
Date: Tue, 23 Sep 2025 08:48:36 GMT   (12937kb)

Title: A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image
  Denoising
Authors: Kuang Xiaodong and Li Bingxuan and Li Yuan and Rao Fan and Ma Gege and
  Xie Qingguo and Mok Greta S P and Liu Huafeng and Zhu Wentao
Categories: cs.CV cs.AI
\\
  Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.
\\ ( https://arxiv.org/abs/2509.18801 ,  12937kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18802
Date: Tue, 23 Sep 2025 08:49:07 GMT   (4696kb)

Title: Surgical Video Understanding with Label Interpolation
Authors: Garam Kim, Tae Kyeong Jeong, and Juyoun Park
Categories: cs.CV
Comments: 8 pages, 10 figures
\\
  Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.
\\ ( https://arxiv.org/abs/2509.18802 ,  4696kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18824
Date: Tue, 23 Sep 2025 09:12:46 GMT   (9635kb)

Title: Hyper-Bagel: A Unified Acceleration Framework for Multimodal
  Understanding and Generation
Authors: Yanzuo Lu, Xin Xia, Manlin Zhang, Huafeng Kuang, Jianbin Zheng, Yuxi
  Ren, Xuefeng Xiao
Categories: cs.CV
Comments: Technical Report
\\
  Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.
\\ ( https://arxiv.org/abs/2509.18824 ,  9635kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18839
Date: Tue, 23 Sep 2025 09:23:31 GMT   (8423kb)

Title: Benchmarking Vision-Language and Multimodal Large Language Models in
  Zero-shot and Few-shot Scenarios: A study on Christian Iconography
Authors: Gianmarco Spinaci (1 and 2), Lukas Klic (2), Giovanni Colavizza (1 and
  3) ((1) Department of Classical Philology and Italian Studies, University of
  Bologna, Italy, (2) Villa i Tatti, The Harvard University Center for Italian
  Renaissance Studies, Florence, Italy, (3) Department of Communication,
  University of Copenhagen, Denmark)
Categories: cs.CV
Comments: 11 pages, 2 figures
\\
  This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.
\\ ( https://arxiv.org/abs/2509.18839 ,  8423kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18840
Date: Tue, 23 Sep 2025 09:25:22 GMT   (1041kb)

Title: ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized
  Graph Construction
Authors: Ismael Elsharkawi, Hossam Sharara, Ahmed Rafea
Categories: cs.CV
Comments: Under Review
ACM-class: I.2.10
\\
  Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.
\\ ( https://arxiv.org/abs/2509.18840 ,  1041kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18847
Date: Tue, 23 Sep 2025 09:35:49 GMT   (20kb)

Title: Failure Makes the Agent Stronger: Enhancing Accuracy through Structured
  Reflection for Reliable Tool Interactions
Authors: Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han,
  Junfeng Luo, Yurui Qiu
Categories: cs.CV cs.AI cs.CL
Comments: 9pages
\\
  Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.
\\ ( https://arxiv.org/abs/2509.18847 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18891
Date: Tue, 23 Sep 2025 10:59:24 GMT   (29221kb)

Title: Attack for Defense: Adversarial Agents for Point Prompt Optimization
  Empowering Segment Anything Model
Authors: Xueyu Liu, Xiaoyi Zhang, Guangze Shi, Meilin Liu, Yexin Lai, Yongfei
  Wu, Mingqiang Wei
Categories: cs.CV
\\
  Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.
\\ ( https://arxiv.org/abs/2509.18891 ,  29221kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18894
Date: Tue, 23 Sep 2025 11:07:18 GMT   (4156kb)

Title: SmartWilds: Multimodal Wildlife Monitoring Dataset
Authors: Jenna Kline, Anirudh Potlapally, Bharath Pillai, Tanishka Wani, Rugved
  Katole, Vedant Patil, Penelope Covey, Hari Subramoni, Tanya Berger-Wolf, and
  Christopher Stewart
Categories: cs.CV
Comments: 8 pages
\\
  We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.
\\ ( https://arxiv.org/abs/2509.18894 ,  4156kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18897
Date: Tue, 23 Sep 2025 11:20:51 GMT   (15347kb)

Title: RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote
  Sensing
Authors: Jiayu Wang, Ruizhi Wang, Jie Song, Haofei Zhang, Mingli Song, Zunlei
  Feng, Li Sun
Categories: cs.CV
Comments: 26 pages, 4 figures
\\
  In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.
\\ ( https://arxiv.org/abs/2509.18897 ,  15347kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18898
Date: Tue, 23 Sep 2025 11:21:54 GMT   (27958kb)

Title: DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust
  Deblurring
Authors: Pengteng Li and Yunfan Lu and Pinhao Song and Weiyu Guo and Huizai Yao
  and F. Richard Yu and Hui Xiong
Categories: cs.CV
\\
  In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.
\\ ( https://arxiv.org/abs/2509.18898 ,  27958kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18910
Date: Tue, 23 Sep 2025 12:33:23 GMT   (682kb)

Title: Moir\'eNet: A Compact Dual-Domain Network for Image Demoir\'eing
Authors: Shuwei Guo, Simin Luan, Yan Ke, Zeyd Boukhers, John See, Cong Yang
Categories: cs.CV
\\
  Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.
\\ ( https://arxiv.org/abs/2509.18910 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18912
Date: Tue, 23 Sep 2025 12:33:48 GMT   (6258kb)

Title: Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual
  Segmentation
Authors: Yunzhe Shen, Kai Peng, Leiye Liu, Wei Ji, Jingjing Li, Miao Zhang,
  Yongri Piao, Huchuan Lu
Categories: cs.CV
\\
  Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.
\\ ( https://arxiv.org/abs/2509.18912 ,  6258kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18913
Date: Tue, 23 Sep 2025 12:33:54 GMT   (3241kb)

Title: xAI-CV: An Overview of Explainable Artificial Intelligence in Computer
  Vision
Authors: Nguyen Van Tu, Pham Nguyen Hai Long, Vo Hoai Viet
Categories: cs.CV
\\
  Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.
\\ ( https://arxiv.org/abs/2509.18913 ,  3241kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18917
Date: Tue, 23 Sep 2025 12:35:07 GMT   (2456kb)

Title: LiDAR Point Cloud Image-based Generation Using Denoising Diffusion
  Probabilistic Models
Authors: Amirhesam Aghanouri, Cristina Olaverri-Monreal
Categories: cs.CV cs.AI
\\
  Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.
\\ ( https://arxiv.org/abs/2509.18917 ,  2456kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18919
Date: Tue, 23 Sep 2025 12:35:32 GMT   (15190kb)

Title: Advancing Metallic Surface Defect Detection via Anomaly-Guided
  Pretraining on a Large Industrial Dataset
Authors: Chuni Liu, Hongjie Li, Jiaqi Du, Yangyang Hou, Qian Sun, Lei Jin, Ke
  Xu
Categories: cs.CV
\\
  The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.
\\ ( https://arxiv.org/abs/2509.18919 ,  15190kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18924
Date: Tue, 23 Sep 2025 12:46:43 GMT   (32267kb)

Title: Audio-Driven Universal Gaussian Head Avatars
Authors: Kartik Teotia, Helge Rhodin, Mohit Mendiratta, Hyeongwoo Kim, Marc
  Habermann, Christian Theobalt
Categories: cs.CV
Comments: (SIGGRAPH Asia 2025) Project page:
  https://kartik-teotia.github.io/UniGAHA/
\\
  We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.
\\ ( https://arxiv.org/abs/2509.18924 ,  32267kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18926
Date: Tue, 23 Sep 2025 12:47:43 GMT   (2435kb)

Title: SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic
  Spines
Authors: Pamela Osuna-Vargas, Altug Kamacioglu, Dominik F. Aschauer, Petros E.
  Vlachos, Sercan Alipek, Jochen Triesch, Simon Rumpel, Matthias Kaschube
Categories: cs.CV
\\
  Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.
\\ ( https://arxiv.org/abs/2509.18926 ,  2435kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18938
Date: Tue, 23 Sep 2025 12:54:52 GMT   (221kb)

Title: No Labels Needed: Zero-Shot Image Classification with Collaborative
  Self-Learning
Authors: Matheus Vin\'icius Todescato and Joel Lu\'is Carbonera
Categories: cs.CV cs.AI cs.LG
Comments: This paper was accepted at International Conference on Tools with
  Artificial Intelligence (ICTAI) 2025
\\
  While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.
\\ ( https://arxiv.org/abs/2509.18938 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18956
Date: Tue, 23 Sep 2025 13:06:00 GMT   (5545kb)

Title: Seeing Through Reflections: Advancing 3D Scene Reconstruction in
  Mirror-Containing Environments with Gaussian Splatting
Authors: Zijing Guo, Yunyang Zhao, Lin Wang
Categories: cs.CV
\\
  Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.
\\ ( https://arxiv.org/abs/2509.18956 ,  5545kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18958
Date: Tue, 23 Sep 2025 13:11:53 GMT   (9060kb)

Title: Generative data augmentation for biliary tract detection on
  intraoperative images
Authors: Cristina Iacono, Mariarosaria Meola, Federica Conte, Laura Mecozzi,
  Umberto Bracale, Pietro Falco, Fanny Ficuciello
Categories: cs.CV cs.LG
\\
  Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.
\\ ( https://arxiv.org/abs/2509.18958 ,  9060kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18973
Date: Tue, 23 Sep 2025 13:26:06 GMT   (3475kb)

Title: Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive
  Semantic Segmentation of Electron Microscopy Images
Authors: Jiabao Chen, Shan Xiong, and Jialin Peng
Categories: cs.CV
Comments: MICCAI2025
\\
  Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.
\\ ( https://arxiv.org/abs/2509.18973 ,  3475kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19002
Date: Tue, 23 Sep 2025 13:46:31 GMT   (7289kb)

Title: VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via
  Travel Video Itinerary Reconstruction
Authors: Hao Wang, Eiki Murata, Lingfang Zhang, Ayako Sato, So Fukuda, Ziqi
  Yin, Wentao Hu, Keisuke Nakao, Yusuke Nakamura, Sebastian Zwirner, Yi-Chia
  Chen, Hiroyuki Otomo, Hiroki Ouchi, and Daisuke Kawahara
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.
\\ ( https://arxiv.org/abs/2509.19002 ,  7289kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19003
Date: Tue, 23 Sep 2025 13:47:32 GMT   (539kb)

Title: Unveiling Chain of Step Reasoning for Vision-Language Models with
  Fine-grained Rewards
Authors: Honghao Chen, Xingzhou Lou, Xiaokun Feng, Kaiqi Huang, Xinlong Wang
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\
  Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.
\\ ( https://arxiv.org/abs/2509.19003 ,  539kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19028
Date: Tue, 23 Sep 2025 14:01:51 GMT   (7047kb)

Title: Weakly Supervised Food Image Segmentation using Vision Transformers and
  Segment Anything Model
Authors: Ioannis Sarafis, Alexandros Papadopoulos and Anastasios Delopoulos
Categories: cs.CV
Comments: Submitted to the 20th International Workshop on Semantic and Social
  Media Adaptation & Personalization
\\
  In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.
\\ ( https://arxiv.org/abs/2509.19028 ,  7047kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19052
Date: Tue, 23 Sep 2025 14:17:01 GMT   (2095kb)

Title: A DyL-Unet framework based on dynamic learning for Temporally Consistent
  Echocardiographic Segmentation
Authors: Jierui Qu and Jianchun Zhao
Categories: cs.CV
\\
  Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.
\\ ( https://arxiv.org/abs/2509.19052 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19070
Date: Tue, 23 Sep 2025 14:33:21 GMT   (4319kb)

Title: ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness
  Tests?
Authors: Zijian Ling, Han Zhang, Yazhuo Zhou, Jiahao Cui
Categories: cs.CV cs.CL
Comments: Accepted at the Open Science for Foundation Models (SCI-FM) Workshop
  at ICLR 2025
\\
  This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.
\\ ( https://arxiv.org/abs/2509.19070 ,  4319kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19073
Date: Tue, 23 Sep 2025 14:34:10 GMT   (1779kb)

Title: WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian
  Object Reconstruction
Authors: Hung Nguyen and Runfa Li and An Le and Truong Nguyen
Categories: cs.CV eess.IV eess.SP
\\
  3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.
\\ ( https://arxiv.org/abs/2509.19073 ,  1779kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19082
Date: Tue, 23 Sep 2025 14:38:25 GMT   (542kb)

Title: 3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA
  Results with Consistent Training and Inference
Authors: Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian
  Leibe
Categories: cs.CV
\\
  Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i
\\ ( https://arxiv.org/abs/2509.19082 ,  542kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19087
Date: Tue, 23 Sep 2025 14:40:52 GMT   (2655kb)

Title: Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal
  Gemini 2.5 Model for Remote Sensing Applications
Authors: Ganesh Mallya, Yotam Gigi, Dahun Kim, Maxim Neumann, Genady Beryozkin,
  Tomer Shekel, Anelia Angelova
Categories: cs.CV
\\
  Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.
\\ ( https://arxiv.org/abs/2509.19087 ,  2655kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19090
Date: Tue, 23 Sep 2025 14:42:31 GMT   (22536kb)

Title: Citrus-V: Advancing Medical Foundation Models with Unified Medical Image
  Grounding for Clinical Reasoning
Authors: Guoxin Wang, Jun Zhao, Xinyi Liu, Yanbo Liu, Xuyang Cao, Chao Li,
  Zhuoyun Liu, Qintian Sun, Fangru Zhou, Haoqiang Xing, Zhenhong Yang
Categories: cs.CV cs.AI cs.CL
\\
  Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.
\\ ( https://arxiv.org/abs/2509.19090 ,  22536kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19096
Date: Tue, 23 Sep 2025 14:47:33 GMT   (12246kb)

Title: Investigating Traffic Accident Detection Using Multimodal Large Language
  Models
Authors: Ilhan Skender, Kailin Tong, Selim Solmaz, Daniel Watzenig
Categories: cs.CV cs.SE
Comments: Accepted for presentation at the 2025 IEEE International Automated
  Vehicle Validation Conference (IAVVC 2025). Final version to appear in IEEE
  Xplore
\\
  Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.
\\ ( https://arxiv.org/abs/2509.19096 ,  12246kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19115
Date: Tue, 23 Sep 2025 15:00:18 GMT   (10460kb)

Title: Track-On2: Enhancing Online Point Tracking with Memory
Authors: G\"orkay Aydemir, Weidi Xie, Fatma G\"uney
Categories: cs.CV
\\
  In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2
\\ ( https://arxiv.org/abs/2509.19115 ,  10460kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19129
Date: Tue, 23 Sep 2025 15:15:37 GMT   (3686kb)

Title: KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic
  Environments
Authors: Adam Romlein, Benjamin X. Hou, Yuval Boss, Cynthia L. Christman,
  Stacie Koslovsky, Erin E. Moreland, Jason Parham, Anthony Hoogs
Categories: cs.CV
Comments: Accepted to the IEEE/CVF International Conference on Computer Vision
  (ICCV 2025)
\\
  We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.
\\ ( https://arxiv.org/abs/2509.19129 ,  3686kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19156
Date: Tue, 23 Sep 2025 15:34:33 GMT   (1156kb)

Title: NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and
  Dynamic Early-Exit
Authors: Maurf Hassan, Steven Davy, Muhammad Zawish, Owais Bin Zuber, and
  Nouman Ashraf
Categories: cs.CV
Comments: This paper was accepted at ICMLA 2025. The official version will
  appear in IEEE Xplore
\\
  Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.
\\ ( https://arxiv.org/abs/2509.19156 ,  1156kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19165
Date: Tue, 23 Sep 2025 15:41:40 GMT   (18585kb)

Title: RoSe: Robust Self-supervised Stereo Matching under Adverse Weather
  Conditions
Authors: Yun Wang, Junjie Hu, Junhui Hou, Chenghao Zhang, Renwei Yang, Dapeng
  Oliver Wu
Categories: cs.CV cs.AI
Journal-ref: IEEE Transactions on Circuits and Systems for Video Technology
  2025
\\
  Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.
\\ ( https://arxiv.org/abs/2509.19165 ,  18585kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19166
Date: Tue, 23 Sep 2025 15:41:44 GMT   (3180kb)

Title: YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and
  Negatives
Authors: Siddharth Gupta and Jitin Singla
Categories: cs.CV
\\
  Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.
\\ ( https://arxiv.org/abs/2509.19166 ,  3180kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19183
Date: Tue, 23 Sep 2025 15:58:13 GMT   (1044kb)

Title: The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware
  Video Segmentation via SeC
Authors: Mingqi Gao and Jingkun Chen and Yunqi Miao and Gengshen Wu and Zhijin
  Qin and Jungong Han
Categories: cs.CV
\\
  This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.
\\ ( https://arxiv.org/abs/2509.19183 ,  1044kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19191
Date: Tue, 23 Sep 2025 16:07:18 GMT   (4243kb)

Title: Reading Images Like Texts: Sequential Image Understanding in
  Vision-Language Models
Authors: Yueyan Li, Chenggong Zhao, Zeyuan Zang, Caixia Yuan, Xiaojie Wang
Categories: cs.CV
\\
  Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.
\\ ( https://arxiv.org/abs/2509.19191 ,  4243kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19203
Date: Tue, 23 Sep 2025 16:22:27 GMT   (14989kb)

Title: Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene
  Descriptions
Authors: Ioanna Ntinou, Alexandros Xenos, Yassine Ouali, Adrian Bulat, Georgios
  Tzimiropoulos
Categories: cs.CV
Comments: Accepted at EMNLP 2025
\\
  Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP
\\ ( https://arxiv.org/abs/2509.19203 ,  14989kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19207
Date: Tue, 23 Sep 2025 16:28:51 GMT   (257kb)

Title: Long Story Short: Disentangling Compositionality and Long-Caption
  Understanding in VLMs
Authors: Israfel Salazar, Desmond Elliott, Yova Kementchedjhieva
Categories: cs.CV
\\
  Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.
\\ ( https://arxiv.org/abs/2509.19207 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19208
Date: Tue, 23 Sep 2025 16:29:13 GMT   (8382kb)

Title: Enabling Plant Phenotyping in Weedy Environments using Multi-Modal
  Imagery via Synthetic and Generated Training Data
Authors: Earl Ranario, Ismael Mayanja, Heesup Yun, Brian N. Bailey, J. Mason
  Earles
Categories: cs.CV
\\
  Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.
\\ ( https://arxiv.org/abs/2509.19208 ,  8382kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19218
Date: Tue, 23 Sep 2025 16:42:16 GMT   (6258kb)

Title: HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and
  Choroid Plexus in Pediatric Hydrocephalus
Authors: Yunzhi Xu, Yushuang Ding, Hu Sun, Hongxi Zhang, Li Zhao
Categories: cs.CV cs.AI
Comments: 10 pages, 7 figures
\\
  Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.
\\ ( https://arxiv.org/abs/2509.19218 ,  6258kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19227
Date: Tue, 23 Sep 2025 16:49:25 GMT   (8267kb)

Title: MsFIN: Multi-scale Feature Interaction Network for Traffic Accident
  Anticipation
Authors: Tongshuai Wu, Chao Lu, Ze Song, Yunlong Lin, Sizhe Fan, Xuemei Chen
Categories: cs.CV cs.AI
\\
  With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.
\\ ( https://arxiv.org/abs/2509.19227 ,  8267kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19230
Date: Tue, 23 Sep 2025 16:52:27 GMT   (1122kb)

Title: DevFD: Developmental Face Forgery Detection by Learning Shared and
  Orthogonal LoRA Subspaces
Authors: Tianshuo Zhang, Li Gao, Siran Peng, Xiangyu Zhu, Zhen Lei
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\
  The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.
\\ ( https://arxiv.org/abs/2509.19230 ,  1122kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19244
Date: Tue, 23 Sep 2025 17:05:46 GMT   (7300kb)

Title: Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal
  Understanding and Generation
Authors: Shufan Li, Jiuxiang Gu, Kangning Liu, Zhe Lin, Zijun Wei, Aditya
  Grover, Jason Kuen
Categories: cs.CV
Comments: 32 pages, 15 figures
\\
  We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.
\\ ( https://arxiv.org/abs/2509.19244 ,  7300kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19245
Date: Tue, 23 Sep 2025 17:06:11 GMT   (28262kb)

Title: ConViS-Bench: Estimating Video Similarity Through Semantic Concepts
Authors: Benedetta Liberatori, Alessandro Conti, Lorenzo Vaquero, Yiming Wang,
  Elisa Ricci, Paolo Rota
Categories: cs.CV
Comments: Accepted to NeurIPS 2025
\\
  What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.
\\ ( https://arxiv.org/abs/2509.19245 ,  28262kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19252
Date: Tue, 23 Sep 2025 17:12:20 GMT   (1334kb)

Title: Adversarially-Refined VQ-GAN with Dense Motion Tokenization for
  Spatio-Temporal Heatmaps
Authors: Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal
  Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi
Categories: cs.CV cs.AI
\\
  Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.
\\ ( https://arxiv.org/abs/2509.19252 ,  1334kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19258
Date: Tue, 23 Sep 2025 17:18:33 GMT   (22895kb)

Title: Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging
  Heterogeneity in Confounding Tumor Pathologies
Authors: Dheerendranath Battalapalli, Apoorva Safai, Maria Jaramillo, Hyemin
  Um, Gustavo Adalfo Pineda Ortiz, Ulas Bagci, Manmeet Singh Ahluwalia, Marwa
  Ismail, and Pallavi Tiwari
Categories: cs.CV
Comments: Under Review: npj Digital Medicine
\\
  A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.
\\ ( https://arxiv.org/abs/2509.19258 ,  22895kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19259
Date: Tue, 23 Sep 2025 17:18:56 GMT   (30435kb)

Title: Moving by Looking: Towards Vision-Driven Avatar Motion Generation
Authors: Markos Diomataris, Berat Mert Albaba, Giorgio Becherini, Partha Ghosh,
  Omid Taheri, Michael J. Black
Categories: cs.CV
\\
  The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.
\\ ( https://arxiv.org/abs/2509.19259 ,  30435kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19282
Date: Tue, 23 Sep 2025 17:50:00 GMT   (23364kb)

Title: OverLayBench: A Benchmark for Layout-to-Image Generation with Dense
  Overlaps
Authors: Bingnan Li, Chen-Yu Wang, Haiyang Xu, Xiang Zhang, Ethan Armand,
  Divyansh Srivastava, Xiaojun Shan, Zeyuan Chen, Jianwen Xie, Zhuowen Tu
Categories: cs.CV
Comments: Accepted to NeurIPS 2025 Dataset&Benchmark Track
\\
  Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.
\\ ( https://arxiv.org/abs/2509.19282 ,  23364kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19296
Date: Tue, 23 Sep 2025 17:58:01 GMT   (8090kb)

Title: Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model
  Self-Distillation
Authors: Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng
  Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic,
  Sanja Fidler, Huan Ling, Jun Gao, Xuanchi Ren
Categories: cs.CV cs.GR
Comments: Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/
\\
  The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.
\\ ( https://arxiv.org/abs/2509.19296 ,  8090kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19297
Date: Tue, 23 Sep 2025 17:59:02 GMT   (5408kb)

Title: VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with
  Voxel-Aligned Prediction
Authors: Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang,
  Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Y. Chen, Bohan Zhuang
Categories: cs.CV
Comments: Project Page: https://lhmd.top/volsplat, Code:
  https://github.com/ziplab/VolSplat
\\
  Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.
\\ ( https://arxiv.org/abs/2509.19297 ,  5408kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19300
Date: Tue, 23 Sep 2025 17:59:31 GMT   (7436kb)

Title: CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target
  for Better Flow Matching
Authors: Chen Chen, Pengsheng Guo, Liangchen Song, Jiasen Lu, Rui Qian, Xinze
  Wang, Tsu-Jui Fu, Wei Liu, Yinfei Yang, Alex Schwing
Categories: cs.CV
\\
  Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.
\\ ( https://arxiv.org/abs/2509.19300 ,  7436kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18472
Date: Mon, 22 Sep 2025 23:22:24 GMT   (951kb)

Title: Bridging Simulation and Silicon: A Study of RISC-V Hardware and FireSim
  Simulation
Authors: Atanu Barai and Kamalavasan Kamalakkannan and Patrick Diehl and Maxim
  Moraru and Jered Dominguez-Trujillo and Howard Pritchard and Nandakishore
  Santhi and Farzad Fatollahi-Fard and Galen Shipman
Categories: cs.DC
Report-no: LA-UR-25-27825
\\
  RISC-V ISA-based processors have recently emerged as both powerful and
energy-efficient computing platforms. The release of the MILK-V Pioneer marked
a significant milestone as the first desktop-grade RISC-V system. With
increasing engagement from both academia and industry, such platforms exhibit
strong potential for adoption in high-performance computing (HPC) environments.
  The open-source, FPGA-accelerated FireSim framework has emerged as a flexible
and scalable tool for architectural exploration, enabling simulation of various
system configurations using RISC-V cores. Despite its capabilities, there
remains a lack of systematic evaluation regarding the feasibility and
performance prediction accuracy of FireSim when compared to physical hardware.
  In this study, we address this gap by modeling a commercially available
single-board computer and a desktop-grade RISC-V CPU within FireSim. To ensure
fidelity between simulation and real hardware, we first measure the performance
of a series of benchmarks to compare runtime behavior under single-core and
four-core configurations. Based on the closest matching simulation parameters,
we subsequently evaluate performance using a representative mini-application
and the LAMMPS molecular dynamics code.
  Our findings indicate that while FireSim provides valuable insights into
architectural performance trends, discrepancies remain between simulated and
measured runtimes. These deviations stem from both inherent limitations of the
simulation environment and the restricted availability of detailed performance
specifications from CPU manufacturers, which hinder precise configuration
matching.
\\ ( https://arxiv.org/abs/2509.18472 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18735
Date: Tue, 23 Sep 2025 07:28:55 GMT   (1210kb)

Title: 6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and
  Non-Linear Precoder Design for Radio Access Networks
Authors: Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ali
  Jamshed, Dean F. Hougen, John M. Cioffi
Categories: cs.DC
Comments: Submitted to IEEE Transactions on Wireless Communications
\\
  This work introduces 6G Twin, the first end-to-end artificial intelligence
(AI)-native radio access network (RAN) design that unifies (i) neural Gaussian
Radio Fields (GRF) for compressed channel state information (CSI) acquisition,
(ii) continual channel prediction with handover persistence, and (iii) an
energy-optimal nonlinear precoder (minPMAC). GRF replaces dense pilots with a
sparse Gaussian field, cutting pilot overhead by about 100x while delivering
1.1 ms inference and less than 2 minutes on-site training, thus enabling
millisecond-scale closed-loop operation. A replay-driven continual learner
sustains accuracy under mobility and cell transitions, improving channel
normalized mean square error (NMSE) by more than 10 dB over frozen predictors
and an additional 2-5 dB over uniform replay, thereby stabilizing performance
across UMi/UMa handovers. Finally, minPMAC solves a convex, order-free MAC
precoder design that recovers the globally optimal order from Broadcast Channel
(BC) duals and minimizes transmit energy subject to minimum-rate guarantees,
achieving 4-10 times lower energy (scenario dependent) with monotonically
increasing bits per joule as SNR grows. This translates to up to 5 times higher
data rate at comparable power or the same rates at substantially lower power.
Together, these components form a practical, GPU-ready framework that attains
real-time CSI, robust tracking in dynamic networks with efficient handovers,
and state-of-the-art throughput-energy tradeoffs under 3GPP-style settings.
\\ ( https://arxiv.org/abs/2509.18735 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18869
Date: Tue, 23 Sep 2025 10:07:29 GMT   (4083kb)

Title: On The Reproducibility Limitations of RAG Systems
Authors: Baiqiang Wang and Dongfang Zhao and Nathan R Tallent and Luanzheng Guo
Categories: cs.DC
\\
  Retrieval-Augmented Generation (RAG) is increasingly employed in generative
AI-driven scientific workflows to integrate rapidly evolving scientific
knowledge bases, yet its reliability is frequently compromised by
non-determinism in their retrieval components. This paper introduces ReproRAG,
a comprehensive benchmarking framework designed to systematically measure and
quantify the reproducibility of vector-based retrieval systems. ReproRAG
investigates sources of uncertainty across the entire pipeline, including
different embedding models, precision, retrieval algorithms, hardware
configurations, and distributed execution environments. Utilizing a suite of
metrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the
proposed framework effectively characterizes the trade-offs between
reproducibility and performance. Our large-scale empirical study reveals
critical insights; for instance, we observe that different embedding models
have remarkable impact on RAG reproducibility. The open-sourced ReproRAG
framework provides researchers and engineers productive tools to validate
deployments, benchmark reproducibility, and make informed design decisions,
thereby fostering more trustworthy AI for science.
\\ ( https://arxiv.org/abs/2509.18869 ,  4083kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18957
Date: Tue, 23 Sep 2025 13:11:42 GMT   (1335kb)

Title: TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources
  via Distributed Reinforcement Learning
Authors: Shengye Song, Minxian Xu, Kan Hu, Wenxia Guo, Kejiang Ye
Categories: cs.DC
Comments: 14 pages, 5 figures
Journal-ref: PDCAT 2025
\\
  Resource scheduling in cloud-edge systems is challenging as edge nodes run
latency-sensitive workloads under tight resource constraints, while existing
centralized schedulers can suffer from performance bottlenecks and user
experience degradation. To address the issues of distributed decisions in
cloud-edge environments, we present TD3-Sched, a distributed reinforcement
learning (DRL) scheduler based on Twin Delayed Deep Deterministic Policy
Gradient (TD3) for continuous control of CPU and memory allocation, which can
achieve optimized decisions for resource provisioning under dynamic workloads.
On a realistic cloud-edge testbed with SockShop application and Alibaba traces,
TD3-Sched achieves reductions of 17.9% to 38.6% in latency under same loads
compared with other reinforcement-learning and rule-based baselines, and 16% to
31.6% under high loads. TD3-Sched also shows superior Service Level Objective
(SLO) compliance with only 0.47% violations. These results indicate faster
convergence, lower latency, and more stable performance while preserving
service quality in container-based cloud-edge environment compared with the
baselines.
\\ ( https://arxiv.org/abs/2509.18957 ,  1335kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19086
Date: Tue, 23 Sep 2025 14:40:24 GMT   (1132kb)

Title: Scheduler-Driven Job Atomization
Authors: Michal Konopa, Jan Fesl, Ladislav Ber\'anek (Faculty of Agriculture
  and Technology, University of South Bohemia)
Categories: cs.DC
Comments: 22 pages
ACM-class: D.4.1; C.4; C.1.4; D.1.3
\\
  Modern GPU clusters, particularly those built on NVIDIA's Multi-Instance GPU
(MIG) architecture, often suffer from inefficiencies because jobs are treated
as rigid, indivisible blocks that occupy a fixed slice until completion. The
reliance on static peak memory estimates exacerbates fragmentation,
underutilization, and job rejections. We propose Scheduler-Driven Job
Atomization (SJA), a new paradigm that establishes a bidirectional interaction
between scheduler and jobs. In SJA, the scheduler advertises available
execution gaps, and jobs respond by signaling interest if they can potentially
generate a subjob that fits the offered time-capacity window. The scheduler may
collect multiple signals for the same slot and, based on its allocation policy
(e.g., fairness, efficiency, or SLA priorities), selects which job is granted
the slot. Only then does the chosen job materialize a safe, self-contained
subjob tailored to that opportunity. Unlike migration or preemption, SJA
proactively shapes workloads before execution, thereby avoiding costly state
transfers and unpredictable interruptions. It aims to increase GPU utilization,
reduce wait times, and minimize migration overhead by aligning jobs with
opportunities in real time, ensuring that each admitted subjob is correct by
construction. This paper is presented as a concept paper: it introduces the
paradigm, defines its building blocks, and outlines future research directions,
rather than offering a full experimental evaluation.
\\ ( https://arxiv.org/abs/2509.19086 ,  1132kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19150
Date: Tue, 23 Sep 2025 15:29:59 GMT   (753kb)

Title: In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow
  Patterns
Authors: Harikrishna Tummalapalli, Riccardo Balin, Christine M. Simpson, Andrew
  Park, Aymen Alsaadi, Andrew E. Shao, Wesley Brewer, Shantenu Jha
Categories: cs.DC
\\
  Coupled AI-Simulation workflows are becoming the major workloads for HPC
facilities, and their increasing complexity necessitates new tools for
performance analysis and prototyping of new in-situ workflows. We present
SimAI-Bench, a tool designed to both prototype and evaluate these coupled
workflows. In this paper, we use SimAI-Bench to benchmark the data transport
performance of two common patterns on the Aurora supercomputer: a one-to-one
workflow with co-located simulation and AI training instances, and a
many-to-one workflow where a single AI model is trained from an ensemble of
simulations. For the one-to-one pattern, our analysis shows that node-local and
DragonHPC data staging strategies provide excellent performance compared Redis
and Lustre file system. For the many-to-one pattern, we find that data
transport becomes a dominant bottleneck as the ensemble size grows. Our
evaluation reveals that file system is the optimal solution among the tested
strategies for the many-to-one pattern.
\\ ( https://arxiv.org/abs/2509.19150 ,  753kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19187
Date: Tue, 23 Sep 2025 16:03:14 GMT   (84kb)

Title: Non-Uniform Content-Oblivious Leader Election on Oriented Asynchronous
  Rings
Authors: J\'er\'emie Chalopin, Yi-Jun Chang, Lyuting Chen, Giuseppe A. Di Luna,
  Haoran Zhou
Categories: cs.DC
\\
  We study the leader election problem in oriented ring networks under
content-oblivious asynchronous message-passing systems, where an adversary may
arbitrarily corrupt message contents.
  Frei et al. (DISC 2024) presented a uniform terminating leader election
algorithm for oriented rings in this setting, with message complexity $O(n
\cdot \mathsf{ID}_{\max})$ on a ring of size $n$, where $\mathsf{ID}_{\max}$ is
the largest identifier in the system, this result has been recently extended by
Chalopin et al. (DISC 2025) to unoriented rings.
  In this paper, we investigate the message complexity of leader election on
ring networks in the content-oblivious model, showing that no uniform algorithm
can solve the problem if each process is limited to sending a constant number
of messages in one direction.
  Interestingly, this limitation hinges on the uniformity assumption. In the
non-uniform setting, where processes know an upper bound $U \geq n$ on the ring
size, we present an algorithm with message complexity $O(n \cdot U \cdot
\mathsf{ID}_{\min})$, in which each process sends $O(U \cdot
\mathsf{ID}_{\min})$ messages clockwise and only three messages
counter-clockwise. Here, $\mathsf{ID}_{\min}$ is the smallest identifier in the
system. This dependence on the identifiers compares favorably with the
dependence on $\mathsf{ID}_{\max}$ of Frei et al.
  We also show a non-uniform algorithm where each process sends $O(U \cdot
\log\mathsf{ID}_{\min})$ messages in one direction and
$O(\log\mathsf{ID}_{\min})$ in the other. The factor $\log \mathsf{ID}_{\min}$
is optimal, matching the lower bound of Frei et al.
  Finally, in the anonymous setting, where processes do not have identifiers,
we propose a randomized algorithm where each process sends only $O(\log^2 U)$
messages, with a success probability of $1 - U^{-c}$.
\\ ( https://arxiv.org/abs/2509.19187 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19294
Date: Tue, 23 Sep 2025 17:56:34 GMT   (241kb)

Title: Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based
  Tenstorrent Wormhole
Authors: Jenny Lynn Almerol, Elisabetta Boella, Mario Spera, Daniele Gregori
Categories: cs.DC astro-ph.IM
ACM-class: D.1.3; J.2
DOI: 10.1145/3731599.3767528
\\
  Although originally developed primarily for artificial intelligence
workloads, RISC-V-based accelerators are also emerging as attractive platforms
for high-performance scientific computing. In this work, we present our
approach to accelerating an astrophysical $N$-body code on the RISC-V-based
Wormhole n300 card developed by Tenstorrent. Our results show that this
platform can be highly competitive for astrophysical simulations employing this
class of algorithms, delivering more than a $2 \times$ speedup and
approximately $2 \times$ energy savings compared to a highly optimized CPU
implementation of the same code.
\\ ( https://arxiv.org/abs/2509.19294 ,  241kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2509.18104 (*cross-listing*)
Date: Tue, 9 Sep 2025 06:45:30 GMT   (3585kb)

Title: Data Valuation and Selection in a Federated Model Marketplace
Authors: Wenqian Li, Youjia Yang, Ruoxi Jia, Yan Pang
Categories: cs.LG cs.AI
\\
  In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.
\\ ( https://arxiv.org/abs/2509.18104 ,  3585kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18105 (*cross-listing*)
Date: Tue, 9 Sep 2025 10:02:41 GMT   (840kb)

Title: BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential
  Equations under Stochastic Demand
Authors: Nachiket N. Naik, Prathamesh Dinesh Joshi, Raj Abhijit Dandekar, Rajat
  Dandekar and Sreedath Panat
Categories: cs.LG cs.AI
\\
  We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.
\\ ( https://arxiv.org/abs/2509.18105 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18108 (*cross-listing*)
Date: Tue, 9 Sep 2025 19:12:00 GMT   (12263kb)

Title: Solve it with EASE
Authors: Adam Viktorin, Tomas Kadavy, Jozef Kovac, Michal Pluhacek, Roman
  Senkerik
Categories: cs.LG cs.AI
Comments: EASE framework landing paper
ACM-class: I.2.2; I.2.7
\\
  This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.
\\ ( https://arxiv.org/abs/2509.18108 ,  12263kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18111 (*cross-listing*)
Date: Tue, 9 Sep 2025 21:03:46 GMT   (5692kb)

Title: Prompt Optimization Meets Subspace Representation Learning for Few-shot
  Out-of-Distribution Detection
Authors: Faizul Rakib Sayem, Shahana Ibrahim
Categories: cs.LG cs.AI cs.CV
\\
  The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2509.18111 ,  5692kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18116 (*cross-listing*)
Date: Wed, 10 Sep 2025 07:03:35 GMT   (282kb)

Title: Amortized Latent Steering: Low-Cost Alternative to Test-Time
  Optimization
Authors: Nathan Egbuna, Saatvik Gaur, Sunishchal Dev, Ashwinee Panda, Maheep
  Chaudhary
Categories: cs.LG cs.AI
\\
  Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}
\\ ( https://arxiv.org/abs/2509.18116 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18119 (*cross-listing*)
Date: Wed, 10 Sep 2025 13:09:27 GMT   (3458kb)

Title: MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents
Authors: Yifan Xu, Xiao Liu, Xinghan Liu, Jiaqi Fu, Hanchen Zhang, Bohao Jing,
  Shudan Zhang, Yuting Wang, Wenyi Zhao, Yuxiao Dong
Categories: cs.LG cs.AI
\\
  Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.
\\ ( https://arxiv.org/abs/2509.18119 ,  3458kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18120 (*cross-listing*)
Date: Wed, 10 Sep 2025 13:29:05 GMT   (3839kb)

Title: A Coopetitive-Compatible Data Generation Framework for Cross-silo
  Federated Learning
Authors: Thanh Linh Nguyen and Quoc-Viet Pham
Categories: cs.LG cs.AI cs.CE cs.DC cs.GT
Comments: Accepted in IEEE GLOBECOM 2025
\\
  Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.
\\ ( https://arxiv.org/abs/2509.18120 ,  3839kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18125 (*cross-listing*)
Date: Wed, 10 Sep 2025 21:41:42 GMT   (284kb)

Title: NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient
  Assignment
Authors: Harsha Koduri
Categories: cs.LG cs.AI
\\
  Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.
\\ ( https://arxiv.org/abs/2509.18125 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18126 (*cross-listing*)
Date: Thu, 11 Sep 2025 03:46:41 GMT   (308kb)

Title: Anomaly Detection in Electric Vehicle Charging Stations Using Federated
  Learning
Authors: Bishal K C, Amr Hilal, Pawan Thapa
Categories: cs.LG cs.AI
\\
  Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.
\\ ( https://arxiv.org/abs/2509.18126 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18127 (*cross-listing*)
Date: Thu, 11 Sep 2025 11:22:43 GMT   (19415kb)

Title: Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language
  Models via Sparse Autoencoder Interpretation Framework
Authors: Jiaqi Weng, Han Zheng, Hanyu Zhang, Qinqin He, Jialing Tao, Hui Xue,
  Zhixuan Chu, Xiting Wang
Categories: cs.LG cs.AI cs.CL
\\
  Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.
\\ ( https://arxiv.org/abs/2509.18127 ,  19415kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18130 (*cross-listing*)
Date: Fri, 12 Sep 2025 12:23:26 GMT   (554kb)

Title: Research on Metro Transportation Flow Prediction Based on the STL-GRU
  Combined Model
Authors: Zijie Zhou, Huichen Ma
Categories: cs.LG cs.AI
\\
  In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.
\\ ( https://arxiv.org/abs/2509.18130 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18131 (*cross-listing*)
Date: Fri, 12 Sep 2025 16:41:23 GMT   (2677kb)

Title: Two ways to knowledge?
Authors: Jean-Michel Tucny, Abhisek Ganguly, Santosh Ansumali, Sauro Succi
Categories: cs.LG cs.AI
\\
  It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.
\\ ( https://arxiv.org/abs/2509.18131 ,  2677kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18133 (*cross-listing*)
Date: Sun, 14 Sep 2025 04:04:19 GMT   (313kb)

Title: Self-Evolving LLMs via Continual Instruction Tuning
Authors: Le Huang, Jiazheng Kang, Cheng Hou, Zhe Zhao, Zhenxiang Yan, Chuan
  Shi, and Ting Bai
Categories: cs.LG cs.AI
\\
  In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.
\\ ( https://arxiv.org/abs/2509.18133 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18135 (*cross-listing*)
Date: Sun, 14 Sep 2025 11:23:12 GMT   (275kb)

Title: SDGF: Fusing Static and Multi-Scale Dynamic Correlations for
  Multivariate Time Series Forecasting
Authors: Shaoxun Wang, Xingjun Zhang, Qianyang Li, Jiawei Cao, Zhendong Tan
Categories: cs.LG cs.AI
\\
  Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.
\\ ( https://arxiv.org/abs/2509.18135 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18136 (*cross-listing*)
Date: Sun, 14 Sep 2025 12:20:39 GMT   (488kb)

Title: From Parameters to Performance: A Data-Driven Study on LLM Structure and
  Development
Authors: Suqing Wang, Zuchao Li, Luohe Shi, Bo Du, Hai Zhao, Yun Li, Qianren
  Wang
Categories: cs.LG cs.AI
Comments: Accepted by EMNLP 2025
\\
  Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset
\\ ( https://arxiv.org/abs/2509.18136 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18137 (*cross-listing*)
Date: Sun, 14 Sep 2025 16:33:17 GMT   (29kb)

Title: LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods
Authors: Shaoheng Wang, Yao Lu, Yuqi Li, Yaxin Gao, Jiaqi Nie, Shanqing Yu,
  Yingli Tian, Qi Xuan
Categories: cs.LG cs.AI
\\
  As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.
\\ ( https://arxiv.org/abs/2509.18137 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18140 (*cross-listing*)
Date: Sun, 14 Sep 2025 19:29:52 GMT   (566kb)

Title: A Machine Learning Framework for Pathway-Driven Therapeutic Target
  Discovery in Metabolic Disorders
Authors: Iram Wajahat, Amritpal Singh, Fazel Keshtkar, and Syed Ahmad Chan
  Bukhari
Categories: cs.LG cs.AI
Comments: 6 pages, 6 figures
\\
  Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.
\\ ( https://arxiv.org/abs/2509.18140 ,  566kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18141 (*cross-listing*)
Date: Mon, 15 Sep 2025 00:38:38 GMT   (2771kb)

Title: KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data
  from Kaplan-Meier Plots
Authors: Yao Zhao, Haoyue Sun, Yantian Ding, Yanxun Xu
Categories: cs.LG cs.AI cs.CV stat.AP stat.ML
\\
  Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.
\\ ( https://arxiv.org/abs/2509.18141 ,  2771kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18143 (*cross-listing*)
Date: Mon, 15 Sep 2025 14:11:34 GMT   (1058kb)

Title: Weight Mapping Properties of a Dual Tree Single Clock Adiabatic
  Capacitive Neuron
Authors: Mike Smart, Sachin Maheshwari, Himadri Singh Raghav, Alexander Serb
Categories: cs.ET cs.AI cs.LG eess.IV
Comments: 11 pages, 10 figures, 6 tables. This work has been submitted to the
  IEEE for possible publication
\\
  Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits
offer the potential for highly energy-efficient Artificial Neural Network (ANN)
computation in full custom analog IC designs. The efficient mapping of
Artificial Neuron (AN) abstract weights, extracted from the software-trained
ANNs, onto physical ACN capacitance values has, however, yet to be fully
researched. In this paper, we explore the unexpected hidden complexities,
challenges and properties of the mapping, as well as, the ramifications for IC
designers in terms accuracy, design and implementation. We propose an optimal,
AN to ACN methodology, that promotes smaller chip sizes and improved overall
classification accuracy, necessary for successful practical deployment. Using
TensorFlow and Larq software frameworks, we train three different ANN networks
and map their weights into the energy-efficient DTSC ACN capacitance value
domain to demonstrate 100% functional equivalency. Finally, we delve into the
impact of weight quantization on ACN performance using novel metrics related to
practical IC considerations, such as IC floor space and comparator
decision-making efficacy.
\\ ( https://arxiv.org/abs/2509.18143 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18144 (*cross-listing*)
Date: Mon, 15 Sep 2025 18:55:56 GMT   (412kb)

Title: AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling
  for Spatio-Temporal Imputation
Authors: Yubo Yang and Yichen Zhu and Bo Jiang
Categories: cs.LG cs.AI
Comments: 9 pages
\\
  Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.
\\ ( https://arxiv.org/abs/2509.18144 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18145 (*cross-listing*)
Date: Mon, 15 Sep 2025 20:40:45 GMT   (303kb)

Title: Early Prediction of Multi-Label Care Escalation Triggers in the
  Intensive Care Unit Using Electronic Health Records
Authors: Syed Ahmad Chan Bukhari, Amritpal Singh, Shifath Hossain, and Iram
  Wajahat
Categories: cs.LG cs.AI
Comments: 7 pages, 3 Figure
\\
  Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.
\\ ( https://arxiv.org/abs/2509.18145 ,  303kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18147 (*cross-listing*)
Date: Tue, 16 Sep 2025 03:02:46 GMT   (1234kb)

Title: ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for
  Convolutional Neural Networks
Authors: Xinyu Mu, Hui Dou, Furao Shen, Jian Zhao
Categories: cs.LG cs.AI
\\
  Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.
\\ ( https://arxiv.org/abs/2509.18147 ,  1234kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18148 (*cross-listing*)
Date: Tue, 16 Sep 2025 06:40:30 GMT   (2135kb)

Title: Augmenting Limited and Biased RCTs through Pseudo-Sample Matching-Based
  Observational Data Fusion Method
Authors: Kairong Han, Weidong Huang, Taiyang Zhou, Peng Zhen, Kun Kuang
Categories: stat.ME cs.AI cs.LG stat.ML
Comments: Accepted by CIKM 2025
\\
  In the online ride-hailing pricing context, companies often conduct
randomized controlled trials (RCTs) and utilize uplift models to assess the
effect of discounts on customer orders, which substantially influences
competitive market outcomes. However, due to the high cost of RCTs, the
proportion of trial data relative to observational data is small, which only
accounts for 0.65\% of total traffic in our context, resulting in significant
bias when generalizing to the broader user base. Additionally, the complexity
of industrial processes reduces the quality of RCT data, which is often subject
to heterogeneity from potential interference and selection bias, making it
difficult to correct. Moreover, existing data fusion methods are challenging to
implement effectively in complex industrial settings due to the high
dimensionality of features and the strict assumptions that are hard to verify
with real-world data. To address these issues, we propose an empirical data
fusion method called pseudo-sample matching. By generating pseudo-samples from
biased, low-quality RCT data and matching them with the most similar samples
from large-scale observational data, the method expands the RCT dataset while
mitigating its heterogeneity. We validated the method through simulation
experiments, conducted offline and online tests using real-world data. In a
week-long online experiment, we achieved a 0.41\% improvement in profit, which
is a considerable gain when scaled to industrial scenarios with hundreds of
millions in revenue. In addition, we discuss the harm to model training,
offline evaluation, and online economic benefits when the RCT data quality is
not high, and emphasize the importance of improving RCT data quality in
industrial scenarios. Further details of the simulation experiments can be
found in the GitHub repository https://github.com/Kairong-Han/Pseudo-Matching.
\\ ( https://arxiv.org/abs/2509.18148 ,  2135kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18150 (*cross-listing*)
Date: Tue, 16 Sep 2025 11:33:20 GMT   (320kb)

Title: Sparse Training Scheme for Multimodal LLM
Authors: Kean Shi, Liang Chen, Haozhe Zhao, Baobao Chang
Categories: cs.LG cs.AI
\\
  Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.
\\ ( https://arxiv.org/abs/2509.18150 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18151 (*cross-listing*)
Date: Tue, 16 Sep 2025 11:49:12 GMT   (1281kb)

Title: HyperNAS: Enhancing Architecture Representation for NAS Predictor via
  Hypernetwork
Authors: Jindi Lv, Yuhao Zhou, Yuxin Tian, Qing Ye, Wentao Feng, Jiancheng Lv
Categories: cs.LG cs.AI
\\
  Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.
\\ ( https://arxiv.org/abs/2509.18151 ,  1281kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18152 (*cross-listing*)
Date: Tue, 16 Sep 2025 14:59:45 GMT   (9754kb)

Title: WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well
  Geological Interpretation
Authors: Zhenyu Qi, Qing Yu, Jichen Wang, Yun-Bo Zhao, Zerui Li and Wenjun Lv
Categories: cs.LG cs.AI
\\
  Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.
\\ ( https://arxiv.org/abs/2509.18152 ,  9754kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18161 (*cross-listing*)
Date: Wed, 17 Sep 2025 03:51:16 GMT   (1363kb)

Title: Developing Training Procedures for Piecewise-linear Spline Activation
  Functions in Neural Networks
Authors: William H Patty
Categories: cs.LG cs.AI
\\
  Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.
\\ ( https://arxiv.org/abs/2509.18161 ,  1363kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18196 (*cross-listing*)
Date: Fri, 19 Sep 2025 20:40:36 GMT   (857kb)

Title: MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal
  Vocalization Recognition in Speech
Authors: Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen,
  Jingyuan Xing, Xiangmin Xu
Categories: cs.SD cs.AI eess.AS
Comments: Submitted to ICASSP 2026
\\
  Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing
lexical content, but largely fail to recognize nonverbal vocalizations (NVs)
embedded in speech, such as sighs, laughs, and coughs. This capability is
important for a comprehensive understanding of human communication, as NVs
convey crucial emotional and intentional cues. Progress in NV-aware ASR has
been hindered by the lack of high-quality, well-annotated datasets. To address
this gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech
dataset. Unlike most existing corpora that rely on model-based detection,
MNV-17's performative nature ensures high-fidelity, clearly articulated NV
instances. To the best of our knowledge, MNV-17 provides the most extensive set
of nonverbal vocalization categories, comprising 17 distinct and well-balanced
classes of common NVs. We benchmarked MNV-17 on four mainstream ASR
architectures, evaluating their joint performance on semantic transcription and
NV classification. The dataset and the pretrained model checkpoints will be
made publicly available to facilitate future research in expressive ASR.
\\ ( https://arxiv.org/abs/2509.18196 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18200 (*cross-listing*)
Date: Sat, 20 Sep 2025 05:25:32 GMT   (3790kb)

Title: Conversational Orientation Reasoning: Egocentric-to-Allocentric
  Navigation with Multimodal Chain-of-Thought
Authors: Yu Ti Huang
Categories: cs.LG cs.AI cs.CL cs.RO
\\
  Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.
\\ ( https://arxiv.org/abs/2509.18200 ,  3790kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18208 (*cross-listing*)
Date: Sun, 21 Sep 2025 02:46:02 GMT   (8453kb)

Title: Variational Task Vector Composition
Authors: Boyuan Zhang and Yingjun Du and Xiantong Zhen and Ling Shao
Categories: cs.LG cs.AI
\\
  Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.
\\ ( https://arxiv.org/abs/2509.18208 ,  8453kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18214 (*cross-listing*)
Date: Sun, 21 Sep 2025 19:55:14 GMT   (4407kb)

Title: Automatic Classification of Magnetic Chirality of Solar Filaments from
  H-Alpha Observations
Authors: Alexis Chalmers, Azim Ahmadzadeh
Categories: astro-ph.SR cs.AI
\\
  In this study, we classify the magnetic chirality of solar filaments from
H-Alpha observations using state-of-the-art image classification models. We
establish the first reproducible baseline for solar filament chirality
classification on the MAGFiLO dataset. The MAGFiLO dataset contains over 10,000
manually-annotated filaments from GONG H-Alpha observations, making it the
largest dataset for filament detection and classification to date. Prior
studies relied on much smaller datasets, which limited their generalizability
and comparability. We fine-tuned several pre-trained, image classification
architectures, including ResNet, WideResNet, ResNeXt, and ConvNeXt, and also
applied data augmentation and per-class loss weights to optimize the models.
Our best model, ConvNeXtBase, achieves a per-class accuracy of 0.69 for left
chirality filaments and $0.73$ for right chirality filaments.
\\ ( https://arxiv.org/abs/2509.18214 ,  4407kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18231 (*cross-listing*)
Date: Mon, 22 Sep 2025 13:47:28 GMT   (76kb)

Title: Enhanced Interpretable Knowledge Tracing for Students Performance
  Prediction with Human understandable Feature Space
Authors: Sein Minn and Roger Nkambou
Categories: cs.CY cs.AI
Comments: International Conference on Artificial Intelligence in Education
\\
  Knowledge Tracing (KT) plays a central role in assessing students skill
mastery and predicting their future performance. While deep learning based KT
models achieve superior predictive accuracy compared to traditional methods,
their complexity and opacity hinder their ability to provide psychologically
meaningful explanations. This disconnect between model parameters and cognitive
theory poses challenges for understanding and enhancing the learning process,
limiting their trustworthiness in educational applications. To address these
challenges, we enhance interpretable KT models by exploring
human-understandable features derived from students interaction data. By
incorporating additional features, particularly those reflecting students
learning abilities, our enhanced approach improves predictive accuracy while
maintaining alignment with cognitive theory. Our contributions aim to balance
predictive power with interpretability, advancing the utility of adaptive
learning systems.
\\ ( https://arxiv.org/abs/2509.18231 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18233 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:07:57 GMT   (877kb)

Title: Perceptions of AI Across Sectors: A Comparative Review of Public
  Attitudes
Authors: Filip Bialy, Mark Elliot and Robert Meckin
Categories: cs.CY cs.AI
\\
  This paper offers a domain-mediated comparative review of 251 studies on
public attitudes toward AI, published between 2011 and 2025. Drawing on a
systematic literature review, we analyse how different factors including
perceived benefits and concerns (or risks) shape public acceptance of - or
resistance to - artificial intelligence across domains and use-cases, including
healthcare, education, security, public administration, generative AI, and
autonomous vehicles. The analysis highlights recurring patterns in individual,
contextual, and technical factors influencing perception, while also tracing
variations in institutional trust, perceived fairness, and ethical concerns. We
show that the public perception in AI is shaped not only by technical design or
performance but also by sector-specific considerations as well as imaginaries,
cultural narratives, and historical legacies. This comparative approach offers
a foundation for developing more tailored and context-sensitive strategies for
responsible AI governance.
\\ ( https://arxiv.org/abs/2509.18233 ,  877kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18282 (*cross-listing*)
Date: Mon, 22 Sep 2025 18:10:14 GMT   (4280kb)

Title: PEEK: Guiding and Minimal Image Representations for Zero-Shot
  Generalization of Robot Manipulation Policies
Authors: Jesse Zhang, Marius Memmel, Kevin Kim, Dieter Fox, Jesse Thomason,
  Fabio Ramos, Erdem B{\i}y{\i}k, Abhishek Gupta, Anqi Li
Categories: cs.RO cs.AI cs.LG
Comments: 11 pages
\\
  Robotic manipulation policies often fail to generalize because they must
simultaneously learn where to attend, what actions to take, and how to execute
them. We argue that high-level reasoning about where and what can be offloaded
to vision-language models (VLMs), leaving policies to specialize in how to act.
We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which
fine-tunes VLMs to predict a unified point-based intermediate representation:
1. end-effector paths specifying what actions to take, and 2. task-relevant
masks indicating where to focus. These annotations are directly overlaid onto
robot observations, making the representation policy-agnostic and transferable
across architectures. To enable scalable training, we introduce an automatic
annotation pipeline, generating labeled data across 20+ robot datasets spanning
9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot
generalization, including a 41.4x real-world improvement for a 3D policy
trained only in simulation, and 2-3.5x gains for both large VLAs and small
manipulation policies. By letting VLMs absorb semantic and visual complexity,
PEEK equips manipulation policies with the minimal cues they need--where, what,
and how. Website at https://peek-robot.github.io/.
\\ ( https://arxiv.org/abs/2509.18282 ,  4280kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18355 (*cross-listing*)
Date: Mon, 22 Sep 2025 19:31:58 GMT   (1068kb)

Title: Chiplet-Based RISC-V SoC with Modular AI Acceleration
Authors: P. Ramkumar and S. S. Bharadwaj
Categories: cs.AR cs.AI
Comments: 3 pages, 3 figures and 2 tables
\\
  Achieving high performance, energy efficiency, and cost-effectiveness while
maintaining architectural flexibility is a critical challenge in the
development and deployment of edge AI devices. Monolithic SoC designs struggle
with this complex balance mainly due to low manufacturing yields (below 16%) at
advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based
RISC-V SoC architecture that addresses these limitations through modular AI
acceleration and intelligent system level optimization. Our proposed design
integrates 4 different key innovations in a 30mm x 30mm silicon interposer:
adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware
Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring
streaming flow control units and compression-aware transfers; distributed
cryptographic security across heterogeneous chiplets; and intelligent
sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V
CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory
stacks, and dedicated power management controllers. Experimental results across
industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video
processing demonstrate significant performance improvements. The AI-optimized
configuration achieves ~14.7% latency reduction, 17.3% throughput improvement,
and 16.2% power reduction compared to previous basic chiplet implementations.
These improvements collectively translate to a 40.1% efficiency gain
corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while
maintaining sub-5ms real-time capability across all experimented workloads.
These performance upgrades demonstrate that modular chiplet designs can achieve
near-monolithic computational density while enabling cost efficiency,
scalability and upgradeability, crucial for next-generation edge AI device
applications.
\\ ( https://arxiv.org/abs/2509.18355 ,  1068kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18361 (*cross-listing*)
Date: Mon, 22 Sep 2025 19:37:42 GMT   (252kb)

Title: Reading Between the Lines: Scalable User Feedback via Implicit Sentiment
  in Developer Prompts
Authors: Daye Nam, Malgorzata Salawa, Satish Chandra
Categories: cs.SE cs.AI cs.HC
\\
  Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.
\\ ( https://arxiv.org/abs/2509.18361 ,  252kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18362 (*cross-listing*)
Date: Tue, 16 Sep 2025 07:36:26 GMT   (309kb)

Title: FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction
Authors: Yuxuan Cai, Xiaozhuan Liang, Xinghua Wang, Jin Ma, Haijin Liang,
  Jinwen Luo, Xinyu Zuo, Lisheng Duan, Yuyang Yin, Xi Chen
Categories: cs.LG cs.AI
\\
  As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.
\\ ( https://arxiv.org/abs/2509.18362 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18367 (*cross-listing*)
Date: Mon, 22 Sep 2025 19:47:44 GMT   (4829kb)

Title: Multi-Worker Selection based Distributed Swarm Learning for Edge IoT
  with Non-i.i.d. Data
Authors: Zhuoyu Yao, Yue Wang, Songyang Zhang, Yingshu Li, Zhipeng Cai and Zhi
  Tian
Categories: cs.LG cs.AI
\\
  Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.
\\ ( https://arxiv.org/abs/2509.18367 ,  4829kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18386 (*cross-listing*)
Date: Mon, 22 Sep 2025 20:15:15 GMT   (4804kb)

Title: Graph Enhanced Trajectory Anomaly Detection
Authors: Jonathan Kabala Mbuya, Dieter Pfoser, Antonios Anastasopoulos
Categories: cs.LG cs.AI
\\
  Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.
\\ ( https://arxiv.org/abs/2509.18386 ,  4804kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18394 (*cross-listing*)
Date: Mon, 22 Sep 2025 20:27:29 GMT   (1232kb)

Title: An Artificial Intelligence Value at Risk Approach: Metrics and Models
Authors: Luis Enriquez Alvarez
Categories: cs.CY cs.AI q-fin.RM
\\
  Artificial intelligence risks are multidimensional in nature, as the same
risk scenarios may have legal, operational, and financial risk dimensions. With
the emergence of new AI regulations, the state of the art of artificial
intelligence risk management seems to be highly immature due to upcoming AI
regulations. Despite the appearance of several methodologies and generic
criteria, it is rare to find guidelines with real implementation value,
considering that the most important issue is customizing artificial
intelligence risk metrics and risk models for specific AI risk scenarios.
Furthermore, the financial departments, legal departments and Government Risk
Compliance teams seem to remain unaware of many technical aspects of AI
systems, in which data scientists and AI engineers emerge as the most
appropriate implementers. It is crucial to decompose the problem of artificial
intelligence risk in several dimensions: data protection, fairness, accuracy,
robustness, and information security. Consequently, the main task is developing
adequate metrics and risk models that manage to reduce uncertainty for
decision-making in order to take informed decisions concerning the risk
management of AI systems.
  The purpose of this paper is to orientate AI stakeholders about the depths of
AI risk management. Although it is not extremely technical, it requires a basic
knowledge of risk management, quantifying uncertainty, the FAIR model, machine
learning, large language models and AI context engineering. The examples
presented pretend to be very basic and understandable, providing simple ideas
that can be developed regarding specific AI customized environments. There are
many issues to solve in AI risk management, and this paper will present a
holistic overview of the inter-dependencies of AI risks, and how to model them
together, within risk scenarios.
\\ ( https://arxiv.org/abs/2509.18394 ,  1232kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18407 (*cross-listing*)
Date: Mon, 22 Sep 2025 20:46:23 GMT   (442kb)

Title: Assistive Decision-Making for Right of Way Navigation at Uncontrolled
  Intersections
Authors: Navya Tiwari, Joseph Vazhaeparampil, Victoria Preston
Categories: cs.RO cs.AI cs.HC
Comments: 6 pages, 5 figures. Accepted as a poster at Northeast Robotics
  Colloquium (NERC 2025). Extended abstract
\\
  Uncontrolled intersections account for a significant fraction of roadway
crashes due to ambiguous right-of-way rules, occlusions, and unpredictable
driver behavior. While autonomous vehicle research has explored
uncertainty-aware decision making, few systems exist to retrofit human-operated
vehicles with assistive navigation support. We present a driver-assist
framework for right-of-way reasoning at uncontrolled intersections, formulated
as a Partially Observable Markov Decision Process (POMDP). Using a custom
simulation testbed with stochastic traffic agents, pedestrians, occlusions, and
adversarial scenarios, we evaluate four decision-making approaches: a
deterministic finite state machine (FSM), and three probabilistic planners:
QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform
the rule-based baseline, achieving up to 97.5 percent collision-free navigation
under partial observability, with POMCP prioritizing safety and DESPOT
balancing efficiency and runtime feasibility. Our findings highlight the
importance of uncertainty-aware planning for driver assistance and motivate
future integration of sensor fusion and environment perception modules for
real-time deployment in realistic traffic environments.
\\ ( https://arxiv.org/abs/2509.18407 ,  442kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18415 (*cross-listing*)
Date: Mon, 22 Sep 2025 20:59:51 GMT   (223kb)

Title: Context Lineage Assurance for Non-Human Identities in Critical
  Multi-Agent Systems
Authors: Sumana Malkapuram, Sameera Gangavarapu, Kailashnath Reddy
  Kavalakuntla, and Ananya Gangavarapu
Categories: cs.CR cs.AI
\\
  The proliferation of autonomous software agents necessitates rigorous
frameworks for establishing secure and verifiable agent-to-agent (A2A)
interactions, particularly when such agents are instantiated as non-human
identities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a
cryptographically grounded mechanism for lineage verification, wherein the
provenance and evolution of NHIs are anchored in append-only Merkle tree
structures modeled after Certificate Transparency (CT) logs. Unlike traditional
A2A models that primarily secure point-to-point interactions, our approach
enables both agents and external verifiers to cryptographically validate
multi-hop provenance, thereby ensuring the integrity of the entire call chain.
  A federated proof server acts as an auditor across one or more Merkle logs,
aggregating inclusion proofs and consistency checks into compact, signed
attestations that external parties can verify without access to the full
execution trace. In parallel, we augment the A2A agent card to incorporate
explicit identity verification primitives, enabling both peer agents and human
approvers to authenticate the legitimacy of NHI representations in a
standardized manner. Together, these contributions establish a cohesive model
that integrates identity attestation, lineage verification, and independent
proof auditing, thereby advancing the security posture of inter-agent
ecosystems and providing a foundation for robust governance of NHIs in
regulated environments such as FedRAMP.
\\ ( https://arxiv.org/abs/2509.18415 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18424 (*cross-listing*)
Date: Mon, 22 Sep 2025 21:08:06 GMT   (2171kb)

Title: Scattering Transformer: A Training-Free Transformer Architecture for
  Heart Murmur Detection
Authors: Rami Zewail
Categories: cs.SD cs.AI eess.AS
\\
  In an attempt to address the need for skilled clinicians in heart sound
interpretation, recent research efforts on automating cardiac auscultation have
explored deep learning approaches. The majority of these approaches have been
based on supervised learning that is always challenged in occasions where
training data is limited. More recently, there has been a growing interest in
potentials of pre-trained self-supervised audio foundation models for
biomedical end tasks. Despite exhibiting promising results, these foundational
models are typically computationally intensive. Within the context of automatic
cardiac auscultation, this study explores a lightweight alternative to these
general-purpose audio foundation models by introducing the Scattering
Transformer, a novel, training-free transformer architecture for heart murmur
detection. The proposed method leverages standard wavelet scattering networks
by introducing contextual dependencies in a transformer-like architecture
without any backpropagation. We evaluate our approach on the public CirCor
DigiScope dataset, directly comparing it against leading general-purpose
foundational models. The Scattering Transformer achieves a Weighted
Accuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697,
demonstrating performance highly competitive with contemporary state of the art
methods. This study establishes the Scattering Transformer as a viable and
promising alternative in resource-constrained setups.
\\ ( https://arxiv.org/abs/2509.18424 ,  2171kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18447 (*cross-listing*)
Date: Mon, 22 Sep 2025 22:05:11 GMT   (4438kb)

Title: PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical
  Human-Robot Interaction
Authors: Rishabh Madan, Jiawei Lin, Mahika Goel, Angchen Xie, Xiaoyu Liang,
  Marcus Lee, Justin Guo, Pranav N. Thakkar, Rohan Banerjee, Jose Barreiros,
  Kate Tsui, Tom Silver, Tapomayukh Bhattacharjee
Categories: cs.RO cs.AI
Comments: Conference on Robot Learning (CoRL)
\\
  Physical human-robot interaction (pHRI) requires robots to adapt to
individual contact preferences, such as where and how much force is applied.
Identifying preferences is difficult for a single contact; with whole-arm
interaction involving multiple simultaneous contacts between the robot and
human, the challenge is greater because different body parts can impose
incompatible force requirements. In caregiving tasks, where contact is frequent
and varied, such conflicts are unavoidable. With multiple preferences across
multiple contacts, no single solution can satisfy all objectives--trade-offs
are inherent, making prioritization essential. We present PrioriTouch, a
framework for ranking and executing control objectives across multiple
contacts. PrioriTouch can prioritize from a general collection of controllers,
making it applicable not only to caregiving scenarios such as bed bathing and
dressing but also to broader multi-contact settings. Our method combines a
novel learning-to-rank approach with hierarchical operational space control,
leveraging simulation-in-the-loop rollouts for data-efficient and safe
exploration. We conduct a user study on physical assistance preferences, derive
personalized comfort thresholds, and incorporate them into PrioriTouch. We
evaluate PrioriTouch through extensive simulation and real-world experiments,
demonstrating its ability to adapt to user contact preferences, maintain task
performance, and enhance safety and comfort. Website:
https://emprise.cs.cornell.edu/prioritouch.
\\ ( https://arxiv.org/abs/2509.18447 ,  4438kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18461 (*cross-listing*)
Date: Mon, 22 Sep 2025 22:33:16 GMT   (3219kb)

Title: Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake
  Content Before It's Created?
Authors: Ayan Sar, Sampurna Roy, Tanupriya Choudhury, and Ajith Abraham
Categories: cs.GR cs.AI cs.CV cs.MM
Comments: Published in Foundations and Trends in Signal Processing (#1 in
  Signal Processing, #3 in Computer Science)
Journal-ref: Foundations and Trends in Signal Processing (2025)
DOI: 10.1561/2000000136
\\
  Generative adversarial networks (GANs) and diffusion models have dramatically
advanced deepfake technology, and its threats to digital security, media
integrity, and public trust have increased rapidly. This research explored
zero-shot deepfake detection, an emerging method even when the models have
never seen a particular deepfake variation. In this work, we studied
self-supervised learning, transformer-based zero-shot classifier, generative
model fingerprinting, and meta-learning techniques that better adapt to the
ever-evolving deepfake threat. In addition, we suggested AI-driven prevention
strategies that mitigated the underlying generation pipeline of the deepfakes
before they occurred. They consisted of adversarial perturbations for creating
deepfake generators, digital watermarking for content authenticity
verification, real-time AI monitoring for content creation pipelines, and
blockchain-based content verification frameworks. Despite these advancements,
zero-shot detection and prevention faced critical challenges such as
adversarial attacks, scalability constraints, ethical dilemmas, and the absence
of standardized evaluation benchmarks. These limitations were addressed by
discussing future research directions on explainable AI for deepfake detection,
multimodal fusion based on image, audio, and text analysis, quantum AI for
enhanced security, and federated learning for privacy-preserving deepfake
detection. This further highlighted the need for an integrated defense
framework for digital authenticity that utilized zero-shot learning in
combination with preventive deepfake mechanisms. Finally, we highlighted the
important role of interdisciplinary collaboration between AI researchers,
cybersecurity experts, and policymakers to create resilient defenses against
the rising tide of deepfake attacks.
\\ ( https://arxiv.org/abs/2509.18461 ,  3219kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18507 (*cross-listing*)
Date: Tue, 23 Sep 2025 01:16:23 GMT   (1210kb)

Title: Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in
  Neural Imaging Data
Authors: Mohammad Hosseini, Maryam M. Shanechi
Categories: q-bio.NC cs.AI cs.CV cs.LG
Comments: Published at the 42nd International Conference on Machine Learning
  (ICML) 2025. Code available at: https://github.com/ShanechiLab/SBIND/
Journal-ref: ICML 2025
\\
  High-dimensional imaging of neural activity, such as widefield calcium and
functional ultrasound imaging, provide a rich source of information for
understanding the relationship between brain activity and behavior. Accurately
modeling neural dynamics in these modalities is crucial for understanding this
relationship but is hindered by the high-dimensionality, complex spatiotemporal
dependencies, and prevalent behaviorally irrelevant dynamics in these
modalities. Existing dynamical models often employ preprocessing steps to
obtain low-dimensional representations from neural image modalities. However,
this process can discard behaviorally relevant information and miss
spatiotemporal structure. We propose SBIND, a novel data-driven deep learning
framework to model spatiotemporal dependencies in neural images and disentangle
their behaviorally relevant dynamics from other neural dynamics. We validate
SBIND on widefield imaging datasets, and show its extension to functional
ultrasound imaging, a recent modality whose dynamical modeling has largely
remained unexplored. We find that our model effectively identifies both local
and long-range spatial dependencies across the brain while also dissociating
behaviorally relevant neural dynamics. Doing so, SBIND outperforms existing
models in neural-behavioral prediction. Overall, SBIND provides a versatile
tool for investigating the neural mechanisms underlying behavior using imaging
modalities.
\\ ( https://arxiv.org/abs/2509.18507 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18520 (*cross-listing*)
Date: Tue, 23 Sep 2025 01:32:28 GMT   (944kb)

Title: Coherence-driven inference for cybersecurity
Authors: Steve Huntsman
Categories: cs.CR cs.AI
Comments: LLM4Sec - Workshop on the use of Large Language Models for
  Cybersecurity (https://llm4sec-workshop.github.io/)
\\
  Large language models (LLMs) can compile weighted graphs on natural language
data to enable automatic coherence-driven inference (CDI) relevant to red and
blue team operations in cybersecurity. This represents an early application of
automatic CDI that holds near- to medium-term promise for decision-making in
cybersecurity and eventually also for autonomous blue team operations.
\\ ( https://arxiv.org/abs/2509.18520 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18521 (*cross-listing*)
Date: Tue, 23 Sep 2025 01:32:36 GMT   (18686kb)

Title: APRIL: Active Partial Rollouts in Reinforcement Learning to tame
  long-tail generation
Authors: Yuzhen Zhou, Jiajun Li, Yusheng Su, Gowtham Ramesh, Zilin Zhu, Xiang
  Long, Chenyang Zhao, Jin Pan, Xiaodong Yu, Ze Wang, Kangrui Du, Jialian Wu,
  Ximeng Sun, Jiang Liu, Qiaolin Yu, Hao Chen, Zicheng Liu, Emad Barsoum
Categories: cs.LG cs.AI
\\
  Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.
\\ ( https://arxiv.org/abs/2509.18521 ,  18686kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18523 (*cross-listing*)
Date: Tue, 23 Sep 2025 01:40:14 GMT   (822kb)

Title: Automatic coherence-driven inference on arguments
Authors: Steve Huntsman
Categories: cs.CY cs.AI
Comments: Workshop on Data Mining and AI for Law
  (https://dmail-workshop.github.io/DMAIL2025/)
\\
  Inconsistencies are ubiquitous in law, administration, and jurisprudence.
Though a cure is too much to hope for, we propose a technological remedy. Large
language models (LLMs) can accurately extract propositions from arguments and
compile them into natural data structures that enable coherence-driven
inference (CDI) via combinatorial optimization. This neurosymbolic architecture
naturally separates concerns and enables meaningful judgments about the
coherence of arguments that can inform legislative and policy analysis and
legal reasoning.
\\ ( https://arxiv.org/abs/2509.18523 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18531 (*cross-listing*)
Date: Tue, 23 Sep 2025 01:51:38 GMT   (221kb)

Title: No Verifiable Reward for Prosody: Toward Preference-Guided Prosody
  Learning in TTS
Authors: Seungyoun Shin, Dongha Ahn, Jiwoo Kim, Sungwook Jeon
Categories: eess.AS cs.AI cs.CL cs.SD
Comments: submitted to ICASSP 2026
\\
  Recent work reports gains in neural text-to-speech (TTS) with Group Relative
Policy Optimization (GRPO). However, in the absence of a verifiable reward for
\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)
lowers error rates yet collapses prosody into monotone, unnatural speech;
adding speaker-similarity further destabilizes training and degrades CER. We
address this with an \textit{iterative Direct Preference Optimization (DPO)}
scheme that uses only a few hundred human-labeled preference pairs per round to
directly optimize prosodic naturalness while regularizing to the current model.
On \textbf{KoCC-TTS}, a curated dataset of authentic Korean call center
interactions capturing task-oriented dialogues, our method attains the highest
human preference (ELO) with competitive CER, outperforming GRPO and strong
commercial baselines. These results suggest that when prosody cannot be
rewarded automatically, \textit{human preference optimization} offers a
practical and data-efficient path to natural and robust TTS. The demo page is
available at \href{https://tts.ch.dev}
\\ ( https://arxiv.org/abs/2509.18531 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18542 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:07:14 GMT   (496kb)

Title: Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent
  Mixture-of-Experts
Authors: Qi Wang, Hanyang Peng, Yue Yu
Categories: cs.LG cs.AI
\\
  Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.
\\ ( https://arxiv.org/abs/2509.18542 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18552 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:24:23 GMT   (4416kb)

Title: Global Minimizers of Sigmoid Contrastive Loss
Authors: Kiril Bangachev, Guy Bresler, Iliyas Noman, Yury Polyanskiy
Categories: cs.LG cs.AI
Comments: Author names listed in alphabetical order. NeurIPS 2025
\\
  The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.
\\ ( https://arxiv.org/abs/2509.18552 ,  4416kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18561 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:36:39 GMT   (3061kb)

Title: SoundCompass: Navigating Target Sound Extraction With Effective
  Directional Clue Integration In Complex Acoustic Scenes
Authors: Dayun Choi, Jung-Woo Choi
Categories: eess.AS cs.AI cs.SD
Comments: 5 pages, 4 figures, submitted to ICASSP 2026
\\
  Recent advances in target sound extraction (TSE) utilize directional clues
derived from direction of arrival (DoA), which represent an inherent spatial
property of sound available in any acoustic scene. However, previous DoA-based
methods rely on hand-crafted features or discrete encodings, which lose
fine-grained spatial information and limit adaptability. We propose
SoundCompass, an effective directional clue integration framework centered on a
Spectral Pairwise INteraction (SPIN) module that captures cross-channel spatial
correlations in the complex spectrogram domain to preserve full spatial
information in multichannel signals. The input feature expressed in terms of
spatial correlations is fused with a DoA clue represented as spherical
harmonics (SH) encoding. The fusion is carried out across overlapping frequency
subbands, inheriting the benefits reported in the previous band-split
architectures. We also incorporate the iterative refinement strategy,
chain-of-inference (CoI), in the TSE framework, which recursively fuses DoA
with sound event activation estimated from the previous inference stage.
Experiments demonstrate that SoundCompass, combining SPIN, SH embedding, and
CoI, robustly extracts target sources across diverse signal classes and spatial
configurations.
\\ ( https://arxiv.org/abs/2509.18561 ,  3061kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18562 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:38:49 GMT   (305kb)

Title: CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese
  Patronizing and Condescending Language Detection
Authors: Jiaxun Yang, Yifei Han, Long Zhang, Liu Yujie, Bin Li, Bo Gao, Yangfan
  He, Kejia Zhan
Categories: cs.MM cs.AI
Comments: Submitted to ICASSP 2025
\\
  Chinese Patronizing and Condescending Language (CPCL) is an implicitly
discriminatory toxic speech targeting vulnerable groups on Chinese video
platforms. The existing dataset lacks user comments, which are a direct
reflection of video content. This undermines the model's understanding of video
content and results in the failure to detect some CPLC videos. To make up for
this loss, this research reconstructs a new dataset PCLMMPLUS that includes
103k comment entries and expands the dataset size. We also propose the
CPCLDetector model with alignment selection and knowledge-enhanced comment
content modules. Extensive experiments show the proposed CPCLDetector
outperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS .
CPLC videos are detected more accurately, supporting content governance and
protecting vulnerable groups. Code and dataset are available at
https://github.com/jiaxunyang256/PCLD.
\\ ( https://arxiv.org/abs/2509.18562 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18569 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:52:54 GMT   (302kb)

Title: Explore the Reinforcement Learning for the LLM based ASR and TTS system
Authors: Changfeng Gao, Yabin Li, Keyu An, Zhifu Gao, Zhihao Du, Han Zhao,
  Xiangang Li
Categories: cs.SD cs.AI eess.AS
\\
  In recent years, large language models (LLMs) have played an important role
in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While
reinforcement learning (RL) has significantly enhanced LLM performance in
text-based tasks, its application to ASR and TTS remains underexplored due to
the complexity of training audio-based models. In this study, we propose a
lightweight RL framework tailored for audio-based LLMs that can process audio
inputs and generate audio outputs. Based on this framework, we evaluate the
effectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR
task, we experiment with different rule-based reward functions within the Group
Relative Policy Optimization (GRPO) framework and investigate the impact of RL
data construction. For the TTS task, we compare GRPO with Differentiable Reward
Optimization (DiffRO) and further combine the two approaches to achieve
improved performance. Our experiments demonstrate that RL can significantly
enhance the performance of both ASR and TTS systems, even with limited training
data and a small number of optimization steps.
\\ ( https://arxiv.org/abs/2509.18569 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18573 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:56:05 GMT   (2562kb)

Title: Interaction Topological Transformer for Multiscale Learning in Porous
  Materials
Authors: Dong Chen, Jian Liu, Chun-Long Chen, Guo-Wei Wei
Categories: cs.LG cond-mat.mtrl-sci cs.AI
Comments: 4 figures, 2 tables
\\
  Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.
\\ ( https://arxiv.org/abs/2509.18573 ,  2562kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18575 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:56:38 GMT   (5327kb)

Title: The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking
Authors: Yaoyao Qian, Yifan Zeng, Yuchao Jiang, Chelsi Jain, Huazheng Wang
Categories: cs.IR cs.AI
Comments: Accepted by EMNLP 2025
\\
  Large Language Models (LLMs) have demonstrated strong performance in
information retrieval tasks like passage ranking. Our research examines how
instruction-following capabilities in LLMs interact with multi-document
comparison tasks, identifying what we term the "Ranking Blind Spot", a
characteristic of LLM decision processes during comparative evaluation. We
analyze how this ranking blind spot affects LLM evaluation systems through two
approaches: Decision Objective Hijacking, which alters the evaluation goal in
pairwise ranking systems, and Decision Criteria Hijacking, which modifies
relevance standards across ranking schemes. These approaches demonstrate how
content providers could potentially influence LLM-based ranking systems to
affect document positioning. These attacks aim to force the LLM ranker to
prefer a specific passage and rank it at the top. Malicious content providers
can exploit this weakness, which helps them gain additional exposure by
attacking the ranker. In our experiment, We empirically show that the proposed
attacks are effective in various LLMs and can be generalized to multiple
ranking schemes. We apply these attack to realistic examples to show their
effectiveness. We also found stronger LLMs are more vulnerable to these
attacks. Our code is available at:
https://github.com/blindspotorg/RankingBlindSpot
\\ ( https://arxiv.org/abs/2509.18575 ,  5327kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18576 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:57:25 GMT   (12089kb)

Title: LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA
Authors: Zeyi Kang (1), Liang He (2), Yanxin Zhang (3), Zuheng Ming (4),
  Kaixing Zhao (5) ((1) Northwestern Polytechnical University, (2) University
  Sorbonne Paris Nord)
Categories: cs.RO cs.AI
\\
  Multimodal semantic learning plays a critical role in embodied intelligence,
especially when robots perceive their surroundings, understand human
instructions, and make intelligent decisions. However, the field faces
technical challenges such as effective fusion of heterogeneous data and
computational efficiency in resource-constrained environments. To address these
challenges, this study proposes the lightweight LCMF cascaded attention
framework, introducing a multi-level cross-modal parameter sharing mechanism
into the Mamba module. By integrating the advantages of Cross-Attention and
Selective parameter-sharing State Space Models (SSMs), the framework achieves
efficient fusion of heterogeneous modalities and semantic complementary
alignment. Experimental results show that LCMF surpasses existing multimodal
baselines with an accuracy of 74.29% in VQA tasks and achieves competitive
mid-tier performance within the distribution cluster of Large Language Model
Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a
4.35-fold reduction in FLOPs relative to the average of comparable baselines
while using only 166.51M parameters (image-text) and 219M parameters
(video-text), providing an efficient solution for Human-Robot Interaction (HRI)
applications in resource-constrained scenarios with strong multimodal decision
generalization capabilities.
\\ ( https://arxiv.org/abs/2509.18576 ,  12089kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18592 (*cross-listing*)
Date: Tue, 23 Sep 2025 03:23:03 GMT   (6334kb)

Title: VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic
  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation
Authors: Neel P. Bhatt, Yunhao Yang, Rohan Siva, Pranay Samineni, Daniel Milan,
  Zhangyang Wang, and Ufuk Topcu
Categories: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY
Comments: Codebase, datasets, and videos for VLN-Zero are available at:
  https://vln-zero.github.io/
\\
  Rapid adaptation in unseen environments is essential for scalable real-world
autonomy, yet existing approaches rely on exhaustive exploration or rigid
navigation policies that fail to generalize. We present VLN-Zero, a two-phase
vision-language navigation framework that leverages vision-language models to
efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic
navigation. In the exploration phase, structured prompts guide VLM-based search
toward informative and diverse trajectories, yielding compact scene graph
representations. In the deployment phase, a neurosymbolic planner reasons over
the scene graph and environmental observations to generate executable plans,
while a cache-enabled execution module accelerates adaptation by reusing
previously computed task-location trajectories. By combining rapid exploration,
symbolic reasoning, and cache-enabled execution, the proposed framework
overcomes the computational inefficiency and poor generalization of prior
vision-language navigation methods, enabling robust and scalable
decision-making in unseen environments. VLN-Zero achieves 2x higher success
rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned
baselines, and reaches goal locations in half the time with 55% fewer VLM calls
on average compared to state-of-the-art models across diverse environments.
Codebase, datasets, and videos for VLN-Zero are available at:
https://vln-zero.github.io/.
\\ ( https://arxiv.org/abs/2509.18592 ,  6334kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18603 (*cross-listing*)
Date: Tue, 23 Sep 2025 03:48:26 GMT   (1028kb)

Title: SynSonic: Augmenting Sound Event Detection through Text-to-Audio
  Diffusion ControlNet and Effective Sample Filtering
Authors: Jiarui Hai, Mounya Elhilali
Categories: eess.AS cs.AI cs.SD
\\
  Data synthesis and augmentation are essential for Sound Event Detection (SED)
due to the scarcity of temporally labeled data. While augmentation methods like
SpecAugment and Mix-up can enhance model performance, they remain constrained
by the diversity of existing samples. Recent generative models offer new
opportunities, yet their direct application to SED is challenging due to the
lack of precise temporal annotations and the risk of introducing noise through
unreliable filtering. To address these challenges and enable generative-based
augmentation for SED, we propose SynSonic, a data augmentation method tailored
for this task. SynSonic leverages text-to-audio diffusion models guided by an
energy-envelope ControlNet to generate temporally coherent sound events. A
joint score filtering strategy with dual classifiers ensures sample quality,
and we explore its practical integration into training pipelines. Experimental
results show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1
and PSDS2), enhancing both temporal localization and sound class
discrimination.
\\ ( https://arxiv.org/abs/2509.18603 ,  1028kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18606 (*cross-listing*)
Date: Tue, 23 Sep 2025 03:52:52 GMT   (432kb)

Title: FlexSED: Towards Open-Vocabulary Sound Event Detection
Authors: Jiarui Hai, Helin Wang, Weizhe Guo, Mounya Elhilali
Categories: eess.AS cs.AI cs.SD
\\
  Despite recent progress in large-scale sound event detection (SED) systems
capable of handling hundreds of sound classes, existing multi-class
classification frameworks remain fundamentally limited. They cannot process
free-text sound queries, which enable more flexible and user-friendly
interaction, and they lack zero-shot capabilities and offer poor few-shot
adaptability. Although text-query-based separation methods have been explored,
they primarily focus on source separation and are ill-suited for SED tasks that
require precise temporal localization and efficient detection across large and
diverse sound vocabularies. In this paper, we propose FlexSED, an
open-vocabulary sound event detection system. FlexSED builds on a pretrained
audio SSL model and the CLAP text encoder, introducing an encoder-decoder
composition and an adaptive fusion strategy to enable effective continuous
training from pretrained weights. To ensure robust supervision, it also employs
large language models (LLMs) to assist in event query selection during
training, addressing challenges related to missing labels. As a result, FlexSED
achieves superior performance compared to vanilla SED models on
AudioSet-Strong, while demonstrating strong zero-shot and few-shot
capabilities. We release the code and pretrained models to support future
research and applications based on FlexSED.
\\ ( https://arxiv.org/abs/2509.18606 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18608 (*cross-listing*)
Date: Tue, 23 Sep 2025 03:56:10 GMT   (6678kb)

Title: End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement
  Learning
Authors: Ana Luiza Mineiro, Francisco Affonso, Marcelo Becker
Categories: cs.RO cs.AI
Comments: Accepted to the 22nd International Conference on Advanced Robotics
  (ICAR 2025). 7 pages
\\
  Reliable navigation in under-canopy agricultural environments remains a
challenge due to GNSS unreliability, cluttered rows, and variable lighting. To
address these limitations, we present an end-to-end learning-based navigation
system that maps raw 3D LiDAR data directly to control commands using a deep
reinforcement learning policy trained entirely in simulation. Our method
includes a voxel-based downsampling strategy that reduces LiDAR input size by
95.83%, enabling efficient policy learning without relying on labeled datasets
or manually designed control interfaces. The policy was validated in
simulation, achieving a 100% success rate in straight-row plantations and
showing a gradual decline in performance as row curvature increased, tested
across varying sinusoidal frequencies and amplitudes.
\\ ( https://arxiv.org/abs/2509.18608 ,  6678kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18611 (*cross-listing*)
Date: Tue, 23 Sep 2025 04:00:41 GMT   (1480kb)

Title: Flow marching for a generative PDE foundation model
Authors: Zituo Chen, Sili Deng
Categories: cs.LG cs.AI
\\
  Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.
\\ ( https://arxiv.org/abs/2509.18611 ,  1480kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18626 (*cross-listing*)
Date: Tue, 23 Sep 2025 04:21:39 GMT   (19676kb)

Title: The Case for Negative Data: From Crash Reports to Counterfactuals for
  Reasonable Driving
Authors: Jay Patrikar, Apoorva Sharma, Sushant Veer, Boyi Li, Sebastian
  Scherer, Marco Pavone
Categories: cs.RO cs.AI
Comments: 8 pages, 5 figures
\\
  Learning-based autonomous driving systems are trained mostly on incident-free
data, offering little guidance near safety-performance boundaries. Real crash
reports contain precisely the contrastive evidence needed, but they are hard to
use: narratives are unstructured, third-person, and poorly grounded to sensor
views. We address these challenges by normalizing crash narratives to
ego-centric language and converting both logs and crashes into a unified
scene-action representation suitable for retrieval. At decision time, our
system adjudicates proposed actions by retrieving relevant precedents from this
unified index; an agentic counterfactual extension proposes plausible
alternatives, retrieves for each, and reasons across outcomes before deciding.
On a nuScenes benchmark, precedent retrieval substantially improves
calibration, with recall on contextually preferred actions rising from 24% to
53%. The counterfactual variant preserves these gains while sharpening
decisions near risk.
\\ ( https://arxiv.org/abs/2509.18626 ,  19676kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18627 (*cross-listing*)
Date: Tue, 23 Sep 2025 04:22:53 GMT   (7314kb)

Title: BRAID: Input-Driven Nonlinear Dynamical Modeling of Neural-Behavioral
  Data
Authors: Parsa Vahidi, Omid G. Sani, Maryam M. Shanechi
Categories: q-bio.NC cs.AI cs.LG
Comments: Published at the International Conference on Learning Representations
  (ICLR) 2025. Code is available at GitHub https://github.com/ShanechiLab/BRAID
Journal-ref: ICLR 2025
\\
  Neural populations exhibit complex recurrent structures that drive behavior,
while continuously receiving and integrating external inputs from sensory
stimuli, upstream regions, and neurostimulation. However, neural populations
are often modeled as autonomous dynamical systems, with little consideration
given to the influence of external inputs that shape the population activity
and behavioral outcomes. Here, we introduce BRAID, a deep learning framework
that models nonlinear neural dynamics underlying behavior while explicitly
incorporating any measured external inputs. Our method disentangles intrinsic
recurrent neural population dynamics from the effects of inputs by including a
forecasting objective within input-driven recurrent neural networks. BRAID
further prioritizes the learning of intrinsic dynamics that are related to a
behavior of interest by using a multi-stage optimization scheme. We validate
BRAID with nonlinear simulations, showing that it can accurately learn the
intrinsic dynamics shared between neural and behavioral modalities. We then
apply BRAID to motor cortical activity recorded during a motor task and
demonstrate that our method more accurately fits the neural-behavioral data by
incorporating measured sensory stimuli into the model and improves the
forecasting of neural-behavioral data compared with various baseline methods,
whether input-driven or not.
\\ ( https://arxiv.org/abs/2509.18627 ,  7314kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18629 (*cross-listing*)
Date: Tue, 23 Sep 2025 04:29:26 GMT   (234kb)

Title: HyperAdapt: Simple High-Rank Adaptation
Authors: Abel Gurung, Joseph Campbell
Categories: cs.LG cs.AI
\\
  Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.
\\ ( https://arxiv.org/abs/2509.18629 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18631 (*cross-listing*)
Date: Tue, 23 Sep 2025 04:32:53 GMT   (7322kb)

Title: Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training
Authors: Shuo Cheng, Liqian Ma, Zhenyang Chen, Ajay Mandlekar, Caelan Garrett,
  Danfei Xu
Categories: cs.RO cs.AI
\\
  Behavior cloning has shown promise for robot manipulation, but real-world
demonstrations are costly to acquire at scale. While simulated data offers a
scalable alternative, particularly with advances in automated demonstration
generation, transferring policies to the real world is hampered by various
simulation and real domain gaps. In this work, we propose a unified
sim-and-real co-training framework for learning generalizable manipulation
policies that primarily leverages simulation and only requires a few real-world
demonstrations. Central to our approach is learning a domain-invariant,
task-relevant feature space. Our key insight is that aligning the joint
distributions of observations and their corresponding actions across domains
provides a richer signal than aligning observations (marginals) alone. We
achieve this by embedding an Optimal Transport (OT)-inspired loss within the
co-training framework, and extend this to an Unbalanced OT framework to handle
the imbalance between abundant simulation data and limited real-world examples.
We validate our method on challenging manipulation tasks, showing it can
leverage abundant simulation data to achieve up to a 30% improvement in the
real-world success rate and even generalize to scenarios seen only in
simulation.
\\ ( https://arxiv.org/abs/2509.18631 ,  7322kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18644 (*cross-listing*)
Date: Tue, 23 Sep 2025 04:56:59 GMT   (2028kb)

Title: Do You Need Proprioceptive States in Visuomotor Policies?
Authors: Juntu Zhao, Wenbo Lu, Di Zhang, Yufeng Liu, Yushen Liang, Tianluo
  Zhang, Yifeng Cao, Junyuan Xie, Yingdong Hu, Shengjie Wang, Junliang Guo,
  Dequan Wang, Yang Gao
Categories: cs.RO cs.AI
Comments: Project page: https://statefreepolicy.github.io
\\
  Imitation-learning-based visuomotor policies have been widely used in robot
manipulation, where both visual observations and proprioceptive states are
typically adopted together for precise control. However, in this study, we find
that this common practice makes the policy overly reliant on the proprioceptive
state input, which causes overfitting to the training trajectories and results
in poor spatial generalization. On the contrary, we propose the State-free
Policy, removing the proprioceptive state input and predicting actions only
conditioned on visual observations. The State-free Policy is built in the
relative end-effector action space, and should ensure the full task-relevant
visual observations, here provided by dual wide-angle wrist cameras. Empirical
results demonstrate that the State-free policy achieves significantly stronger
spatial generalization than the state-based policy: in real-world tasks such as
pick-and-place, challenging shirt-folding, and complex whole-body manipulation,
spanning multiple robot embodiments, the average success rate improves from 0\%
to 85\% in height generalization and from 6\% to 64\% in horizontal
generalization. Furthermore, they also show advantages in data efficiency and
cross-embodiment adaptation, enhancing their practicality for real-world
deployment.
\\ ( https://arxiv.org/abs/2509.18644 ,  2028kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18648 (*cross-listing*)
Date: Tue, 23 Sep 2025 05:03:00 GMT   (11668kb)

Title: SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer
Authors: Yarden As, Chengrui Qu, Benjamin Unger, Dongho Kang, Max van der Hart,
  Laixi Shi, Stelian Coros, Adam Wierman, Andreas Krause
Categories: cs.RO cs.AI
\\
  Safety remains a major concern for deploying reinforcement learning (RL) in
real-world applications. Simulators provide safe, scalable training
environments, but the inevitable sim-to-real gap introduces additional safety
concerns, as policies must satisfy constraints in real-world conditions that
differ from simulation. To address this challenge, robust safe RL techniques
offer principled methods, but are often incompatible with standard scalable
training pipelines. In contrast, domain randomization, a simple and popular
sim-to-real technique, stands out as a promising alternative, although it often
results in unsafe behaviors in practice. We present SPiDR, short for
Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with
provable guarantees for safe sim-to-real transfer. SPiDR uses domain
randomization to incorporate the uncertainty about the sim-to-real gap into the
safety constraints, making it versatile and highly compatible with existing
training pipelines. Through extensive experiments on sim-to-sim benchmarks and
two distinct real-world robotic platforms, we demonstrate that SPiDR
effectively ensures safety despite the sim-to-real gap while maintaining strong
performance.
\\ ( https://arxiv.org/abs/2509.18648 ,  11668kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18672 (*cross-listing*)
Date: Tue, 23 Sep 2025 05:45:11 GMT   (6704kb)

Title: NaviSense: A Multimodal Assistive Mobile application for Object
  Retrieval by Persons with Visual Impairment
Authors: Ajay Narayanan Sridhar (1), Fuli Qiao (1), Nelson Daniel Troncoso
  Aldas (2), Yanpei Shi (3), Mehrdad Mahdavi (1), Laurent Itti (3),
  Vijaykrishnan Narayanan (1) ((1) The Pennsylvania State University, (2)
  Independent Researcher, (3) University of Southern California)
Categories: cs.HC cs.AI
\\
  People with visual impairments often face significant challenges in locating
and retrieving objects in their surroundings. Existing assistive technologies
present a trade-off: systems that offer precise guidance typically require
pre-scanning or support only fixed object categories, while those with
open-world object recognition lack spatial feedback for reaching the object. To
address this gap, we introduce 'NaviSense', a mobile assistive system that
combines conversational AI, vision-language models, augmented reality (AR), and
LiDAR to support open-world object detection with real-time audio-haptic
guidance. Users specify objects via natural language and receive continuous
spatial feedback to navigate toward the target without needing prior setup.
Designed with insights from a formative study and evaluated with 12 blind and
low-vision participants, NaviSense significantly reduced object retrieval time
and was preferred over existing tools, demonstrating the value of integrating
open-world perception with precise, accessible guidance.
\\ ( https://arxiv.org/abs/2509.18672 ,  6704kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18691 (*cross-listing*)
Date: Tue, 23 Sep 2025 06:20:41 GMT   (585kb)

Title: An overview of neural architectures for self-supervised audio
  representation learning from masked spectrograms
Authors: Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan
Categories: cs.SD cs.AI eess.AS
\\
  In recent years, self-supervised learning has amassed significant interest
for training deep neural representations without labeled data. One such
self-supervised learning approach is masked spectrogram modeling, where the
objective is to learn semantically rich contextual representations by
predicting removed or hidden portions of the input audio spectrogram. With the
Transformer neural architecture at its core, masked spectrogram modeling has
emerged as the prominent approach for learning general purpose audio
representations, a.k.a. audio foundation models. Meanwhile, addressing the
issues of the Transformer architecture, in particular the underlying Scaled
Dot-product Attention operation, which scales quadratically with input sequence
length, has led to renewed interest in recurrent sequence modeling approaches.
Among them, Selective structured state space models (such as Mamba) and
extended Long Short-Term Memory (xLSTM) are the two most promising approaches
which have experienced widespread adoption. While the body of work on these two
topics continues to grow, there is currently a lack of an adequate overview
encompassing the intersection of these topics. In this paper, we present a
comprehensive overview of the aforementioned research domains, covering masked
spectrogram modeling and the previously mentioned neural sequence modeling
architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and
xLSTM based masked spectrogram models in a unified, reproducible framework on
ten diverse downstream audio classification tasks, which will help interested
readers to make informed decisions regarding suitability of the evaluated
approaches to adjacent applications.
\\ ( https://arxiv.org/abs/2509.18691 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18714 (*cross-listing*)
Date: Tue, 23 Sep 2025 07:02:05 GMT   (221kb)

Title: A Generalized Bisimulation Metric of State Similarity between Markov
  Decision Processes: From Theoretical Propositions to Applications
Authors: Zhenyu Tao, Wei Xu, Xiaohu You
Categories: cs.LG cs.AI
Comments: This paper is accepted by the 39th Conference on Neural Information
  Processing Systems (NeurIPS 2025)
\\
  The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.
\\ ( https://arxiv.org/abs/2509.18714 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18757 (*cross-listing*)
Date: Tue, 23 Sep 2025 07:53:05 GMT   (6653kb)

Title: MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning
Authors: Omar Rayyan, John Abanes, Mahmoud Hafez, Anthony Tzes, Fares Abu-Dakka
Categories: cs.RO cs.AI
Comments: For project website and videos, see https https://mv-umi.github.io
\\
  Recent advances in imitation learning have shown great promise for developing
robust robot manipulation policies from demonstrations. However, this promise
is contingent on the availability of diverse, high-quality datasets, which are
not only challenging and costly to collect but are often constrained to a
specific robot embodiment. Portable handheld grippers have recently emerged as
intuitive and scalable alternatives to traditional robotic teleoperation
methods for data collection. However, their reliance solely on first-person
view wrist-mounted cameras often creates limitations in capturing sufficient
scene contexts. In this paper, we present MV-UMI (Multi-View Universal
Manipulation Interface), a framework that integrates a third-person perspective
with the egocentric camera to overcome this limitation. This integration
mitigates domain shifts between human demonstration and robot deployment,
preserving the cross-embodiment advantages of handheld data-collection devices.
Our experimental results, including an ablation study, demonstrate that our
MV-UMI framework improves performance in sub-tasks requiring broad scene
understanding by approximately 47% across 3 tasks, confirming the effectiveness
of our approach in expanding the range of feasible manipulation tasks that can
be learned using handheld gripper systems, without compromising the
cross-embodiment advantages inherent to such systems.
\\ ( https://arxiv.org/abs/2509.18757 ,  6653kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18758 (*cross-listing*)
Date: Tue, 23 Sep 2025 07:53:27 GMT   (1920kb)

Title: Complexity of Activity Patterns in a Bio-Inspired Hopfield-Type Network
  in Different Topologies
Authors: Marco Cafiso and Paolo Paradisi
Categories: q-bio.NC cs.AI nlin.AO physics.bio-ph
\\
  Neural network models capable of storing memory have been extensively studied
in computer science and computational neuroscience. The Hopfield network is a
prototypical example of a model designed for associative, or
content-addressable, memory and has been analyzed in many forms. Further, ideas
and methods from complex network theory have been incorporated into artificial
neural networks and learning, emphasizing their structural properties.
Nevertheless, the temporal dynamics also play a vital role in biological neural
networks, whose temporal structure is a crucial feature to examine. Biological
neural networks display complex intermittency and, thus, can be studied through
the lens of the temporal complexity (TC) theory. The TC approach look at the
metastability of self-organized states, characterized by a power-law decay in
the inter-event time distribution and in the total activity distribution or a
scaling behavior in the corresponding event-driven diffusion processes. In this
study, we present a temporal complexity (TC) analysis of a
biologically-inspired Hopfield-type neural network model. We conducted a
comparative assessment between scale-free and random network topologies, with
particular emphasis on their global activation patterns. Our parametric
analysis revealed comparable dynamical behaviors across both neural network
architectures. Furthermore, our investigation into temporal complexity
characteristics uncovered that seemingly distinct dynamical patterns exhibit
similar temporal complexity behaviors. In particular, similar power-law decay
in the activity distribution and similar complexity levels are observed in both
topologies, but with a much reduced noise in the scale-free topology. Notably,
most of the complex dynamical profiles were consistently observed in scale-free
network configurations, thus confirming the crucial role of hubs in neural
network dynamics.
\\ ( https://arxiv.org/abs/2509.18758 ,  1920kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18761 (*cross-listing*)
Date: Tue, 23 Sep 2025 07:55:35 GMT   (47kb)

Title: Security smells in infrastructure as code: a taxonomy update beyond the
  seven sins
Authors: Aicha War, Serge L.B. Nikiema, Jordan Samhi, Jacques Klein, Tegawende
  F. Bissyande
Categories: cs.CR cs.AI cs.LG cs.SE
\\
  Infrastructure as Code (IaC) has become essential for modern software
management, yet security flaws in IaC scripts can have severe consequences, as
exemplified by the recurring exploits of Cloud Web Services. Prior work has
recognized the need to build a precise taxonomy of security smells in IaC
scripts as a first step towards developing approaches to improve IaC security.
This first effort led to the unveiling of seven sins, limited by the focus on a
single IaC tool as well as by the extensive, and potentially biased, manual
effort that was required. We propose, in our work, to revisit this taxonomy:
first, we extend the study of IaC security smells to a more diverse dataset
with scripts associated with seven popular IaC tools, including Terraform,
Ansible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some
automation for the analysis by relying on an LLM. While we leverage LLMs for
initial pattern processing, all taxonomic decisions underwent systematic human
validation and reconciliation with established security standards. Our study
yields a comprehensive taxonomy of 62 security smell categories, significantly
expanding beyond the previously known seven. We demonstrate actionability by
implementing new security checking rules within linters for seven popular IaC
tools, often achieving 1.00 precision score. Our evolution study of security
smells in GitHub projects reveals that these issues persist for extended
periods, likely due to inadequate detection and mitigation tools. This work
provides IaC practitioners with insights for addressing common security smells
and systematically adopting DevSecOps practices to build safer infrastructure
code.
\\ ( https://arxiv.org/abs/2509.18761 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18778 (*cross-listing*)
Date: Tue, 23 Sep 2025 08:15:30 GMT   (1193kb)

Title: VGGT-DP: Generalizable Robot Control via Vision Foundation Models
Authors: Shijia Ge, Yinxin Zhang, Shuzhao Xie, Weixiang Zhang, Mingcai Zhou,
  Zhi Wang
Categories: cs.RO cs.AI
Comments: submitted to AAAI 2026
\\
  Visual imitation learning frameworks allow robots to learn manipulation
skills from expert demonstrations. While existing approaches mainly focus on
policy design, they often neglect the structure and capacity of visual
encoders, limiting spatial understanding and generalization. Inspired by
biological vision systems, which rely on both visual and proprioceptive cues
for robust control, we propose VGGT-DP, a visuomotor policy framework that
integrates geometric priors from a pretrained 3D perception model with
proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer
(VGGT) as the visual encoder and introduce a proprioception-guided visual
learning strategy to align perception with internal robot states, improving
spatial grounding and closed-loop control. To reduce inference latency, we
design a frame-wise token reuse mechanism that compacts multi-view tokens into
an efficient spatial representation. We further apply random token pruning to
enhance policy robustness and reduce overfitting. Experiments on challenging
MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines
such as DP and DP3, particularly in precision-critical and long-horizon
scenarios.
\\ ( https://arxiv.org/abs/2509.18778 ,  1193kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18790 (*cross-listing*)
Date: Tue, 23 Sep 2025 08:28:49 GMT   (219kb)

Title: Detection of security smells in IaC scripts through semantics-aware code
  and language processing
Authors: Aicha War, Adnan A. Rawass, Abdoul K. Kabore, Jordan Samhi, Jacques
  Klein, and Tegawende F. Bissyande
Categories: cs.CR cs.AI cs.LG cs.SE
\\
  Infrastructure as Code (IaC) automates the provisioning and management of IT
infrastructure through scripts and tools, streamlining software deployment.
Prior studies have shown that IaC scripts often contain recurring security
misconfigurations, and several detection and mitigation approaches have been
proposed. Most of these rely on static analysis, using statistical code
representations or Machine Learning (ML) classifiers to distinguish insecure
configurations from safe code.
  In this work, we introduce a novel approach that enhances static analysis
with semantic understanding by jointly leveraging natural language and code
representations. Our method builds on two complementary ML models: CodeBERT, to
capture semantics across code and text, and LongFormer, to represent long IaC
scripts without losing contextual information. We evaluate our approach on
misconfiguration datasets from two widely used IaC tools, Ansible and Puppet.
To validate its effectiveness, we conduct two ablation studies (removing code
text from the natural language input and truncating scripts to reduce context)
and compare against four large language models (LLMs) and prior work. Results
show that semantic enrichment substantially improves detection, raising
precision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from
0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively.
\\ ( https://arxiv.org/abs/2509.18790 ,  219kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18831 (*cross-listing*)
Date: Tue, 23 Sep 2025 09:17:18 GMT   (23234kb)

Title: Text Slider: Efficient and Plug-and-Play Continuous Concept Control for
  Image/Video Synthesis via LoRA Adapters
Authors: Pin-Yen Chiu, I-Sheng Fang, Jun-Cheng Chen
Categories: cs.GR cs.AI cs.CV cs.LG cs.MM
\\
  Recent advances in diffusion models have significantly improved image and
video synthesis. In addition, several concept control methods have been
proposed to enable fine-grained, continuous, and flexible control over
free-form text prompts. However, these methods not only require intensive
training time and GPU memory usage to learn the sliders or embeddings but also
need to be retrained for different diffusion backbones, limiting their
scalability and adaptability. To address these limitations, we introduce Text
Slider, a lightweight, efficient and plug-and-play framework that identifies
low-rank directions within a pre-trained text encoder, enabling continuous
control of visual concepts while significantly reducing training time, GPU
memory consumption, and the number of trainable parameters. Furthermore, Text
Slider supports multi-concept composition and continuous control, enabling
fine-grained and flexible manipulation in both image and video synthesis. We
show that Text Slider enables smooth and continuous modulation of specific
attributes while preserving the original spatial layout and structure of the
input. Text Slider achieves significantly better efficiency: 5$\times$ faster
training than Concept Slider and 47$\times$ faster than Attribute Control,
while reducing GPU memory usage by nearly 2$\times$ and 4$\times$,
respectively.
\\ ( https://arxiv.org/abs/2509.18831 ,  23234kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18851 (*cross-listing*)
Date: Tue, 23 Sep 2025 09:38:10 GMT   (269kb)

Title: NGRPO: Negative-enhanced Group Relative Policy Optimization
Authors: Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei
  Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, Xingzhong Xu
Categories: cs.LG cs.AI
\\
  RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.
\\ ( https://arxiv.org/abs/2509.18851 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18874 (*cross-listing*)
Date: Tue, 23 Sep 2025 10:10:37 GMT   (2466kb)

Title: When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and
  LLM Profiling Risks
Authors: Baiyu Chen, Benjamin Tag, Hao Xue, Daniel Angus, Flora Salim
Categories: cs.HC cs.AI cs.CY
\\
  Automated ad targeting on social media is opaque, creating risks of
exploitation and invisibility to external scrutiny. Users may be steered toward
harmful content while independent auditing of these processes remains blocked.
Large Language Models (LLMs) raise a new concern: the potential to
reverse-engineer sensitive user attributes from exposure alone. We introduce a
multi-stage auditing framework to investigate these risks. First, a large-scale
audit of over 435,000 ad impressions delivered to 891 Australian Facebook users
reveals algorithmic biases, including disproportionate Gambling and Politics
ads shown to socioeconomically vulnerable and politically aligned groups.
Second, a multimodal LLM can reconstruct users' demographic profiles from ad
streams, outperforming census-based baselines and matching or exceeding human
performance. Our results provide the first empirical evidence that ad streams
constitute rich digital footprints for public AI inference, highlighting urgent
privacy risks and the need for content-level auditing and governance.
\\ ( https://arxiv.org/abs/2509.18874 ,  2466kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18900 (*cross-listing*)
Date: Tue, 23 Sep 2025 11:28:30 GMT   (1528kb)

Title: The AI Literacy Heptagon: A Structured Approach to AI Literacy in Higher
  Education
Authors: Veronika Hackl, Alexandra Mueller, Maximilian Sailer
Categories: cs.CY cs.AI
Comments: 4 figures
\\
  The integrative literature review addresses the conceptualization and
implementation of AI Literacy (AIL) in Higher Education (HE) by examining
recent research literature. Through an analysis of publications (2021-2024), we
explore (1) how AIL is defined and conceptualized in current research,
particularly in HE, and how it can be delineated from related concepts such as
Data Literacy, Media Literacy, and Computational Literacy; (2) how various
definitions can be synthesized into a comprehensive working definition, and (3)
how scientific insights can be effectively translated into educational
practice. Our analysis identifies seven central dimensions of AIL: technical,
applicational, critical thinking, ethical, social, integrational, and legal.
These are synthesized in the AI Literacy Heptagon, deepening conceptual
understanding and supporting the structured development of AIL in HE. The study
aims to bridge the gap between theoretical AIL conceptualizations and the
practical implementation in academic curricula.
\\ ( https://arxiv.org/abs/2509.18900 ,  1528kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18930 (*cross-listing*)
Date: Tue, 23 Sep 2025 12:49:25 GMT   (373kb)

Title: Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined
  through Reinforcement Learning
Authors: Alex Schutz, Victor-Alexandru Darvariu, Efimia Panagiotaki, Bruno
  Lacerda, Nick Hawes
Categories: cs.LG cs.AI
\\
  Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.
\\ ( https://arxiv.org/abs/2509.18930 ,  373kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18933 (*cross-listing*)
Date: Tue, 23 Sep 2025 12:52:01 GMT   (3041kb)

Title: Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine
  Learning
Authors: Gabriele Formis, Gianluca Cena, Lukasz Wisniewski, Stefano Scanzio
Categories: cs.NI cs.AI cs.LG
Comments: accepted version in IEEE Transactions on Industrial Informatics, 12
  pages, 2025
DOI: 10.1109/TII.2025.3609224
\\
  Wireless communications are characterized by their unpredictability, posing
challenges for maintaining consistent communication quality. This paper
presents a comprehensive analysis of various prediction models, with a focus on
achieving accurate and efficient Wi-Fi link quality forecasts using machine
learning techniques. Specifically, the paper evaluates the performance of
data-driven models based on the linear combination of exponential moving
averages, which are designed for low-complexity implementations and are then
suitable for hardware platforms with limited processing resources. Accuracy of
the proposed approaches was assessed using experimental data from a real-world
Wi-Fi testbed, considering both channel-dependent and channel-independent
training data. Remarkably, channel-independent models, which allow for
generalized training by equipment manufacturers, demonstrated competitive
performance. Overall, this study provides insights into the practical
deployment of machine learning-based prediction models for enhancing Wi-Fi
dependability in industrial environments.
\\ ( https://arxiv.org/abs/2509.18933 ,  3041kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18949 (*cross-listing*)
Date: Tue, 23 Sep 2025 12:58:32 GMT   (282kb)

Title: Towards Privacy-Aware Bayesian Networks: A Credal Approach
Authors: Niccol\`o Rocchi and Fabio Stella and Cassio de Campos
Categories: cs.LG cs.AI
Comments: Accepted at ECAI2025 conference, 20 pages, 1 figure
\\
  Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.
\\ ( https://arxiv.org/abs/2509.18949 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18953 (*cross-listing*)
Date: Tue, 23 Sep 2025 13:02:23 GMT   (3413kb)

Title: Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under
  Real-World Physical Variations
Authors: Hanqing Liu, Jiahuan Long, Junqi Wu, Jiacheng Hou, Huili Tang,
  Tingsong Jiang, Weien Zhou, Wen Yao
Categories: cs.RO cs.AI
\\
  Vision-Language-Action (VLA) models have emerged as promising solutions for
robotic manipulation, yet their robustness to real-world physical variations
remains critically underexplored. To bridge this gap, we propose Eva-VLA, the
first unified framework that systematically evaluates the robustness of VLA
models by transforming discrete physical variations into continuous
optimization problems. However, comprehensively assessing VLA robustness
presents two key challenges: (1) how to systematically characterize diverse
physical variations encountered in real-world deployments while maintaining
evaluation reproducibility, and (2) how to discover worst-case scenarios
without prohibitive real-world data collection costs efficiently. To address
the first challenge, we decompose real-world variations into three critical
domains: object 3D transformations that affect spatial reasoning, illumination
variations that challenge visual perception, and adversarial patches that
disrupt scene understanding. For the second challenge, we introduce a
continuous black-box optimization framework that transforms discrete physical
variations into parameter optimization, enabling systematic exploration of
worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models
across multiple benchmarks reveal alarming vulnerabilities: all variation types
trigger failure rates exceeding 60%, with object transformations causing up to
97.8% failure in long-horizon tasks. Our findings expose critical gaps between
controlled laboratory success and unpredictable deployment readiness, while the
Eva-VLA framework provides a practical pathway for hardening VLA-based robotic
manipulation models against real-world deployment challenges.
\\ ( https://arxiv.org/abs/2509.18953 ,  3413kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19012 (*cross-listing*)
Date: Tue, 23 Sep 2025 13:53:52 GMT   (1792kb)

Title: Pure Vision Language Action (VLA) Models: A Comprehensive Survey
Authors: Dapeng Zhang, Jin Sun, Chenghui Hu, Xiaoyan Wu, Zhenlong Yuan, Rui
  Zhou, Fei Shen, and Qingguo Zhou
Categories: cs.RO cs.AI
\\
  The emergence of Vision Language Action (VLA) models marks a paradigm shift
from traditional policy-based control to generalized robotics, reframing Vision
Language Models (VLMs) from passive sequence generators into active agents for
manipulation and decision-making in complex, dynamic environments. This survey
delves into advanced VLA methods, aiming to provide a clear taxonomy and a
systematic, comprehensive review of existing research. It presents a
comprehensive analysis of VLA applications across different scenarios and
classifies VLA approaches into several paradigms: autoregression-based,
diffusion-based, reinforcement-based, hybrid, and specialized methods; while
examining their motivations, core strategies, and implementations in detail. In
addition, foundational datasets, benchmarks, and simulation platforms are
introduced. Building on the current VLA landscape, the review further proposes
perspectives on key challenges and future directions to advance research in VLA
models and generalizable robotics. By synthesizing insights from over three
hundred recent studies, this survey maps the contours of this rapidly evolving
field and highlights the opportunities and challenges that will shape the
development of scalable, general-purpose VLA methods.
\\ ( https://arxiv.org/abs/2509.19012 ,  1792kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19017 (*cross-listing*)
Date: Tue, 23 Sep 2025 13:57:13 GMT   (15669kb)

Title: Fully Learnable Neural Reward Machines
Authors: Hazem Dewidar, Elena Umili
Categories: cs.LG cs.AI
\\
  Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).
\\ ( https://arxiv.org/abs/2509.19017 ,  15669kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19023 (*cross-listing*)
Date: Tue, 23 Sep 2025 13:58:36 GMT   (555kb)

Title: Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free
  Humanoid Locomotion
Authors: Shuai Liu, Meng Cheng Lau
Categories: cs.RO cs.AI
Comments: 11 pages, 5 figures, 1 table, Computational Science Graduate Project
\\
  We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a
two-stage reinforcement learning framework for humanoid walking that requires
no motion capture data or elaborate reward shaping. In the first stage, a
compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via
Proximal Policy Optimization. This generates energy-efficient gait templates.
In the second stage, those dynamically consistent trajectories guide a
full-body policy trained with Soft Actor--Critic augmented by an adversarial
discriminator, ensuring the student's five-dimensional gait feature
distribution matches the ROM's demonstrations. Experiments at 1
meter-per-second and 4 meter-per-second show that ROM-GRL produces stable,
symmetric gaits with substantially lower tracking error than a pure-reward
baseline. By distilling lightweight ROM guidance into high-dimensional
policies, ROM-GRL bridges the gap between reward-only and imitation-based
locomotion methods, enabling versatile, naturalistic humanoid behaviors without
any human demonstrations.
\\ ( https://arxiv.org/abs/2509.19023 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19063 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:27:44 GMT   (1759kb)

Title: Beyond Backpropagation: Exploring Innovative Algorithms for
  Energy-Efficient Deep Neural Network Training
Authors: Przemys{\l}aw Spyra
Categories: cs.LG cs.AI
MSC-class: 68T07
\\
  The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.
\\ ( https://arxiv.org/abs/2509.19063 ,  1759kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19080 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:38:15 GMT   (3672kb)

Title: World4RL: Diffusion World Models for Policy Refinement with
  Reinforcement Learning for Robotic Manipulation
Authors: Zhennan Jiang, Kai Liu, Yuxin Qin, Shuai Tian, Yupeng Zheng, Mingcai
  Zhou, Chao Yu, Haoran Li, Dongbin Zhao
Categories: cs.RO cs.AI
\\
  Robotic manipulation policies are commonly initialized through imitation
learning, but their performance is limited by the scarcity and narrow coverage
of expert data. Reinforcement learning can refine polices to alleviate this
limitation, yet real-robot training is costly and unsafe, while training in
simulators suffers from the sim-to-real gap. Recent advances in generative
models have demonstrated remarkable capabilities in real-world simulation, with
diffusion models in particular excelling at generation. This raises the
question of how diffusion model-based world models can be combined to enhance
pre-trained policies in robotic manipulation. In this work, we propose
World4RL, a framework that employs diffusion-based world models as
high-fidelity simulators to refine pre-trained policies entirely in imagined
environments for robotic manipulation. Unlike prior works that primarily employ
world models for planning, our framework enables direct end-to-end policy
optimization. World4RL is designed around two principles: pre-training a
diffusion world model that captures diverse dynamics on multi-task datasets and
refining policies entirely within a frozen world model to avoid online
real-world interactions. We further design a two-hot action encoding scheme
tailored for robotic manipulation and adopt diffusion backbones to improve
modeling fidelity. Extensive simulation and real-world experiments demonstrate
that World4RL provides high-fidelity environment modeling and enables
consistent policy refinement, yielding significantly higher success rates
compared to imitation learning and other baselines. More visualization results
are available at https://world4rl.github.io/.
\\ ( https://arxiv.org/abs/2509.19080 ,  3672kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19084 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:39:09 GMT   (1146kb)

Title: Graph Neural Networks with Similarity-Navigated Probabilistic Feature
  Copying
Authors: Asela Hevapathige
Categories: cs.LG cs.AI
\\
  Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.
\\ ( https://arxiv.org/abs/2509.19084 ,  1146kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19088 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:42:14 GMT   (6836kb)

Title: A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and
  Opportunities for Further Improvement
Authors: Tiany Peng, George Gui, Daniel J. Merlau, Grace Jiarui Fan, Malek Ben
  Sliman, Melanie Brucks, Eric J. Johnson, Vicki Morwitz, Abdullah Althenayyan,
  Silvia Bellezza, Dante Donati, Hortense Fong, Elizabeth Friedman, Ariana
  Guevara, Mohamed Hussein, Kinshuk Jerath, Bruce Kogut, Kristen Lane, Hannah
  Li, Patryk Perkowski, Oded Netzer, Olivier Toubia
Categories: cs.CY cs.AI cs.HC stat.AP
\\
  Do "digital twins" capture individual responses in surveys and experiments?
We run 19 pre-registered studies on a national U.S. panel and their LLM-powered
digital twins (constructed based on previously-collected extensive
individual-level data) and compare twin and human answers across 164 outcomes.
The correlation between twin and human answers is modest (approximately 0.2 on
average) and twin responses are less variable than human responses. While
constructing digital twins based on rich individual-level data improves our
ability to capture heterogeneity across participants and predict relative
differences between them, it does not substantially improve our ability to
predict the exact answers given by specific participants or enhance predictions
of population means. Twin performance varies by domain and is higher among more
educated, higher-income, and ideologically moderate participants. These results
suggest current digital twins can capture some degree of relative differences
but are unreliable for individual-level predictions and sample mean and
variance estimation, underscoring the need for careful validation before use.
Our data and code are publicly available for researchers and practitioners
interested in optimizing digital twin pipelines.
\\ ( https://arxiv.org/abs/2509.19088 ,  6836kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19091 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:43:27 GMT   (4362kb)

Title: Training Flow Matching Models with Reliable Labels via Self-Purification
Authors: Hyeongju Kim, Yechan Yu, June Young Yi, Juheon Lee
Categories: eess.AS cs.AI cs.SD
Comments: 5 pages, 3 figures, preprint
\\
  Training datasets are inherently imperfect, often containing mislabeled
samples due to human annotation errors, limitations of tagging models, and
other sources of noise. Such label contamination can significantly degrade the
performance of a trained model. In this work, we introduce Self-Purifying Flow
Matching (SPFM), a principled approach to filtering unreliable data within the
flow-matching framework. SPFM identifies suspicious data using the model itself
during the training process, bypassing the need for pretrained models or
additional modules. Our experiments demonstrate that models trained with SPFM
generate samples that accurately adhere to the specified conditioning, even
when trained on noisy labels. Furthermore, we validate the robustness of SPFM
on the TITW dataset, which consists of in-the-wild speech data, achieving
performance that surpasses existing baselines.
\\ ( https://arxiv.org/abs/2509.19091 ,  4362kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19100 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:48:58 GMT   (66788kb)

Title: Algorithms for Adversarially Robust Deep Learning
Authors: Alexander Robey
Categories: cs.LG cs.AI
Comments: PhD thesis
\\
  Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.
\\ ( https://arxiv.org/abs/2509.19100 ,  66788kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19102 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:49:05 GMT   (4012kb)

Title: FUNCanon: Learning Pose-Aware Action Primitives via Functional Object
  Canonicalization for Generalizable Robotic Manipulation
Authors: Hongli Xu, Lei Zhang, Xiaoyue Hu, Boyang Zhong, Kaixin Bai,
  Zolt\'an-Csaba M\'arton, Zhenshan Bing, Zhaopeng Chen, Alois Christian Knoll,
  Jianwei Zhang
Categories: cs.RO cs.AI cs.CV
Comments: project website: https://sites.google.com/view/funcanon, 11 pages
\\
  General-purpose robotic skills from end-to-end demonstrations often leads to
task-specific policies that fail to generalize beyond the training
distribution. Therefore, we introduce FunCanon, a framework that converts
long-horizon manipulation tasks into sequences of action chunks, each defined
by an actor, verb, and object. These chunks focus policy learning on the
actions themselves, rather than isolated tasks, enabling compositionality and
reuse. To make policies pose-aware and category-general, we perform functional
object canonicalization for functional alignment and automatic manipulation
trajectory transfer, mapping objects into shared functional frames using
affordance cues from large vision language models. An object centric and action
centric diffusion policy FuncDiffuser trained on this aligned data naturally
respects object affordances and poses, simplifying learning and improving
generalization ability. Experiments on simulated and real-world benchmarks
demonstrate category-level generalization, cross-task behavior reuse, and
robust sim2real deployment, showing that functional canonicalization provides a
strong inductive bias for scalable imitation learning in complex manipulation
domains. Details of the demo and supplemental material are available on our
project website https://sites.google.com/view/funcanon.
\\ ( https://arxiv.org/abs/2509.19102 ,  4012kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19112 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:58:50 GMT   (729kb)

Title: Towards Practical Multi-label Causal Discovery in High-Dimensional Event
  Sequences via One-Shot Graph Aggregation
Authors: Hugo Math, Rainer Lienhart
Categories: cs.LG cs.AI
Comments: Accepted at NeuRIPS2025 Workshop on Structured Probabilistic
  Inference and Generative Modeling
\\
  Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.
\\ ( https://arxiv.org/abs/2509.19112 ,  729kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19120 (*cross-listing*)
Date: Tue, 23 Sep 2025 15:06:04 GMT   (477kb)

Title: FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy
  Federated Learning in Healthcare AI
Authors: Ferdinand Kahenga, Antoine Bagula, Sajal K. Das, Patrick Sello
Categories: cs.LG cs.AI cs.DC
\\
  Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.
\\ ( https://arxiv.org/abs/2509.19120 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19122 (*cross-listing*)
Date: Tue, 23 Sep 2025 15:08:25 GMT   (1507kb)

Title: Analysis on distribution and clustering of weight
Authors: Chunming Ye, Wenquan Tian, Yalan Gao, Songzhou Li
Categories: cs.LG cs.AI
Comments: 14page,16 figures
MSC-class: 68T50
ACM-class: I.2.7
\\
  The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.
\\ ( https://arxiv.org/abs/2509.19122 ,  1507kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19135 (*cross-listing*)
Date: Tue, 23 Sep 2025 15:20:38 GMT   (195kb)

Title: GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility
  Understanding
Authors: Wenying Luo, Zhiyuan Lin, Wenhao Xu, Minghao Liu, Zhi Li
Categories: cs.LG cs.AI
\\
  Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.
\\ ( https://arxiv.org/abs/2509.19135 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19136 (*cross-listing*)
Date: Tue, 23 Sep 2025 15:20:40 GMT   (421kb)

Title: On the Soundness and Consistency of LLM Agents for Executing Test Cases
  Written in Natural Language
Authors: S\'ebastien Salva and Redha Taguelmimt
Categories: cs.SE cs.AI
ACM-class: D.2.4; D.2.5; F.3.1
\\
  The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.
\\ ( https://arxiv.org/abs/2509.19136 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19147 (*cross-listing*)
Date: Tue, 23 Sep 2025 15:27:00 GMT   (1570kb)

Title: Generative Propaganda
Authors: Madeleine I. G. Daepp, Alejandro Cuevas, Robert Osazuwa Ness, Vickie
  Yu-Ping Wang, Bharat Kumar Nayak, Dibyendu Mishra, Ti-Chung Cheng, Shaily
  Desai, Joyojeet Pal
Categories: cs.CY cs.AI cs.SI
Comments: Working Paper
ACM-class: K.4.2
\\
  Generative propaganda is the use of generative artificial intelligence (AI)
to shape public opinion. To characterize its use in real-world settings, we
conducted interviews with defenders (e.g., factcheckers, journalists,
officials) in Taiwan and creators (e.g., influencers, political consultants,
advertisers) as well as defenders in India, centering two places characterized
by high levels of online propaganda. The term "deepfakes", we find, exerts
outsized discursive power in shaping defenders' expectations of misuse and, in
turn, the interventions that are prioritized. To better characterize the space
of generative propaganda, we develop a taxonomy that distinguishes between
obvious versus hidden and promotional versus derogatory use. Deception was
neither the main driver nor the main impact vector of AI's use; instead, Indian
creators sought to persuade rather than to deceive, often making AI's use
obvious in order to reduce legal and reputational risks, while Taiwan's
defenders saw deception as a subset of broader efforts to distort the
prevalence of strategic narratives online. AI was useful and used, however, in
producing efficiency gains in communicating across languages and modes, and in
evading human and algorithmic detection. Security researchers should reconsider
threat models to clearly differentiate deepfakes from promotional and obvious
uses, to complement and bolster the social factors that constrain misuse by
internal actors, and to counter efficiency gains globally.
\\ ( https://arxiv.org/abs/2509.19147 ,  1570kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19182 (*cross-listing*)
Date: Tue, 23 Sep 2025 15:57:42 GMT   (5159kb)

Title: YAC: Bridging Natural Language and Interactive Visual Exploration with
  Generative AI for Biomedical Data Discovery
Authors: Devin Lange, Shanghua Gao, Pengwei Sui, Austen Money, Priya Misner,
  Marinka Zitnik, Nils Gehlenborg
Categories: cs.HC cs.AI
\\
  Incorporating natural language input has the potential to improve the
capabilities of biomedical data discovery interfaces. However, user interface
elements and visualizations are still powerful tools for interacting with data,
even in the new world of generative AI. In our prototype system, YAC, Yet
Another Chatbot, we bridge the gap between natural language and interactive
visualizations by generating structured declarative output with a multi-agent
system and interpreting that output to render linked interactive visualizations
and apply data filters. Furthermore, we include widgets, which allow users to
adjust the values of that structured output through user interface elements. We
reflect on the capabilities and design of this system with an analysis of its
technical dimensions and illustrate the capabilities through four usage
scenarios.
\\ ( https://arxiv.org/abs/2509.19182 ,  5159kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19220 (*cross-listing*)
Date: Tue, 23 Sep 2025 16:46:06 GMT   (405kb)

Title: FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders
  for Robust Adaptation under Label Scarcity
Authors: Ferdinand Kahenga, Antoine Bagula, Patrick Sello, and Sajal K. Das
Categories: cs.LG cs.AI cs.DC
\\
  Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.
\\ ( https://arxiv.org/abs/2509.19220 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19231 (*cross-listing*)
Date: Tue, 23 Sep 2025 16:53:07 GMT   (544kb)

Title: Finding My Voice: Generative Reconstruction of Disordered Speech for
  Automated Clinical Evaluation
Authors: Karen Rosero, Eunjung Yeo, David R. Mortensen, Cortney Van't Slot,
  Rami R. Hallac, Carlos Busso
Categories: cs.SD cs.AI cs.CL
\\
  We present ChiReSSD, a speech reconstruction framework that preserves
children speaker's identity while suppressing mispronunciations. Unlike prior
approaches trained on healthy adult speech, ChiReSSD adapts to the voices of
children with speech sound disorders (SSD), with particular emphasis on pitch
and prosody. We evaluate our method on the STAR dataset and report substantial
improvements in lexical accuracy and speaker identity preservation.
Furthermore, we automatically predict the phonetic content in the original and
reconstructed pairs, where the proportion of corrected consonants is comparable
to the percentage of correct consonants (PCC), a clinical speech assessment
metric. Our experiments show Pearson correlation of 0.63 between automatic and
human expert annotations, highlighting the potential to reduce the manual
transcription burden. In addition, experiments on the TORGO dataset demonstrate
effective generalization for reconstructing adult dysarthric speech. Our
results indicate that disentangled, style-based TTS reconstruction can provide
identity-preserving speech across diverse clinical populations.
\\ ( https://arxiv.org/abs/2509.19231 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19277 (*cross-listing*)
Date: Tue, 23 Sep 2025 17:42:24 GMT   (4394kb)

Title: MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion
  interactive segmentation of neurobromas in whole-body MRI
Authors: Georgii Kolokolnikov, Marie-Lena Schmalhofer, Sophie G \"otz, Lennart
  Well, Said Farschtschi, Victor-Felix Mautner, Inka Ristow, Rene Werner
Categories: eess.IV cs.AI cs.CV cs.LG
\\
  Background and Objectives: Neurofibromatosis type 1 is a genetic disorder
characterized by the development of numerous neurofibromas (NFs) throughout the
body. Whole-body MRI (WB-MRI) is the clinical standard for detection and
longitudinal surveillance of NF tumor growth. Existing interactive segmentation
methods fail to combine high lesion-wise precision with scalability to hundreds
of lesions. This study proposes a novel interactive segmentation model tailored
to this challenge.
  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation
model that extends the state-of-the-art, transformer-based, promptable Segment
Anything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was
trained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using
T2-weighted fat-suppressed sequences. The dataset was split at the patient
level into a training set and four test sets (one in-domain and three
reflecting different domain shift scenarios, e.g., MRI field strength
variation, low tumor burden, differences in clinical site and scanner vendor).
  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of
0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:
0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained
under MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:
0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1
scores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader
variability analysis showed model-to-expert agreement (DSC: 0.62-0.68),
comparable to inter-expert agreement (DSC: 0.57-0.69).
  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable
interactive segmentation of NFs in WB-MRI with minimal user input and strong
generalization, supporting integration into clinical workflows.
\\ ( https://arxiv.org/abs/2509.19277 ,  4394kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19292 (*cross-listing*)
Date: Tue, 23 Sep 2025 17:54:47 GMT   (2755kb)

Title: SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold
  Exploration
Authors: Yang Jin, Jun Lv, Han Xue, Wendi Chen, Chuan Wen, Cewu Lu
Categories: cs.RO cs.AI cs.LG
\\
  Intelligent agents progress by continually refining their capabilities
through actively exploring environments. Yet robot policies often lack
sufficient exploration capability due to action mode collapse. Existing methods
that encourage exploration typically rely on random perturbations, which are
unsafe and induce unstable, erratic behaviors, thereby limiting their
effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a
framework that enhances policy exploration and improvement in robotic
manipulation. SOE learns a compact latent representation of task-relevant
factors and constrains exploration to the manifold of valid actions, ensuring
safety, diversity, and effectiveness. It can be seamlessly integrated with
arbitrary policy models as a plug-in module, augmenting exploration without
degrading the base policy performance. Moreover, the structured latent space
enables human-guided exploration, further improving efficiency and
controllability. Extensive experiments in both simulation and real-world tasks
demonstrate that SOE consistently outperforms prior methods, achieving higher
task success rates, smoother and safer exploration, and superior sample
efficiency. These results establish on-manifold exploration as a principled
approach to sample-efficient policy self-improvement. Project website:
https://ericjin2002.github.io/SOE
\\ ( https://arxiv.org/abs/2509.19292 ,  2755kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19295 (*cross-listing*)
Date: Tue, 23 Sep 2025 17:57:44 GMT   (355kb)

Title: Audio-Based Pedestrian Detection in the Presence of Vehicular Noise
Authors: Yonghyun Kim, Chaeyeon Han, Akash Sarode, Noah Posner, Subhrajit
  Guhathakurta, Alexander Lerch
Categories: eess.AS cs.AI cs.LG cs.SD
Comments: Accepted to the 10th Workshop on Detection and Classification of
  Acoustic Scenes and Events (DCASE), 2025
\\
  Audio-based pedestrian detection is a challenging task and has, thus far,
only been explored in noise-limited environments. We present a new dataset,
results, and a detailed analysis of the state-of-the-art in audio-based
pedestrian detection in the presence of vehicular noise. In our study, we
conduct three analyses: (i) cross-dataset evaluation between noisy and
noise-limited environments, (ii) an assessment of the impact of noisy data on
model performance, highlighting the influence of acoustic context, and (iii) an
evaluation of the model's predictive robustness on out-of-domain sounds. The
new dataset is a comprehensive 1321-hour roadside dataset. It incorporates
traffic-rich soundscapes. Each recording includes 16kHz audio synchronized with
frame-level pedestrian annotations and 1fps video thumbnails.
\\ ( https://arxiv.org/abs/2509.19295 ,  355kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18169 (*cross-listing*)
Date: Wed, 17 Sep 2025 10:15:25 GMT   (5495kb)

Title: PiMoE: Token-Level Routing for Integrating High-Precision Computation
  and Reasoning
Authors: Hengbo Xiao, Jingyuan Fan, Xin Tong, Jingzhao Zhang, Chao Lu, Guannan
  He
Categories: cs.LG cs.CE cs.CL
\\
  Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.
\\ ( https://arxiv.org/abs/2509.18169 ,  5495kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18173 (*cross-listing*)
Date: Wed, 17 Sep 2025 15:00:03 GMT   (4788kb)

Title: TurnBack: A Geospatial Route Cognition Benchmark for Large Language
  Models through Reverse Route
Authors: Hongyi Luo, Qing Cheng, Daniel Matos, Hari Krishna Gadi, Yanfeng
  Zhang, Lu Liu, Yongliang Wang, Niclas Zeller, Daniel Cremers, Liqiu Meng
Categories: cs.LG cs.CL
Comments: Accepted to EMNLP 2025 (Main). This is the camera-ready/author
  version
\\
  Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}
\\ ( https://arxiv.org/abs/2509.18173 ,  4788kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18570 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:53:38 GMT   (202kb)

Title: HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for
  Multi-Task Speech Language Modeling
Authors: Yuke Si, Runyan Yang, Yingying Gao, Junlan Feng, Chao Deng, Shilei
  Zhang
Categories: eess.AS cs.CL cs.SD
Comments: 5 pages; submitted to ICASSP 2026
\\
  Recent advances in large language models have facilitated the development of
unified speech language models (SLMs) capable of supporting multiple speech
tasks within a shared architecture. However, tasks such as automatic speech
recognition (ASR) and speech emotion recognition (SER) rely on distinct types
of information: ASR primarily depends on linguistic content, whereas SER
requires the integration of both linguistic and paralinguistic cues. Existing
multitask SLMs typically adopt naive parameter sharing or prompt-based
conditioning without explicitly modeling the differences in information
composition required by each task. Such designs risk task interference and
performance degradation, especially under limited data conditions. To address
these limitations, we propose HarmoniFuse, a component-selective and
prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse
is designed to harmonize heterogeneous task demands by selecting and fusing
task-relevant components of speech representations. Specifically, it integrates
a gated speech encoder to extract task-specific acoustic features and a
prompt-adaptive dynamic fusion module to aggregate transformer layers based on
task characteristics. In addition, a batch-interleaved training strategy
enables leveraging separate ASR and SER datasets without requiring joint
annotation. Experimental results demonstrate that HarmoniFuse improves both ASR
and SER performance, offering a scalable and robust solution for multitask
speech understanding under realistic data constraints.
\\ ( https://arxiv.org/abs/2509.18570 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18579 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:58:16 GMT   (131kb)

Title: Teaching Audio Models to Reason: A Unified Framework for Source- and
  Layer-wise Distillation
Authors: Runyan Yang, Yuke Si, Yingying Gao, Junlan Feng, Chao Deng, Shilei
  Zhang
Categories: eess.AS cs.CL cs.SD
Comments: 5 pages; submitted to ICASSP 2026
\\
  While large audio language models excel at tasks like ASR and emotion
recognition, they still struggle with complex reasoning due to the modality gap
between audio and text as well as the lack of structured intermediate
supervision. To address this, we propose a unified knowledge distillation
framework to transfer reasoning capabilities from a high-capacity textual
teacher model to a student audio models while preserving its acoustic
competence. Our method introduces two key dimensions: source-wise distillation,
which leverages both textual and acoustic teachers to provide complementary
modality-specific supervision; and layer-wise distillation, which aligns
teacher signals with appropriate student layers to improve transfer efficiency.
This dual-dimensional strategy enables fine-grained control over the
distillation process, effectively bridging the gap between symbolic reasoning
and speech representations. Experimental results show significant improvements
in audio reasoning performance, demonstrating the effectiveness of our
framework as a reasoning transfer solution for audio modeling.
\\ ( https://arxiv.org/abs/2509.18579 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18661 (*cross-listing*)
Date: Tue, 23 Sep 2025 05:28:43 GMT   (636kb)

Title: Agentic AutoSurvey: Let LLMs Survey LLMs
Authors: Yixin Liu, Yonghui Wu, Denghui Zhang, Lichao Sun
Categories: cs.IR cs.CL cs.HC
Comments: 29 pages, 7 figures
\\
  The exponential growth of scientific literature poses unprecedented
challenges for researchers attempting to synthesize knowledge across rapidly
evolving fields. We present \textbf{Agentic AutoSurvey}, a multi-agent
framework for automated survey generation that addresses fundamental
limitations in existing approaches. Our system employs four specialized agents
(Paper Search Specialist, Topic Mining \& Clustering, Academic Survey Writer,
and Quality Evaluator) working in concert to generate comprehensive literature
surveys with superior synthesis quality. Through experiments on six
representative LLM research topics from COLM 2024 categories, we demonstrate
that our multi-agent approach achieves significant improvements over existing
baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent
architecture processes 75--443 papers per topic (847 total across six topics)
while targeting high citation coverage (often $\geq$80\% on 75--100-paper sets;
lower on very large sets such as RLHF) through specialized agent orchestration.
Our 12-dimension evaluation captures organization, synthesis integration, and
critical analysis beyond basic metrics. These findings demonstrate that
multi-agent architectures represent a meaningful advancement for automated
literature survey generation in rapidly evolving scientific domains.
\\ ( https://arxiv.org/abs/2509.18661 ,  636kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18816 (*cross-listing*)
Date: Tue, 23 Sep 2025 09:02:15 GMT   (145kb)

Title: Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal
  Attention in Large Audio Language Models
Authors: Junyu Wang and Ziyang Ma and Zhengding Luo and Tianrui Wang and Meng
  Ge and Xiaobao Wang and Longbiao Wang
Categories: cs.SD cs.CL cs.MM eess.AS
Comments: Submitted to ICASSP 2026
\\
  Large Audio-Language Models (LALMs) often suffer from audio-textual attention
imbalance, prioritizing text over acoustic information, particularly in the
multi-modal fusion layers of the Transformer architecture. This bias hinders
their ability to fully utilize acoustic cues, causing suboptimal performance on
audio reasoning tasks. To mitigate this, we propose \textbf{MATA}, a novel
training-free method that dynamically pushes LALMs to pay \textbf{M}ore
\textbf{A}ttention \textbf{T}o \textbf{A}udio tokens within the self-attention
mechanism. Specifically, MATA intervenes post raw attention scoring, targeting
only the last token in intermediate layers without introducing additional
parameters or computational overhead. Experiments on the MMAU and MMAR
benchmarks confirm MATA's effectiveness, with consistent performance gains.
Notably, on MMAR, MATA enables an open-source model to surpass the proprietary
Gemini 2.0 Flash for the first time. Our work provides an efficient solution to
mitigate attention bias and opens a new research direction for enhancing the
audio-processing capabilities of multi-modal models.
\\ ( https://arxiv.org/abs/2509.18816 ,  145kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18110 (*cross-listing*)
Date: Tue, 9 Sep 2025 20:13:51 GMT   (4671kb)

Title: Localized PCA-Net Neural Operators for Scalable Solution Reconstruction
  of Elliptic PDEs
Authors: Mrigank Dhingra, Romit Maulik, Adil Rasheed, Omer San
Categories: cs.LG cs.CV
\\
  Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.
\\ ( https://arxiv.org/abs/2509.18110 ,  4671kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18154 (*cross-listing*)
Date: Tue, 16 Sep 2025 19:41:48 GMT   (8747kb)

Title: MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and
  Training Recipe
Authors: Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui
  He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, Bokai Xu, Junbo
  Cui, Yingjing Xu, Liqing Ruan, Luoyuan Zhang, Hanyu Liu, Jingkun Tang,
  Hongyuan Liu, Qining Guo, Wenhao Hu, Bingxiang He, Jie Zhou, Jie Cai, Ji Qi,
  Zonghao Guo, Chi Chen, Guoyang Zeng, Yuxuan Li, Ganqu Cui, Ning Ding, Xu Han,
  Yuan Yao, Zhiyuan Liu and Maosong Sun
Categories: cs.LG cs.CV
Comments: Project Website: https://github.com/OpenBMB/MiniCPM-V
\\
  Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.
\\ ( https://arxiv.org/abs/2509.18154 ,  8747kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18342 (*cross-listing*)
Date: Mon, 22 Sep 2025 19:04:31 GMT   (3070kb)

Title: Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation
Authors: Rajitha de Silva and Jonathan Cox and James R. Heselden and Marija
  Popovic and Cesar Cadena and Riccardo Polvara
Categories: cs.RO cs.CV
Comments: Sumbitted to ICRA 2026
\\
  Accurate localisation is critical for mobile robots in structured outdoor
environments, yet LiDAR-based methods often fail in vineyards due to repetitive
row geometry and perceptual aliasing. We propose a semantic particle filter
that incorporates stable object-level detections, specifically vine trunks and
support poles into the likelihood estimation process. Detected landmarks are
projected into a birds eye view and fused with LiDAR scans to generate semantic
observations. A key innovation is the use of semantic walls, which connect
adjacent landmarks into pseudo-structural constraints that mitigate row
aliasing. To maintain global consistency in headland regions where semantics
are sparse, we introduce a noisy GPS prior that adaptively supports the filter.
Experiments in a real vineyard demonstrate that our approach maintains
localisation within the correct row, recovers from deviations where AMCL fails,
and outperforms vision-based SLAM methods such as RTAB-Map.
\\ ( https://arxiv.org/abs/2509.18342 ,  3070kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18378 (*cross-listing*)
Date: Mon, 22 Sep 2025 20:01:32 GMT   (4500kb)

Title: Neural Network-Driven Direct CBCT-Based Dose Calculation for
  Head-and-Neck Proton Treatment Planning
Authors: Muheng Li, Evangelia Choulilitsa, Lisa Fankhauser, Francesca
  Albertini, Antony Lomax and Ye Zhang
Categories: physics.med-ph cs.CV
\\
  Accurate dose calculation on cone beam computed tomography (CBCT) images is
essential for modern proton treatment planning workflows, particularly when
accounting for inter-fractional anatomical changes in adaptive treatment
scenarios. Traditional CBCT-based dose calculation suffers from image quality
limitations, requiring complex correction workflows. This study develops and
validates a deep learning approach for direct proton dose calculation from CBCT
images using extended Long Short-Term Memory (xLSTM) neural networks. A
retrospective dataset of 40 head-and-neck cancer patients with paired planning
CT and treatment CBCT images was used to train an xLSTM-based neural network
(CBCT-NN). The architecture incorporates energy token encoding and
beam's-eye-view sequence modelling to capture spatial dependencies in proton
dose deposition patterns. Training utilized 82,500 paired beam configurations
with Monte Carlo-generated ground truth doses. Validation was performed on 5
independent patients using gamma analysis, mean percentage dose error
assessment, and dose-volume histogram comparison. The CBCT-NN achieved gamma
pass rates of 95.1 $\pm$ 2.7% using 2mm/2% criteria. Mean percentage dose
errors were 2.6 $\pm$ 1.4% in high-dose regions ($>$90% of max dose) and 5.9
$\pm$ 1.9% globally. Dose-volume histogram analysis showed excellent
preservation of target coverage metrics (Clinical Target Volume V95%
difference: -0.6 $\pm$ 1.1%) and organ-at-risk constraints (parotid mean dose
difference: -0.5 $\pm$ 1.5%). Computation time is under 3 minutes without
sacrificing Monte Carlo-level accuracy. This study demonstrates the
proof-of-principle of direct CBCT-based proton dose calculation using xLSTM
neural networks. The approach eliminates traditional correction workflows while
achieving comparable accuracy and computational efficiency suitable for
adaptive protocols.
\\ ( https://arxiv.org/abs/2509.18378 ,  4500kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18391 (*cross-listing*)
Date: Mon, 22 Sep 2025 20:23:33 GMT   (1203kb)

Title: Does Embodiment Matter to Biomechanics and Function? A Comparative
  Analysis of Head-Mounted and Hand-Held Assistive Devices for Individuals with
  Blindness and Low Vision
Authors: Gaurav Seth, Hoa Pham, Giles Hamilton-Fletcher, Charles Leclercq,
  John-Ross Rizzo
Categories: cs.HC cs.CV
Comments: 30 pages, 7 figures, 5 tables. Pre-print submitted to International
  Journal of Human-Computer Interaction. Also to appear as a late-breaking
  poster at ACRM. Limited AI (ChatGPT-4/5) used for language refinement and
  figure schematics under author supervision. One author (CL) is CEO of ARx
  Vision; others report no conflicts
ACM-class: K.4.2; H.5.2; I.3.6
\\
  Visual assistive technologies, such as Microsoft Seeing AI, can improve
access to environmental information for persons with blindness or low vision
(pBLV). Yet, the physical and functional implications of different device
embodiments remain unclear. In this study, 11 pBLV participants used Seeing AI
on a hand-held smartphone and on a head-mounted ARx Vision system to perform
six activities of daily living, while their movements were captured with Xsens
motion capture. Functional outcomes included task time, success rate, and
number of attempts, and biomechanical measures included joint range of motion,
angular path length, working volume, and movement smoothness. The head-mounted
system generally reduced upper-body movement and task time, especially for
document-scanning style tasks, whereas the hand-held system yielded higher
success rates for tasks involving small or curved text. These findings indicate
that both embodiments are viable, but they differ in terms of physical demands
and ease of use. Incorporating biomechanical measures into assistive technology
evaluations can inform designs that optimise user experience by balancing
functional efficiency, physical sustainability, and intuitive interaction.
\\ ( https://arxiv.org/abs/2509.18391 ,  1203kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18428 (*cross-listing*)
Date: Mon, 22 Sep 2025 21:19:10 GMT   (5756kb)

Title: Latent Action Pretraining Through World Modeling
Authors: Bahey Tharwat, Yara Nasser, Ali Abouzeid, Ian Reid
Categories: cs.RO cs.CV
\\
  Vision-Language-Action (VLA) models have gained popularity for learning
robotic manipulation tasks that follow language instructions. State-of-the-art
VLAs, such as OpenVLA and $\pi_{0}$, were trained on large-scale, manually
labeled action datasets collected through teleoperation. More recent
approaches, including LAPA and villa-X, introduce latent action representations
that enable unsupervised pretraining on unlabeled datasets by modeling abstract
visual changes between frames. Although these methods have shown strong
results, their large model sizes make deployment in real-world settings
challenging. In this work, we propose LAWM, a model-agnostic framework to
pretrain imitation learning models in a self-supervised way, by learning latent
action representations from unlabeled video data through world modeling. These
videos can be sourced from robot recordings or videos of humans performing
actions with everyday objects. Our framework is designed to be effective for
transferring across tasks, environments, and embodiments. It outperforms models
trained with ground-truth robotics actions and similar pretraining methods on
the LIBERO benchmark and real-world setup, while being significantly more
efficient and practical for real-world settings.
\\ ( https://arxiv.org/abs/2509.18428 ,  5756kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18479 (*cross-listing*)
Date: Tue, 23 Sep 2025 00:32:37 GMT   (2317kb)

Title: Machine learning approach to single-shot multiparameter estimation for
  the non-linear Schr\"odinger equation
Authors: Louis Rossignol, Tangui Aladjidi, Myrann Baker-Rasooli, and Quentin
  Glorieux
Categories: quant-ph cs.CV physics.optics
Comments: 10 pages, 4 figures
\\
  The nonlinear Schr\"odinger equation (NLSE) is a fundamental model for wave
dynamics in nonlinear media ranging from optical fibers to Bose-Einstein
condensates. Accurately estimating its parameters, which are often strongly
correlated, from a single measurement remains a significant challenge. We
address this problem by treating parameter estimation as an inverse problem and
training a neural network to invert the NLSE mapping. We combine a fast
numerical solver with a machine learning approach based on the ConvNeXt
architecture and a multivariate Gaussian negative log-likelihood loss function.
From single-shot field (density and phase) images, our model estimates three
key parameters: the nonlinear coefficient $n_2$, the saturation intensity
$I_{sat}$, and the linear absorption coefficient $\alpha$. Trained on 100,000
simulated images, the model achieves a mean absolute error of $3.22\%$ on
12,500 unseen test samples, demonstrating strong generalization and close
agreement with ground-truth values. This approach provides an efficient route
for characterizing nonlinear systems and has the potential to bridge
theoretical modeling and experimental data when realistic noise is
incorporated.
\\ ( https://arxiv.org/abs/2509.18479 ,  2317kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18497 (*cross-listing*)
Date: Tue, 23 Sep 2025 01:02:31 GMT   (26995kb)

Title: Differentiable Light Transport with Gaussian Surfels via Adapted
  Radiosity for Efficient Relighting and Geometry Reconstruction
Authors: Kaiwen Jiang, Jia-Mu Sun, Zilu Li, Dan Wang, Tzu-Mao Li, Ravi
  Ramamoorthi
Categories: cs.GR cs.CV
\\
  Radiance fields have gained tremendous success with applications ranging from
novel view synthesis to geometry reconstruction, especially with the advent of
Gaussian splatting. However, they sacrifice modeling of material reflective
properties and lighting conditions, leading to significant geometric
ambiguities and the inability to easily perform relighting. One way to address
these limitations is to incorporate physically-based rendering, but it has been
prohibitively expensive to include full global illumination within the inner
loop of the optimization. Therefore, previous works adopt simplifications that
make the whole optimization with global illumination effects efficient but less
accurate. In this work, we adopt Gaussian surfels as the primitives and build
an efficient framework for differentiable light transport, inspired from the
classic radiosity theory. The whole framework operates in the coefficient space
of spherical harmonics, enabling both diffuse and specular materials. We extend
the classic radiosity into non-binary visibility and semi-opaque primitives,
propose novel solvers to efficiently solve the light transport, and derive the
backward pass for gradient optimizations, which is more efficient than
auto-differentiation. During inference, we achieve view-independent rendering
where light transport need not be recomputed under viewpoint changes, enabling
hundreds of FPS for global illumination effects, including view-dependent
reflections using a spherical harmonics representation. Through extensive
qualitative and quantitative experiments, we demonstrate superior geometry
reconstruction, view synthesis and relighting than previous inverse rendering
baselines, or data-driven baselines given relatively sparse datasets with known
or unknown lighting conditions.
\\ ( https://arxiv.org/abs/2509.18497 ,  26995kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18553 (*cross-listing*)
Date: Tue, 23 Sep 2025 02:25:44 GMT   (3978kb)

Title: Efficient Breast and Ovarian Cancer Classification via ViT-Based
  Preprocessing and Transfer Learning
Authors: Richa Rawat and Faisal Ahmed
Categories: eess.IV cs.CV cs.LG
Comments: 10 pages, 3 figures
\\
  Cancer is one of the leading health challenges for women, specifically breast
and ovarian cancer. Early detection can help improve the survival rate through
timely intervention and treatment. Traditional methods of detecting cancer
involve manually examining mammograms, CT scans, ultrasounds, and other imaging
types. However, this makes the process labor-intensive and requires the
expertise of trained pathologists. Hence, making it both time-consuming and
resource-intensive. In this paper, we introduce a novel vision transformer
(ViT)-based method for detecting and classifying breast and ovarian cancer. We
use a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for both
binary and multi-class classification tasks using publicly available
histopathological image datasets. Further, we use a preprocessing pipeline that
converts raw histophological images into standardized PyTorch tensors, which
are compatible with the ViT architecture and also help improve the model
performance. We evaluated the performance of our model on two benchmark
datasets: the BreakHis dataset for binary classification and the UBC-OCEAN
dataset for five-class classification without any data augmentation. Our model
surpasses existing CNN, ViT, and topological data analysis-based approaches in
binary classification. For multi-class classification, it is evaluated against
recent topological methods and demonstrates superior performance. Our study
highlights the effectiveness of Vision Transformer-based transfer learning
combined with efficient preprocessing in oncological diagnostics.
\\ ( https://arxiv.org/abs/2509.18553 ,  3978kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18783 (*cross-listing*)
Date: Tue, 23 Sep 2025 08:21:53 GMT   (2046kb)

Title: Reconstruction of Optical Coherence Tomography Images from
  Wavelength-space Using Deep-learning
Authors: Maryam Viqar, Erdem Sahin, Elena Stoykova and Violeta Madjarova
Categories: physics.optics cs.CV cs.LG
Journal-ref: SENSORS 2024
DOI: 10.3390/s25010093
\\
  Conventional Fourier-domain Optical Coherence Tomography (FD-OCT) systems
depend on resampling into wavenumber (k) domain to extract the depth profile.
This either necessitates additional hardware resources or amplifies the
existing computational complexity. Moreover, the OCT images also suffer from
speckle noise, due to systemic reliance on low coherence interferometry. We
propose a streamlined and computationally efficient approach based on
Deep-Learning (DL) which enables reconstructing speckle-reduced OCT images
directly from the wavelength domain. For reconstruction, two encoder-decoder
styled networks namely Spatial Domain Convolution Neural Network (SD-CNN) and
Fourier Domain CNN (FD-CNN) are used sequentially. The SD-CNN exploits the
highly degraded images obtained by Fourier transforming the domain fringes to
reconstruct the deteriorated morphological structures along with suppression of
unwanted noise. The FD-CNN leverages this output to enhance the image quality
further by optimization in Fourier domain (FD). We quantitatively and visually
demonstrate the efficacy of the method in obtaining high-quality OCT images.
Furthermore, we illustrate the computational complexity reduction by harnessing
the power of DL models. We believe that this work lays the framework for
further innovations in the realm of OCT image reconstruction.
\\ ( https://arxiv.org/abs/2509.18783 ,  2046kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18786 (*cross-listing*)
Date: Tue, 23 Sep 2025 08:23:51 GMT   (4835kb)

Title: Human-Interpretable Uncertainty Explanations for Point Cloud
  Registration
Authors: Johannes A. Gaus, Loris Schneider, Yitian Shi, Jongseok Lee, Rania
  Rayyes, Rudolph Triebel
Categories: cs.RO cs.CV
\\
  In this paper, we address the point cloud registration problem, where
well-known methods like ICP fail under uncertainty arising from sensor noise,
pose-estimation errors, and partial overlap due to occlusion. We develop a
novel approach, Gaussian Process Concept Attribution (GP-CA), which not only
quantifies registration uncertainty but also explains it by attributing
uncertainty to well-known sources of errors in registration problems. Our
approach leverages active learning to discover new uncertainty sources in the
wild by querying informative instances. We validate GP-CA on three publicly
available datasets and in our real-world robot experiment. Extensive ablations
substantiate our design choices. Our approach outperforms other
state-of-the-art methods in terms of runtime, high sample-efficiency with
active learning, and high accuracy. Our real-world experiment clearly
demonstrates its applicability. Our video also demonstrates that GP-CA enables
effective failure-recovery behaviors, yielding more robust robotic perception.
\\ ( https://arxiv.org/abs/2509.18786 ,  4835kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18830 (*cross-listing*)
Date: Tue, 23 Sep 2025 09:16:34 GMT   (30903kb)

Title: DexSkin: High-Coverage Conformable Robotic Skin for Learning
  Contact-Rich Manipulation
Authors: Suzannah Wistreich, Baiyu Shi, Stephen Tian, Samuel Clarke, Michael
  Nath, Chengyi Xu, Zhenan Bao, Jiajun Wu
Categories: cs.RO cs.CV cs.LG
Comments: Accepted to CoRL 2025
\\
  Human skin provides a rich tactile sensing stream, localizing intentional and
unintentional contact events over a large and contoured region. Replicating
these tactile sensing capabilities for dexterous robotic manipulation systems
remains a longstanding challenge. In this work, we take a step towards this
goal by introducing DexSkin. DexSkin is a soft, conformable capacitive
electronic skin that enables sensitive, localized, and calibratable tactile
sensing, and can be tailored to varying geometries. We demonstrate its efficacy
for learning downstream robotic manipulation by sensorizing a pair of parallel
jaw gripper fingers, providing tactile coverage across almost the entire finger
surfaces. We empirically evaluate DexSkin's capabilities in learning
challenging manipulation tasks that require sensing coverage across the entire
surface of the fingers, such as reorienting objects in hand and wrapping
elastic bands around boxes, in a learning-from-demonstration framework. We then
show that, critically for data-driven approaches, DexSkin can be calibrated to
enable model transfer across sensor instances, and demonstrate its
applicability to online reinforcement learning on real robots. Our results
highlight DexSkin's suitability and practicality for learning real-world,
contact-rich manipulation. Please see our project webpage for videos and
visualizations: https://dex-skin.github.io/.
\\ ( https://arxiv.org/abs/2509.18830 ,  30903kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18947 (*cross-listing*)
Date: Tue, 23 Sep 2025 12:58:12 GMT   (1752kb)

Title: Quantum Random Synthetic Skyrmion Texture Generation, a Qiskit
  Simulation
Authors: Hillol Biswas
Categories: quant-ph cs.CV
\\
  An integer winding, i.e., topological charge, is a characteristic of
skyrmions, which are topologically nontrivial spin patterns in magnets. They
emerge when smooth two-dimensional spin configurations are stabilized by
conflicting interactions such as exchange, anisotropy, the
Dzyaloshinskii-Moriya interaction, or geometric frustration. These nanoscale
textures, which are typically a few to tens of nanometers in size, are strong
'particle-like' excitations because they are shielded by energy barriers
connected to their topology. By exploiting their helicity, i.e., spin rotation
angle or associated internal modes, as a two-level system, skyrmions can
function as quantum bits or qubits. Two quantized helicity states of a
nanometer-scale skyrmion encode the logical value states in a 'skyrmion qubit.'
Interestingly, skyrmion qubits are topologically protected and macroscopic,
i.e., they involve a large number of spins; however, external influences can
still affect them. When the texture is tiny and disconnected, the helicity
angle of the skyrmion becomes quantized. A qubit basis is made up of the lowest
two energy eigenstates, i.e., symmetric or antisymmetric superpositions of
opposite helicity, for example. Therefore, Skyrmion textures can provide
valuable insights for different purposes. However, is it possible to
synthetically generate skyrmion textures using quantum computing? This paper
investigates the possibility and generates a few hundred different textures,
producing sample comparisons from various types, which indicate a novel
direction for skyrmion-based research based on quantum randomness and other
criteria.
\\ ( https://arxiv.org/abs/2509.18947 ,  1752kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18948 (*cross-listing*)
Date: Tue, 23 Sep 2025 12:58:15 GMT   (27787kb)

Title: One-shot Embroidery Customization via Contrastive LoRA Modulation
Authors: Jun Ma, Qian He, Gaofeng He, Huang Chen, Chen Liu, Xiaogang Jin,
  Huamin Wang
Categories: cs.GR cs.CV
Comments: Accepted to ACM Transactions on Graphics (TOG), SIGGRAPH Asia 2025
\\
  Diffusion models have significantly advanced image manipulation techniques,
and their ability to generate photorealistic images is beginning to transform
retail workflows, particularly in presale visualization. Beyond artistic style
transfer, the capability to perform fine-grained visual feature transfer is
becoming increasingly important. Embroidery is a textile art form characterized
by intricate interplay of diverse stitch patterns and material properties,
which poses unique challenges for existing style transfer methods. To explore
the customization for such fine-grained features, we propose a novel
contrastive learning framework that disentangles fine-grained style and content
features with a single reference image, building on the classic concept of
image analogy. We first construct an image pair to define the target style, and
then adopt a similarity metric based on the decoupled representations of
pretrained diffusion models for style-content separation. Subsequently, we
propose a two-stage contrastive LoRA modulation technique to capture
fine-grained style features. In the first stage, we iteratively update the
whole LoRA and the selected style blocks to initially separate style from
content. In the second stage, we design a contrastive learning strategy to
further decouple style and content through self-knowledge distillation.
Finally, we build an inference pipeline to handle image or text inputs with
only the style blocks. To evaluate our method on fine-grained style transfer,
we build a benchmark for embroidery customization. Our approach surpasses prior
methods on this task and further demonstrates strong generalization to three
additional domains: artistic style transfer, sketch colorization, and
appearance transfer.
\\ ( https://arxiv.org/abs/2509.18948 ,  27787kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18954 (*cross-listing*)
Date: Tue, 23 Sep 2025 13:02:44 GMT   (8902kb)

Title: Towards Robust LiDAR Localization: Deep Learning-based Uncertainty
  Estimation
Authors: Minoo Dolatabadi, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi
  Javanmardi
Categories: cs.RO cs.CV
\\
  LiDAR-based localization and SLAM often rely on iterative matching
algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align
sensor data with pre-existing maps or previous scans. However, ICP is prone to
errors in featureless environments and dynamic scenes, leading to inaccurate
pose estimation. Accurately predicting the uncertainty associated with ICP is
crucial for robust state estimation but remains challenging, as existing
approaches often rely on handcrafted models or simplified assumptions.
Moreover, a few deep learning-based methods for localizability estimation
either depend on a pre-built map, which may not always be available, or provide
a binary classification of localizable versus non-localizable, which fails to
properly model uncertainty. In this work, we propose a data-driven framework
that leverages deep learning to estimate the registration error covariance of
ICP before matching, even in the absence of a reference map. By associating
each LiDAR scan with a reliable 6-DoF error covariance estimate, our method
enables seamless integration of ICP within Kalman filtering, enhancing
localization accuracy and robustness. Extensive experiments on the KITTI
dataset demonstrate the effectiveness of our approach, showing that it
accurately predicts covariance and, when applied to localization using a
pre-built map or SLAM, reduces localization errors and improves robustness.
\\ ( https://arxiv.org/abs/2509.18954 ,  8902kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18979 (*cross-listing*)
Date: Tue, 23 Sep 2025 13:29:32 GMT   (4294kb)

Title: Category-Level Object Shape and Pose Estimation in Less Than a
  Millisecond
Authors: Lorenzo Shaikewitz, Tim Nguyen, Luca Carlone
Categories: cs.RO cs.CV
\\
  Object shape and pose estimation is a foundational robotics problem,
supporting tasks from manipulation to scene understanding and navigation. We
present a fast local solver for shape and pose estimation which requires only
category-level object priors and admits an efficient certificate of global
optimality. Given an RGB-D image of an object, we use a learned front-end to
detect sparse, category-level semantic keypoints on the target object. We
represent the target object's unknown shape using a linear active shape model
and pose a maximum a posteriori optimization problem to solve for position,
orientation, and shape simultaneously. Expressed in unit quaternions, this
problem admits first-order optimality conditions in the form of an eigenvalue
problem with eigenvector nonlinearities. Our primary contribution is to solve
this problem efficiently with self-consistent field iteration, which only
requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector
pair at each iterate. Solving a linear system for the corresponding Lagrange
multipliers gives a simple global optimality certificate. One iteration of our
solver runs in about 100 microseconds, enabling fast outlier rejection. We test
our method on synthetic data and a variety of real-world settings, including
two public datasets and a drone tracking scenario. Code is released at
https://github.com/MIT-SPARK/Fast-ShapeAndPose.
\\ ( https://arxiv.org/abs/2509.18979 ,  4294kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19044 (*cross-listing*)
Date: Tue, 23 Sep 2025 14:12:41 GMT   (2557kb)

Title: Latent Danger Zone: Distilling Unified Attention for Cross-Architecture
  Black-box Attacks
Authors: Yang Li, Chenyu Wang, Tingrui Wang, Yongwei Wang, Haonan Li, Zhunga
  Liu, Quan Pan
Categories: cs.LG cs.CV
\\
  Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.
\\ ( https://arxiv.org/abs/2509.19044 ,  2557kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18129 (*cross-listing*)
Date: Thu, 11 Sep 2025 21:47:24 GMT   (501kb)

Title: Pareto-optimal Tradeoffs Between Communication and Computation with
  Flexible Gradient Tracking
Authors: Yan Huang, Jinming Xu, Li Chai, Jiming Chen, and Karl H. Johansson
Categories: math.OC cs.DC cs.LG
Comments: 25 pages
MSC-class: 90C06, 90C25, 90C26
\\
  This paper addresses distributed optimization problems in non-i.i.d.
scenarios, focusing on the interplay between communication and computation
efficiency. To this end, we propose FlexGT, a flexible snapshot gradient
tracking method with tunable numbers of local updates and neighboring
communications in each round. Leveraging a unified convergence analysis
framework, we prove that FlexGT achieves a linear or sublinear convergence rate
depending on objective-specific properties--from (strongly) convex to
nonconvex--and the above-mentioned tunable parameters. FlexGT is provably
robust to the heterogeneity across nodes and attains the best-known
communication and computation complexity among existing results. Moreover, we
introduce an accelerated gossip-based variant, termed Acc-FlexGT, and show that
with prior knowledge of the graph, it achieves a Pareto-optimal trade-off
between communication and computation. Particularly, Acc-FlexGT achieves the
optimal iteration complexity of $\tilde{\mathcal{O}} \left( L/\epsilon +L\sigma
^2/\left( n\epsilon^2 \sqrt{1-\sqrt{\rho _W}} \right) \right) $ for the
nonconvex case, matching the existing lower bound up to a logarithmic factor,
and improves the existing results for the strongly convex case by a factor of
$\tilde{\mathcal{O}} \left( 1/\sqrt{\epsilon} \right)$, where $\epsilon$ is the
targeted accuracy, $n$ the number of nodes, $L$ the Lipschitz constant,
$\rho_W$ the spectrum gap of the graph, and $\sigma$ the stochastic gradient
variance. Numerical examples are provided to demonstrate the effectiveness of
the proposed methods.
\\ ( https://arxiv.org/abs/2509.18129 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18519 (*cross-listing*)
Date: Tue, 23 Sep 2025 01:31:20 GMT   (14kb)

Title: Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network
  Paths
Authors: Michael Luby, John Byers
Categories: cs.NI cs.DC
Comments: Technical report, 18 pages. Includes formal proofs of packet spray
  discrepancy bounds and example path profile updates
\\
  We present Whack-a-Mole, a deterministic packet spraying algorithm for
distributing packets across multiple network paths with provably tight
discrepancy bounds. The algorithm is motivated by large-scale distributed AI/ML
training and inference workloads, where collective completion time (CCT) and
effective training time ratio (ETTR) are highly sensitive to tail latency and
transport imbalance. Whack-a-Mole represents the path profile as a discrete
allocation of $m$ selection units across $n$ paths and uses a bit-reversal
counter to choose a path for each packet. We prove that the discrepancy between
expected and actual packet counts per path is bounded by $O(\log m)$ over any
contiguous packet sequence. The algorithm responds quickly to congestion
feedback by reducing allocations to degraded paths and redistributing load to
healthier ones. This combination of deterministic distribution, low per-packet
overhead, and compatibility with erasure-coded transport makes Whack-a-Mole an
effective building block for multipath transport protocols that aim to minimize
CCT and maximize GPU utilization.
\\ ( https://arxiv.org/abs/2509.18519 ,  14kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18191 (*cross-listing*)
Date: Fri, 19 Sep 2025 08:45:05 GMT   (2911kb)

Title: Introducing a novel Location-Assignment Algorithm for Activity-Based
  Transport Models: CARLA
Authors: Felix Petre, Lasse Bienzeisler, Bernhard Friedrich
Categories: cs.OH cs.CY cs.MA math.OC
Comments: 9 pages, 5 figures, 3 tables. Presented as a short paper at the 13th
  Symposium of the European Association for Research in Transportation (hEART
  2025), hosted by TU Munich
\\
  This paper introduces CARLA (spatially Constrained Anchor-based Recursive
Location Assignment), a recursive algorithm for assigning secondary or any
activity locations in activity-based travel models. CARLA minimizes distance
deviations while integrating location potentials, ensuring more realistic
activity distributions. The algorithm decomposes trip chains into smaller
subsegments, using geometric constraints and configurable heuristics to
efficiently search the solution space. Compared to a state-of-the-art
relaxation-discretization approach, CARLA achieves significantly lower mean
deviations, even under limited runtimes. It is robust to real-world data
inconsistencies, such as infeasible distances, and can flexibly adapt to
various priorities, such as emphasizing location attractiveness or distance
accuracy. CARLA's versatility and efficiency make it a valuable tool for
improving the spatial accuracy of activity-based travel models and agent-based
transport simulations. Our implementation is available at
https://github.com/tnoud/carla.
\\ ( https://arxiv.org/abs/2509.18191 ,  2911kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18371 (*cross-listing*)
Date: Mon, 22 Sep 2025 19:52:16 GMT   (12104kb)

Title: Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear
  Multi-Agent Games
Authors: Eduardo Sebasti\'an, Maitrayee Keskar, Eeman Iqbal, Eduardo Montijano,
  Carlos Sag\"u\'es and Nikolay Atanasov
Categories: eess.SY cs.MA cs.RO cs.SY
\\
  Multi-agent games in dynamic nonlinear settings are challenging due to the
time-varying interactions among the agents and the non-stationarity of the
(potential) Nash equilibria. In this paper we consider model-free games, where
agent transitions and costs are observed without knowledge of the transition
and cost functions that generate them. We propose a policy gradient approach to
learn distributed policies that follow the communication structure in
multi-team games, with multiple agents per team. Our formulation is inspired by
the structure of distributed policies in linear quadratic games, which take the
form of time-varying linear feedback gains. In the nonlinear case, we model the
policies as nonlinear feedback gains, parameterized by self-attention layers to
account for the time-varying multi-agent communication topology. We demonstrate
that our distributed policy gradient approach achieves strong performance in
several settings, including distributed linear and nonlinear regulation, and
simulated and real multi-robot pursuit-and-evasion games.
\\ ( https://arxiv.org/abs/2509.18371 ,  12104kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18793 (*cross-listing*)
Date: Tue, 23 Sep 2025 08:36:08 GMT   (5886kb)

Title: Application Management in C-ITS: Orchestrating Demand-Driven Deployments
  and Reconfigurations
Authors: Lukas Zanger, Bastian Lampe, Lennart Reiher, Lutz Eckstein
Categories: cs.RO cs.MA cs.SE
Comments: 7 pages, 2 figures, 2 tables; Accepted to be published as part of the
  2025 IEEE International Conference on Intelligent Transportation Systems
  (ITSC 2025), Gold Coast, Australia, November 18-21, 2025
\\
  Vehicles are becoming increasingly automated and interconnected, enabling the
formation of cooperative intelligent transport systems (C-ITS) and the use of
offboard services. As a result, cloud-native techniques, such as microservices
and container orchestration, play an increasingly important role in their
operation. However, orchestrating applications in a large-scale C-ITS poses
unique challenges due to the dynamic nature of the environment and the need for
efficient resource utilization. In this paper, we present a demand-driven
application management approach that leverages cloud-native techniques -
specifically Kubernetes - to address these challenges. Taking into account the
demands originating from different entities within the C-ITS, the approach
enables the automation of processes, such as deployment, reconfiguration,
update, upgrade, and scaling of microservices. Executing these processes on
demand can, for example, reduce computing resource consumption and network
traffic. A demand may include a request for provisioning an external supporting
service, such as a collective environment model. The approach handles changing
and new demands by dynamically reconciling them through our proposed
application management framework built on Kubernetes and the Robot Operating
System (ROS 2). We demonstrate the operation of our framework in the C-ITS use
case of collective environment perception and make the source code of the
prototypical framework publicly available at
https://github.com/ika-rwth-aachen/application_manager .
\\ ( https://arxiv.org/abs/2509.18793 ,  5886kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19246 (*cross-listing*)
Date: Tue, 23 Sep 2025 17:06:52 GMT   (14018kb)

Title: Proactive-reactive detection and mitigation of intermittent faults in
  robot swarms
Authors: Sinan O\u{g}uz, Emanuele Garone, Marco Dorigo, Mary Katherine Heinrich
Categories: cs.RO cs.MA cs.SY eess.SY
\\
  Intermittent faults are transient errors that sporadically appear and
disappear. Although intermittent faults pose substantial challenges to
reliability and coordination, existing studies of fault tolerance in robot
swarms focus instead on permanent faults. One reason for this is that
intermittent faults are prohibitively difficult to detect in the fully
self-organized ad-hoc networks typical of robot swarms, as their network
topologies are transient and often unpredictable. However, in the recently
introduced self-organizing nervous systems (SoNS) approach, robot swarms are
able to self-organize persistent network structures for the first time, easing
the problem of detecting intermittent faults. To address intermittent faults in
robot swarms that have persistent networks, we propose a novel
proactive-reactive strategy to detection and mitigation, based on
self-organized backup layers and distributed consensus in a multiplex network.
Proactively, the robots self-organize dynamic backup paths before faults occur,
adapting to changes in the primary network topology and the robots' relative
positions. Reactively, robots use one-shot likelihood ratio tests to compare
information received along different paths in the multiplex network, enabling
early fault detection. Upon detection, communication is temporarily rerouted in
a self-organized way, until the detected fault resolves. We validate the
approach in representative scenarios of faulty positional data occurring during
formation control, demonstrating that intermittent faults are prevented from
disrupting convergence to desired formations, with high fault detection
accuracy and low rates of false positives.
\\ ( https://arxiv.org/abs/2509.19246 ,  14018kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2501.08074
replaced with revised version Mon, 22 Sep 2025 18:56:06 GMT   (1353kb)

Title: Artificial Liver Classifier: A New Alternative to Conventional Machine
  Learning Models
Authors: Mahmood A. Jumaah, Yossra H. Ali, Tarik A. Rashid
Categories: cs.AI
Comments: 22 pages
Journal-ref: Front. Artif. Intell. 8:1639720 (2025)
DOI: 10.3389/frai.2025.1639720
\\ ( https://arxiv.org/abs/2501.08074 ,  1353kb)
------------------------------------------------------------------------------
\\
arXiv:2501.17310
replaced with revised version Tue, 23 Sep 2025 15:17:03 GMT   (3483kb)

Title: Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds
  Decoding
Authors: Yun-Shiuan Chuang, Sameer Narendran, Nikunj Harlalka, Alexander
  Cheung, Sizhe Gao, Siddharth Suresh, Junjie Hu, Timothy T. Rogers
Categories: cs.AI cs.HC
\\ ( https://arxiv.org/abs/2501.17310 ,  3483kb)
------------------------------------------------------------------------------
\\
arXiv:2503.19470
replaced with revised version Tue, 23 Sep 2025 03:45:42 GMT   (191kb)

Title: ReSearch: Learning to Reason with Search for LLMs via Reinforcement
  Learning
Authors: Mingyang Chen, Linzhuang Sun, Tianpeng Li, Haoze Sun, Yijie Zhou,
  Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang,
  Zenan Zhou, Weipeng Chen
Categories: cs.AI cs.CL
Comments: Accepted to NeurIPS 2025
\\ ( https://arxiv.org/abs/2503.19470 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2503.23329
replaced with revised version Tue, 23 Sep 2025 08:36:02 GMT   (28137kb)

Title: A Multi-Agent Framework with Automated Decision Rule Optimization for
  Cross-Domain Misinformation Detection
Authors: Hui Li, Ante Wang, kunquan li, Zhihao Wang, Liang Zhang, Delai Qiu,
  Qingsong Liu, Jinsong Su
Categories: cs.AI
\\ ( https://arxiv.org/abs/2503.23329 ,  28137kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05684
replaced with revised version Tue, 23 Sep 2025 00:50:57 GMT   (319kb)

Title: Meta-Semantics Augmented Few-Shot Relational Learning
Authors: Han Wu, Jie Yin
Categories: cs.AI cs.CL cs.LG
Comments: Accepted by EMNLP 2025
\\ ( https://arxiv.org/abs/2505.05684 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11189
replaced with revised version Tue, 23 Sep 2025 15:19:17 GMT   (95kb)

Title: Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule
  Extraction vs RuleSHAP
Authors: Francesco Sovrano
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2505.11189 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16147
replaced with revised version Tue, 23 Sep 2025 02:26:53 GMT   (0kb,I)

Title: Losing is for Cherishing: Data Valuation Based on Machine Unlearning and
  Shapley Value
Authors: Le Ma, Shirao Yang, Zihao Wang, Yinggui Wang, Lei Wang, Tao Wei, Kejun
  Zhang
Categories: cs.AI cs.LG
Comments: There are theoretical mistakes in Section 3.2, where the definition
  of utility should be fixed. Therefore, this paper requires a major revision
  in its methodology
\\ ( https://arxiv.org/abs/2505.16147 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19892
replaced with revised version Tue, 23 Sep 2025 07:08:26 GMT   (1288kb)

Title: OptMerge: Unifying Multimodal LLM Capabilities and Modalities via Model
  Merging
Authors: Yongxian Wei, Runxi Cheng, Weike Jin, Enneng Yang, Li Shen, Lu Hou,
  Sinan Du, Chun Yuan, Xiaochun Cao, Dacheng Tao
Categories: cs.AI
\\ ( https://arxiv.org/abs/2505.19892 ,  1288kb)
------------------------------------------------------------------------------
\\
arXiv:2505.24846
replaced with revised version Mon, 22 Sep 2025 19:02:16 GMT   (2560kb)

Title: MiCRo: Mixture Modeling and Context-aware Routing for Personalized
  Preference Learning
Authors: Jingyan Shen, Jiarui Yao, Rui Yang, Yifan Sun, Feng Luo, Rui Pan, Tong
  Zhang, Han Zhao
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2505.24846 ,  2560kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11555
replaced with revised version Tue, 23 Sep 2025 08:42:57 GMT   (7915kb)

Title: RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware
  Reasoning
Authors: Yu Wang, Shiwan Zhao, Zhihu Wang, Ming Fan, Xicheng Zhang, Yubo Zhang,
  Zhengfan Wang, Heyuan Huang, Ting Liu
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2506.11555 ,  7915kb)
------------------------------------------------------------------------------
\\
arXiv:2507.03293
replaced with revised version Tue, 23 Sep 2025 04:36:17 GMT   (485kb)

Title: LogicGuard: Improving Embodied LLM agents through Temporal Logic based
  Critics
Authors: Anand Gokhale and Vaibhav Srivastava and Francesco Bullo
Categories: cs.AI cs.CL cs.LG cs.SY eess.SY
Comments: Modified version of prior LTLCrit work with new robotics dataset
\\ ( https://arxiv.org/abs/2507.03293 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2507.03616
replaced with revised version Tue, 23 Sep 2025 14:00:26 GMT   (391kb)

Title: EvoAgentX: An Automated Framework for Evolving Agentic Workflows
Authors: Yingxu Wang, Siwei Liu, Jinyuan Fang, Zaiqiao Meng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2507.03616 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2507.06993
replaced with revised version Tue, 23 Sep 2025 03:59:02 GMT   (18483kb)

Title: IMAIA: Interactive Maps AI Assistant for Travel Planning and Geo-Spatial
  Intelligence
Authors: Jieren Deng, Zhizhang Hu, Ziyan He, Aleksandar Cvetkovic, Pak Kiu
  Chung, Dragomir Yankov and Chiqun Zhang
Categories: cs.AI cs.CV
\\ ( https://arxiv.org/abs/2507.06993 ,  18483kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01561
replaced with revised version Tue, 23 Sep 2025 15:38:52 GMT   (3676kb)

Title: One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear
  Temporal Logic Requirements in Multi-Task Reinforcement Learning
Authors: Zijian Guo, \.Ilker I\c{s}{\i}k, H. M. Sabbir Ahmad, Wenchao Li
Categories: cs.AI
\\ ( https://arxiv.org/abs/2508.01561 ,  3676kb)
------------------------------------------------------------------------------
\\
arXiv:2509.06278
replaced with revised version Tue, 23 Sep 2025 02:57:07 GMT   (2130kb)

Title: TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table
  Reasoning
Authors: Chuang Jiang, Mingyue Cheng, Xiaoyu Tao, Qingyang Mao, Jie Ouyang, Qi
  Liu
Categories: cs.AI
Comments: Comments: 10 pages, 6 figures. Submitted to WSDM 2026
\\ ( https://arxiv.org/abs/2509.06278 ,  2130kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10401
replaced with revised version Tue, 23 Sep 2025 14:45:53 GMT   (747kb)

Title: Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure
  Attribution in Multi-Agent Systems
Authors: Alva West, Yixuan Weng, Minjun Zhu, Zhen Lin, Zhiyuan Ning, Yue Zhang
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2509.10401 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11079
replaced with revised version Tue, 23 Sep 2025 13:32:37 GMT   (1132kb)

Title: Difficulty-Aware Agent Orchestration in LLM-Powered Workflows
Authors: Jinwei Su, Yinghui Xia, Qizhen Lan, Xinyuan Song, Chen Chen, Yang
  Jingsong, Lewei He, Tianyu Shi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.11079 ,  1132kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14778
replaced with revised version Tue, 23 Sep 2025 01:37:30 GMT   (1359kb)

Title: OpenLens AI: Fully Autonomous Research Agent for Health Infomatics
Authors: Yuxiao Cheng, Jinli Suo
Categories: cs.AI cs.MA
\\ ( https://arxiv.org/abs/2509.14778 ,  1359kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17393
replaced with revised version Tue, 23 Sep 2025 01:29:22 GMT   (313kb)

Title: Program Synthesis via Test-Time Transduction
Authors: Kang-il Lee, Jahyun Koo, Seunghyun Yoon, Minbeom Kim, Hyukhun Koh,
  Dongryeol Lee, Kyomin Jung
Categories: cs.AI cs.CL
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2509.17393 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17544
replaced with revised version Tue, 23 Sep 2025 14:32:50 GMT   (4906kb)

Title: A Multimodal Conversational Assistant for the Characterization of
  Agricultural Plots from Geospatial Open Data
Authors: Juan Ca\~nada, Ra\'ul Alonso, Julio Molleda, Fidel D\'iez
Categories: cs.AI
Comments: Accepted at 2025 4th International Conference on Geographic
  Information and Remote Sensing Technology
\\ ( https://arxiv.org/abs/2509.17544 ,  4906kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17706
replaced with revised version Tue, 23 Sep 2025 08:35:07 GMT   (121kb)

Title: Virtual Arc Consistency for Linear Constraints in Cost Function Networks
Authors: Pierre Montalbano, Simon de Givry and George Katsirelos
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.17706 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17905
replaced with revised version Tue, 23 Sep 2025 05:27:09 GMT   (440kb)

Title: Mitigating Strategy-Selection Bias in Reasoning for More Effective
  Test-Time Scaling
Authors: Zongqian Wu, Baoduo Xu, Tianyu Li, Zhu Sun, Xiaofeng Zhu, Lei Feng
Categories: cs.AI
Comments: 23 pages, 9 figures
\\ ( https://arxiv.org/abs/2509.17905 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2409.02889
replaced with revised version Mon, 22 Sep 2025 19:16:45 GMT   (3882kb)

Title: LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a
  Hybrid Architecture
Authors: Xidong Wang, Dingjie Song, Shunian Chen, Junyin Chen, Zhenyang Cai,
  Chen Zhang, Lichao Sun, Benyou Wang
Categories: cs.CL cs.AI cs.CV cs.MM
Comments: Accepted to EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2409.02889 ,  3882kb)
------------------------------------------------------------------------------
\\
arXiv:2409.04183
replaced with revised version Tue, 23 Sep 2025 03:53:05 GMT   (253kb)

Title: GALLa: Graph Aligned Large Language Models for Improved Source Code
  Understanding
Authors: Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang
Categories: cs.CL cs.AI
Comments: ACL 2025
\\ ( https://arxiv.org/abs/2409.04183 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2409.12887
replaced with revised version Tue, 23 Sep 2025 04:46:17 GMT   (10262kb)

Title: Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data
  Augmentation and Gaussian-Decayed Contrastive Learning
Authors: Peichao Lai, Zhengfeng Zhang, Wentao Zhang, Fangcheng Fu, Bin Cui
Categories: cs.CL
\\ ( https://arxiv.org/abs/2409.12887 ,  10262kb)
------------------------------------------------------------------------------
\\
arXiv:2410.05401
replaced with revised version Tue, 23 Sep 2025 01:45:34 GMT   (1934kb)

Title: Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs:
  Thematic Insights and Fairness Evaluation
Authors: Tunazzina Islam, Dan Goldwasser
Categories: cs.CL cs.AI cs.CY cs.SI
Comments: Accepted at Findings of 2025 Conference on Empirical Methods in
  Natural Language Processing (EMNLP 2025)
\\ ( https://arxiv.org/abs/2410.05401 ,  1934kb)
------------------------------------------------------------------------------
\\
arXiv:2410.12613
replaced with revised version Tue, 23 Sep 2025 17:26:13 GMT   (3193kb)

Title: Exploring Model Kinship for Merging Large Language Models
Authors: Yedi Hu, Yunzhi Yao, Ningyu Zhang, Huajun Chen, Shumin Deng
Categories: cs.CL cs.AI cs.CV cs.LG cs.MA
Comments: EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2410.12613 ,  3193kb)
------------------------------------------------------------------------------
\\
arXiv:2411.08019
replaced with revised version Mon, 22 Sep 2025 21:11:32 GMT   (3135kb)

Title: Language Models as Causal Effect Generators
Authors: Lucius E.J. Bynum and Kyunghyun Cho
Categories: cs.CL cs.AI cs.LG stat.AP stat.ME stat.ML
\\ ( https://arxiv.org/abs/2411.08019 ,  3135kb)
------------------------------------------------------------------------------
\\
arXiv:2411.10927
replaced with revised version Tue, 23 Sep 2025 07:12:25 GMT   (10483kb)

Title: Compositional Phoneme Approximation for L1-Grounded L2 Pronunciation
  Training
Authors: Jisang Park, Minu Kim, DaYoung Hong, and Jongha Lee
Categories: cs.CL cs.SD eess.AS
ACM-class: H.5.5
\\ ( https://arxiv.org/abs/2411.10927 ,  10483kb)
------------------------------------------------------------------------------
\\
arXiv:2501.19093
replaced with revised version Tue, 23 Sep 2025 04:49:18 GMT   (1880kb)

Title: Improving Low-Resource Sequence Labeling with Knowledge Fusion and
  Contextual Label Explanations
Authors: Peichao Lai, Jiaxin Gan, Feiyang Ye, Yilei Wang and Bin Cui
Categories: cs.CL
\\ ( https://arxiv.org/abs/2501.19093 ,  1880kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11361
replaced with revised version Tue, 23 Sep 2025 15:07:33 GMT   (18736kb)

Title: VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment
Authors: Shaina Raza, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza
  Khazaie, Syed Raza Bashir, Elham Dolatabadi, Gias Uddin, Christos
  Emmanouilidis, Rizwan Qureshi, Mubarak Shah
Categories: cs.CL
Comments: under review
\\ ( https://arxiv.org/abs/2502.11361 ,  18736kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13329
replaced with revised version Mon, 22 Sep 2025 18:41:38 GMT   (593kb)

Title: Language Models Can Predict Their Own Behavior
Authors: Dhananjay Ashok, Jonathan May
Categories: cs.CL cs.AI cs.LG
Comments: Presented at the Thirty-Ninth Annual Conference on Neural Information
  Processing Systems (2025)
\\ ( https://arxiv.org/abs/2502.13329 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2502.14359
replaced with revised version Tue, 23 Sep 2025 08:53:37 GMT   (2025kb)

Title: Triangulating LLM Progress through Benchmarks, Games, and Cognitive
  Tests
Authors: Filippo Moment\`e, Alessandro Suglia, Mario Giulianelli, Ambra
  Ferrari, Alexander Koller, Oliver Lemon, David Schlangen, Raquel Fern\'andez,
  Raffaella Bernardi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2502.14359 ,  2025kb)
------------------------------------------------------------------------------
\\
arXiv:2502.15589
replaced with revised version Tue, 23 Sep 2025 17:20:22 GMT   (9418kb)

Title: LightThinker: Thinking Step-by-Step Compression
Authors: Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du,
  Da Zheng, Huajun Chen, Ningyu Zhang
Categories: cs.CL cs.AI cs.IR cs.LG cs.MM
Comments: EMNLP 2025 (oral)
\\ ( https://arxiv.org/abs/2502.15589 ,  9418kb)
------------------------------------------------------------------------------
\\
arXiv:2502.18156
replaced with revised version Tue, 23 Sep 2025 06:10:32 GMT   (167kb)

Title: Can LLMs Explain Themselves Counterfactually?
Authors: Zahra Dehghanighobadi and Asja Fischer and Muhammad Bilal Zafar
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2502.18156 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2502.18795
replaced with revised version Tue, 23 Sep 2025 03:08:44 GMT   (313kb)

Title: Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning
  in LMs
Authors: Xiulin Yang, Tatsuya Aoyama, Yuekun Yao, Ethan Wilcox
Categories: cs.CL
Comments: ACL 2025
\\ ( https://arxiv.org/abs/2502.18795 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2502.20475
replaced with revised version Tue, 23 Sep 2025 02:52:22 GMT   (13385kb)

Title: Promote, Suppress, Iterate: How Language Models Answer One-to-Many
  Factual Queries
Authors: Tianyi Lorena Yan and Robin Jia
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to EMNLP 2025
\\ ( https://arxiv.org/abs/2502.20475 ,  13385kb)
------------------------------------------------------------------------------
\\
arXiv:2502.21074
replaced with revised version Tue, 23 Sep 2025 08:16:08 GMT   (2371kb)

Title: CODI: Compressing Chain-of-Thought into Continuous Space via
  Self-Distillation
Authors: Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, Yulan He
Categories: cs.CL
Comments: 17 pages. Code available at https://github.com/zhenyi4/codi
\\ ( https://arxiv.org/abs/2502.21074 ,  2371kb)
------------------------------------------------------------------------------
\\
arXiv:2503.16356
replaced with revised version Tue, 23 Sep 2025 17:10:14 GMT   (1161kb)

Title: CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners
Authors: Yunzhi Yao, Jizhan Fang, Jia-Chen Gu, Ningyu Zhang, Shumin Deng,
  Huajun Chen, Nanyun Peng
Categories: cs.CL cs.AI cs.CV cs.IR cs.LG
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2503.16356 ,  1161kb)
------------------------------------------------------------------------------
\\
arXiv:2503.19041
replaced with revised version Tue, 23 Sep 2025 17:04:18 GMT   (1278kb)

Title: LookAhead Tuning: Safer Language Models via Partial Answer Previews
Authors: Kangwei Liu, Mengru Wang, Yujie Luo, Yuan Lin, Mengshu Sun, Lei Liang,
  Zhiqiang Zhang, Jun Zhou, Bryan Hooi, Shumin Deng
Categories: cs.CL cs.AI cs.CV cs.LG cs.MM
Comments: Work in progress
\\ ( https://arxiv.org/abs/2503.19041 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2504.12734
replaced with revised version Tue, 23 Sep 2025 11:15:44 GMT   (0kb,I)

Title: Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning
  Across Diverse Structured Knowledge
Authors: Yongrui Chen, Junhao He, Linbo Fu, Shenyu Zhang, Rihui Jin, Xinbang
  Dai, Jiaqi Li, Dehai Min, Nan Hu, Yuxin Zhang, Guilin Qi, Yi Huang, Tongtong
  Wu
Categories: cs.CL cs.AI
Comments: New version is arXiv:2508.17905
\\ ( https://arxiv.org/abs/2504.12734 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2505.06538
replaced with revised version Tue, 23 Sep 2025 03:33:21 GMT   (8598kb)

Title: Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in
  Multimodal Large Reasoning Model
Authors: Xinyue Lou, You Li, Jinan Xu, Xiangyu Shi, Chi Chen, Kaiyu Huang
Categories: cs.CL
Comments: Accepted by EMNLP2025-main(Oral Presentation, SAC score: 10)
\\ ( https://arxiv.org/abs/2505.06538 ,  8598kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11341
replaced with revised version Tue, 23 Sep 2025 17:07:48 GMT   (1056kb)

Title: Benchmarking Critical Questions Generation: A Challenging Reasoning Task
  for Large Language Models
Authors: Banca Calvo Figueras, Rodrigo Agerri
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.11341 ,  1056kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12543
replaced with revised version Mon, 22 Sep 2025 18:36:48 GMT   (209kb)

Title: Disambiguation in Conversational Question Answering in the Era of LLMs
  and Agents: A Survey
Authors: Md Mehrab Tanjim, Yeonjun In, Xiang Chen, Victor S. Bursztyn, Ryan A.
  Rossi, Sungchul Kim, Guang-Jie Ren, Vaishnavi Muppala, Shun Jiang, Yongsung
  Kim, Chanyoung Park
Categories: cs.CL
Comments: 14 pages, 2 figures, Accepted at EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2505.12543 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14305
replaced with revised version Tue, 23 Sep 2025 02:27:14 GMT   (1264kb)

Title: JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy
  Schema Sampling
Authors: Jinwang Song, Hongying Zan, Kunli Zhang, Lingling Mu, Yingjie Han,
  Haobo Hua, Min Peng
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2505.14305 ,  1264kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15389
replaced with revised version Tue, 23 Sep 2025 15:14:42 GMT   (2182kb)

Title: Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark
  Study
Authors: DongGeon Lee, Joonwon Jang, Jihae Jeong, Hwanjo Yu
Categories: cs.CL cs.CR cs.CV
Comments: Accepted to EMNLP 2025
\\ ( https://arxiv.org/abs/2505.15389 ,  2182kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16216
replaced with revised version Tue, 23 Sep 2025 07:17:12 GMT   (3074kb)

Title: Memorization or Reasoning? Exploring the Idiom Understanding of LLMs
Authors: Jisu Kim, Youngwoo Shin, Uiji Hwang, Jihun Choi, Richeng Xuan and
  Taeuk Kim
Categories: cs.CL
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2505.16216 ,  3074kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17091
replaced with revised version Tue, 23 Sep 2025 03:02:04 GMT   (2745kb)

Title: Large Language Models Implicitly Learn to See and Hear Just By Reading
Authors: Prateek Verma, Mert Pilanci
Categories: cs.CL cs.AI cs.CV cs.LG cs.SD eess.AS
Comments: 6 pages, 3 figures, 4 tables. Added BLIP reference
\\ ( https://arxiv.org/abs/2505.17091 ,  2745kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17510
replaced with revised version Mon, 22 Sep 2025 20:30:41 GMT   (1829kb)

Title: Large Language Models Do Multi-Label Classification Differently
Authors: Marcus Ma, Georgios Chochlakis, Niyantha Maruthu Pandiyan, Jesse
  Thomason, Shrikanth Narayanan
Categories: cs.CL
Comments: To be published in the Main Conference Proceedings of EMNLP 2025, 24
  pages, 16 figures, 7 tables
\\ ( https://arxiv.org/abs/2505.17510 ,  1829kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18383
replaced with revised version Mon, 22 Sep 2025 22:02:16 GMT   (2721kb)

Title: NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for
  Local Communities
Authors: Abdellah El Mekki, Houdaifa Atou, Omer Nacar, Shady Shehata, Muhammad
  Abdul-Mageed
Categories: cs.CL
Comments: Accepted to EMNLP 2025 (Main Conference). Camera-ready version. Data
  & models: https://github.com/UBC-NLP/nilechat
\\ ( https://arxiv.org/abs/2505.18383 ,  2721kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18555
replaced with revised version Tue, 23 Sep 2025 13:50:21 GMT   (40170kb)

Title: Unraveling Misinformation Propagation in LLM Reasoning
Authors: Yiyang Feng, Yichen Wang, Shaobo Cui, Boi Faltings, Mina Lee, Jiawei
  Zhou
Categories: cs.CL
Comments: Accepted to EMNLP 2025 (Findings)
\\ ( https://arxiv.org/abs/2505.18555 ,  40170kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22848
replaced with revised version Tue, 23 Sep 2025 09:52:20 GMT   (258kb)

Title: LiTEx: A Linguistic Taxonomy of Explanations for Understanding
  Within-Label Variation in Natural Language Inference
Authors: Pingjun Hong, Beiduo Chen, Siyao Peng, Marie-Catherine de Marneffe,
  Barbara Plank
Categories: cs.CL
Comments: Accepted by EMNLP 2025 Main, 22 pages, 7 figures
\\ ( https://arxiv.org/abs/2505.22848 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2506.04521
replaced with revised version Mon, 22 Sep 2025 21:51:10 GMT   (2772kb)

Title: Please Translate Again: Two Simple Experiments on Whether Human-Like
  Reasoning Helps Translation
Authors: Di Wu, Seth Aycock, Christof Monz
Categories: cs.CL
Comments: EMNLP Main 2025 17 pages, 15 figures
\\ ( https://arxiv.org/abs/2506.04521 ,  2772kb)
------------------------------------------------------------------------------
\\
arXiv:2506.06561
replaced with revised version Mon, 22 Sep 2025 21:32:08 GMT   (3177kb)

Title: LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure
  Profiles
Authors: Ho Yin 'Sam' Ng, Ting-Yao Hsu, Aashish Anantha Ramakrishnan, Branislav
  Kveton, Nedim Lipka, Franck Dernoncourt, Dongwon Lee, Tong Yu, Sungchul Kim,
  Ryan A. Rossi, Ting-Hao 'Kenneth' Huang
Categories: cs.CL cs.AI cs.CV
Comments: Accepted to EMNLP 2025 Findings. The LaMP-CAP dataset is publicly
  available at: https://github.com/Crowd-AI-Lab/lamp-cap
\\ ( https://arxiv.org/abs/2506.06561 ,  3177kb)
------------------------------------------------------------------------------
\\
arXiv:2506.14901
replaced with revised version Mon, 22 Sep 2025 18:52:28 GMT   (718kb)

Title: Combining Constrained and Unconstrained Decoding via Boosting: BoostCD
  and Its Application to Information Extraction
Authors: Marija \v{S}akota, Robert West
Categories: cs.CL
\\ ( https://arxiv.org/abs/2506.14901 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21808
replaced with revised version Mon, 22 Sep 2025 18:35:07 GMT   (624kb)

Title: A suite of allotaxonometric tools for the comparison of complex systems
  using rank-turbulence divergence
Authors: Jonathan St-Onge, Ashley M. A. Fehr, Carter Ward, Calla G. Beauregard,
  Michael V. Arnold, Samuel F. Rosenblatt, Benjamin Cooley, Christopher M.
  Danforth, and Peter Sheridan Dodds
Categories: cs.CL
Comments: 4 pages, 2 figures
\\ ( https://arxiv.org/abs/2506.21808 ,  624kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13255
replaced with revised version Tue, 23 Sep 2025 03:15:44 GMT   (20970kb)

Title: Automating Steering for Safe Multimodal Large Language Models
Authors: Lyucheng Wu, Mengru Wang, Ziwen Xu, Tri Cao, Nay Oo, Bryan Hooi,
  Shumin Deng
Categories: cs.CL cs.AI cs.IR cs.LG cs.MM
Comments: EMNLP 2025 Main Conference. 23 pages (8+ for main); 25 figures; 1
  table
\\ ( https://arxiv.org/abs/2507.13255 ,  20970kb)
------------------------------------------------------------------------------
\\
arXiv:2507.23577
replaced with revised version Tue, 23 Sep 2025 14:22:04 GMT   (640kb)

Title: T-Detect: Tail-Aware Statistical Normalization for Robust Detection of
  Adversarial Machine-Generated Text
Authors: Alva West, Luodan Zhang, Liuliu Zhang, Minjun Zhu, Yixuan Weng, Yue
  Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2507.23577 ,  640kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01754
replaced with revised version Tue, 23 Sep 2025 14:58:23 GMT   (1951kb)

Title: AI-Generated Text is Non-Stationary: Detection via Temporal Tomography
Authors: Alva West, Yixuan Weng, Minjun Zhu, Luodan Zhang, Zhen Lin, Guangsheng
  Bao, Yue Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2508.01754 ,  1951kb)
------------------------------------------------------------------------------
\\
arXiv:2508.09403
replaced with revised version Tue, 23 Sep 2025 16:19:06 GMT   (4229kb)

Title: Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large
  Language Models
Authors: Ting Cai, Stephen Sheen, AnHai Doan
Categories: cs.CL cs.DB
Comments: Accepted to Findings of EMNLP 2025; 19 pages, 14 figures
\\ ( https://arxiv.org/abs/2508.09403 ,  4229kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15044
replaced with revised version Tue, 23 Sep 2025 17:25:59 GMT   (163kb)

Title: Reward-Shifted Speculative Sampling Is An Efficient Test-Time
  Weak-to-Strong Aligner
Authors: Bolian Li, Yanran Wu, Xinyu Luo, Ruqi Zhang
Categories: cs.CL
Comments: EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2508.15044 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15139
replaced with revised version Mon, 22 Sep 2025 21:03:25 GMT   (267kb)

Title: Identifying and Answering Questions with False Assumptions: An
  Interpretable Approach
Authors: Zijie Wang and Eduardo Blanco
Categories: cs.CL
Comments: To appear at EMNLP 2025 Main conference
\\ ( https://arxiv.org/abs/2508.15139 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16048
replaced with revised version Tue, 23 Sep 2025 02:28:48 GMT   (170kb)

Title: OpenWHO: A Document-Level Parallel Corpus for Health Translation in
  Low-Resource Languages
Authors: Rapha\"el Merx, Hanna Suominen, Trevor Cohn, Ekaterina Vylomova
Categories: cs.CL cs.AI
Comments: Accepted at WMT 2025
\\ ( https://arxiv.org/abs/2508.16048 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2508.17078
replaced with revised version Tue, 23 Sep 2025 14:02:49 GMT   (6576kb)

Title: Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer
  on Low-resource Languages
Authors: Yuemei Xu, Kexin Xu, Jian Zhou, Ling Hu, Lin Gui
Categories: cs.CL cs.AI
Comments: Accepted by EMNLP 2025
\\ ( https://arxiv.org/abs/2508.17078 ,  6576kb)
------------------------------------------------------------------------------
\\
arXiv:2508.19813
replaced with revised version Tue, 23 Sep 2025 07:48:36 GMT   (3930kb)

Title: T2R-bench: A Benchmark for Generating Article-Level Reports from Real
  World Industrial Tables
Authors: Jie Zhang, Changzai Pan, Kaiwen Wei, Sishi Xiong, Yu Zhao, Xiangyu Li,
  Jiaxin Peng, Xiaoyan Gu, Jian Yang, Wenhan Chang, Zhenhe Wu, Jiang Zhong,
  Shuangyong Song, Yongxiang Li, Xuelong Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2508.19813 ,  3930kb)
------------------------------------------------------------------------------
\\
arXiv:2509.04467
replaced with revised version Tue, 23 Sep 2025 08:31:26 GMT   (841kb)

Title: PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference
Authors: Hao Zhang, Mengsi Lyu, Zhuo Chen, Xingrun Xing, Yulong Ao, Yonghua Lin
Categories: cs.CL cs.AI
Comments: 22 pages
\\ ( https://arxiv.org/abs/2509.04467 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2509.04802
replaced with revised version Mon, 22 Sep 2025 20:19:43 GMT   (4951kb)

Title: Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in
  LLMs with Action Graphs
Authors: Ilham Wicaksono, Zekun Wu, Rahul Patel, Theo King, Adriano Koshiyama,
  Philip Treleaven
Categories: cs.CL
\\ ( https://arxiv.org/abs/2509.04802 ,  4951kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11101
replaced with revised version Tue, 23 Sep 2025 02:12:08 GMT   (15kb)

Title: Seeing is Not Understanding: A Benchmark on Perception-Cognition
  Disparities in Large Language Models
Authors: Haokun Li, Yazhou Zhang, Jizhi Ding, Qiuchi Li, Peng Zhang
Categories: cs.CL
Comments: I need to modify the content of the article
\\ ( https://arxiv.org/abs/2509.11101 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12955
replaced with revised version Tue, 23 Sep 2025 14:57:23 GMT   (1872kb)

Title: Automated Generation of Research Workflows from Academic Papers: A
  Full-text Mining Framework
Authors: Heng Zhang, Chengzhi Zhang
Categories: cs.CL cs.DL cs.IR
Journal-ref: Journal of Informetrics, 2025
\\ ( https://arxiv.org/abs/2509.12955 ,  1872kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15260
replaced with revised version Tue, 23 Sep 2025 07:50:08 GMT   (1862kb)

Title: Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's
  Low-Resource Languages
Authors: Yujia Hu, Ming Shan Hee, Preslav Nakov, Roy Ka-Wei Lee
Categories: cs.CL
Comments: 9 pages, EMNLP 2025
\\ ( https://arxiv.org/abs/2509.15260 ,  1862kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15335
replaced with revised version Tue, 23 Sep 2025 11:42:25 GMT   (980kb)

Title: PolBiX: Detecting LLMs' Political Bias in Fact-Checking through
  X-phemisms
Authors: Charlott Jakob, David Harbecke, Patrick Parschan, Pia Wenzel Neves,
  Vera Schmitt
Categories: cs.CL
Comments: Accepted at Findings of EMNLP 2025, camera-ready version
\\ ( https://arxiv.org/abs/2509.15335 ,  980kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15587
replaced with revised version Tue, 23 Sep 2025 14:48:18 GMT   (319kb)

Title: DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation
  in Large Language Models
Authors: Tsz Ting Chung, Lemao Liu, Mo Yu, Dit-Yan Yeung
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by EMNLP 2025. Project Page:
  https://ttchungc.github.io/projects/divlogiceval/
\\ ( https://arxiv.org/abs/2509.15587 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16198
replaced with revised version Tue, 23 Sep 2025 01:00:38 GMT   (5263kb)

Title: RPG: A Repository Planning Graph for Unified and Scalable Codebase
  Generation
Authors: Jane Luo, Xin Zhang, Steven Liu, Jie Wu, Yiming Huang, Yangyu Huang,
  Chengyu Yin, Ying Xin, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qi Chen, Scarlett
  Li, Mao Yang
Categories: cs.CL cs.AI cs.SE
\\ ( https://arxiv.org/abs/2509.16198 ,  5263kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16264
replaced with revised version Tue, 23 Sep 2025 03:43:30 GMT   (1514kb)

Title: Gender and Political Bias in Large Language Models: A Demonstration
  Platform
Authors: Wenjie Lin, Hange Liu, Xutao Mao, Yingying Zhuang, Jingwei Shi, Xudong
  Han, Tianyu Shi, Jinrui Yang
Categories: cs.CL cs.AI cs.HC cs.LG
Comments: online demo: https://euro-parl-vote-demo.vercel.app/; Video:
  https://www.youtube.com/@Jinrui-sf2jg
\\ ( https://arxiv.org/abs/2509.16264 ,  1514kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16598
replaced with revised version Tue, 23 Sep 2025 07:28:16 GMT   (761kb)

Title: PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality
Authors: Byeongho Yu, Changhun Lee, Jungyu Jin, Eunhyeok Park
Categories: cs.CL cs.AI
Comments: accepted at EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2509.16598 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16889
replaced with revised version Tue, 23 Sep 2025 02:52:42 GMT   (3322kb)

Title: Can GRPO Boost Complex Multimodal Table Understanding?
Authors: Xiaoqiang Kang, Shengen Wu, Zimu Wang, Yilin Liu, Xiaobo Jin, Kaizhu
  Huang, Wei Wang, Yutao Yue, Xiaowei Huang, Qiufeng Wang
Categories: cs.CL
Comments: EMNLP 2025
Journal-ref: EMNLP 2025
\\ ( https://arxiv.org/abs/2509.16889 ,  3322kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16929
replaced with revised version Tue, 23 Sep 2025 12:42:19 GMT   (492kb)

Title: K-DeCore: Facilitating Knowledge Transfer in Continual Structured
  Knowledge Reasoning via Knowledge Decoupling
Authors: Yongrui Chen, Yi Huang, Yunchang Liu, Shenyu Zhang, Junhao He,
  Tongtong Wu, Guilin Qi, Tianxing Wu
Categories: cs.CL
Comments: Accepted in Neurips 2025 (poster)
\\ ( https://arxiv.org/abs/2509.16929 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17428
replaced with revised version Tue, 23 Sep 2025 07:49:57 GMT   (2548kb)

Title: QWHA: Quantization-Aware Walsh-Hadamard Adaptation for
  Parameter-Efficient Fine-Tuning on Large Language Models
Authors: Hyesung Jeon, Seojune Lee, Beomseok Kang, Yulhwa Kim, Jae-Joon Kim
Categories: cs.CL
Comments: 25 pages, 9 figures, 14 tables
\\ ( https://arxiv.org/abs/2509.17428 ,  2548kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17737
replaced with revised version Tue, 23 Sep 2025 08:18:07 GMT   (397kb)

Title: Breaking Token Into Concepts: Exploring Extreme Compression in Token
  Representation Via Compositional Shared Semantics
Authors: Kavin R V and Pawan Goyal
Categories: cs.CL
Comments: 5 pages, 1 figure Accepted at EMNLP 2025 Findings (Short)
Journal-ref: EMNLP 2025 Findings (Short)
\\ ( https://arxiv.org/abs/2509.17737 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17830
replaced with revised version Tue, 23 Sep 2025 03:46:06 GMT   (7440kb)

Title: Fine-Grained Detection of AI-Generated Text Using Sentence-Level
  Segmentation
Authors: Lekkala Sai Teja and Annepaka Yadagiri and Partha Pakray and Chukhu
  Chunka and Mangadoddi Srikar Vardhan
Categories: cs.CL cs.AI
Comments: 14 pages, 14 figures
\\ ( https://arxiv.org/abs/2509.17830 ,  7440kb)
------------------------------------------------------------------------------
\\
arXiv:1903.07162
replaced with revised version Tue, 23 Sep 2025 09:36:10 GMT   (7834kb)

Title: Evaluation Framework of Superpixel Methods with a Global Regularity
  Measure
Authors: R\'emi Giraud, Vinh-Thong Ta, Nicolas Papadakis
Categories: cs.CV
Comments: Journal of Electronic Imaging (JEI), 2017 Special issue on
  Superpixels for Image Processing and Computer Vision
\\ ( https://arxiv.org/abs/1903.07162 ,  7834kb)
------------------------------------------------------------------------------
\\
arXiv:2206.14263
replaced with revised version Tue, 23 Sep 2025 03:25:14 GMT   (174kb)

Title: ZoDIAC: Zoneout Dropout Injection Attention Calculation
Authors: Zanyar Zohourianshahzadi, Terrance E. Boult, Jugal K. Kalita
Categories: cs.CV
Comments: This work has been published in IEEE AIxSET 2024 and is available
  conference proceedings
DOI: 10.1109/AIxSET62544.2024.00008
\\ ( https://arxiv.org/abs/2206.14263 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11593
replaced with revised version Tue, 23 Sep 2025 16:12:45 GMT   (3852kb)

Title: Improving Image Captioning Descriptiveness by Ranking and LLM-based
  Fusion
Authors: Luigi Celona and Simone Bianco and Marco Donzella and Paolo Napoletano
Categories: cs.CV cs.AI cs.CL cs.DB cs.LG
Comments: This manuscript has been accepted for publication in Springer Neural
  Computing and Applications
\\ ( https://arxiv.org/abs/2306.11593 ,  3852kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09804
replaced with revised version Tue, 23 Sep 2025 08:04:53 GMT   (6217kb)

Title: Fix your downsampling ASAP! Be natively more robust via Aliasing and
  Spectral Artifact free Pooling
Authors: Julia Grabinski, Steffen Jung, Janis Keuper and Margret Keuper
Categories: cs.CV eess.IV
\\ ( https://arxiv.org/abs/2307.09804 ,  6217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06837
replaced with revised version Tue, 23 Sep 2025 15:57:07 GMT   (6878kb)

Title: Individualized Mapping of Aberrant Cortical Thickness via Stochastic
  Cortical Self-Reconstruction
Authors: Christian Wachinger, Dennis Hedderich, Melissa Thalhammer, Fabian
  Bongratz
Categories: cs.CV
Comments: Accepted for publication in Medical Image Analysis
\\ ( https://arxiv.org/abs/2403.06837 ,  6878kb)
------------------------------------------------------------------------------
\\
arXiv:2405.09806
replaced with revised version Tue, 23 Sep 2025 04:56:13 GMT   (32812kb)

Title: MediSyn: A Generalist Text-Guided Latent Diffusion Model For Diverse
  Medical Image Synthesis
Authors: Joseph Cho, Mrudang Mathur, Cyril Zakka, Dhamanpreet Kaur, Matthew
  Leipzig, Alex Dalal, Aravind Krishnan, Eubee Koo, Karen Wai, Cindy S. Zhao,
  Akshay Chaudhari, Matthew Duda, Ashley Choi, Ehsan Rahimy, Lyna Azzouz, Robyn
  Fong, Rohan Shad, William Hiesinger
Categories: cs.CV cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2405.09806 ,  32812kb)
------------------------------------------------------------------------------
\\
arXiv:2405.16116
replaced with revised version Tue, 23 Sep 2025 08:12:53 GMT   (2496kb)

Title: REACT: Real-time Efficiency and Accuracy Compromise for Tradeoffs in
  Scene Graph Generation
Authors: Ma\"elic Neau, Paulo E. Santos, Anne-Gwenn Bosser, C\'edric Buche and
  Akihiro Sugimoto
Categories: cs.CV
Comments: Accepted at the 2025 British Machine Vision Conference (BMVC)
\\ ( https://arxiv.org/abs/2405.16116 ,  2496kb)
------------------------------------------------------------------------------
\\
arXiv:2407.17354
replaced with revised version Tue, 23 Sep 2025 13:50:58 GMT   (17910kb)

Title: Deep Spherical Superpixels
Authors: R\'emi Giraud, Micha\"el Cl\'ement
Categories: cs.CV
\\ ( https://arxiv.org/abs/2407.17354 ,  17910kb)
------------------------------------------------------------------------------
\\
arXiv:2408.08182
replaced with revised version Tue, 23 Sep 2025 15:41:29 GMT   (2011kb)

Title: Your Turn: At Home Turning Angle Estimation for Parkinson's Disease
  Severity Assessment
Authors: Qiushuo Cheng, Catherine Morgan, Arindam Sikdar, Alessandro Masullo,
  Alan Whone, Majid Mirmehdi
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2408.08182 ,  2011kb)
------------------------------------------------------------------------------
\\
arXiv:2410.03592
replaced with revised version Tue, 23 Sep 2025 05:06:45 GMT   (34619kb)

Title: Variational Bayes Gaussian Splatting
Authors: Toon Van de Maele, Ozan Catal, Alexander Tschantz, Christopher L.
  Buckley, Tim Verbelen
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2410.03592 ,  34619kb)
------------------------------------------------------------------------------
\\
arXiv:2410.22629
replaced with revised version Tue, 23 Sep 2025 14:53:07 GMT   (24813kb)

Title: CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable
  Remote Sensing Semantic Segmentation
Authors: Ziyang Gong, Zhixiang Wei, Di Wang, Xiaoxing Hu, Xianzheng Ma,
  Hongruixuan Chen, Yuru Jia, Yupeng Deng, Zhenming Ji, Xiangwei Zhu, Xue Yang,
  Naoto Yokoya, Jing Zhang, Bo Du, Junchi Yan, Liangpei Zhang
Categories: cs.CV
Comments: The codes and models will be available at
  https://github.com/Cuzyoung/CrossEarth
\\ ( https://arxiv.org/abs/2410.22629 ,  24813kb)
------------------------------------------------------------------------------
\\
arXiv:2410.23262
replaced with revised version Tue, 23 Sep 2025 04:19:59 GMT   (7908kb)

Title: EMMA: End-to-End Multimodal Model for Autonomous Driving
Authors: Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji,
  Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, Yin Zhou,
  James Guo, Dragomir Anguelov, Mingxing Tan
Categories: cs.CV cs.AI cs.CL cs.LG cs.RO
Comments: Accepted by TMLR. Blog post:
  https://waymo.com/blog/2024/10/introducing-emma/
\\ ( https://arxiv.org/abs/2410.23262 ,  7908kb)
------------------------------------------------------------------------------
\\
arXiv:2411.06478
replaced with revised version Tue, 23 Sep 2025 09:52:02 GMT   (11247kb)

Title: Superpixel Segmentation: A Long-Lasting Ill-Posed Problem
Authors: R\'emi Giraud, Micha\"el Cl\'ement
Categories: cs.CV
\\ ( https://arxiv.org/abs/2411.06478 ,  11247kb)
------------------------------------------------------------------------------
\\
arXiv:2412.06028
replaced with revised version Tue, 23 Sep 2025 02:43:59 GMT   (44576kb)

Title: SparseDiT: Token Sparsification for Efficient Diffusion Transformer
Authors: Shuning Chang, Pichao Wang, Jiasheng Tang, Fan Wang, Yi Yang
Categories: cs.CV
Comments: 39th Conference on Neural Information Processing Systems (NeurIPS
  2025)
\\ ( https://arxiv.org/abs/2412.06028 ,  44576kb)
------------------------------------------------------------------------------
\\
arXiv:2412.14487
replaced with revised version Tue, 23 Sep 2025 17:03:44 GMT   (13245kb)

Title: Token Preference Optimization with Self-Calibrated Visual-Anchored
  Rewards for Hallucination Mitigation
Authors: Jihao Gu, Yingyao Wang, Meng Cao, Pi Bu, Jun Song, Yancheng He,
  Shilong Li, Bo Zheng
Categories: cs.CV
\\ ( https://arxiv.org/abs/2412.14487 ,  13245kb)
------------------------------------------------------------------------------
\\
arXiv:2412.20201
replaced with revised version Tue, 23 Sep 2025 10:10:52 GMT   (3376kb)

Title: Injecting Explainability and Lightweight Design into Weakly Supervised
  Video Anomaly Detection Systems
Authors: Wen-Dong Jiang, Chih-Yung Chang, Hsiang-Chuan Chang, Ji-Yuan Chen,
  Diptendu Sinha Roy
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2412.20201 ,  3376kb)
------------------------------------------------------------------------------
\\
arXiv:2501.01346
replaced with revised version Tue, 23 Sep 2025 16:40:27 GMT   (1280kb)

Title: Large Vision-Language Model Alignment and Misalignment: A Survey Through
  the Lens of Explainability
Authors: Dong Shu, Haiyan Zhao, Jingyu Hu, Weiru Liu, Ali Payani, Lu Cheng,
  Mengnan Du
Categories: cs.CV cs.CL
Comments: EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2501.01346 ,  1280kb)
------------------------------------------------------------------------------
\\
arXiv:2501.13707
replaced with revised version Tue, 23 Sep 2025 09:53:54 GMT   (6234kb)

Title: EventVL: Understand Event Streams via Multimodal Large Language Model
Authors: Pengteng Li and Yunfan Lu and Pinghao Song and Wuyang Li and Huizai
  Yao and Hui Xiong
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2501.13707 ,  6234kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11381
replaced with revised version Tue, 23 Sep 2025 12:39:26 GMT   (11105kb)

Title: Without Paired Labeled Data: End-to-End Self-Supervised Learning for
  Drone-view Geo-Localization
Authors: Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong, Guoqi Li
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2502.11381 ,  11105kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13407
replaced with revised version Tue, 23 Sep 2025 10:25:20 GMT   (7484kb)

Title: JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust
  Multi-Teacher Knowledge Distillation Framework
Authors: Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, and Yuantao
  Gu
Categories: cs.CV cs.AI
Comments: 17 pages, 9 figures
\\ ( https://arxiv.org/abs/2502.13407 ,  7484kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16972
replaced with revised version Tue, 23 Sep 2025 09:47:03 GMT   (2329kb)

Title: SCoT: Straight Consistent Trajectory for Pre-Trained Diffusion Model
  Distillations
Authors: Zhangkai Wu, Xuhui Fan, Hongyu Wu, Longbing Cao
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2502.16972 ,  2329kb)
------------------------------------------------------------------------------
\\
arXiv:2502.19200
replaced with revised version Tue, 23 Sep 2025 10:18:06 GMT   (0kb,I)

Title: HDM: Hybrid Diffusion Model for Unified Image Anomaly Detection
Authors: Zekang Weng, Jinjin Shi, Jinwei Wang, Zeming Han
Categories: cs.CV
Comments: The paper is withdrawn owing to issues found in the experimental
  results
\\ ( https://arxiv.org/abs/2502.19200 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2503.00046
replaced with revised version Tue, 23 Sep 2025 17:34:10 GMT   (24320kb)

Title: Leveraging Large Models to Evaluate Novel Content: A Case Study on
  Advertisement Creativity
Authors: Zhaoyi Joey Hou, Adriana Kovashka, Xiang Lorraine Li
Categories: cs.CV cs.AI
Comments: To Appear in EMNLP2025
\\ ( https://arxiv.org/abs/2503.00046 ,  24320kb)
------------------------------------------------------------------------------
\\
arXiv:2503.04130
replaced with revised version Mon, 22 Sep 2025 21:53:47 GMT   (15077kb)

Title: STORM: Token-Efficient Long Video Understanding for Multimodal LLMs
Authors: Jindong Jiang, Xiuyu Li, Zhijian Liu, Muyang Li, Guo Chen, Zhiqi Li,
  De-An Huang, Guilin Liu, Zhiding Yu, Kurt Keutzer, Sungjin Ahn, Jan Kautz,
  Hongxu Yin, Yao Lu, Song Han, Wonmin Byeon
Categories: cs.CV
\\ ( https://arxiv.org/abs/2503.04130 ,  15077kb)
------------------------------------------------------------------------------
\\
arXiv:2503.20429
replaced with revised version Sat, 20 Sep 2025 09:13:04 GMT   (10560kb)

Title: Latent Beam Diffusion Models for Generating Visual Sequences
Authors: Guilherme Fernandes, Vasco Ramos, Regev Cohen, Idan Szpektor, Jo\~ao
  Magalh\~aes
Categories: cs.CV
\\ ( https://arxiv.org/abs/2503.20429 ,  10560kb)
------------------------------------------------------------------------------
\\
arXiv:2504.08727
replaced with revised version Tue, 23 Sep 2025 17:04:48 GMT   (31637kb)

Title: Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections
  of Images
Authors: Boyang Deng, Songyou Peng, Kyle Genova, Gordon Wetzstein, Noah
  Snavely, Leonidas Guibas, Thomas Funkhouser
Categories: cs.CV cs.AI cs.CY
Comments: ICCV 2025, Project page: https://boyangdeng.com/visual-chronicles ,
  second and third listed authors have equal contributions
\\ ( https://arxiv.org/abs/2504.08727 ,  31637kb)
------------------------------------------------------------------------------
\\
arXiv:2504.10716
replaced with revised version Tue, 23 Sep 2025 17:30:33 GMT   (39645kb)

Title: SpinMeRound: Consistent Multi-View Identity Generation Using Diffusion
  Models
Authors: Stathis Galanakis, Alexandros Lattas, Stylianos Moschoglou, Bernhard
  Kainz, Stefanos Zafeiriou
Categories: cs.CV
\\ ( https://arxiv.org/abs/2504.10716 ,  39645kb)
------------------------------------------------------------------------------
\\
arXiv:2504.11366
replaced with revised version Tue, 23 Sep 2025 14:13:06 GMT   (4212kb)

Title: A Decade of Wheat Mapping for Lebanon
Authors: Hasan Wehbi, Hasan Nasrallah, Mohamad Hasan Zahweh, Zeinab Takach,
  Veera Ganesh Yalla, Ali J. Ghandour
Categories: cs.CV
\\ ( https://arxiv.org/abs/2504.11366 ,  4212kb)
------------------------------------------------------------------------------
\\
arXiv:2504.20690
replaced with revised version Tue, 23 Sep 2025 02:34:36 GMT   (18859kb)

Title: In-Context Edit: Enabling Instructional Image Editing with In-Context
  Generation in Large Scale Diffusion Transformer
Authors: Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, Yi Yang
Categories: cs.CV
Comments: Accepted by NeurIPS 2025, there will be future updates for camera
  ready version. Code: https://github.com/River-Zhang/ICEdit
\\ ( https://arxiv.org/abs/2504.20690 ,  18859kb)
------------------------------------------------------------------------------
\\
arXiv:2505.01571
replaced with revised version Tue, 23 Sep 2025 02:18:00 GMT   (26946kb)

Title: PainFormer: a Vision Foundation Model for Automatic Pain Assessment
Authors: Stefanos Gkikas, Raul Fernandez Rojas, Manolis Tsiknakis
Categories: cs.CV
Journal-ref: IEEE Transactions on Affective Computing; 2025
DOI: 10.1109/TAFFC.2025.3605475
\\ ( https://arxiv.org/abs/2505.01571 ,  26946kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05023
replaced with revised version Tue, 23 Sep 2025 03:36:32 GMT   (2474kb)

Title: Split Matching for Inductive Zero-shot Semantic Segmentation
Authors: Jialei Chen, Xu Zheng, Dongyue Li, Chong Yi, Seigo Ito, Danda Pani
  Paudel, Luc Van Gool, Hiroshi Murase, Daisuke Deguchi
Categories: cs.CV
Comments: Accepted by BMVC 2025
\\ ( https://arxiv.org/abs/2505.05023 ,  2474kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13817
replaced with revised version Tue, 23 Sep 2025 03:13:10 GMT   (10417kb)

Title: InstanceBEV: Unifying Instance and BEV Representation for 3D Panoptic
  Segmentation
Authors: Feng Li, Zhaoyue Wang, Enyuan Zhang, Mohammad Masum Billah, Yunduan
  Cui, Kun Xu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2505.13817 ,  10417kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14257
replaced with revised version Tue, 23 Sep 2025 02:23:21 GMT   (2057kb)

Title: Mitigating Hallucination in Large Vision-Language Models through
  Aligning Attention Distribution to Information Flow
Authors: Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng
Categories: cs.CV
Comments: Accepted to Findings of EMNLP 2025
\\ ( https://arxiv.org/abs/2505.14257 ,  2057kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14359
replaced with revised version Tue, 23 Sep 2025 07:16:18 GMT   (31336kb)

Title: Dual Data Alignment Makes AI-Generated Image Detector Easier
  Generalizable
Authors: Ruoxin Chen, Junwei Xi, Zhiyuan Yan, Ke-Yue Zhang, Shuang Wu, Jingyi
  Xie, Xu Chen, Lei Xu, Isabel Guan, Taiping Yao, Shouhong Ding
Categories: cs.CV
Comments: NeurIPS2025 Spotlight. 12 Pages, 9 figures
\\ ( https://arxiv.org/abs/2505.14359 ,  31336kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15173
replaced with revised version Tue, 23 Sep 2025 14:29:40 GMT   (13161kb)

Title: AvatarShield: Visual Reinforcement Learning for Human-Centric Synthetic
  Video Detection
Authors: Zhipei Xu, Xuanyu Zhang, Qing Huang, Xing Zhou, Jian Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2505.15173 ,  13161kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17423
replaced with revised version Mon, 22 Sep 2025 20:11:46 GMT   (1269kb)

Title: VIBE: Annotation-Free Video-to-Text Information Bottleneck Evaluation
  for TL;DR
Authors: Shenghui Chen, Po-han Li, Sandeep Chinchali, Ufuk Topcu
Categories: cs.CV cs.HC cs.IT math.IT
\\ ( https://arxiv.org/abs/2505.17423 ,  1269kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05312
replaced with revised version Tue, 23 Sep 2025 15:05:01 GMT   (7883kb)

Title: Do It Yourself: Learning Semantic Correspondence from Pseudo-Labels
Authors: Olaf D\"unkel, Thomas Wimmer, Christian Theobalt, Christian Rupprecht,
  Adam Kortylewski
Categories: cs.CV
Comments: ICCV 2025. Project page: https://genintel.github.io/DIY-SC
\\ ( https://arxiv.org/abs/2506.05312 ,  7883kb)
------------------------------------------------------------------------------
\\
arXiv:2506.07122
replaced with revised version Mon, 22 Sep 2025 21:11:14 GMT   (812kb)

Title: Image Segmentation and Classification of E-waste for Training Robots for
  Waste Segregation
Authors: Prakriti Tripathi
Categories: cs.CV cs.AI
Comments: 3 pages, 2 figures, submitted to 2025 5th International Conference on
  AI-ML-Systems (AIMLSystems)
ACM-class: I.2.10
\\ ( https://arxiv.org/abs/2506.07122 ,  812kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09534
replaced with revised version Tue, 23 Sep 2025 01:35:58 GMT   (14608kb)

Title: Gaussian Herding across Pens: An Optimal Transport Perspective on Global
  Gaussian Reduction for 3DGS
Authors: Tao Wang and Mengyu Li and Geduo Zeng and Cheng Meng and Qiong Zhang
Categories: cs.CV
Comments: 26 pages, 15 figures
ACM-class: I.4.5
\\ ( https://arxiv.org/abs/2506.09534 ,  14608kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11168
replaced with revised version Tue, 23 Sep 2025 08:06:09 GMT   (405kb)

Title: WaveFormer: A Lightweight Transformer Model for sEMG-based Gesture
  Recognition
Authors: Yanlong Chen, Mattia Orlandi, Pierangelo Maria Rapa, Simone Benatti,
  Luca Benini, and Yawei Li
Categories: cs.CV
Comments: 6 pages, 3 figures, accepted to IEEE EMBS Conference on Neural
  Engineering (NER) 2025 Code and data are available at
  https://github.com/ForeverBlue816/WaveFormer
\\ ( https://arxiv.org/abs/2506.11168 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2506.14765
replaced with revised version Tue, 23 Sep 2025 17:56:55 GMT   (6018kb)

Title: Earth Observation Foundation Model PhilEO: Pretraining on the MajorTOM
  and FastTOM Datasets
Authors: Nikolaos Dionelis, Riccardo Musto, Jente Bosmans, Simone Sarti,
  Giancarlo Paoletti, S\'ebastien Lef\`evre, Bertrand Le Saux, Nicolas
  Long\'ep\'e
Categories: cs.CV
Comments: 15 pages, 22 figures, 2 tables, 64 references
\\ ( https://arxiv.org/abs/2506.14765 ,  6018kb)
------------------------------------------------------------------------------
\\
arXiv:2506.16742
replaced with revised version Mon, 22 Sep 2025 22:35:43 GMT   (911kb)

Title: Uncertainty-Aware Information Pursuit for Interpretable and Reliable
  Medical Image Analysis
Authors: Md Nahiduzzaman, Steven Korevaar, Zongyuan Ge, Feng Xia, Alireza
  Bab-Hadiashar, Ruwan Tennakoon
Categories: cs.CV
\\ ( https://arxiv.org/abs/2506.16742 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21731
replaced with revised version Mon, 22 Sep 2025 22:05:16 GMT   (5428kb)

Title: Exploring Image Generation via Mutually Exclusive Probability Spaces and
  Local Correlation Hypothesis
Authors: Chenqiu Zhao, Anup Basu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2506.21731 ,  5428kb)
------------------------------------------------------------------------------
\\
arXiv:2507.07838
replaced with revised version Tue, 23 Sep 2025 14:58:35 GMT   (4739kb)

Title: 3D-ADAM: A Dataset for 3D Anomaly Detection in Additive Manufacturing
Authors: Paul McHard, Florent P. Audonnet, Oliver Summerell, Sebastian Andraos,
  Paul Henderson, Gerardo Aragon-Camarasa
Categories: cs.CV
\\ ( https://arxiv.org/abs/2507.07838 ,  4739kb)
------------------------------------------------------------------------------
\\
arXiv:2507.15690
replaced with revised version Tue, 23 Sep 2025 09:25:42 GMT   (14663kb)

Title: DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian
  Splatting
Authors: Hung Nguyen and Runfa Li and An Le and Truong Nguyen
Categories: cs.CV eess.IV eess.SP
Comments: Accepted to VCIP 2025
\\ ( https://arxiv.org/abs/2507.15690 ,  14663kb)
------------------------------------------------------------------------------
\\
arXiv:2507.18237
replaced with revised version Tue, 23 Sep 2025 07:27:41 GMT   (8422kb)

Title: DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in
  Collaborative Perception
Authors: Chengchang Tian, Jianwei Ma, Yan Huang, Zhanye Chen, Honghao Wei, Hui
  Zhang, Wei Hong
Categories: cs.CV
Comments: ICCV 2025, accepted as poster. 22 pages including supplementary
  materials
\\ ( https://arxiv.org/abs/2507.18237 ,  8422kb)
------------------------------------------------------------------------------
\\
arXiv:2508.03485
replaced with revised version Tue, 23 Sep 2025 05:30:13 GMT   (44977kb)

Title: LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion
  Transformers for Image and Video Generation
Authors: Lianwei Yang, Haokun Lin, Tianchen Zhao, Yichen Wu, Hongyu Zhu, Ruiqi
  Xie, Zhenan Sun, Yu Wang, Qingyi Gu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.03485 ,  44977kb)
------------------------------------------------------------------------------
\\
arXiv:2509.03277
replaced with revised version Tue, 23 Sep 2025 17:13:41 GMT   (3824kb)

Title: PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly
  Detection
Authors: Qihang Zhou, Shibo He, Jiangtao Yan, Wenchao Meng, Jiming Chen
Categories: cs.CV
Comments: Submitted to TPAMI
\\ ( https://arxiv.org/abs/2509.03277 ,  3824kb)
------------------------------------------------------------------------------
\\
arXiv:2509.04545
replaced with revised version Tue, 23 Sep 2025 08:56:30 GMT   (13929kb)

Title: PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via
  Chain-of-Thought Prompt Rewriting
Authors: Linqing Wang, Ximing Xing, Yiji Cheng, Zhiyuan Zhao, Donghao Li,
  Tiankai Hang, Jiale Tao, Qixun Wang, Ruihuang Li, Comi Chen, Xin Li, Mingrui
  Wu, Xinchi Deng, Shuyang Gu, Chunyu Wang, Qinglin Lu
Categories: cs.CV
Comments: Technical Report. Project Page:
  https://hunyuan-promptenhancer.github.io/
\\ ( https://arxiv.org/abs/2509.04545 ,  13929kb)
------------------------------------------------------------------------------
\\
arXiv:2509.06035
replaced with revised version Tue, 23 Sep 2025 07:02:16 GMT   (19435kb)

Title: TinyDef-DETR: A DETR-based Framework for Defect Detection in
  Transmission Lines from UAV Imagery
Authors: Feng Shen, Jiaming Cui, Shuai Zhou, Wenqiang Li and Ruifeng Qin
Categories: cs.CV cs.AI cs.CE
\\ ( https://arxiv.org/abs/2509.06035 ,  19435kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07021
replaced with revised version Tue, 23 Sep 2025 17:01:06 GMT   (8925kb)

Title: MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians
  and Unified Pruning
Authors: Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan
  Liu, Yujing Sun, Wenping Wang
Categories: cs.CV cs.AI
Comments: 20 pages, 8 figures. Project page at https://megs-2.github.io/
\\ ( https://arxiv.org/abs/2509.07021 ,  8925kb)
------------------------------------------------------------------------------
\\
arXiv:2509.08422
replaced with revised version Tue, 23 Sep 2025 07:40:50 GMT   (21226kb)

Title: LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations
Authors: Payal Varshney, Adriano Lucieri, Christoph Balada, Sheraz Ahmed,
  Andreas Dengel
Categories: cs.CV cs.LG
Comments: 30 pages
\\ ( https://arxiv.org/abs/2509.08422 ,  21226kb)
------------------------------------------------------------------------------
\\
arXiv:2509.08805
replaced with revised version Tue, 23 Sep 2025 09:34:27 GMT   (9514kb)

Title: Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching
Authors: Matthieu Vilain, R\'emi Giraud, Yannick Berthoumieu, Guillaume
  Bourmaud
Categories: cs.CV
Journal-ref: Presented at ICIP 2025
\\ ( https://arxiv.org/abs/2509.08805 ,  9514kb)
------------------------------------------------------------------------------
\\
arXiv:2509.08908
replaced with revised version Mon, 22 Sep 2025 21:01:29 GMT   (2411kb)

Title: Diffusion-Based Action Recognition Generalizes to Untrained Domains
Authors: Rogerio Guimaraes, Frank Xiao, Pietro Perona, Markus Marks
Categories: cs.CV
Comments: Project page: https://www.vision.caltech.edu/actiondiff. Code:
  https://github.com/frankyaoxiao/ActionDiff
\\ ( https://arxiv.org/abs/2509.08908 ,  2411kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10026
replaced with revised version Tue, 23 Sep 2025 02:37:42 GMT   (4186kb)

Title: LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization
  for Real-World Multilingual VQA
Authors: Jing Huang, Zhiya Tan, Shutao Gong, Fanwei Zeng, Joey Tianyi Zhou,
  Jianshu Li
Categories: cs.CV
Comments: 12 Pages, 12 Figures, 2 Tables
\\ ( https://arxiv.org/abs/2509.10026 ,  4186kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12197
replaced with revised version Tue, 23 Sep 2025 16:13:36 GMT   (17594kb)

Title: 3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review
Authors: Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet
Categories: cs.CV
Comments: under review
\\ ( https://arxiv.org/abs/2509.12197 ,  17594kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14001
replaced with revised version Tue, 23 Sep 2025 07:52:31 GMT   (9521kb)

Title: MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment
Authors: Elena Camuffo, Francesco Barbato, Mete Ozay, Simone Milani, Umberto
  Michieli
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2509.14001 ,  9521kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15235
replaced with revised version Tue, 23 Sep 2025 07:13:20 GMT   (2497kb)

Title: ViSpec: Accelerating Vision-Language Models with Vision-Aware
  Speculative Decoding
Authors: Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen
Categories: cs.CV cs.CL
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2509.15235 ,  2497kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16421
replaced with revised version Tue, 23 Sep 2025 00:52:32 GMT   (2137kb)

Title: AHA - Predicting What Matters Next: Online Highlight Detection Without
  Looking Ahead
Authors: Aiden Chang, Celso De Melo, Stephanie M. Lukin
Categories: cs.CV cs.AI
Comments: Accepted at NeurIPS 2025, 32 pages, 5 figures
\\ ( https://arxiv.org/abs/2509.16421 ,  2137kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16423
replaced with revised version Tue, 23 Sep 2025 00:47:20 GMT   (37823kb)

Title: 3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction
Authors: Maria Taktasheva, Lily Goli, Alessandro Fiorini, Zhen Li, Daniel
  Rebain, Andrea Tagliasacchi
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.16423 ,  37823kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16727
replaced with revised version Tue, 23 Sep 2025 00:50:14 GMT   (1097kb)

Title: Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain
  Assessment
Authors: Xin Lei Lin, Soroush Mehraban, Abhishek Moturu, Babak Taati
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2509.16727 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16738
replaced with revised version Tue, 23 Sep 2025 07:34:23 GMT   (2879kb)

Title: Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental
  Learning
Authors: Kai Jiang, Zhengyan Shi, Dell Zhang, Hongyuan Zhang and Xuelong Li
Categories: cs.CV cs.LG
Comments: Accepted by NeurIPS 2025. Code is available at
  https://github.com/ASCIIJK/MiN-NeurIPS2025
\\ ( https://arxiv.org/abs/2509.16738 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16832
replaced with revised version Tue, 23 Sep 2025 08:02:05 GMT   (28201kb)

Title: L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR
  Point Clouds and Semantic 3D City Models
Authors: Ziyang Xu, Benedikt Schwab, Yihui Yang, Thomas H. Kolbe, Christoph
  Holst
Categories: cs.CV cs.RO eess.IV
Comments: Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing
\\ ( https://arxiv.org/abs/2509.16832 ,  28201kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16886
replaced with revised version Tue, 23 Sep 2025 09:14:07 GMT   (529kb)

Title: SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in
  Medical Segmentation
Authors: Yingzhen Hu, Yiheng Zhong, Ruobing Li, Yingxue Su, Jiabao An, Feilong
  Tang, Jionglong Su, Imran Razzak
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.16886 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16888
replaced with revised version Tue, 23 Sep 2025 03:06:34 GMT   (5821kb)

Title: Rethinking Evaluation of Infrared Small Target Detection
Authors: Youwei Pang, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu, Georges El Fakhri,
  Xiaofeng Liu, Shijian Lu
Categories: cs.CV
Comments: NeurIPS 2025; Evaluation Toolkit:
  https://github.com/lartpang/PyIRSTDMetrics; Correct a few typos
\\ ( https://arxiv.org/abs/2509.16888 ,  5821kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16968
replaced with revised version Tue, 23 Sep 2025 16:17:58 GMT   (27359kb)

Title: Penalizing Boundary Activation for Object Completeness in Diffusion
  Models
Authors: Haoyang Xu, Tianhao Zhao, Sibei Yang and Yutian Lin
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.16968 ,  27359kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17083
replaced with revised version Tue, 23 Sep 2025 03:08:14 GMT   (6618kb)

Title: HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel
  View Synthesis
Authors: Zipeng Wang, Dan Xu
Categories: cs.CV
Comments: Accepted at NeurIPS 2025
\\ ( https://arxiv.org/abs/2509.17083 ,  6618kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17429
replaced with revised version Tue, 23 Sep 2025 06:52:04 GMT   (8409kb)

Title: Multi-scale Temporal Prediction via Incremental Generation and
  Multi-agent Collaboration
Authors: Zhitao Zeng and Guojian Yuan and Junyuan Mao and Yuxuan Wang and
  Xiaoshuang Jia and Yueming Jin
Categories: cs.CV
Comments: 20 pages, 6 figures
MSC-class: 68T45
ACM-class: I.2.10
Journal-ref: NeurIPS 2025
\\ ( https://arxiv.org/abs/2509.17429 ,  8409kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17430
replaced with revised version Tue, 23 Sep 2025 03:58:25 GMT   (13338kb)

Title: EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian
  Splats from a Mobile Device
Authors: Gunjan Chhablani, Xiaomeng Ye, Muhammad Zubair Irshad, Zsolt Kira
Categories: cs.CV cs.RO
Comments: 16 pages, 18 figures, paper accepted at ICCV, 2025
\\ ( https://arxiv.org/abs/2509.17430 ,  13338kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17431
replaced with revised version Tue, 23 Sep 2025 05:56:37 GMT   (26627kb)

Title: Hierarchical Neural Semantic Representation for 3D Semantic
  Correspondence
Authors: Keyu Du, Jingyu Hu, Haipeng Li, Hao Xu, Haibing Huang, Chi-Wing Fu,
  Shuaicheng Liu
Categories: cs.CV
Comments: This paper is accepted by Siggraph Asia 2025 conference track
\\ ( https://arxiv.org/abs/2509.17431 ,  26627kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17492
replaced with revised version Tue, 23 Sep 2025 01:40:38 GMT   (1391kb)

Title: Multimodal Medical Image Classification via Synergistic Learning
  Pre-training
Authors: Qinghua Lin, Guang-Hai Liu, Zuoyong Li, Yang Li, Yuting Jiang and
  Xiang Wu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2509.17492 ,  1391kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17537
replaced with revised version Tue, 23 Sep 2025 04:04:41 GMT   (514kb)

Title: SimToken: A Simple Baseline for Referring Audio-Visual Segmentation
Authors: Dian Jin, Yanghao Zhou, Jinxing Zhou, Jiaqi Ma, Ruohao Guo, Dan Guo
Categories: cs.CV
Comments: Project page: https://github.com/DianJin-HFUT/SimToken
\\ ( https://arxiv.org/abs/2509.17537 ,  514kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17562
replaced with revised version Tue, 23 Sep 2025 04:33:22 GMT   (420kb)

Title: Visual Instruction Pretraining for Domain-Specific Foundation Models
Authors: Yuxuan Li, Yicheng Zhang, Wenhao Tang, Yimian Dai, Ming-Ming Cheng,
  Xiang Li, Jian Yang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.17562 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17654
replaced with revised version Tue, 23 Sep 2025 06:05:25 GMT   (3017kb)

Title: Clothing agnostic Pre-inpainting Virtual Try-ON
Authors: Sehyun Kim, Hye Jun Lee, Jiwoo Lee, Taemin Lee
Categories: cs.CV
Comments: Github : https://github.com/DevChoco/CAP-VTON
\\ ( https://arxiv.org/abs/2509.17654 ,  3017kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17660
replaced with revised version Tue, 23 Sep 2025 05:41:34 GMT   (14178kb)

Title: Development and validation of an AI foundation model for endoscopic
  diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep
  learning study
Authors: Yikun Ma, Bo Li, Ying Chen, Zijie Yue, Shuchang Xu, Jingyao Li, Lei
  Ma, Liang Zhong, Duowu Zou, Leiming Xu, Yunshi Zhong, Xiaobo Li, Weiqun Ding,
  Minmin Zhang, Dongli He, Zhenghong Li, Ye Chen, Ye Zhao, Jialong Zhuo,
  Xiaofen Wu, Lisha Yi, Miaojing Shi, Huihui Sun
Categories: cs.CV
Comments: Accepted to eClinicalMedicine, Part of The Lancet Discovery Science
\\ ( https://arxiv.org/abs/2509.17660 ,  14178kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17743
replaced with revised version Tue, 23 Sep 2025 07:31:00 GMT   (4681kb)

Title: Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA
Authors: Chenglin Li, Feng Han, Feng Tao, Ruilin Li, Qianglong Chen, Jingqi
  Tong, Yin Zhang, Jiaqi Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.17743 ,  4681kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17993
replaced with revised version Tue, 23 Sep 2025 13:36:08 GMT   (24589kb)

Title: StableGuard: Towards Unified Copyright Protection and Tamper
  Localization in Latent Diffusion Models
Authors: Haoxin Yang, Bangzhen Liu, Xuemiao Xu, Cheng Xu, Yuyang Yu, Zikai
  Huang, Yi Wang, Shengfeng He
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2509.17993 ,  24589kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12820
replaced with revised version Tue, 23 Sep 2025 08:40:27 GMT   (2426kb)

Title: Atomic Smart Contract Interoperability with High Efficiency via
  Cross-Chain Integrated Execution
Authors: Chaoyue Yin, Mingzhe Li, Jin Zhang, You Lin, Qingsong Wei, Siow Mong
  Rick Goh
Categories: cs.DC
Comments: Accepted by IEEE Transactions on Parallel and Distributed Systems
\\ ( https://arxiv.org/abs/2502.12820 ,  2426kb)
------------------------------------------------------------------------------
\\
arXiv:2503.21476
replaced with revised version Tue, 23 Sep 2025 03:56:58 GMT   (7012kb)

Title: Robust DNN Partitioning and Resource Allocation Under Uncertain
  Inference Time
Authors: Zhaojun Nan, Yunchu Han, Sheng Zhou, and Zhisheng Niu
Categories: cs.DC cs.IT cs.LG math.IT
\\ ( https://arxiv.org/abs/2503.21476 ,  7012kb)
------------------------------------------------------------------------------
\\
arXiv:2505.01616
replaced with revised version Tue, 23 Sep 2025 14:01:26 GMT   (1847kb)

Title: Phantora: Maximizing Code Reuse in Simulation-based Machine Learning
  System Performance Estimation
Authors: Jianxing Qin, Jingrong Chen, Xinhao Kong, Yongji Wu, Tianjun Yuan,
  Liang Luo, Zhaodong Wang, Ying Zhang, Tingjun Chen, Alvin R. Lebeck, Danyang
  Zhuo
Categories: cs.DC cs.LG cs.PF
\\ ( https://arxiv.org/abs/2505.01616 ,  1847kb)
------------------------------------------------------------------------------
\\
arXiv:2506.01979
replaced with revised version Tue, 23 Sep 2025 05:29:24 GMT   (7988kb)

Title: Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch
  Parallelism
Authors: Yuhao Shen, Junyi Shen, Quan Kong, Tianyu Liu, Yao Lu and Cong Wang
Categories: cs.DC cs.AI
\\ ( https://arxiv.org/abs/2506.01979 ,  7988kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04013
replaced with revised version Mon, 22 Sep 2025 20:21:47 GMT   (1900kb)

Title: High-Performance Statistical Computing (HPSC): Challenges,
  Opportunities, and Future Directions
Authors: Sameh Abdulah, Mary Lai O. Salvana, Ying Sun, David E. Keyes, and Marc
  G. Genton
Categories: cs.DC
\\ ( https://arxiv.org/abs/2508.04013 ,  1900kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11697
replaced with revised version Tue, 23 Sep 2025 02:31:43 GMT   (1176kb)

Title: Towards the Distributed Large-scale k-NN Graph Construction by Graph
  Merge
Authors: Cheng Zhang and Wan-Lei Zhao and Shihai Xiao and Jiajie Yao and
  Xuecang Zhang
Categories: cs.DC
Comments: 16 pages, 17 figures
\\ ( https://arxiv.org/abs/2509.11697 ,  1176kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13978
replaced with revised version Tue, 23 Sep 2025 13:31:18 GMT   (846kb)

Title: LLM Agents for Interactive Workflow Provenance: Reference Architecture
  and Evaluation Methodology
Authors: Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal
  Gueroudji, Woong Shin, Prasanna Balaprakash, Rafael Ferreira da Silva
Categories: cs.DC cs.AI cs.DB
Comments: Paper accepted in the proceedings of the Supercomputing Conference
  (SC). Cite it as Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal
  Gueroudji, Woong Shin, Prasanna Balaprakash, and Rafael Ferreira da Silva.
  LLM Agents for Interactive Workflow Provenance: Reference Architecture and
  Evaluation Methodology. In WORKS at the ACM/IEEE International Conference on
  Supercomputing, 2025
MSC-class: 68M14, 68M20, 68T07
ACM-class: C.2.4; D.1.3; I.2.0
DOI: 10.1145/3731599.3767582
\\ ( https://arxiv.org/abs/2509.13978 ,  846kb)
------------------------------------------------------------------------------
\\
arXiv:2008.12846
replaced with revised version Tue, 23 Sep 2025 01:16:37 GMT   (1188kb)

Title: Formal Methods for An Iterated Volunteer's Dilemma
Authors: Jacob Dineen, A S M Ahsan-Ul Haque, Matthew Bielskas
Categories: cs.MA cs.GT
Comments: 9 pages, 4 figures, 5 tables
DOI: 10.1007/978-3-030-80387-2_8
\\ ( https://arxiv.org/abs/2008.12846 ,  1188kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13841
replaced with revised version Mon, 22 Sep 2025 19:43:57 GMT   (2411kb)

Title: Is Pre-training Truly Better Than Meta-Learning?
Authors: Brando Miranda, Patrick Yu, Saumya Goyal, Yu-Xiong Wang, Sanmi Koyejo
Categories: cs.LG cs.AI cs.CL cs.CV
Journal-ref: Proceedings of the 40th International Conference on Machine
  Learning 2023 DMLR Workshop
\\ ( https://arxiv.org/abs/2306.13841 ,  2411kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05652
replaced with revised version Mon, 22 Sep 2025 21:20:37 GMT   (22963kb)

Title: "What is Different Between These Datasets?" A Framework for Explaining
  Data Distribution Shifts
Authors: Varun Babbar, Zhicheng Guo, Cynthia Rudin
Categories: cs.LG cs.AI
Journal-ref: J. Mach. Learn. Res. 26(180):1-64, 2025
\\ ( https://arxiv.org/abs/2403.05652 ,  22963kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07560
replaced with revised version Tue, 23 Sep 2025 15:28:51 GMT   (3235kb)

Title: Socially Pertinent Robots in Gerontological Healthcare
Authors: Xavier Alameda-Pineda and Angus Addlesee and Daniel Hern\'andez
  Garc\'ia and Chris Reinke and Soraya Arias and Federica Arrigoni and Alex
  Auternaud and Lauriane Blavette and Cigdem Beyan and Luis Gomez Camara and
  Ohad Cohen and Alessandro Conti and S\'ebastien Dacunha and Christian Dondrup
  and Yoav Ellinson and Francesco Ferro and Sharon Gannot and Florian Gras and
  Nancie Gunson and Radu Horaud and Moreno D'Inc\`a and Imad Kimouche and
  S\'everin Lemaignan and Oliver Lemon and Cyril Liotard and Luca Marchionni
  and Mordehay Moradi and Tomas Pajdla and Maribel Pino and Michal Polic and
  Matthieu Py and Ariel Rado and Bin Ren and Elisa Ricci and Anne-Sophie Rigaud
  and Paolo Rota and Marta Romeo and Nicu Sebe and Weronika Siei\'nska and
  Pinchas Tandeitnik and Francesco Tonini and Nicolas Turro and Timoth\'ee
  Wintz and Yanchao Yu
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2404.07560 ,  3235kb)
------------------------------------------------------------------------------
\\
arXiv:2409.19375
replaced with revised version Tue, 23 Sep 2025 01:58:07 GMT   (3222kb)

Title: DOTA: Distributional Test-Time Adaptation of Vision-Language Models
Authors: Zongbo Han, Jialong Yang, Guangyu Wang, Junfan Li, Qianli Xu, Mike
  Zheng Shou, Changqing Zhang
Categories: cs.LG cs.AI cs.CL cs.CV cs.HC
Comments: Accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2409.19375 ,  3222kb)
------------------------------------------------------------------------------
\\
arXiv:2412.16905
replaced with revised version Tue, 23 Sep 2025 14:27:14 GMT   (713kb)

Title: Backdoor Attack with Invisible Triggers Based on Model Architecture
  Modification
Authors: Yuan Ma, Jiankang Wei, Yilun Lyu, Kehao Chen and Jingtong Huang
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2412.16905 ,  713kb)
------------------------------------------------------------------------------
\\
arXiv:2412.19191 (*cross-listing*)
replaced with revised version Tue, 23 Sep 2025 12:55:03 GMT   (7084kb)

Title: Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence
  Understanding Capability of Large Language Models
Authors: Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao
  Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong,
  Wanli Ouyang, Dongzhan Zhou, Peng Ye
Categories: q-bio.BM cs.AI cs.LG
Comments: EMNLP 2025 findings
\\ ( https://arxiv.org/abs/2412.19191 ,  7084kb)
------------------------------------------------------------------------------
\\
arXiv:2502.06106
replaced with revised version Tue, 23 Sep 2025 16:07:40 GMT   (438kb)

Title: Fine-Tuning is Subgraph Search: A New Lens on Learning Dynamics
Authors: Yueyan Li, Wenhao Gao, Caixia Yuan, Xiaojie Wang
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2502.06106 ,  438kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12623
replaced with revised version Tue, 23 Sep 2025 15:01:37 GMT   (9558kb)

Title: DeepResonance: Enhancing Multimodal Music Understanding via
  Music-centric Multi-way Instruction Tuning
Authors: Zhuoyuan Mao, Mengjie Zhao, Qiyu Wu, Hiromi Wakaki, Yuki Mitsufuji
Categories: cs.SD cs.AI cs.CL cs.MM eess.AS
Comments: Accepted to EMNLP 2025 main conference
\\ ( https://arxiv.org/abs/2502.12623 ,  9558kb)
------------------------------------------------------------------------------
\\
arXiv:2502.14334 (*cross-listing*)
replaced with revised version Tue, 23 Sep 2025 08:07:30 GMT   (30kb)

Title: Purest Quantum State Identification
Authors: Yingqi Yu, Honglin Chen, Jun Wu, Wei Xie, Xiangyang Li
Categories: quant-ph cs.AI
\\ ( https://arxiv.org/abs/2502.14334 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2502.15855 (*cross-listing*)
replaced with revised version Tue, 23 Sep 2025 12:47:28 GMT   (1084kb)

Title: THFlow: A Temporally Hierarchical Flow Matching Framework for 3D Peptide
  Design
Authors: Dengdeng Huang and Shikui Tu
Categories: q-bio.QM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2502.15855 ,  1084kb)
------------------------------------------------------------------------------
\\
arXiv:2502.17928
replaced with revised version Tue, 23 Sep 2025 02:50:13 GMT   (499kb)

Title: Structure-prior Informed Diffusion Model for Graph Source Localization
  with Limited Data
Authors: Hongyi Chen, Jingtao Ding, Xiaojun Liang, Yong Li, Xiao-Ping Zhang
Categories: cs.SI cs.AI cs.LG
Comments: CIKM 2025
DOI: 10.1145/3746252.3761146
\\ ( https://arxiv.org/abs/2502.17928 ,  499kb)
------------------------------------------------------------------------------
\\
arXiv:2503.02495
replaced with revised version Tue, 23 Sep 2025 07:09:46 GMT   (3057kb)

Title: Union of Experts: Adapting Hierarchical Routing to Equivalently
  Decomposed Transformer
Authors: Yujiao Yang, Jing Lian, Linhui Li
Categories: cs.LG cs.AI cs.CL
MSC-class: 68T07
ACM-class: I.5.1; I.2.0
\\ ( https://arxiv.org/abs/2503.02495 ,  3057kb)
------------------------------------------------------------------------------
\\
arXiv:2503.05613
replaced with revised version Tue, 23 Sep 2025 16:48:10 GMT   (346kb)

Title: A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of
  Large Language Models
Authors: Dong Shu, Xuansheng Wu, Haiyan Zhao, Daking Rai, Ziyu Yao, Ninghao
  Liu, Mengnan Du
Categories: cs.LG cs.AI cs.CL
Comments: EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2503.05613 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2505.08080
replaced with revised version Tue, 23 Sep 2025 16:43:51 GMT   (203kb)

Title: Beyond Input Activations: Identifying Influential Latents by Gradient
  Sparse Autoencoders
Authors: Dong Shu, Xuansheng Wu, Haiyan Zhao, Mengnan Du, Ninghao Liu
Categories: cs.LG cs.AI cs.CL
Comments: EMNLP 2025 Main
\\ ( https://arxiv.org/abs/2505.08080 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2505.09558 (*cross-listing*)
replaced with revised version Tue, 23 Sep 2025 09:57:50 GMT   (10691kb)

Title: WavReward: Spoken Dialogue Models With Generalist Reward Evaluators
Authors: Shengpeng Ji, Tianle Liang, Yangzhuo Li, Jialong Zuo, Minghui Fang,
  Jinzheng He, Yifu Chen, Zhengqing Liu, Ziyue Jiang, Xize Cheng, Siqi Zheng,
  Jin Xu, Junyang Lin, Zhou Zhao
Categories: eess.AS cs.AI cs.LG cs.MM cs.SD
\\ ( https://arxiv.org/abs/2505.09558 ,  10691kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19164
replaced with revised version Mon, 22 Sep 2025 22:40:08 GMT   (1496kb)

Title: BroadGen: A Framework for Generating Effective and Efficient Advertiser
  Broad Match Keyphrase Recommendations
Authors: Ashirbad Mishra, Jinyu Zhao, Soumik Dey, Hansi Wu, Binbin Li, Kamesh
  Madduri
Categories: cs.IR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2505.19164 ,  1496kb)
------------------------------------------------------------------------------
\\
arXiv:2506.00329
replaced with revised version Mon, 22 Sep 2025 19:20:33 GMT   (21661kb)

Title: Foresight: Adaptive Layer Reuse for Accelerated and High-Quality
  Text-to-Video Generation
Authors: Muhammad Adnan, Nithesh Kurella, Akhil Arunkumar, Prashant J. Nair
Categories: cs.LG cs.AI cs.CV
Comments: Accepted at the 39th Conference on Neural Information Processing
  Systems (NeurIPS), 2025
\\ ( https://arxiv.org/abs/2506.00329 ,  21661kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09532
replaced with revised version Tue, 23 Sep 2025 05:57:54 GMT   (881kb)

Title: Athena: Enhancing Multimodal Reasoning with Data-efficient Process
  Reward Models
Authors: Shuai Wang, Zhenhua Liu, Jiaheng Wei, Xuanwu Yin, Dong Li, Emad
  Barsoum
Categories: cs.LG cs.AI cs.CL cs.CV
\\ ( https://arxiv.org/abs/2506.09532 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2507.05829
replaced with revised version Tue, 23 Sep 2025 05:51:13 GMT   (16903kb)

Title: Intra-DP: A High Performance Collaborative Inference System for Mobile
  Edge Computing
Authors: Zekai Sun, Xiuxian Guan, Zheng Lin, Zihan Fang, Xiangming Cai, Zhe
  Chen, Fangming Liu, Heming Cui, Jie Xiong, Wei Ni, Chau Yuen
Categories: cs.NI cs.AI cs.LG
Comments: 14 pages, 19 figures
\\ ( https://arxiv.org/abs/2507.05829 ,  16903kb)
------------------------------------------------------------------------------
\\
arXiv:2507.09790
replaced with revised version Tue, 23 Sep 2025 09:52:43 GMT   (309kb)

Title: Prompting for Performance: Exploring LLMs for Configuring Software
Authors: Helge Spieker, Th\'eo Matricon, Nassim Belmecheri, J{\o}rn Eirik
  Betten, Gauthier Le Bartz Lyan, Heraldo Borges, Quentin Mazouni, Dennis
  Gross, Arnaud Gotlieb, Mathieu Acher
Categories: cs.SE cs.AI cs.PF
Comments: ICTAI 2025
\\ ( https://arxiv.org/abs/2507.09790 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2507.12642
replaced with revised version Tue, 23 Sep 2025 16:36:24 GMT   (1715kb)

Title: QSpark: Towards Reliable Qiskit Code Generation
Authors: Kiana Kheiri and Aamna Aamir and Andriy Miranskyy and Chen Ding
Categories: cs.SE cs.AI quant-ph
Journal-ref: In Proceedings of AIQxQIA 2025: International Workshop on AI for
  Quantum and Quantum for AI | co-located with ECAI 2025, Bologna, Italy
\\ ( https://arxiv.org/abs/2507.12642 ,  1715kb)
------------------------------------------------------------------------------
\\
arXiv:2507.17291
replaced with revised version Tue, 23 Sep 2025 16:15:07 GMT   (37kb)

Title: Integrating Belief Domains into Probabilistic Logic Programs
Authors: Damiano Azzolini, Fabrizio Riguzzi, Theresa Swift
Categories: cs.LO cs.AI
Comments: Under consideration in Theory and Practice of Logic Programming
  (TPLP)
DOI: 10.1017/S1471068425100161
\\ ( https://arxiv.org/abs/2507.17291 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04488 (*cross-listing*)
replaced with revised version Tue, 23 Sep 2025 03:27:39 GMT   (17kb)

Title: Benchmarking Quantum and Classical Sequential Models for Urban
  Telecommunication Forecasting
Authors: Chi-Sheng Chen and Samuel Yen-Chi Chen and Yun-Cheng Tsai
Categories: quant-ph cs.AI
\\ ( https://arxiv.org/abs/2508.04488 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2508.12104
replaced with revised version Tue, 23 Sep 2025 17:35:19 GMT   (1157kb)

Title: Generative Medical Event Models Improve with Scale
Authors: Shane Waxler, Paul Blazek, Davis White, Daniel Sneider, Kevin Chung,
  Mani Nagarathnam, Patrick Williams, Hank Voeller, Karen Wong, Matthew
  Swanhorst, Sheng Zhang, Naoto Usuyama, Cliff Wong, Tristan Naumann, Hoifung
  Poon, Andrew Loza, Daniella Meeker, Seth Hain, and Rahul Shah
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2508.12104 ,  1157kb)
------------------------------------------------------------------------------
\\
arXiv:2508.13057
replaced with revised version Tue, 23 Sep 2025 15:43:58 GMT   (2805kb)

Title: Hierarchical Evaluation Function: A Multi-Metric Approach for Optimizing
  Demand Forecasting Models
Authors: Adolfo Gonz\'alez and V\'ictor Parada
Categories: cs.LG cs.AI cs.PF
Comments: 31 pages, 15 figures, 25 tables. Submitted as a preprint. The
  manuscript introduces the Hierarchical Evaluation Function, a multi-metric
  framework for optimizing demand forecasting models under high uncertainty.
  Includes extensive experimental validation using real-world datasets and a
  comparative analysis against classical and modern methods
MSC-class: 62M10, 90C59, 68T05
ACM-class: I.2.6; I.5.1; I.5.2; I.5.4; G.1.6
\\ ( https://arxiv.org/abs/2508.13057 ,  2805kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16313
replaced with revised version Tue, 23 Sep 2025 02:08:36 GMT   (11313kb)

Title: Retrieval Enhanced Feedback via In-context Neural Error-book
Authors: Jongyeop Hyun, Bumsoo Kim
Categories: cs.LG cs.AI cs.CL
Comments: Accepted at EMNLP 2025 main conference
\\ ( https://arxiv.org/abs/2508.16313 ,  11313kb)
------------------------------------------------------------------------------
\\
arXiv:2508.17681
replaced with revised version Tue, 23 Sep 2025 15:40:35 GMT   (27kb)

Title: Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative
  Scientific Discovery
Authors: Robert Yang
Categories: cs.LG cs.AI
Comments: 6 pages. Accepted to NeurIPS 2025 AI4Science Workshop
\\ ( https://arxiv.org/abs/2508.17681 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2508.19843
replaced with revised version Tue, 23 Sep 2025 07:31:42 GMT   (413kb)

Title: SoK: Large Language Model Copyright Auditing via Fingerprinting
Authors: Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao,
  Zhan Qin
Categories: cs.CR cs.AI cs.CL
\\ ( https://arxiv.org/abs/2508.19843 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01479
replaced with revised version Tue, 23 Sep 2025 14:27:05 GMT   (82kb)

Title: An Information-Flow Perspective on Explainability Requirements:
  Specification and Verification
Authors: Bernd Finkbeiner, Hadar Frenkel, Julian Siber
Categories: cs.LO cs.AI
Comments: 22nd International Conference on Principles of Knowledge
  Representation and Reasoning (KR 2025)
\\ ( https://arxiv.org/abs/2509.01479 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2509.02783
replaced with revised version Tue, 23 Sep 2025 16:43:24 GMT   (23102kb)

Title: The Transparent Earth: A Multimodal Foundation Model for the Earth's
  Subsurface
Authors: Arnab Mazumder, Javier E. Santos, Noah Hobbs, Mohamed Mehana, Daniel
  O'Malley
Categories: cs.LG cs.AI physics.geo-ph
Comments: Accepted at the Neurips 2025 AI4Science Workshop
\\ ( https://arxiv.org/abs/2509.02783 ,  23102kb)
------------------------------------------------------------------------------
\\
arXiv:2509.06216
replaced with revised version Tue, 23 Sep 2025 01:01:15 GMT   (1512kb)

Title: Agentic Software Engineering: Foundational Pillars and a Research
  Roadmap
Authors: Ahmed E. Hassan, Hao Li, Dayi Lin, Bram Adams, Tse-Hsun Chen, Yutaro
  Kashiwa, Dong Qiu
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2509.06216 ,  1512kb)
------------------------------------------------------------------------------
\\
arXiv:2509.06743
replaced with revised version Tue, 23 Sep 2025 08:22:42 GMT   (2954kb)

Title: Long-Range Graph Wavelet Networks
Authors: Filippo Guerranti, Fabrizio Forte, Simon Geisler, Stephan G\"unnemann
Categories: cs.LG cs.AI
Comments: 39th Conference on Neural Information Processing Systems (NeurIPS
  2025) Workshop: New Perspectives in Advancing Graph Machine Learning
\\ ( https://arxiv.org/abs/2509.06743 ,  2954kb)
------------------------------------------------------------------------------
\\
arXiv:2509.06984
replaced with revised version Tue, 23 Sep 2025 04:55:03 GMT   (4613kb)

Title: FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under
  Missing Modalities
Authors: Lishan Yang, Wei Emma Zhang, Nam Kha Nguygen, Po Hu, Yanjun Shu,
  Weitong Chen and Mong Yuan Sim
Categories: cs.LG cs.AI
Comments: 8 pages, 7 figures
ACM-class: I.2.7; I.2.11
\\ ( https://arxiv.org/abs/2509.06984 ,  4613kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09177
replaced with revised version Tue, 23 Sep 2025 15:39:49 GMT   (2154kb)

Title: Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level
  RL
Authors: Hanyi Mao, Quanjia Xiao, Lei Pang, Haixiao Liu
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2509.09177 ,  2154kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10918
replaced with revised version Tue, 23 Sep 2025 02:10:29 GMT   (12505kb)

Title: ToMA: Token Merge with Attention for Diffusion Models
Authors: Wenbo Lu, Shaoyi Zheng, Yuxuan Xia, Shengjie Wang
Categories: cs.LG cs.AI
Comments: In proceedings of the 42nd International Conference on Machine
  Learning (ICML 2025). Code available at https://github.com/wenboluu/ToMA
\\ ( https://arxiv.org/abs/2509.10918 ,  12505kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11044
replaced with revised version Tue, 23 Sep 2025 16:41:27 GMT   (931kb)

Title: FragmentGPT: A Unified GPT Model for Fragment Growing, Linking, and
  Merging in Molecular Design
Authors: Xuefeng Liu, Songhao Jiang, Qinan Huang, Tinson Xu, Ian Foster, Mengdi
  Wang, Hening Lin, Rick Stevens
Categories: cs.LG cs.AI q-bio.BM
\\ ( https://arxiv.org/abs/2509.11044 ,  931kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13232
replaced with revised version Tue, 23 Sep 2025 14:19:47 GMT   (262kb)

Title: Single-stream Policy Optimization
Authors: Zhongwen Xu and Zihan Ding
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2509.13232 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13400
replaced with revised version Tue, 23 Sep 2025 06:37:56 GMT   (100kb)

Title: Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer
  Reviews
Authors: Sai Suresh Marchala Vasu, Ivaxi Sheth, Hui-Po Wang, Ruta Binkyte,
  Mario Fritz
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2509.13400 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15908 (*cross-listing*)
replaced with revised version Tue, 23 Sep 2025 10:48:03 GMT   (5495kb)

Title: Interpretable Nanoporous Materials Design with Symmetry-Aware Networks
Authors: Zhenhao Zhou, Salman Bin Kashif, Jin-Hu Dou, Chris Wolverton, Kaihang
  Shi, Tao Deng, Zhenpeng Yao
Categories: cond-mat.mtrl-sci cs.AI
\\ ( https://arxiv.org/abs/2509.15908 ,  5495kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16339
replaced with revised version Tue, 23 Sep 2025 14:07:12 GMT   (1356kb)

Title: Highly Imbalanced Regression with Tabular Data in SEP and Other
  Applications
Authors: Josias K. Moukpe, Philip K. Chan, Ming Zhang
Categories: cs.LG cs.AI
Comments: ICMLA 2025
\\ ( https://arxiv.org/abs/2509.16339 ,  1356kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17046 (*cross-listing*)
replaced with revised version Tue, 23 Sep 2025 02:04:18 GMT   (1253kb)

Title: A Chain-of-thought Reasoning Breast Ultrasound Dataset Covering All
  Histopathology Categories
Authors: Haojun Yu, Youcheng Li, Zihan Niu, Nan Zhang, Xuantong Gong, Huan Li,
  Zhiying Zou, Haifeng Qi, Zhenxiao Cao, Zijie Lan, Xingjian Yuan, Jiating He,
  Haokai Zhang, Shengtao Zhang, Zicheng Wang, Dong Wang, Ziwei Zhao, Congying
  Chen, Yong Wang, Wangyan Qin, Qingli Zhu, Liwei Wang
Categories: eess.IV cs.AI cs.CV
\\ ( https://arxiv.org/abs/2509.17046 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17446
replaced with revised version Tue, 23 Sep 2025 02:50:01 GMT   (755kb)

Title: MVCL-DAF++: Enhancing Multimodal Intent Recognition via Prototype-Aware
  Contrastive Alignment and Coarse-to-Fine Dynamic Attention Fusion
Authors: Haofeng Huang, Yifei Han, Long Zhang, Bin Li, Yangfan He
Categories: cs.LG cs.AI
Comments: Submitted to ICASSP 2026
\\ ( https://arxiv.org/abs/2509.17446 ,  755kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17970
replaced with revised version Tue, 23 Sep 2025 02:59:09 GMT   (9522kb)

Title: Joint Memory Frequency and Computing Frequency Scaling for
  Energy-efficient DNN Inference
Authors: Yunchu Han, Zhaojun Nan, Sheng Zhou, and Zhisheng Niu
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2509.17970 ,  9522kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17998
replaced with revised version Tue, 23 Sep 2025 12:57:08 GMT   (5158kb)

Title: Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with
  LLMs
Authors: Richard Cornelius Suwandi, Feng Yin, Juntao Wang, Renjie Li, Tsung-Hui
  Chang, Sergios Theodoridis
Categories: cs.LG cs.AI
Comments: Accepted as Poster at NeurIPS 2025
\\ ( https://arxiv.org/abs/2509.17998 ,  5158kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17999
replaced with revised version Tue, 23 Sep 2025 14:28:10 GMT   (1812kb)

Title: The Narcissus Hypothesis: Descending to the Rung of Illusion
Authors: Riccardo Cadei, Christian Intern\`o
Categories: cs.CY cs.AI cs.HC cs.LG
\\ ( https://arxiv.org/abs/2509.17999 ,  1812kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18057
replaced with revised version Tue, 23 Sep 2025 17:34:30 GMT   (8359kb)

Title: Reinforced Generation of Combinatorial Structures: Applications to
  Complexity Theory
Authors: Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta
Categories: cs.LG cs.AI cs.CC math.CO
\\ ( https://arxiv.org/abs/2509.18057 ,  8359kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18058
replaced with revised version Tue, 23 Sep 2025 17:34:27 GMT   (343kb)

Title: Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier
  LLMs
Authors: Alexander Panfilov, Evgenii Kortukov, Kristina Nikoli\'c, Matthias
  Bethge, Sebastian Lapuschkin, Wojciech Samek, Ameya Prabhu, Maksym
  Andriushchenko, Jonas Geiping
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2509.18058 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09707 (*cross-listing*)
replaced with revised version Tue, 23 Sep 2025 06:08:57 GMT   (1665kb)

Title: When and How Long Did Therapy Happen? Soft-Supervising Temporal
  Localization Using Audio-Language Models
Authors: Suhas BN, Andrew M. Sherrill, Jyoti Alaparthi, Dominik Mattioli, Rosa
  I. Arriaga, Chris W. Wiese, and Saeed Abdullah
Categories: eess.AS cs.CL cs.HC
Comments: 5 pages, 2 figures
MSC-class: 68T07
ACM-class: I.2.7; I.5.4; H.5.2
\\ ( https://arxiv.org/abs/2506.09707 ,  1665kb)
------------------------------------------------------------------------------
\\
arXiv:2508.18370
replaced with revised version Tue, 23 Sep 2025 03:30:33 GMT   (534kb)

Title: Training Language Model Agents to Find Vulnerabilities with CTF-Dojo
Authors: Terry Yue Zhuo, Dingmin Wang, Hantian Ding, Varun Kumar, Zijian Wang
Categories: cs.SE cs.CL cs.CR cs.LG
\\ ( https://arxiv.org/abs/2508.18370 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13625
replaced with revised version Tue, 23 Sep 2025 02:40:24 GMT   (227kb)

Title: Privacy-Aware In-Context Learning for Large Language Models
Authors: Bishnu Bhusal, Manoj Acharya, Ramneet Kaur, Colin Samplawski, Anirban
  Roy, Adam D. Cobb, Rohit Chadha, Susmit Jha
Categories: cs.LG cs.CL cs.CR
\\ ( https://arxiv.org/abs/2509.13625 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15561
replaced with revised version Tue, 23 Sep 2025 09:10:27 GMT   (543kb)

Title: Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning
Authors: Om Naphade, Saksham Bansal, Parikshit Pareek
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2509.15561 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2408.15555 (*cross-listing*)
replaced with revised version Mon, 22 Sep 2025 19:09:54 GMT   (248kb)

Title: GlaLSTM: A Concurrent LSTM Stream Framework for Glaucoma Detection via
  Biomarker Mining
Authors: Cheng Huang and Weizheng Xie and Tsengdar Lee and Karanjit Kooner and
  Ning Zhang and Jia Zhang
Categories: eess.IV cs.CV cs.LG
Comments: IEEE 47th EMBC (Poster)
\\ ( https://arxiv.org/abs/2408.15555 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2504.06610
replaced with revised version Mon, 22 Sep 2025 20:17:58 GMT   (3480kb)

Title: Disentangle and Regularize: Sign Language Production with
  Articulator-Based Disentanglement and Channel-Aware Regularization
Authors: Sumeyye Meryem Tasyurek and Tugce Kiziltepe and Hacer Yalim Keles
Categories: cs.LG cs.CV
Comments: 12 pages, 7 figures, 5 tables
\\ ( https://arxiv.org/abs/2504.06610 ,  3480kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17912 (*cross-listing*)
replaced with revised version Mon, 22 Sep 2025 19:24:08 GMT   (12160kb)

Title: UltraBoneUDF: Self-supervised Bone Surface Reconstruction from
  Ultrasound Based on Neural Unsigned Distance Functions
Authors: Luohong Wu, Matthias Seibold, Nicola A. Cavalcanti, Giuseppe Loggia,
  Lisa Reissner, Bastian Sigrist, Jonas Hein, Lilian Calvet, Arnd Vieh\"ofer,
  Philipp F\"urnstahl
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2505.17912 ,  12160kb)
------------------------------------------------------------------------------
\\
arXiv:2507.03866
replaced with revised version Mon, 22 Sep 2025 21:18:37 GMT   (4212kb)

Title: A Rigorous Behavior Assessment of CNNs Using a Data-Domain Sampling
  Regime
Authors: Shuning Jiang, Wei-Lun Chao, Daniel Haehn, Hanspeter Pfister, Jian
  Chen
Categories: cs.LG cs.CV cs.HC
Comments: This is a preprint of a paper that has been accepted for publication
  at IEEE VIS 2025. The final version may be different upon publication. 9
  pages main text, 11 pages supplementary contents, 37 figures
\\ ( https://arxiv.org/abs/2507.03866 ,  4212kb)
------------------------------------------------------------------------------
\\
arXiv:2507.07712
replaced with revised version Tue, 23 Sep 2025 03:19:50 GMT   (1781kb)

Title: Class-wise Balancing Data Replay for Federated Class-Incremental
  Learning
Authors: Zhuang Qi, Ying-Peng Tang, Lei Meng, Han Yu, Xiaoxiao Li, Xiangxu Meng
Categories: cs.LG cs.CV
Comments: NeurIPS'25 Accepted, Oral
\\ ( https://arxiv.org/abs/2507.07712 ,  1781kb)
------------------------------------------------------------------------------
\\
arXiv:2508.18705
replaced with revised version Tue, 23 Sep 2025 10:01:38 GMT   (3634kb)

Title: Enhancing Video-Based Robot Failure Detection Using Task Knowledge
Authors: Santosh Thoduka and Sebastian Houben and Juergen Gall and Paul G.
  Pl\"oger
Categories: cs.RO cs.CV
Comments: Accepted at ECMR 2025
DOI: 10.1109/ECMR65884.2025.11162998
\\ ( https://arxiv.org/abs/2508.18705 ,  3634kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16223 (*cross-listing*)
replaced with revised version Tue, 23 Sep 2025 17:59:51 GMT   (284kb)

Title: mRadNet: A Compact Radar Object Detector with MetaFormer
Authors: Huaiyu Chen, Fahed Hassanat, Robert Laganiere, Martin Bouchard
Categories: eess.SP cs.CV
Comments: 5 pages, 2 figures, submitted to IEEE ICASSP 2026. Code availble at
  https://github.com/huaiyu-chen/mRadNet
\\ ( https://arxiv.org/abs/2509.16223 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16875
replaced with revised version Tue, 23 Sep 2025 15:26:12 GMT   (4252kb)

Title: Towards Interpretable and Efficient Attention: Compressing All by
  Contracting a Few
Authors: Qishuai Wen, Zhiyuan Huang, Chun-Guang Li
Categories: cs.LG cs.CV
Comments: NeurIPS2025 Spotlight
\\ ( https://arxiv.org/abs/2509.16875 ,  4252kb)
------------------------------------------------------------------------------
\\
arXiv:2412.20301
replaced with revised version Mon, 22 Sep 2025 21:31:18 GMT   (1355kb)

Title: Distributed Hybrid Sketching for $\ell_2$-Embeddings
Authors: Neophytos Charalambides, Arya Mazumdar
Categories: math.NA cs.DC cs.IT cs.NA eess.SP math.IT
Comments: 23 pages, 13 figures, 1 table
MSC-class: 65F10, 65F20, 65F55, 65B99, 65Z05, 68P20, 68P27, 68P30, 68U01,
  68W10, 68W15, 68W20, 68W25, 94D99
ACM-class: G.1.2; G.1.3; E.4
\\ ( https://arxiv.org/abs/2412.20301 ,  1355kb)
------------------------------------------------------------------------------
\\
arXiv:2503.23817
replaced with revised version Tue, 23 Sep 2025 01:26:23 GMT   (2294kb)

Title: MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM
  Acceleration
Authors: Tatsuya Kubo, Daichi Tokuda, Tomoya Nagatani, Masayuki Usui, Lei Qu,
  Ting Cao and Shinya Takamaeda-Yamazaki
Categories: cs.AR cs.DC
\\ ( https://arxiv.org/abs/2503.23817 ,  2294kb)
------------------------------------------------------------------------------
\\
arXiv:2508.17341
replaced with revised version Tue, 23 Sep 2025 07:30:51 GMT   (2491kb)

Title: MetaFed: Advancing Privacy, Performance, and Sustainability in Federated
  Metaverse Systems
Authors: Muhammet Anil Yagiz, Zeynep Sude Cengiz, Polat Goktas
Categories: cs.LG cs.CR cs.CY cs.DC cs.ET
Comments: 2025 IEEE International Symposium on Emerging Metaverse (ISEMV),
  co-located with the 2025 IEEE/CVF International Conference on Computer Vision
  (ICCV)
\\ ( https://arxiv.org/abs/2508.17341 ,  2491kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
