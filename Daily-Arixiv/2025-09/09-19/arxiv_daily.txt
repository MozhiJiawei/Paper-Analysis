Gmail	李嘉维 <qetwe0000@gmail.com>
cs daily Subj-class mailing 8004a1 1
send mail ONLY to cs <no-reply@arxiv.org>	2025年9月23日 13:02
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Computer Vision and Pattern Recognition
Distributed, Parallel, and Cluster Computing
Multiagent Systems
 received from  Fri 19 Sep 25 18:00:00 GMT  to  Mon 22 Sep 25 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2509.16288
Date: Fri, 19 Sep 2025 08:04:14 GMT   (27kb)

Title: Identifying Critical Pathways in Coronary Heart Disease via Fuzzy
  Subgraph Connectivity
Authors: Shanookha Ali and Nitha Niralda P C
Categories: cs.AI
MSC-class: 05C22, 05C90, 68R10 05C22, 05C90, 68R10 05C22, 05C90, 68R10
\\
  Coronary heart disease (CHD) arises from complex interactions among
uncontrollable factors, controllable lifestyle factors, and clinical
indicators, where relationships are often uncertain. Fuzzy subgraph
connectivity (FSC) provides a systematic tool to capture such imprecision by
quantifying the strength of association between vertices and subgraphs in fuzzy
graphs. In this work, a fuzzy CHD graph is constructed with vertices for
uncontrollable, controllable, and indicator components, and edges weighted by
fuzzy memberships. Using FSC, we evaluate connectivity to identify strongest
diagnostic routes, dominant risk factors, and critical bridges. Results show
that FSC highlights influential pathways, bounds connectivity between weakest
and strongest correlations, and reveals critical edges whose removal reduces
predictive strength. Thus, FSC offers an interpretable and robust framework for
modeling uncertainty in CHD risk prediction and supporting clinical
decision-making.
\\ ( https://arxiv.org/abs/2509.16288 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16298
Date: Fri, 19 Sep 2025 17:14:17 GMT   (697kb)

Title: A global view of diverse construction methods of fuzzy implication
  functions rooted on F-chains
Authors: Raquel Fernandez-Peralta and Juan Vicente Riera
Categories: cs.AI
\\
  Fuzzy implication functions are one of the most important operators used in
the fuzzy logic framework. While their flexible definition allows for diverse
families with distinct properties, this variety needs a deeper theoretical
understanding of their structural relationships. In this work, we focus on the
study of construction methods, which employ different techniques to generate
new fuzzy implication functions from existing ones. Particularly, we generalize
the $F$-chain-based construction, recently introduced by Mesiar et al. to
extend a method for constructing aggregation functions to the context of fuzzy
implication functions. Our generalization employs collections of fuzzy
implication functions rather than single ones, and uses two different
increasing functions instead of a unique $F$-chain. We analyze property
preservation under this construction and establish sufficient conditions.
Furthermore, we demonstrate that our generalized $F$-chain-based construction
is a unifying framework for several existing methods. In particular, we show
that various construction techniques, such as contraposition, aggregation, and
generalized vertical/horizontal threshold methods, can be reformulated within
our approach. This reveals structural similarities between seemingly distinct
construction strategies and provides a cohesive perspective on fuzzy
implication construction methods.
\\ ( https://arxiv.org/abs/2509.16298 ,  697kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16299
Date: Fri, 19 Sep 2025 17:21:45 GMT   (60kb)

Title: On the Non-Uniqueness of Representation of $(U,N)$-Implications
Authors: Raquel Fernandez-Peralta and Andrea Mesiarov\'a-Zem\'ankov\'a
Categories: cs.AI
\\
  Fuzzy implication functions constitute fundamental operators in fuzzy logic
systems, extending classical conditionals to manage uncertainty in logical
inference. Among the extensive families of these operators, generalizations of
the classical material implication have received considerable theoretical
attention, particularly $(S,N)$-implications constructed from t-conorms and
fuzzy negations, and their further generalizations to $(U,N)$-implications
using disjunctive uninorms. Prior work has established characterization
theorems for these families under the assumption that the fuzzy negation $N$ is
continuous, ensuring uniqueness of representation. In this paper, we disprove
this last fact for $(U,N)$-implications and we show that they do not
necessarily possess a unique representation, even if the fuzzy negation is
continuous. Further, we provide a comprehensive study of uniqueness conditions
for both uninorms with continuous and non-continuous underlying functions. Our
results offer important theoretical insights into the structural properties of
these operators.
\\ ( https://arxiv.org/abs/2509.16299 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16330
Date: Fri, 19 Sep 2025 18:13:32 GMT   (1397kb)

Title: Generalizability of Large Language Model-Based Agents: A Comprehensive
  Survey
Authors: Minxing Zhang, Yi Yang, Roy Xie, Bhuwan Dhingra, Shuyan Zhou, Jian Pei
Categories: cs.AI
\\
  Large Language Model (LLM)-based agents have emerged as a new paradigm that
extends LLMs' capabilities beyond text generation to dynamic interaction with
external environments. By integrating reasoning with perception, memory, and
tool use, agents are increasingly deployed in diverse domains like web
navigation and household robotics. A critical challenge, however, lies in
ensuring agent generalizability - the ability to maintain consistent
performance across varied instructions, tasks, environments, and domains,
especially those beyond agents' fine-tuning data. Despite growing interest, the
concept of generalizability in LLM-based agents remains underdefined, and
systematic approaches to measure and improve it are lacking. In this survey, we
provide the first comprehensive review of generalizability in LLM-based agents.
We begin by emphasizing agent generalizability's importance by appealing to
stakeholders and clarifying the boundaries of agent generalizability by
situating it within a hierarchical domain-task ontology. We then review
datasets, evaluation dimensions, and metrics, highlighting their limitations.
Next, we categorize methods for improving generalizability into three groups:
methods for the backbone LLM, for agent components, and for their interactions.
Moreover, we introduce the distinction between generalizable frameworks and
generalizable agents and outline how generalizable frameworks can be translated
into agent-level generalizability. Finally, we identify critical challenges and
future directions, including developing standardized frameworks, variance- and
cost-based metrics, and approaches that integrate methodological innovations
with architecture-level designs. By synthesizing progress and highlighting
opportunities, this survey aims to establish a foundation for principled
research on building LLM-based agents that generalize reliably across diverse
applications.
\\ ( https://arxiv.org/abs/2509.16330 ,  1397kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16332
Date: Fri, 19 Sep 2025 18:19:56 GMT   (2418kb)

Title: Psychometric Personality Shaping Modulates Capabilities and Safety in
  Language Models
Authors: Stephen Fitz, Peter Romero, Steven Basart, Sipeng Chen, Jose
  Hernandez-Orallo
Categories: cs.AI cs.CL
\\
  Large Language Models increasingly mediate high-stakes interactions,
intensifying research on their capabilities and safety. While recent work has
shown that LLMs exhibit consistent and measurable synthetic personality traits,
little is known about how modulating these traits affects model behavior. We
address this gap by investigating how psychometric personality control grounded
in the Big Five framework influences AI behavior in the context of capability
and safety benchmarks. Our experiments reveal striking effects: for example,
reducing conscientiousness leads to significant drops in safety-relevant
metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well
as reduction in general capabilities as measured by MMLU. These findings
highlight personality shaping as a powerful and underexplored axis of model
control that interacts with both safety and general competence. We discuss the
implications for safety evaluation, alignment strategies, steering model
behavior after deployment, and risks associated with possible exploitation of
these findings. Our findings motivate a new line of research on
personality-sensitive safety evaluations and dynamic behavioral control in
LLMs.
\\ ( https://arxiv.org/abs/2509.16332 ,  2418kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16348
Date: Fri, 19 Sep 2025 18:40:47 GMT   (1099kb)

Title: A Unified AI Approach for Continuous Monitoring of Human Health and
  Diseases from Intensive Care Unit to Home with Physiological Foundation
  Models (UNIPHY+)
Authors: Minxiao Wang, Saurabh Kataria, Juntong Ni, Timothy G. Buchman, Jocelyn
  Grunwell, Mark Mai, Wei Jin, Matthew Clark, Stephanie Brown, Michael Fundora,
  Puneet Sharma, Tony Pan, Sam Khan, Timothy Ruchti, Naveen Muthu, Kevin Maher,
  Sivasubramanium V Bhavani, Xiao Hu
Categories: cs.AI
\\
  We present UNIPHY+, a unified physiological foundation model (physioFM)
framework designed to enable continuous human health and diseases monitoring
across care settings using ubiquitously obtainable physiological data. We
propose novel strategies for incorporating contextual information during
pretraining, fine-tuning, and lightweight model personalization via multi-modal
learning, feature fusion-tuning, and knowledge distillation. We advocate
testing UNIPHY+ with a broad set of use cases from intensive care to ambulatory
monitoring in order to demonstrate that UNIPHY+ can empower generalizable,
scalable, and personalized physiological AI to support both clinical
decision-making and long-term health monitoring.
\\ ( https://arxiv.org/abs/2509.16348 ,  1099kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16372
Date: Fri, 19 Sep 2025 19:28:12 GMT   (998kb)

Title: Evaluation of Causal Reasoning for Large Language Models in
  Contextualized Clinical Scenarios of Laboratory Test Interpretation
Authors: Balu Bhasuran, Mattia Prosperi, Karim Hanna, John Petrilli, Caretia
  JeLayne Washington, Zhe He
Categories: cs.AI
\\
  This study evaluates causal reasoning in large language models (LLMs) using
99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of
Causation: association, intervention, and counterfactual reasoning. We examined
common laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and
paired them with relevant causal factors including age, gender, obesity, and
smoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with
responses evaluated by four medically trained human experts. GPT-o1
demonstrated stronger discriminative performance (AUROC overall = 0.80 +/-
0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores
across association (0.75 vs 0.72), intervention (0.84 vs 0.70), and
counterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and
specificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings
showing similar trends. Both models performed best on intervention questions
and worst on counterfactuals, particularly in altered outcome scenarios. These
findings suggest GPT-o1 provides more consistent causal reasoning, but
refinement is required before adoption in high-stakes clinical applications.
\\ ( https://arxiv.org/abs/2509.16372 ,  998kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16399
Date: Fri, 19 Sep 2025 20:22:13 GMT   (1714kb)

Title: VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided
  Reward Shaping
Authors: Guojun Xiong and Milind Tambe
Categories: cs.AI
Comments: 28pages, 19figures
\\
  In social impact optimization, AI decision systems often rely on solvers that
optimize well-calibrated mathematical objectives. However, these solvers cannot
directly accommodate evolving human preferences, typically expressed in natural
language rather than formal constraints. Recent approaches address this by
using large language models (LLMs) to generate new reward functions from
preference descriptions. While flexible, they risk sacrificing the system's
core utility guarantees. In this paper, we propose \texttt{VORTEX}, a
language-guided reward shaping framework that preserves established
optimization goals while adaptively incorporating human feedback. By
formalizing the problem as multi-objective optimization, we use LLMs to
iteratively generate shaping rewards based on verbal reinforcement and
text-gradient prompt updates. This allows stakeholders to steer decision
behavior via natural language without modifying solvers or specifying trade-off
weights. We provide theoretical guarantees that \texttt{VORTEX} converges to
Pareto-optimal trade-offs between utility and preference satisfaction.
Empirical results in real-world allocation tasks demonstrate that
\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage
goals while maintaining high task performance. This work introduces a practical
and theoretically grounded paradigm for human-AI collaborative optimization
guided by natural language.
\\ ( https://arxiv.org/abs/2509.16399 ,  1714kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16431
Date: Fri, 19 Sep 2025 21:15:31 GMT   (1050kb)

Title: Proactive Statistical Process Control Using AI: A Time Series
  Forecasting Approach for Semiconductor Manufacturing
Authors: Mohammad Iqbal Rasul Seeam, Victor S. Sheng
Categories: cs.AI
Comments: 7 pages, 3 figures, no .bbl file needed because bibliography already
  in main.tex file
\\
  In the manufacturing industry, it is very important to keep machines and
processes running smoothly and without unexpected problems. One of the most
common tools used to check if everything is working properly is called
Statistical Process Control (SPC). Traditional SPC methods work by checking
whether recent measurements are within acceptable limits. However, they only
react after a problem has already occurred. This can lead to wasted materials,
machine downtime, and increased costs. In this paper, we present a smarter way
to use SPC. Instead of just reacting to issues after they happen, our system
can predict future problems before they occur. We use a machine learning tool
called Facebook Prophet, which is designed to work with time-series data (data
that changes over time). Prophet looks at past data and forecasts what the next
value will be. Then, we use SPC rules to decide if the predicted value is in a
Safe zone (no problem), a Warning zone (needs attention), or a Critical zone
(may require shutting down the process). We applied this system to real data
from a semiconductor manufacturing company. One of the challenges with this
data is that the measurements are not taken at regular time intervals. This
makes it harder to predict future values accurately. Despite this, our model
was able to make strong predictions and correctly classify the risk level of
future measurements. The main benefit of our system is that it gives engineers
and technicians a chance to act early - before something goes wrong. This helps
reduce unexpected failures and improves the overall stability and reliability
of the production process. By combining machine learning with traditional SPC,
we make quality control more proactive, accurate, and useful for modern
industry.
\\ ( https://arxiv.org/abs/2509.16431 ,  1050kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16444
Date: Fri, 19 Sep 2025 21:46:47 GMT   (704kb)

Title: Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered
  Mental Health Chatbots
Authors: Chenhan Lyu, Yutong Song, Pengfei Zhang, Amir M. Rahmani
Categories: cs.AI
\\
  Mental health applications have emerged as a critical area in computational
health, driven by rising global rates of mental illness, the integration of AI
in psychological care, and the need for scalable solutions in underserved
communities. These include therapy chatbots, crisis detection, and wellness
platforms handling sensitive data, requiring specialized AI safety beyond
general safeguards due to emotional vulnerability, risks like misdiagnosis or
symptom exacerbation, and precise management of vulnerable states to avoid
severe outcomes such as self-harm or loss of trust. Despite AI safety advances,
general safeguards inadequately address mental health-specific challenges,
including crisis intervention accuracy to avert escalations, therapeutic
guideline adherence to prevent misinformation, scale limitations in
resource-constrained settings, and adaptation to nuanced dialogues where
generics may introduce biases or miss distress signals. We introduce an
approach to apply Constitutional AI training with domain-specific mental health
principles for safe, domain-adapted CAI systems in computational mental health
applications.
\\ ( https://arxiv.org/abs/2509.16444 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16456
Date: Fri, 19 Sep 2025 22:30:23 GMT   (245kb)

Title: GPO: Learning from Critical Steps to Improve LLM Reasoning
Authors: Jiahao Yu, Zelei Cheng, Xian Wu, Xinyu Xing
Categories: cs.AI
Comments: 39th Conference on Neural Information Processing Systems (NeurIPS
  2025)
\\
  Large language models (LLMs) are increasingly used in various domains,
showing impressive potential on different tasks. Recently, reasoning LLMs have
been proposed to improve the \textit{reasoning} or \textit{thinking}
capabilities of LLMs to solve complex problems. Despite the promising results
of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs
still remains a significant challenge. While existing optimization methods have
advanced the LLM reasoning capabilities, they often treat reasoning
trajectories as a whole, without considering the underlying critical steps
within the trajectory. In this paper, we introduce \textbf{G}uided
\textbf{P}ivotal \textbf{O}ptimization (GPO), a novel fine-tuning strategy that
dives into the reasoning process to enable more effective improvements. GPO
first identifies the `critical step' within a reasoning trajectory - a point
that the model must carefully proceed to succeed at the problem. We locate the
critical step by estimating the advantage function. GPO then resets the policy
to the critical step, samples the new rollout and prioritizes the learning
process on those rollouts. This focus allows the model to learn more
effectively from pivotal moments within the reasoning process to improve the
reasoning performance. We demonstrate that GPO is a general strategy that can
be integrated with various optimization methods to improve reasoning
performance. Besides theoretical analysis, our experiments across challenging
reasoning benchmarks show that GPO can consistently and significantly enhance
the performance of existing optimization methods, showcasing its effectiveness
and generalizability in improving LLM reasoning by concentrating on pivotal
moments within the generation process.
\\ ( https://arxiv.org/abs/2509.16456 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16547
Date: Sat, 20 Sep 2025 06:15:47 GMT   (108kb)

Title: Checking extracted rules in Neural Networks
Authors: Adrian Wurm
Categories: cs.AI cs.LG
Comments: 7 pages, one figure
\\
  In this paper we investigate formal verification of extracted rules for
Neural Networks under a complexity theoretic point of view. A rule is a global
property or a pattern concerning a large portion of the input space of a
network. These rules are algorithmically extracted from networks in an effort
to better understand their inner way of working. Here, three problems will be
in the focus: Does a given set of rules apply to a given network? Is a given
set of rules consistent or do the rules contradict themselves? Is a given set
of rules exhaustive in the sense that for every input the output is determined?
Finding algorithms that extract such rules out of networks has been
investigated over the last 30 years, however, to the author's current
knowledge, no attempt in verification was made until now. A lot of attempts of
extracting rules use heuristics involving randomness and over-approximation, so
it might be beneficial to know whether knowledge obtained in that way can
actually be trusted.
  We investigate the above questions for neural networks with ReLU-activation
as well as for Boolean networks, each for several types of rules. We
demonstrate how these problems can be reduced to each other and show that most
of them are co-NP-complete.
\\ ( https://arxiv.org/abs/2509.16547 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16561
Date: Sat, 20 Sep 2025 07:38:58 GMT   (1619kb)

Title: SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric
  for Chain-of-Thought Reasoning
Authors: Yue Xin, Chen Shen, Shaotian Yan, Xiaosong Yuan, Yaoming Wang,
  Xiaofeng Zhang, Chenxi Huang, Jieping Ye
Categories: cs.AI cs.CL
Comments: accpeted by EMNLP 2025
\\
  Chain-of-Thought (CoT) prompting enhances the math reasoning capability of
large language models (LLMs) to a large margin. However, the mechanism
underlying such improvements remains unexplored. In this paper, we present
\textbf{SalaMAnder} (\textbf{S}h\textbf{a}p\textbf{l}ey-b\textbf{a}sed
\textbf{M}athematical Expression \textbf{A}ttribution a\textbf{nd}
M\textbf{e}t\textbf{r}ic), a theoretically grounded methodology as well as a
mathematically rigorous evaluation metric for quantifying component-level
contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley
value for mathematical expression attribution and develop an efficient
stratified sampling algorithm that significantly reduces the computational
complexity. Besides, we develop the \textbf{CoSP} (\textbf{C}ardinality
\textbf{o}f \textbf{S}hapley \textbf{P}ositives) metric through covariance
analysis. Comprehensive validation across popular LLM models and diverse
mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder
framework exhibits a robust monotonic correlation with model performance, not
only providing theoretical explanations for the empirical success of existing
few-shot CoT but also establishing mathematically rigorous principles for
prompt construction optimization. Furthermore, we verify the reliability of the
explanation, based on which we unify the insights of previous work.
\\ ( https://arxiv.org/abs/2509.16561 ,  1619kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16578
Date: Sat, 20 Sep 2025 08:46:38 GMT   (2528kb)

Title: Zero-Shot Human Mobility Forecasting via Large Language Model with
  Hierarchical Reasoning
Authors: Wenyao Li, Ran Zhang, Pengyang Wang, Yuanchun Zhou, and Pengfei Wang
Categories: cs.AI cs.IR
\\
  Human mobility forecasting is important for applications such as
transportation planning, urban management, and personalized recommendations.
However, existing methods often fail to generalize to unseen users or locations
and struggle to capture dynamic intent due to limited labeled data and the
complexity of mobility patterns. We propose ZHMF, a framework for zero-shot
human mobility forecasting that combines a semantic enhanced retrieval and
reflection mechanism with a hierarchical language model based reasoning system.
The task is reformulated as a natural language question answering paradigm.
Leveraging LLMs semantic understanding of user histories and context, our
approach handles previously unseen prediction scenarios. We further introduce a
hierarchical reflection mechanism for iterative reasoning and refinement by
decomposing forecasting into an activity level planner and a location level
selector, enabling collaborative modeling of long term user intentions and
short term contextual preferences. Experiments on standard human mobility
datasets show that our approach outperforms existing models. Ablation studies
reveal the contribution of each module, and case studies illustrate how the
method captures user intentions and adapts to diverse contextual scenarios.
\\ ( https://arxiv.org/abs/2509.16578 ,  2528kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16590
Date: Sat, 20 Sep 2025 09:26:44 GMT   (149kb)

Title: Question Answering with LLMs and Learning from Answer Sets
Authors: Manuel Borroto, Katie Gallagher, Antonio Ielo, Irfan Kareem, Francesco
  Ricca, Alessandra Russo
Categories: cs.AI cs.CL cs.LO
Comments: Under consideration for TPLP journal
\\
  Large Language Models (LLMs) excel at understanding natural language but
struggle with explicit commonsense reasoning. A recent trend of research
suggests that the combination of LLM with robust symbolic reasoning systems can
overcome this problem on story-based question answering tasks. In this setting,
existing approaches typically depend on human expertise to manually craft the
symbolic component. We argue, however, that this component can also be
automatically learned from examples. In this work, we introduce LLM2LAS, a
hybrid system that effectively combines the natural language understanding
capabilities of LLMs, the rule induction power of the Learning from Answer Sets
(LAS) system ILASP, and the formal reasoning strengths of Answer Set
Programming (ASP). LLMs are used to extract semantic structures from text,
which ILASP then transforms into interpretable logic rules. These rules allow
an ASP solver to perform precise and consistent reasoning, enabling correct
answers to previously unseen questions. Empirical results outline the strengths
and weaknesses of our automatic approach for learning and reasoning in a
story-based question answering benchmark.
\\ ( https://arxiv.org/abs/2509.16590 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16648
Date: Sat, 20 Sep 2025 11:50:22 GMT   (12071kb)

Title: FESTA: Functionally Equivalent Sampling for Trust Assessment of
  Multimodal LLMs
Authors: Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy
Categories: cs.AI cs.CL cs.LG
Comments: Accepted in the Findings of EMNLP, 2025
Journal-ref: EMNLP 2025
\\
  The accurate trust assessment of multimodal large language models (MLLMs)
generated predictions, which can enable selective prediction and improve user
confidence, is challenging due to the diverse multi-modal input paradigms. We
propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a
multimodal input sampling technique for MLLMs, that generates an uncertainty
measure based on the equivalent and complementary input samplings. The proposed
task-preserving sampling approach for uncertainty quantification expands the
input space to probe the consistency (through equivalent samples) and
sensitivity (through complementary samples) of the model. FESTA uses only
input-output access of the model (black-box), and does not require ground truth
(unsupervised). The experiments are conducted with various off-the-shelf
multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA
uncertainty estimate achieves significant improvement (33.3% relative
improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in
selective prediction performance, based on
area-under-receiver-operating-characteristic curve (AUROC) metric in detecting
mispredictions. The code implementation is open-sourced.
\\ ( https://arxiv.org/abs/2509.16648 ,  12071kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16656
Date: Sat, 20 Sep 2025 12:05:47 GMT   (989kb)

Title: NUMINA: A Natural Understanding Benchmark for Multi-dimensional
  Intelligence and Numerical Reasoning Abilities
Authors: Changyu Zeng, Yifan Wang, Zimu Wang, Wei Wang, Zhengni Yang, Muyi Bao,
  Jiming Xiao, Ahn Nguyen, and Yutao Yue
Categories: cs.AI
\\
  Recent advancements in 2D multimodal large language models (MLLMs) have
significantly improved performance in vision-language tasks. However, extending
these capabilities to 3D environments remains a distinct challenge due to the
complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often
lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability
to perform precise spatial measurements and complex numerical reasoning. To
address this gap, we introduce NUMINA, the first Natural Understanding
benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities
to enhance multimodal indoor perceptual understanding. NUMINA features
multi-scale annotations and various question-answer pairs, generated using
NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and
rule-based self-verification. We evaluate the performance of various
state-of-the-art LLMs on NUMINA following the Chat-Scene framework,
demonstrating that current LLMs struggle with multimodal numerical reasoning,
particularly in performing precise computations such as distance and volume
estimation, highlighting the need for further advancements in 3D models. The
dataset and source codes can be obtained from
https://github.com/fengshun124/NUMINA.
\\ ( https://arxiv.org/abs/2509.16656 ,  989kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16742
Date: Sat, 20 Sep 2025 17:09:14 GMT   (10682kb)

Title: Sycophancy Mitigation Through Reinforcement Learning with
  Uncertainty-Aware Adaptive Reasoning Trajectories
Authors: Mohammad Beigi, Ying Shen, Parshin Shojaee, Qifan Wang, Zichao Wang,
  Chandan Reddy, Ming Jin, Lifu Huang
Categories: cs.AI
\\
  Despite the remarkable capabilities of large language models, current
training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency
of a model to agree with or reinforce user-provided information even when it's
factually incorrect. To address this challenge, we introduce \textbf{SMART}
(Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes
sycophancy as a \textit{reasoning optimization problem} rather than an output
alignment issue. SMART is a two-stage framework comprising: (1)
Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically
adjusts model exploration based on state-level uncertainty to collect
high-quality, diverse reasoning trajectories alongside both stepwise progress
and final outcome rewards; and (2) progress-based reinforcement learning, which
fine-tunes the model using the collected trajectories and reward signals to
reinforce effective reasoning patterns. Through extensive experiments, we show
that SMART significantly reduces sycophantic behavior while preserving strong
performance on out-of-distribution inputs and maintaining general capabilities.
These results underscore the importance of optimizing internal reasoning
mechanisms to build more truthful and aligned AI assistants.
\\ ( https://arxiv.org/abs/2509.16742 ,  10682kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16810
Date: Sat, 20 Sep 2025 21:11:33 GMT   (10832kb)

Title: Automated Procedural Analysis via Video-Language Models for AI-assisted
  Nursing Skills Assessment
Authors: Shen Chang, Dennis Liu, Renran Tian, Kristen L. Swartzell, Stacie L.
  Klingler, Amy M. Nagle, and Nan Kong
Categories: cs.AI
\\
  Consistent high-quality nursing care is essential for patient safety, yet
current nursing education depends on subjective, time-intensive instructor
feedback in training future nurses, which limits scalability and efficiency in
their training, and thus hampers nursing competency when they enter the
workforce. In this paper, we introduce a video-language model (VLM) based
framework to develop the AI capability of automated procedural assessment and
feedback for nursing skills training, with the potential of being integrated
into existing training programs. Mimicking human skill acquisition, the
framework follows a curriculum-inspired progression, advancing from high-level
action recognition, fine-grained subaction decomposition, and ultimately to
procedural reasoning. This design supports scalable evaluation by reducing
instructor workload while preserving assessment quality. The system provides
three core capabilities: 1) diagnosing errors by identifying missing or
incorrect subactions in nursing skill instruction videos, 2) generating
explainable feedback by clarifying why a step is out of order or omitted, and
3) enabling objective, consistent formative evaluation of procedures.
Validation on synthesized videos demonstrates reliable error detection and
temporal localization, confirming its potential to handle real-world training
variability. By addressing workflow bottlenecks and supporting large-scale,
standardized evaluation, this work advances AI applications in nursing
education, contributing to stronger workforce development and ultimately safer
patient care.
\\ ( https://arxiv.org/abs/2509.16810 ,  10832kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16811
Date: Sat, 20 Sep 2025 21:22:56 GMT   (1693kb)

Title: Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of
  Long-Form, Story-Driven Media
Authors: Zihan Ding, Junlong Chen, Per Ola Kristensson, Junxiao Shen, Xinyi
  Wang
Categories: cs.AI cs.HC
\\
  Creators struggle to edit long-form, narrative-rich videos not because of UI
complexity, but due to the cognitive demands of searching, storyboarding, and
sequencing hours of footage. Existing transcript- or embedding-based methods
fall short for creative workflows, as models struggle to track characters,
infer motivations, and connect dispersed events. We present a prompt-driven,
modular editing system that helps creators restructure multi-hour content
through free-form prompts rather than timelines. At its core is a semantic
indexing pipeline that builds a global narrative via temporal segmentation,
guided memory compression, and cross-granularity fusion, producing
interpretable traces of plot, dialogue, emotion, and context. Users receive
cinematic edits while optionally refining transparent intermediate outputs.
Evaluated on 400+ videos with expert ratings, QA, and preference studies, our
system scales prompt-driven editing, preserves narrative coherence, and
balances automation with creator control.
\\ ( https://arxiv.org/abs/2509.16811 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16839
Date: Sat, 20 Sep 2025 23:31:53 GMT   (9130kb)

Title: Roundtable Policy: Improving Scientific Reasoning and Narratives through
  Confidence-Weighted Consensus of LLMs
Authors: Yu Yao, Jiayi Dong, Ju Li, Yang Yang, Yilun Du
Categories: cs.AI
Comments: Equal contribution: Yu Yao and Jiayi Dong. Equal advising: Ju Li,
  Yang Yang, and Yilun Du. Affiliations: Massachusetts Institute of Technology
  (Yu Yao, Ju Li), University of California, Los Angeles (Jiayi Dong, Yang
  Yang), Harvard University (Yilun Du)
\\
  Large language models (LLMs) have demonstrated remarkable capabilities not
only in language generation but also in advancing scientific discovery. A
growing body of work has explored ways to improve their reasoning, from
self-consistency and chain-of-thought to multi-agent debate. Inspired by the
dynamics of scientific committees and the "Society of Mind," we introduce
Roundtable Policy, a complementary inference-time reasoning framework that
performs inference through the weighted consensus of multiple LLMs. Our
findings indicate that this approach significantly enhances reasoning in
complex heterogeneous scientific tasks and improves scientific narratives in
terms of creativity, rigor, and logical coherence, while reducing
hallucinations that single models are prone to. Our approach emphasizes
structured and interpretable consensus rather than opaque convergence, while
requiring only black-box access and uniform procedures, making it broadly
applicable to multi-LLM reasoning.
\\ ( https://arxiv.org/abs/2509.16839 ,  9130kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16859
Date: Sun, 21 Sep 2025 01:11:30 GMT   (756kb)

Title: The Principles of Human-like Conscious Machine
Authors: Fangfang Li and Xiaojie Zhang
Categories: cs.AI q-bio.NC
\\
  Determining whether another system, biological or artificial, possesses
phenomenal consciousness has long been a central challenge in consciousness
studies. This attribution problem has become especially pressing with the rise
of large language models and other advanced AI systems, where debates about "AI
consciousness" implicitly rely on some criterion for deciding whether a given
system is conscious. In this paper, we propose a substrate-independent,
logically rigorous, and counterfeit-resistant sufficiency criterion for
phenomenal consciousness. We argue that any machine satisfying this criterion
should be regarded as conscious with at least the same level of confidence with
which we attribute consciousness to other humans. Building on this criterion,
we develop a formal framework and specify a set of operational principles that
guide the design of systems capable of meeting the sufficiency condition. We
further argue that machines engineered according to this framework can, in
principle, realize phenomenal consciousness. As an initial validation, we show
that humans themselves can be viewed as machines that satisfy this framework
and its principles. If correct, this proposal carries significant implications
for philosophy, cognitive science, and artificial intelligence. It offers an
explanation for why certain qualia, such as the experience of red, are in
principle irreducible to physical description, while simultaneously providing a
general reinterpretation of human information processing. Moreover, it suggests
a path toward a new paradigm of AI beyond current statistics-based approaches,
potentially guiding the construction of genuinely human-like AI.
\\ ( https://arxiv.org/abs/2509.16859 ,  756kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16865
Date: Sun, 21 Sep 2025 01:30:30 GMT   (2785kb)

Title: Large Language Models as End-to-end Combinatorial Optimization Solvers
Authors: Xia Jiang, Yaoxin Wu, Minshuo Li, Zhiguang Cao, Yingqian Zhang
Categories: cs.AI
\\
  Combinatorial optimization (CO) problems, central to decision-making
scenarios like logistics and manufacturing, are traditionally solved using
problem-specific algorithms requiring significant domain expertise. While large
language models (LLMs) have shown promise in automating CO problem solving,
existing approaches rely on intermediate steps such as code generation or
solver invocation, limiting their generality and accessibility. This paper
introduces a novel framework that empowers LLMs to serve as end-to-end CO
solvers by directly mapping natural language problem descriptions to solutions.
We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts
LLMs with solution generation patterns from domain-specific solvers, while a
feasibility-and-optimality-aware reinforcement learning (FOARL) process
explicitly mitigates constraint violations and refines solution quality.
Evaluation across seven NP-hard CO problems shows that our method achieves a
high feasibility rate and reduces the average optimality gap to 1.03-8.20% by
tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o),
reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our
method establishes a unified language-based pipeline for CO without extensive
code execution or manual architectural adjustments for different problems,
offering a general and language-driven alternative to traditional solver design
while maintaining relative feasibility guarantees.
\\ ( https://arxiv.org/abs/2509.16865 ,  2785kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16866
Date: Sun, 21 Sep 2025 01:32:13 GMT   (8688kb)

Title: seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of
  LLMs
Authors: Mohammad Ramezanali, Mo Vazifeh, Paolo Santi
Categories: cs.AI cs.CL cs.LG
\\
  We introduce seqBench, a parametrized benchmark for probing sequential
reasoning limits in Large Language Models (LLMs) through precise,
multi-dimensional control over several key complexity dimensions. seqBench
allows systematic variation of (1) the logical depth, defined as the number of
sequential actions required to solve the task; (2) the number of backtracking
steps along the optimal path, quantifying how often the agent must revisit
prior states to satisfy deferred preconditions (e.g., retrieving a key after
encountering a locked door); and (3) the noise ratio, defined as the ratio
between supporting and distracting facts about the environment. Our evaluations
on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses
exponentially beyond a model-specific logical depth. Unlike existing
benchmarks, seqBench's fine-grained control facilitates targeted analyses of
these reasoning failures, illuminating universal scaling laws and statistical
limits, as detailed in this paper alongside its generation methodology and
evaluation metrics. We find that even top-performing models systematically fail
on seqBench's structured reasoning tasks despite minimal search complexity,
underscoring key limitations in their commonsense reasoning capabilities.
Designed for future evolution to keep pace with advancing models, the seqBench
datasets are publicly released to spur deeper scientific inquiry into LLM
reasoning, aiming to establish a clearer understanding of their true potential
and current boundaries for robust real-world application.
\\ ( https://arxiv.org/abs/2509.16866 ,  8688kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16891
Date: Sun, 21 Sep 2025 03:02:59 GMT   (6792kb)

Title: LLMs as Layout Designers: A Spatial Reasoning Perspective
Authors: Sha Li
Categories: cs.AI
\\
  While Large Language Models (LLMs) have demonstrated impressive reasoning and
planning abilities in textual domains and can effectively follow instructions
for complex tasks, their capacity for spatial understanding and reasoning
remains limited. Such capabilities, however, are critical for applications like
content-aware graphic layout design, which demands precise placement,
alignment, and structural organization of multiple elements within constrained
visual spaces. To address this gap, we propose LaySPA, a reinforcement
learning-based framework that augments LLM agents with explicit spatial
reasoning capabilities. LaySPA leverages hybrid reward signals that capture
geometric validity, structural fidelity, and visual quality, enabling agents to
model inter-element relationships, navigate the canvas, and optimize spatial
arrangements. Through iterative self-exploration and adaptive policy
optimization, LaySPA produces both interpretable reasoning traces and
structured layouts. Experimental results demonstrate that LaySPA generates
structurally sound and visually appealing layouts, outperforming larger
general-purpose LLMs and achieving results on par with state-of-the-art
specialized layout models.
\\ ( https://arxiv.org/abs/2509.16891 ,  6792kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16924
Date: Sun, 21 Sep 2025 05:11:09 GMT   (489kb)

Title: Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for
  Audio-Visual Navigation
Authors: Jia Li, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng
Categories: cs.AI cs.SD
Comments: Main paper (14 pages). Accepted for publication by ICONIP(
  International Conference on Neural Information Processing) 2025
\\
  In audio-visual navigation (AVN) tasks, an embodied agent must autonomously
localize a sound source in unknown and complex 3D environments based on
audio-visual signals. Existing methods often rely on static modality fusion
strategies and neglect the spatial cues embedded in stereo audio, leading to
performance degradation in cluttered or occluded scenes. To address these
issues, we propose an end-to-end reinforcement learning-based AVN framework
with two key innovations: (1) a \textbf{S}tereo-Aware \textbf{A}ttention
\textbf{M}odule (\textbf{SAM}), which learns and exploits the spatial disparity
between left and right audio channels to enhance directional sound perception;
and (2) an \textbf{A}udio-\textbf{G}uided \textbf{D}ynamic \textbf{F}usion
Module (\textbf{AGDF}), which dynamically adjusts the fusion ratio between
visual and auditory features based on audio cues, thereby improving robustness
to environmental changes. Extensive experiments are conducted on two realistic
3D scene datasets, Replica and Matterport3D, demonstrating that our method
significantly outperforms existing approaches in terms of navigation success
rate and path efficiency. Notably, our model achieves over 40\% improvement
under audio-only conditions compared to the best-performing baselines. These
results highlight the importance of explicitly modeling spatial cues from
stereo channels and performing deep multi-modal fusion for robust and efficient
audio-visual navigation.
\\ ( https://arxiv.org/abs/2509.16924 ,  489kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16958
Date: Sun, 21 Sep 2025 07:41:49 GMT   (134kb)

Title: Quantum Abduction: A New Paradigm for Reasoning under Uncertainty
Authors: Remo Pareschi
Categories: cs.AI
Comments: 23 pages, 8 figures, 3 tables; submitted to Sci, MDPI
ACM-class: I.2.0; I.2.1; I.2.3; I.2.8
\\
  Abductive reasoning - the search for plausible explanations - has long been
central to human inquiry, from forensics to medicine and scientific discovery.
Yet formal approaches in AI have largely reduced abduction to eliminative
search: hypotheses are treated as mutually exclusive, evaluated against
consistency constraints or probability updates, and pruned until a single
"best" explanation remains. This reductionist framing overlooks the way human
reasoners sustain multiple explanatory lines in suspension, navigate
contradictions, and generate novel syntheses. This paper introduces quantum
abduction, a non-classical paradigm that models hypotheses in superposition,
allows them to interfere constructively or destructively, and collapses only
when coherence with evidence is reached. Grounded in quantum cognition and
implemented with modern NLP embeddings and generative AI, the framework
supports dynamic synthesis rather than premature elimination. Case studies span
historical mysteries (Ludwig II of Bavaria, the "Monster of Florence"),
literary demonstrations ("Murder on the Orient Express"), medical diagnosis,
and scientific theory change. Across these domains, quantum abduction proves
more faithful to the constructive and multifaceted nature of human reasoning,
while offering a pathway toward expressive and transparent AI reasoning
systems.
\\ ( https://arxiv.org/abs/2509.16958 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17037
Date: Sun, 21 Sep 2025 11:15:43 GMT   (382kb)

Title: KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for
  Financial Data Narration
Authors: Yajing Yang, Tony Deng, Min-Yen Kan
Categories: cs.AI
Comments: Accepted at EMNLP 2025 Findings
\\
  We propose KAHAN, a knowledge-augmented hierarchical framework that
systematically extracts insights from raw tabular data at entity, pairwise,
group, and system levels. KAHAN uniquely leverages LLMs as domain experts to
drive the analysis. On DataTales financial reporting benchmark, KAHAN
outperforms existing approaches by over 20% on narrative quality (GPT-4o),
maintains 98.2% factuality, and demonstrates practical utility in human
evaluation. Our results reveal that knowledge quality drives model performance
through distillation, hierarchical analysis benefits vary with market
complexity, and the framework transfers effectively to healthcare domains. The
data and code are available at https://github.com/yajingyang/kahan.
\\ ( https://arxiv.org/abs/2509.17037 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17062
Date: Sun, 21 Sep 2025 12:41:56 GMT   (77kb)

Title: From domain-landmark graph learning to problem-landmark graph generation
Authors: Cristian P\'erez-Corral, Antonio Garrido, Laura Sebastia
Categories: cs.AI
\\
  Landmarks have long played a pivotal role in automated planning, serving as
crucial elements for improving the planning algorithms. The main limitation of
classical landmark extraction methods is their sensitivity to specific planning
tasks. This results in landmarks fully tailored to individual instances,
thereby limiting their applicability across other instances of the same
planning domain. We propose a novel approach that learns landmark relationships
from multiple planning tasks of a planning domain. This leads to the creation
of a \textit{probabilistic lifted ordering graph}, as a structure that captures
weighted abstractions of relationships between parameterized landmarks.
Although these orderings are not 100\% true (they are probabilistic), they can
still be very useful in planning. Next, given a new planning task for that
domain, we instantiate the relationships from that graph to this particular
instance. This instantiation operates in two phases. First, it generates two
graphs: the former instantiating information from the initial state and the
latter from the goal state. Second, it combines these two graphs into one
unified graph by searching equivalences to extract landmark orderings. We
evaluate the precision and recallof the information found by our approach over
well-known planning domains.
\\ ( https://arxiv.org/abs/2509.17062 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17066
Date: Sun, 21 Sep 2025 12:52:28 GMT   (755kb)

Title: RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation
  with Geographical Reranking
Authors: Kunrong Li, Kwan Hui Lim
Categories: cs.AI cs.IR
Comments: PRICAI 2025
\\
  Next point-of-interest (POI) recommendation predicts a user's next
destination from historical movements. Traditional models require intensive
training, while LLMs offer flexible and generalizable zero-shot solutions but
often generate generic or geographically irrelevant results due to missing
trajectory and spatial context. To address these issues, we propose RALLM-POI,
a framework that couples LLMs with retrieval-augmented generation and
self-rectification. We first propose a Historical Trajectory Retriever (HTR)
that retrieves relevant past trajectories to serve as contextual references,
which are then reranked by a Geographical Distance Reranker (GDR) for
prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier
(ALR) is designed to refine outputs through self-reflection. Without additional
training, RALLM-POI achieves substantial accuracy gains across three real-world
Foursquare datasets, outperforming both conventional and LLM-based baselines.
Code is released at https://github.com/LKRcrocodile/RALLM-POI.
\\ ( https://arxiv.org/abs/2509.17066 ,  755kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17068
Date: Sun, 21 Sep 2025 12:57:22 GMT   (678kb)

Title: Intention-aware Hierarchical Diffusion Model for Long-term Trajectory
  Anomaly Detection
Authors: Chen Wang, Sarah Erfani, Tansu Alpcan, Christopher Leckie
Categories: cs.AI
Comments: 15 pages, 5 figures
\\
  Long-term trajectory anomaly detection is a challenging problem due to the
diversity and complex spatiotemporal dependencies in trajectory data. Existing
trajectory anomaly detection methods fail to simultaneously consider both the
high-level intentions of agents as well as the low-level details of the agent's
navigation when analysing an agent's trajectories. This limits their ability to
capture the full diversity of normal trajectories. In this paper, we propose an
unsupervised trajectory anomaly detection method named Intention-aware
Hierarchical Diffusion model (IHiD), which detects anomalies through both
high-level intent evaluation and low-level sub-trajectory analysis. Our
approach leverages Inverse Q Learning as the high-level model to assess whether
a selected subgoal aligns with an agent's intention based on predicted
Q-values. Meanwhile, a diffusion model serves as the low-level model to
generate sub-trajectories conditioned on subgoal information, with anomaly
detection based on reconstruction error. By integrating both models, IHiD
effectively utilises subgoal transition knowledge and is designed to capture
the diverse distribution of normal trajectories. Our experiments show that the
proposed method IHiD achieves up to 30.2% improvement in anomaly detection
performance in terms of F1 score over state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2509.17068 ,  678kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17087
Date: Sun, 21 Sep 2025 14:04:25 GMT   (914kb)

Title: Governing Automated Strategic Intelligence
Authors: Nicholas Kruus, Madhavendra Thakur, Adam Khoja, Leonhard Nagel,
  Maximilian Nicholson, Abeer Sharma, Jason Hausenloy, Alberto KoTafoya, Aliya
  Mukhanova, Alli Katila-Miikkulainen, Harish Chandran, Ivan Zhang, Jessie
  Chen, Joel Raj, Jord Nguyen, Lai Hsien Hao, Neja Jayasundara, Soham Sen,
  Sophie Zhang, Ashley Dora Kokui Tamaklo, Bhavya Thakur, Henry Close, Janghee
  Lee, Nina Sefton, Raghavendra Thakur, Shiv Munagala, Yeeun Kim
Categories: cs.AI
\\
  Military and economic strategic competitiveness between nation-states will
increasingly be defined by the capability and cost of their frontier artificial
intelligence models. Among the first areas of geopolitical advantage granted by
such systems will be in automating military intelligence. Much discussion has
been devoted to AI systems enabling new military modalities, such as lethal
autonomous weapons, or making strategic decisions. However, the ability of a
country of "CIA analysts in a data-center" to synthesize diverse data at scale,
and its implications, have been underexplored. Multimodal foundation models
appear on track to automate strategic analysis previously done by humans. They
will be able to fuse today's abundant satellite imagery, phone-location traces,
social media records, and written documents into a single queryable system. We
conduct a preliminary uplift study to empirically evaluate these capabilities,
then propose a taxonomy of the kinds of ground truth questions these systems
will answer, present a high-level model of the determinants of this system's AI
capabilities, and provide recommendations for nation-states to remain
strategically competitive within the new paradigm of automated intelligence.
\\ ( https://arxiv.org/abs/2509.17087 ,  914kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17116
Date: Sun, 21 Sep 2025 15:17:44 GMT   (273kb)

Title: MCTS-EP: Empowering Embodied Planning with Online Preference
  Optimization
Authors: Hang Xu, Zang Yu, Yehui Tang, Pengbo Hu, Yuhao Tang, Hao Dong
Categories: cs.AI
\\
  This paper introduces MCTS-EP, an online learning framework that combines
large language models (LLM) with Monte Carlo Tree Search (MCTS) for training
embodied agents. MCTS-EP integrates three key components: MCTS-guided
exploration for preference data collection, efficient multi-modal reasoning
mechanism, and iterative training pipeline based on preference optimization. We
theoretically prove that MCTS-EP achieves better performance bounds than
conventional on-policy algorithms when the loss function is strongly convex,
and demonstrate that it can be formulated as a search-enhanced variant of GAIL.
MCTS-EP achieves state-of-the-art performace across serval benchmarks. In
ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks.
In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average
interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code
available at: https://github.com/xuhang-2/Embodied-Agent-Planning
\\ ( https://arxiv.org/abs/2509.17116 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17158
Date: Sun, 21 Sep 2025 16:59:45 GMT   (8565kb)

Title: ARE: Scaling Up Agent Environments and Evaluations
Authors: Pierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo
  Bettini, Amar Budhiraja, Ricardo Silveira Cabral, Virginie Do, Romain Froger,
  Emilien Garreau, Jean-Baptiste Gaya, Hugo Lauren\c{c}on, Maxime Lecanu, Kunal
  Malkan, Dheeraj Mekala, Pierre M\'enard, Gr\'egoire Mialon, Ulyana Piterbarg,
  Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav
  Vorotilov, Mengjue Wang, Ian Yu
Categories: cs.AI cs.CL
\\
  We introduce Meta Agents Research Environments (ARE), a research platform for
scalable creation of environments, integration of synthetic or real
applications, and execution of agentic orchestrations. ARE provides simple
abstractions to build complex and diverse environments, each with their own
rules, tools, content, and verifiers, helping to bridge the gap between model
development and real-world deployment. We also propose Gaia2, a benchmark built
in ARE and designed to measure general agent capabilities. Beyond search and
execution, Gaia2 requires agents to handle ambiguities and noise, adapt to
dynamic environments, collaborate with other agents, and operate under temporal
constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new
failure modes that are invisible in static settings. Our experiments show that
no system dominates across the intelligence spectrum: stronger reasoning often
comes at the cost of efficiency, and budget scaling curves plateau,
highlighting the need for new architectures and adaptive compute strategies.
Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2
to other environments, empowering the community to rapidly create new
benchmarks tailored to their domains. In AI's second half, progress
increasingly depends on defining meaningful tasks and robust evaluations to
drive frontier capabilities forward.
\\ ( https://arxiv.org/abs/2509.17158 ,  8565kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17192
Date: Sun, 21 Sep 2025 18:37:17 GMT   (530kb)

Title: Shall We Play a Game? Language Models for Open-ended Wargames
Authors: Glenn Matlin, Parv Mahajan, Isaac Song, Yixiong Hao, Ryan Bard, Stu
  Topp, Evan Montoya, M. Rehan Parwani, Soham Shetty, Mark Riedl
Categories: cs.AI
\\
  Wargames are multi-faceted, multi-player depictions of conflict in which
participants' decisions influence future events. Wargames are often used to
explore the strategic implications of decision-making. However, it also
encompasses entertainment-oriented simulations, ranging from _Chess_ to
tabletop role-playing games like _Dungeons & Dragons_ (D&D). On the more
open-ended side of the spectrum of wargames, players use natural language to
convey their moves, and adjudicators propose outcomes. Language Models (LMs)
are increasingly being considered for how they can provide insights into
real-world, consequential decisions. We conduct a scoping literature review of
a curated selection of 100 recent works on AI in wargames, from which we
construct an ontology of wargames in terms of the creativity afforded to either
the players or adjudicators. Focusing on the space of wargames with the most
open-endedness for players and adjudicators, we distill a set of considerations
for when and how to use LMs in different application areas. We also present a
set of safety considerations, best practices for deploying LMs in open-ended
wargames, and conclude with a set of high-impact open research challenges.
\\ ( https://arxiv.org/abs/2509.17192 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17238
Date: Sun, 21 Sep 2025 21:05:29 GMT   (516kb)

Title: MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with
  RoE
Authors: Soheil Zibakhsh, Mohammad Samragh, Kumari Nishu, Lauren Hannah, Arnav
  Kundu, Minsik Cho
Categories: cs.AI cs.CL cs.ET cs.LG
\\
  The generation quality of large language models (LLMs) is often improved by
utilizing inference-time sequence-level scaling methods (e.g.,
Chain-of-Thought). We introduce hyper-parallel scaling, a complementary
framework that improves prediction quality at the token level. Hyper-parallel
scaling computes and aggregates multiple output proposals for a single token
from the model. We implement this concept in Mixture-of-Experts (MoE) models,
which we refer to as Roster of Experts (RoE). RoE is a training-free inference
algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects
controlled stochasticity into the expert routing mechanism, enabling it to
sample multiple diverse experts for each token and aggregate their outputs for
a more accurate final prediction.To overcome the computational cost, we
introduce an efficient batching strategy and a specialized KV-caching mechanism
that minimizes compute and memory overhead. For example, RoE enables a 7B MoE
model to match the performance of a 10.5B MoE model while using 30% less
compute for inference. These gains are achieved without any fine-tuning of
model parameters.
\\ ( https://arxiv.org/abs/2509.17238 ,  516kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17240
Date: Sun, 21 Sep 2025 21:17:23 GMT   (1852kb)

Title: Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with
  LLM-based Multi-Agent System
Authors: Abdullah Mushtaq, Muhammad Rafay Naeem, Ibrahim Ghaznavi, Alaa
  Abd-alrazaq, Aliya Tabassum, Junaid Qadir
Categories: cs.AI cs.CL cs.LG cs.MA
\\
  Systematic Literature Reviews (SLRs) are foundational to evidence-based
research but remain labor-intensive and prone to inconsistency across
disciplines. We present an LLM-based SLR evaluation copilot built on a
Multi-Agent System (MAS) architecture to assist researchers in assessing the
overall quality of the systematic literature reviews. The system automates
protocol validation, methodological assessment, and topic relevance checks
using a scholarly database. Unlike conventional single-agent methods, our
design integrates a specialized agentic approach aligned with PRISMA guidelines
to support more structured and interpretable evaluations. We conducted an
initial study on five published SLRs from diverse domains, comparing system
outputs to expert-annotated PRISMA scores, and observed 84% agreement. While
early results are promising, this work represents a first step toward scalable
and accurate NLP-driven systems for interdisciplinary workflows and reveals
their capacity for rigorous, domain-agnostic knowledge aggregation to
streamline the review process.
\\ ( https://arxiv.org/abs/2509.17240 ,  1852kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17259
Date: Sun, 21 Sep 2025 22:18:34 GMT   (4772kb)

Title: Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with
  Action-Graph Observability on GPT-OSS-20B
Authors: Ilham Wicaksono, Zekun Wu, Rahul Patel, Theo King, Adriano Koshiyama,
  Philip Treleaven
Categories: cs.AI
Comments: Winner of the OpenAI GPT-OSS-20B Red Teaming Challenge (Kaggle, 2025)
\\
  As the industry increasingly adopts agentic AI systems, understanding their
unique vulnerabilities becomes critical. Prior research suggests that security
flaws at the model level do not fully capture the risks present in agentic
deployments, where models interact with tools and external environments. This
paper investigates this gap by conducting a comparative red teaming analysis of
GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability
framework AgentSeer to deconstruct agentic systems into granular actions and
components, we apply iterative red teaming attacks with harmful objectives from
HarmBench at two distinct levels: the standalone model and the model operating
within an agentic loop. Our evaluation reveals fundamental differences between
model level and agentic level vulnerability profiles. Critically, we discover
the existence of agentic-only vulnerabilities, attack vectors that emerge
exclusively within agentic execution contexts while remaining inert against
standalone models. Agentic level iterative attacks successfully compromise
objectives that completely failed at the model level, with tool-calling
contexts showing 24\% higher vulnerability than non-tool contexts. Conversely,
certain model-specific exploits work exclusively at the model level and fail
when transferred to agentic contexts, demonstrating that standalone model
vulnerabilities do not always generalize to deployed systems.
\\ ( https://arxiv.org/abs/2509.17259 ,  4772kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17318
Date: Mon, 22 Sep 2025 02:48:50 GMT   (3533kb)

Title: CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning
  in Large Language Models
Authors: Zhuofan Chen, Jiyuan He, Yichi Zhang, Xing Hu, Haoxing Wen, Jun Bai,
  Wenge Rong
Categories: cs.AI cs.CL cs.LG
\\
  Mathematical reasoning poses significant challenges for Large Language Models
(LLMs) due to its demand for multi-step reasoning and abstract conceptual
integration. While recent test-time scaling techniques rely heavily on
high-quality, challenging problems, the scarcity of Olympiad-level math
problems remains a bottleneck. We introduce CogAtom, a novel cognitive
atom-based framework for synthesizing mathematically rigorous and cognitively
diverse problems. Unlike prior approaches, CogAtom models problem construction
as a process of selecting and recombining fundamental reasoning units,
cognitive atoms, extracted from human-authored solutions. A diversity-promoting
random walk algorithm enables exploration of the cognitive atom space, while a
constraint-based recombination mechanism ensures logical soundness and
structural validity. The combinatorial nature of the graph structure provides a
near-infinite space of reasoning paths, and the walk algorithm systematically
explores this space to achieve large-scale synthesis of high-quality problems;
meanwhile, by controlling the number of cognitive atoms, we can precisely
adjust problem difficulty, ensuring diversity, scalability, and controllability
of the generated problems. Experimental results demonstrate that CogAtom
outperforms existing methods in accuracy, reasoning depth, and diversity,
generating problems that closely match the difficulty of AIME while exceeding
it in structural variation. Our work offers a cognitively grounded pathway
toward scalable, high-quality math problem generation.Our code is publicly
available at https://github.com/Icarus-1111/CogAtom.
\\ ( https://arxiv.org/abs/2509.17318 ,  3533kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17337
Date: Mon, 22 Sep 2025 03:14:22 GMT   (399kb)

Title: LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about
  Source Code
Authors: Ala Jararweh, Michael Adams, Avinash Sahu, Abdullah Mueen, Afsah Anwar
Categories: cs.AI cs.CL
Journal-ref: A. Jararweh, M. Adams, A. Sahu, A. Mueen and A. Anwar, "LLaVul: A
  Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code,"
  2025 5th Intelligent Cybersecurity Conference (ICSC), Tampa, FL, USA, 2025,
  pp. 232-241
DOI: 10.1109/ICSC65596.2025.11140501
\\
  Increasing complexity in software systems places a growing demand on
reasoning tools that unlock vulnerabilities manifest in source code. Many
current approaches focus on vulnerability analysis as a classifying task,
oversimplifying the nuanced and context-dependent real-world scenarios. Even
though current code large language models (LLMs) excel in code understanding,
they often pay little attention to security-specific reasoning. We propose
LLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code
through question-answering (QA). Our model is trained to integrate paired code
and natural queries into a unified space, enhancing reasoning and
context-dependent insights about code vulnerability. To evaluate our model
performance, we construct a curated dataset of real-world vulnerabilities
paired with security-focused questions and answers. Our model outperforms
state-of-the-art general-purpose and code LLMs in the QA and detection tasks.
We further explain decision-making by conducting qualitative analysis to
highlight capabilities and limitations. By integrating code and QA, LLaVul
enables more interpretable and security-focused code understanding.
\\ ( https://arxiv.org/abs/2509.17337 ,  399kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17353
Date: Mon, 22 Sep 2025 04:31:27 GMT   (8601kb)

Title: Medical AI Consensus: A Multi-Agent Framework for Radiology Report
  Generation and Evaluation
Authors: Ahmed T. Elboardy, Ghada Khoriba, Essam A. Rashed
Categories: cs.AI eess.IV physics.med-ph
Comments: NeurIPS2025 Workshop: Evaluating the Evolving LLM Lifecycle:
  Benchmarks, Emergent Abilities, and Scaling
\\
  Automating radiology report generation poses a dual challenge: building
clinically reliable systems and designing rigorous evaluation protocols. We
introduce a multi-agent reinforcement learning framework that serves as both a
benchmark and evaluation environment for multimodal clinical reasoning in the
radiology ecosystem. The proposed framework integrates large language models
(LLMs) and large vision models (LVMs) within a modular architecture composed of
ten specialized agents responsible for image analysis, feature extraction,
report generation, review, and evaluation. This design enables fine-grained
assessment at both the agent level (e.g., detection and segmentation accuracy)
and the consensus level (e.g., report quality and clinical relevance). We
demonstrate an implementation using chatGPT-4o on public radiology datasets,
where LLMs act as evaluators alongside medical radiologist feedback. By
aligning evaluation protocols with the LLM development lifecycle, including
pretraining, finetuning, alignment, and deployment, the proposed benchmark
establishes a path toward trustworthy deviance-based radiology report
generation.
\\ ( https://arxiv.org/abs/2509.17353 ,  8601kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17354
Date: Mon, 22 Sep 2025 05:17:54 GMT   (1117kb)

Title: Multi-Scenario Highway Lane-Change Intention Prediction: A
  Physics-Informed AI Framework for Three-Class Classification
Authors: Jiazhao Shi, Yichen Lin, Yiheng Hua, Ziyu Wang, Zijian Zhang, Wenjia
  Zheng, Yun Song, Kuan Lu, and Shoufeng Lu
Categories: cs.AI cs.LG
\\
  Lane-change maneuvers are a leading cause of highway accidents, underscoring
the need for accurate intention prediction to improve the safety and
decision-making of autonomous driving systems. While prior studies using
machine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers)
have shown promise, most approaches remain limited by binary classification,
lack of scenario diversity, and degraded performance under longer prediction
horizons. In this study, we propose a physics-informed AI framework that
explicitly integrates vehicle kinematics, interaction feasibility, and
traffic-safety metrics (e.g., distance headway, time headway,
time-to-collision, closing gap time) into the learning process. lane-change
prediction is formulated as a three-class problem that distinguishes left
change, right change, and no change, and is evaluated across both straight
highway segments (highD) and complex ramp scenarios (exiD). By integrating
vehicle kinematics with interaction features, our machine learning models,
particularly LightGBM, achieve state-of-the-art accuracy and strong
generalization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD,
and 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon,
outperforming a two-layer stacked LSTM baseline. These findings demonstrate the
practical advantages of a physics-informed and feature-rich machine learning
framework for real-time lane-change intention prediction in autonomous driving
systems.
\\ ( https://arxiv.org/abs/2509.17354 ,  1117kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17380
Date: Mon, 22 Sep 2025 06:44:44 GMT   (940kb)

Title: Correlation or Causation: Analyzing the Causal Structures of LLM and LRM
  Reasoning Process
Authors: Zhizhang FU, Guangsheng Bao, Hongbo Zhang, Chenkai Hu, Yue Zhang
Categories: cs.AI
\\
  LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and
inconsistency, since they lack robust causal underpinnings and may rely on
superficial correlations rather than genuine understanding. Successive LRMs
have emerged as a promising alternative, leveraging advanced training
techniques such as reinforcement learning (RL) and distillation to improve task
accuracy. However, the impact of these training methods on causality remains
largely unexplored. In this study, we conduct a systematic causal analysis on
LLMs and LRMs, examining structural causal models (SCMs) of four key variables:
problem instruction (Z), thinking process (T), reasoning steps (X), and answer
(Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal
reasoning capabilities, aligning more closely with ideal causal structures,
while LLMs and distilled LRMs fail to address causality-related deficiencies.
Our further investigation indicates that RLVR reduces spurious correlations and
strengthens genuine causal patterns, thereby mitigating unfaithfulness and
bias. In addition, our inspection on the dynamics of the RLVR training process
observes a high correlation between reduced spurious features and improved
causal structures, where the causal relationships consistently improve in the
training process. This study contributes to the understanding of causality in
reasoning models, highlights the critical role of RLVR in enhancing causal
reasoning, and provides insights for designing future AI systems with stronger
causal foundations. We release our code and data at
https://github.com/Harryking1999/CoT_Causal_Analysis.
\\ ( https://arxiv.org/abs/2509.17380 ,  940kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17393
Date: Mon, 22 Sep 2025 06:53:32 GMT   (313kb)

Title: Program Synthesis via Test-Time Transduction
Authors: Kang-il Lee, Jahyun Koo, Seunghyun Yoon, Minbeom Kim, Hyukhun Koh,
  Dongryeol Lee, Kyomin Jung
Categories: cs.AI cs.CL
Comments: NeurIPS 2025
\\
  We introduce transductive program synthesis, a new formulation of the program
synthesis task that explicitly leverages test inputs during synthesis. While
prior approaches to program synthesis--whether based on natural language
descriptions or input-output examples--typically aim to generalize from
training examples, they often struggle with robustness, especially in
real-world settings where training examples are limited and test inputs involve
various edge cases. To address this, we propose a novel framework that improves
robustness by treating synthesis as an active learning over a finite hypothesis
class defined by programs' outputs. We use an LLM to predict outputs for
selected test inputs and eliminate inconsistent hypotheses, where the inputs
are chosen via a greedy maximin algorithm to minimize the number of LLM queries
required. We evaluate our approach on two real-world datasets: Playgol, a
string transformation benchmark, and MBPP+, a Python code generation benchmark.
We demonstrate that our method significantly improves program synthesis in both
accuracy and efficiency. We release our code at
https://github.com/klee972/SYNTRA.
\\ ( https://arxiv.org/abs/2509.17393 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17425
Date: Mon, 22 Sep 2025 07:17:26 GMT   (4514kb)

Title: Evaluating Multimodal Large Language Models with Daily Composite Tasks
  in Home Environments
Authors: Zhenliang Zhang, Yuxi Wang, Hongzhao Xie, Shiyun Zhao, Mingyuan Liu,
  Yujie Lu, Xinyi He, Zhenku Cheng, Yujia Peng
Categories: cs.AI
\\
  A key feature differentiating artificial general intelligence (AGI) from
traditional AI is that AGI can perform composite tasks that require a wide
range of capabilities. Although embodied agents powered by multimodal large
language models (MLLMs) offer rich perceptual and interactive capabilities, it
remains largely unexplored whether they can solve composite tasks. In the
current work, we designed a set of composite tasks inspired by common daily
activities observed in early childhood development. Within a dynamic and
simulated home environment, these tasks span three core domains: object
understanding, spatial intelligence, and social activity. We evaluated 17
leading proprietary and open-source MLLMs on these tasks. The results
consistently showed poor performance across all three domains, indicating a
substantial gap between current capabilities and general intelligence
requirements. Together, our tasks offer a preliminary framework for evaluating
the general capabilities of embodied agents, marking an early but significant
step toward the development of embodied MLLMs and their real-world deployment.
\\ ( https://arxiv.org/abs/2509.17425 ,  4514kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17439
Date: Mon, 22 Sep 2025 07:28:22 GMT   (40079kb)

Title: SPICED: A Synaptic Homeostasis-Inspired Framework for Unsupervised
  Continual EEG Decoding
Authors: Yangxuan Zhou, Sha Zhao, Jiquan Wang, Haiteng Jiang, Shijian Li, Tao
  Li, Gang Pan
Categories: cs.AI cs.LG
Comments: 21 pages, 13 figures
\\
  Human brain achieves dynamic stability-plasticity balance through synaptic
homeostasis. Inspired by this biological principle, we propose SPICED: a
neuromorphic framework that integrates the synaptic homeostasis mechanism for
unsupervised continual EEG decoding, particularly addressing practical
scenarios where new individuals with inter-individual variability emerge
continually. SPICED comprises a novel synaptic network that enables dynamic
expansion during continual adaptation through three bio-inspired neural
mechanisms: (1) critical memory reactivation; (2) synaptic consolidation and
(3) synaptic renormalization. The interplay within synaptic homeostasis
dynamically strengthens task-discriminative memory traces and weakens
detrimental memories. By integrating these mechanisms with continual learning
system, SPICED preferentially replays task-discriminative memory traces that
exhibit strong associations with newly emerging individuals, thereby achieving
robust adaptations. Meanwhile, SPICED effectively mitigates catastrophic
forgetting by suppressing the replay prioritization of detrimental memories
during long-term continual learning. Validated on three EEG datasets, SPICED
show its effectiveness.
\\ ( https://arxiv.org/abs/2509.17439 ,  40079kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17460
Date: Mon, 22 Sep 2025 07:54:58 GMT   (23529kb)

Title: AI Pangaea: Unifying Intelligence Islands for Adapting Myriad Tasks
Authors: Jianlong Chang, Haixin Wang, Zhiyuan Dang, Li Huang, Zhiyu Wang, Ruoqi
  Cao, Shihao Piao, Dongzhe Li, Dianyu Gao, Dongsheng Wang, Yin Li, Jinan Sun,
  Lu Fang, Zhouchen Lin
Categories: cs.AI cs.LG
Comments: 65 pages, 28 figures, paper under review
\\
  The pursuit of artificial general intelligence continuously demands
generalization in one model across myriad tasks, even those not seen before.
However, current AI models are isolated from each other for being limited to
specific tasks, now first defined as Intelligence Islands. To unify
Intelligence Islands into one, we propose Pangaea, the first AI supercontinent
akin to the geological Pangaea. Pangaea encodes any data into a unified format
and accumulates universal knowledge through pre-training on 296 datasets across
diverse modalities. Eventually, it demonstrates remarkable generalization
across 45 general tasks and 15 scientific tasks encompassing a wide range of
scientific subjects. By investigating Pangaea deeper, the scaling effect of
modality is revealed, quantifying the universal knowledge accumulation across
modalities as the cumulative distribution function of a geometric distribution.
On the whole, Pangaea shows strong potential to handle myriad tasks, indicating
a new direction toward artificial general intelligence.
\\ ( https://arxiv.org/abs/2509.17460 ,  23529kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17544
Date: Mon, 22 Sep 2025 09:02:53 GMT   (4905kb)

Title: A Multimodal Conversational Assistant for the Characterization of
  Agricultural Plots from Geospatial Open Data
Authors: Juan Ca\~nada, Ra\'ul Alonso, Julio Molleda, Fidel D\'iez
Categories: cs.AI
\\
  The increasing availability of open Earth Observation (EO) and agricultural
datasets holds great potential for supporting sustainable land management.
However, their high technical entry barrier limits accessibility for non-expert
users. This study presents an open-source conversational assistant that
integrates multimodal retrieval and large language models (LLMs) to enable
natural language interaction with heterogeneous agricultural and geospatial
data. The proposed architecture combines orthophotos, Sentinel-2 vegetation
indices, and user-provided documents through retrieval-augmented generation
(RAG), allowing the system to flexibly determine whether to rely on multimodal
evidence, textual knowledge, or both in formulating an answer. To assess
response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a
zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional
quantitative evaluation framework. Preliminary results show that the system is
capable of generating clear, relevant, and context-aware responses to
agricultural queries, while remaining reproducible and scalable across
geographic regions. The primary contributions of this work include an
architecture for fusing multimodal EO and textual knowledge sources, a
demonstration of lowering the barrier to access specialized agricultural
information through natural language interaction, and an open and reproducible
design.
\\ ( https://arxiv.org/abs/2509.17544 ,  4905kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17550
Date: Mon, 22 Sep 2025 09:09:13 GMT   (2216kb)

Title: Is It Certainly a Deepfake? Reliability Analysis in Detection &
  Generation Ecosystem
Authors: Neslihan Kose, Anthony Rhodes, Umur Aybars Ciftci, Ilke Demir
Categories: cs.AI cs.CV cs.LG
Comments: Accepted for publication at the ICCV 2025 STREAM workshop
\\
  As generative models are advancing in quality and quantity for creating
synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors
are proposed to counter this effect, however, misuse of detectors claiming fake
content as real or vice versa further fuels this misinformation problem. We
present the first comprehensive uncertainty analysis of deepfake detectors,
systematically investigating how generative artifacts influence prediction
confidence. As reflected in detectors' responses, deepfake generators also
contribute to this uncertainty as their generative residues vary, so we cross
the uncertainty analysis of deepfake detectors and generators. Based on our
observations, the uncertainty manifold holds enough consistent information to
leverage uncertainty for deepfake source detection. Our approach leverages
Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and
epistemic uncertainties across diverse detector architectures. We evaluate
uncertainty on two datasets with nine generators, with four blind and two
biological detectors, compare different uncertainty methods, explore region-
and pixel-based uncertainty, and conduct ablation studies. We conduct and
analyze binary real/fake, multi-class real/fake, source detection, and
leave-one-out experiments between the generator/detector combinations to share
their generalization capability, model calibration, uncertainty, and robustness
against adversarial attacks. We further introduce uncertainty maps that
localize prediction confidence at the pixel level, revealing distinct patterns
correlated with generator-specific artifacts. Our analysis provides critical
insights for deploying reliable deepfake detection systems and establishes
uncertainty quantification as a fundamental requirement for trustworthy
synthetic media detection.
\\ ( https://arxiv.org/abs/2509.17550 ,  2216kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17553
Date: Mon, 22 Sep 2025 09:17:41 GMT   (862kb)

Title: MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target
  Data Instances
Authors: Congcong Ge, Yachuan Liu, Yixuan Tang, Yifan Zhu, Yaofeng Tu, Yunjun
  Gao
Categories: cs.AI cs.DB cs.LG
\\
  In commercial systems, a pervasive requirement for automatic data preparation
(ADP) is to transfer relational data from disparate sources to targets with
standardized schema specifications. Previous methods rely on labor-intensive
supervision signals or target table data access permissions, limiting their
usage in real-world scenarios. To tackle these challenges, we propose an
effective end-to-end ADP framework MontePrep, which enables training-free
pipeline synthesis with zero target-instance requirements. MontePrep is
formulated as an open-source large language model (LLM) powered tree-structured
search problem. It consists of three pivot components, i.e., a data preparation
action sandbox (DPAS), a fundamental pipeline generator (FPG), and an
execution-aware pipeline optimizer (EPO). We first introduce DPAS, a
lightweight action sandbox, to navigate the search-based pipeline generation.
The design of DPAS circumvents exploration of infeasible pipelines. Then, we
present FPG to build executable DP pipelines incrementally, which explores the
predefined action sandbox by the LLM-powered Monte Carlo Tree Search.
Furthermore, we propose EPO, which invokes pipeline execution results from
sources to targets to evaluate the reliability of the generated pipelines in
FPG. In this way, unreasonable pipelines are eliminated, thus facilitating the
search process from both efficiency and effectiveness perspectives. Extensive
experimental results demonstrate the superiority of MontePrep with significant
improvement against five state-of-the-art competitors.
\\ ( https://arxiv.org/abs/2509.17553 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17567
Date: Mon, 22 Sep 2025 10:59:32 GMT   (1655kb)

Title: LIMI: Less is More for Agency
Authors: Yang Xiao, Mohan Jiang, Jie Sun, Keyu Li, Jifan Lin, Yumin Zhuang, Ji
  Zeng, Shijie Xia, Qishuo Hua, Xuefeng Li, Xiaojie Cai, Tongyu Wang, Yue
  Zhang, Liming Liu, Xia Wu, Jinlong Hou, Yuan Cheng, Wenjie Li, Xiang Wang,
  Dequan Wang, Pengfei Liu
Categories: cs.AI
\\
  We define Agency as the emergent capacity of AI systems to function as
autonomous agents actively discovering problems, formulating hypotheses, and
executing solutions through self-directed engagement with environments and
tools. This fundamental capability marks the dawn of the Age of AI Agency,
driven by a critical industry shift: the urgent need for AI systems that don't
just think, but work. While current AI excels at reasoning and generating
responses, industries demand autonomous agents that can execute tasks, operate
tools, and drive real-world outcomes. As agentic intelligence becomes the
defining characteristic separating cognitive systems from productive workers,
efficiently cultivating machine autonomy becomes paramount. Current approaches
assume that more data yields better agency, following traditional scaling laws
from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is
More for Intelligent Agency) demonstrates that agency follows radically
different development principles. Through strategic focus on collaborative
software development and scientific research workflows, we show that
sophisticated agentic intelligence can emerge from minimal but strategically
curated demonstrations of autonomous behavior. Using only 78 carefully designed
training samples, LIMI achieves 73.5% on comprehensive agency benchmarks,
dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),
DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).
Most strikingly, LIMI demonstrates 53.7% improvement over models trained on
10,000 samples-achieving superior agentic intelligence with 128 times fewer
samples. Our findings establish the Agency Efficiency Principle: machine
autonomy emerges not from data abundance but from strategic curation of
high-quality agentic demonstrations.
\\ ( https://arxiv.org/abs/2509.17567 ,  1655kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17589
Date: Mon, 22 Sep 2025 11:13:48 GMT   (1199kb)

Title: Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images
  via Reinforced Multimodal Language Models
Authors: Jun Ling, Yao Qi, Tao Huang, Shibo Zhou, Yanqin Huang, Jiang Yang,
  Ziqi Song, Ying Zhou, Yang Yang, Heng Tao Shen, Peng Wang
Categories: cs.AI
Comments: NeurIPS 2025
\\
  In this work, we address the task of table image to LaTeX code generation,
with the goal of automating the reconstruction of high-quality,
publication-ready tables from visual inputs. A central challenge of this task
lies in accurately handling complex tables -- those with large sizes, deeply
nested structures, and semantically rich or irregular cell content -- where
existing methods often fail. We begin with a comprehensive analysis,
identifying key challenges and highlighting the limitations of current
evaluation protocols. To overcome these issues, we propose a reinforced
multimodal large language model (MLLM) framework, where a pre-trained MLLM is
fine-tuned on a large-scale table-to-LaTeX dataset. To further improve
generation quality, we introduce a dual-reward reinforcement learning strategy
based on Group Relative Policy Optimization (GRPO). Unlike standard approaches
that optimize purely over text outputs, our method incorporates both a
structure-level reward on LaTeX code and a visual fidelity reward computed from
rendered outputs, enabling direct optimization of the visual output quality. We
adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and
show that our method achieves state-of-the-art performance, particularly on
structurally complex tables, demonstrating the effectiveness and robustness of
our approach.
\\ ( https://arxiv.org/abs/2509.17589 ,  1199kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17677
Date: Mon, 22 Sep 2025 12:20:27 GMT   (1120kb)

Title: EngiBench: A Benchmark for Evaluating Large Language Models on
  Engineering Problem Solving
Authors: Xiyuan Zhou, Xinlei Wang, Yirui He, Yang Wu, Ruixi Zou, Yuheng Cheng,
  Yulu Xie, Wenxuan Liu, Huan Zhao, Yan Xu, Jinjin Gu, and Junhua Zhao
Categories: cs.AI
\\
  Large language models (LLMs) have shown strong performance on mathematical
reasoning under well-posed conditions. However, real-world engineering problems
require more than mathematical symbolic computation -- they need to deal with
uncertainty, context, and open-ended scenarios. Existing benchmarks fail to
capture these complexities. We introduce EngiBench, a hierarchical benchmark
designed to evaluate LLMs on solving engineering problems. It spans three
levels of increasing difficulty (foundational knowledge retrieval, multi-step
contextual reasoning, and open-ended modeling) and covers diverse engineering
subfields. To facilitate a deeper understanding of model performance, we
systematically rewrite each problem into three controlled variants (perturbed,
knowledge-enhanced, and math abstraction), enabling us to separately evaluate
the model's robustness, domain-specific knowledge, and mathematical reasoning
abilities. Experiment results reveal a clear performance gap across levels:
models struggle more as tasks get harder, perform worse when problems are
slightly changed, and fall far behind human experts on the high-level
engineering tasks. These findings reveal that current LLMs still lack the
high-level reasoning needed for real-world engineering, highlighting the need
for future models with deeper and more reliable problem-solving capabilities.
Our source code and data are available at
https://github.com/EngiBench/EngiBench.
\\ ( https://arxiv.org/abs/2509.17677 ,  1120kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17706
Date: Mon, 22 Sep 2025 12:44:52 GMT   (121kb)

Title: Virtual Arc Consistency for Linear Constraints inCost Function Networks
Authors: Pierre Montalbano, Simon de Givry and George Katsirelos
Categories: cs.AI
\\
  In Constraint Programming, solving discrete minimization problems with hard
and soft constraints can be done either using (i) soft global constraints, (ii)
a reformulation into a linear program, or (iii) a reformulation into local cost
functions. Approach (i) benefits from a vast catalog of constraints. Each soft
constraint propagator communicates with other soft constraints only through the
variable domains, resulting in weak lower bounds. Conversely, the approach (ii)
provides a global view with strong bounds, but the size of the reformulation
can be problematic. We focus on approach (iii) in which soft arc consistency
(SAC) algorithms produce bounds of intermediate quality. Recently, the
introduction of linear constraints as local cost functions increases their
modeling expressiveness. We adapt an existing SAC algorithm to handle linear
constraints. We show that our algorithm significantly improves the lower bounds
compared to the original algorithm on several benchmarks, reducing solving time
in some cases.
\\ ( https://arxiv.org/abs/2509.17706 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17711
Date: Mon, 22 Sep 2025 12:48:42 GMT   (141kb)

Title: DA-Mamba: Dialogue-aware selective state-space model for multimodal
  engagement estimation
Authors: Shenwei Kang, Xin Zhang, Wen Liu, Bin Li, Yujie Liu, Bo Gao
Categories: cs.AI
\\
  Human engagement estimation in conversational scenarios is essential for
applications such as adaptive tutoring, remote healthcare assessment, and
socially aware human--computer interaction. Engagement is a dynamic, multimodal
signal conveyed by facial expressions, speech, gestures, and behavioral cues
over time. In this work we introduce DA-Mamba, a dialogue-aware multimodal
architecture that replaces attention-heavy dialogue encoders with Mamba-based
selective state-space processing to achieve linear time and memory complexity
while retaining expressive cross-modal reasoning. We design a Mamba
dialogue-aware selective state-space model composed of three core modules: a
Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group
Fusion and Partner-Group Fusion, these modules achieve expressive dialogue
understanding. Extensive experiments on three standard benchmarks (NoXi,
NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art
(SOTA) methods in concordance correlation coefficient (CCC), while reducing
training time and peak memory; these gains enable processing much longer
sequences and facilitate real-time deployment in resource-constrained,
multi-party conversational settings. The source code will be available at:
https://github.com/kksssssss-ssda/MMEA.
\\ ( https://arxiv.org/abs/2509.17711 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17774
Date: Mon, 22 Sep 2025 13:37:52 GMT   (75kb)

Title: Efficient & Correct Predictive Equivalence for Decision Trees
Authors: Joao Marques-Silva and Alexey Ignatiev
Categories: cs.AI cs.LG cs.LO
\\
  The Rashomon set of decision trees (DTs) finds importance uses. Recent work
showed that DTs computing the same classification function, i.e. predictive
equivalent DTs, can represent a significant fraction of the Rashomon set. Such
redundancy is undesirable. For example, feature importance based on the
Rashomon set becomes inaccurate due the existence of predictive equivalent DTs,
i.e. DTs with the same prediction for every possible input. In recent work,
McTavish et al. proposed solutions for several computational problems related
with DTs, including that of deciding predictive equivalent DTs. This approach,
which this paper refers to as MBDSR, consists of applying the well-known method
of Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal
form) representations of DTs, which are then used for comparing DTs for
predictive equivalence. Furthermore, the minimum-size DNF representation was
also applied to computing explanations for the predictions made by DTs, and to
finding predictions in the presence of missing data. However, the problem of
formula minimization is hard for the second level of the polynomial hierarchy,
and the QM method may exhibit worst-case exponential running time and space.
This paper first demonstrates that there exist decision trees that trigger the
worst-case exponential running time and space of the QM method. Second, the
paper shows that the MBDSR approach can produce incorrect results for the
problem of deciding predictive equivalence. Third, the paper shows that any of
the problems to which the minimum-size DNF representation has been applied to
can in fact be solved in polynomial time, in the size of the DT. The
experiments confirm that, for DTs for which the the worst-case of the QM method
is triggered, the algorithms proposed in this paper are orders of magnitude
faster than the ones proposed by McTavish et al.
\\ ( https://arxiv.org/abs/2509.17774 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17905
Date: Mon, 22 Sep 2025 15:30:56 GMT   (440kb)

Title: Mitigating Strategy-Selection Bias in Reasoning for More Effective
  Test-Time Scaling
Authors: Zongqian Wu, Baoduo Xu, Tianyu Li, Zhu Sun, Xiaofeng Zhu, Lei Feng
Categories: cs.AI
Comments: 23 pages, 9 figures
\\
  Test-time scaling (TTS) has been shown to improve the performance of large
language models (LLMs) by sampling and aggregating diverse reasoning paths.
However, existing research has overlooked a critical issue: selection bias of
reasoning strategies during scaling. Specifically, when generating reasoning
processes, LLMs tend to follow certain strategies (e.g., algebraic solutions
for math problems) while neglecting other valid alternatives (e.g., geometric
solutions), resulting in insufficient exploration of the solution space. To
further understand the impact of this bias, we present a theoretical analysis
that reveals when it undermines the effectiveness of test-time scaling.
Motivated by this theoretical insight, we introduce TTS-Uniform, a framework
designed to mitigate the selection bias of reasoning strategies. It (i)
identifies potential strategies, (ii) uniformly allocates the sampling budget
across them, and (iii) filters out unstable strategies prior to aggregation.
Experimental results show that TTS-Uniform significantly enhances scaling
effectiveness across multiple mainstream LLMs and benchmark datasets.
\\ ( https://arxiv.org/abs/2509.17905 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17907
Date: Mon, 22 Sep 2025 15:32:42 GMT   (46456kb)

Title: MEF: A Systematic Evaluation Framework for Text-to-Image Models
Authors: Xiaojing Dong, Weilin Huang, Liang Li, Yiying Li, Shu Liu, Tongtong
  Ou, Shuang Ouyang, Yu Tian, Fengxuan Zhao
Categories: cs.AI
\\
  Rapid advances in text-to-image (T2I) generation have raised higher
requirements for evaluation methodologies. Existing benchmarks center on
objective capabilities and dimensions, but lack an application-scenario
perspective, limiting external validity. Moreover, current evaluations
typically rely on either ELO for overall ranking or MOS for dimension-specific
scoring, yet both methods have inherent shortcomings and limited
interpretability. Therefore, we introduce the Magic Evaluation Framework (MEF),
a systematic and practical approach for evaluating T2I models. First, we
propose a structured taxonomy encompassing user scenarios, elements, element
compositions, and text expression forms to construct the Magic-Bench-377, which
supports label-level assessment and ensures a balanced coverage of both user
scenarios and capabilities. On this basis, we combine ELO and
dimension-specific MOS to generate model rankings and fine-grained assessments
respectively. This joint evaluation method further enables us to quantitatively
analyze the contribution of each dimension to user satisfaction using
multivariate logistic regression. By applying MEF to current T2I models, we
obtain a leaderboard and key characteristics of the leading models. We release
our evaluation framework and make Magic-Bench-377 fully open-source to advance
research in the evaluation of visual generative models.
\\ ( https://arxiv.org/abs/2509.17907 ,  46456kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17917
Date: Mon, 22 Sep 2025 15:40:31 GMT   (1093kb)

Title: Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent
Authors: Junyu Lu, Songxin Zhang, Zejian Xie, Zhuoyang Song, Jiaxing Zhang
Categories: cs.AI
\\
  Recent advances in GUI agents have achieved remarkable grounding and
action-prediction performance, yet existing models struggle with unreliable
reward signals and limited online trajectory generation. In this paper, we
introduce Orcust, a framework that integrates Principle-Constrained Reward
Modeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to
enhance reasoning reliability and data efficiency in interactive GUI tasks. We
leverages environment-verifiable and LLM-derived principle to enforce
interpretable reward signals that constrain long chain-of-thought reasoning and
rule-based feedback. OVTC spins up instrumented virtual machines to
autonomously collect structured GUI interaction trajectories with explicit
procedural and structural objectives, enabling the training of a stepwise
reward model that robustly captures human preferences and adheres to
task-specific constraints. Extensive experiments on standard GUI benchmarks
covering perceptual grounding, foundational operations, and end-to-end task
execution reveal that Orcust achieves state-of-the-art performance, improving
by 22.2\% on ScreenSpot and 23.9\% on ScreenSpot-Pro over the base model (i.e.
Qwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the
reasoning, adaptability and scalability of GUI agents across various
environments and task complexities.
\\ ( https://arxiv.org/abs/2509.17917 ,  1093kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17956
Date: Mon, 22 Sep 2025 16:12:12 GMT   (9258kb)

Title: "I think this is fair'': Uncovering the Complexities of Stakeholder
  Decision-Making in AI Fairness Assessment
Authors: Lin Luo, Yuri Nakao, Mathieu Chollet, Hiroya Inakoshi, Simone Stumpf
Categories: cs.AI cs.HC
\\
  Assessing fairness in artificial intelligence (AI) typically involves AI
experts who select protected features, fairness metrics, and set fairness
thresholds. However, little is known about how stakeholders, particularly those
affected by AI outcomes but lacking AI expertise, assess fairness. To address
this gap, we conducted a qualitative study with 30 stakeholders without AI
expertise, representing potential decision subjects in a credit rating
scenario, to examine how they assess fairness when placed in the role of
deciding on features with priority, metrics, and thresholds. We reveal that
stakeholders' fairness decisions are more complex than typical AI expert
practices: they considered features far beyond legally protected features,
tailored metrics for specific contexts, set diverse yet stricter fairness
thresholds, and even preferred designing customized fairness. Our results
extend the understanding of how stakeholders can meaningfully contribute to AI
fairness governance and mitigation, underscoring the importance of
incorporating stakeholders' nuanced fairness judgments.
\\ ( https://arxiv.org/abs/2509.17956 ,  9258kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17957
Date: Mon, 22 Sep 2025 16:13:06 GMT   (4019kb)

Title: On the Variational Costs of Changing Our Minds
Authors: David Hyland and Mahault Albarracin
Categories: cs.AI cs.IT math.IT
Comments: Accepted as a full paper at the 6th International Workshop on Active
  Inference
\\
  The human mind is capable of extraordinary achievements, yet it often appears
to work against itself. It actively defends its cherished beliefs even in the
face of contradictory evidence, conveniently interprets information to conform
to desired narratives, and selectively searches for or avoids information to
suit its various purposes. Despite these behaviours deviating from common
normative standards for belief updating, we argue that such 'biases' are not
inherently cognitive flaws, but rather an adaptive response to the significant
pragmatic and cognitive costs associated with revising one's beliefs. This
paper introduces a formal framework that aims to model the influence of these
costs on our belief updating mechanisms.
  We treat belief updating as a motivated variational decision, where agents
weigh the perceived 'utility' of a belief against the informational cost
required to adopt a new belief state, quantified by the Kullback-Leibler
divergence from the prior to the variational posterior. We perform
computational experiments to demonstrate that simple instantiations of this
resource-rational model can be used to qualitatively emulate commonplace human
behaviours, including confirmation bias and attitude polarisation. In doing so,
we suggest that this framework makes steps toward a more holistic account of
the motivated Bayesian mechanics of belief change and provides practical
insights for predicting, compensating for, and correcting deviations from
desired belief updating processes.
\\ ( https://arxiv.org/abs/2509.17957 ,  4019kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17978
Date: Mon, 22 Sep 2025 16:24:17 GMT   (5688kb)

Title: The STAR-XAI Protocol: An Interactive Framework for Inducing
  Second-Order Agency in AI Agents
Authors: Antoni Guasch and Maria Isabel Valdez
Categories: cs.AI cs.LO
Comments: Paper 1 of 4 in The STAR-XAI Protocol series. Paper 2
  [arXiv:ID_to_be_added], Paper 3 [arXiv:ID_to_be_added], Paper 4
  [arXiv:ID_to_be_added]
\\
  Current Large Reasoning Models (LRMs) exhibit significant limitations in
reliability and transparency, often showing a collapse in reasoning
capabilities when faced with high-complexity, long-horizon tasks. This
"illusion of thinking" is frequently an artifact of non-agentic, black-box
evaluation paradigms that fail to cultivate robust problem-solving processes.
In response, we introduce The STAR-XAI Protocol (Socratic, Transparent,
Agentic, Reasoning - for eXplainable Artificial Intelligence), a novel
methodology for training and operating verifiably reliable AI agents. Our
method reframes the human-AI interaction as a structured, Socratic dialogue,
governed by an explicit and evolving rulebook, the Consciousness Transfer
Package (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc
strategic justification and a state-locking Checksum that prevents error
accumulation, the protocol transforms a powerful but opaque LRM into a
disciplined "Clear Box" agent. We demonstrate the efficacy of this method
through an exhaustive 25-move case study in the complex strategic game "Caps i
Caps". The agent not only solved the high-complexity puzzle but also
demonstrated Second-Order Agency, identifying flaws in its own
supervisor-approved plans and adapting its core integrity protocols mid-task.
The STAR-XAI Protocol offers a practical pathway to creating AI agents that are
not just high-performing, but also transparent, auditable, and trustworthy by
design.
\\ ( https://arxiv.org/abs/2509.17978 ,  5688kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18076
Date: Mon, 22 Sep 2025 17:55:14 GMT   (1555kb)

Title: Improving Large Language Models Function Calling and Interpretability
  via Guided-Structured Templates
Authors: Hy Dang, Tianyi Liu, Zhuofeng Wu, Jingfeng Yang, Haoming Jiang, Tao
  Yang, Pei Chen, Zhengyang Wang, Helen Wang, Huasheng Li, Bing Yin, Meng Jiang
Categories: cs.AI
Comments: Accepted to EMNLP 2025 Main Conference
\\
  Large language models (LLMs) have demonstrated strong reasoning and tool-use
capabilities, yet they often fail in real-world tool-interactions due to
incorrect parameterization, poor tool selection, or misinterpretation of user
intent. These issues often stem from an incomplete understanding of user goals
and inadequate comprehension of tool documentation. While Chain-of-Thought
(CoT) prompting has proven effective for enhancing reasoning in general
contexts, our analysis reveals that free-form CoT is insufficient and sometimes
counterproductive for structured function-calling tasks. To address this, we
introduce a curriculum-inspired framework that leverages structured reasoning
templates to guide LLMs through more deliberate step-by-step instructions for
generating function callings. Experimental results show that our method reduces
tool-use errors, achieving 3-12% relative improvements over strong baselines
across diverse model series and approaches. Moreover, our framework enhances
the robustness, interpretability, and transparency of tool-using agents,
advancing the development of more reliable AI assistants for real-world
applications.
\\ ( https://arxiv.org/abs/2509.18076 ,  1555kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18083
Date: Mon, 22 Sep 2025 17:56:38 GMT   (6958kb)

Title: Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning
Authors: Valentin Lacombe, Valentin Quesnel, Damien Sileo
Categories: cs.AI cs.CL
\\
  We introduce Reasoning Core, a new scalable environment for Reinforcement
Learning with Verifiable Rewards (RLVR), designed to advance foundational
symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks
that focus on games or isolated puzzles, Reasoning Core procedurally generates
problems across core formal domains, including PDDL planning, first-order
logic, context-free grammar parsing, causal reasoning, and system equation
solving. The environment is built on key design principles of high-generality
problem distributions, verification via external tools, and continuous
difficulty control, which together provide a virtually infinite supply of novel
training instances. Initial zero-shot evaluations with frontier LLMs confirm
the difficulty of Reasoning Core's tasks, positioning it as a promising
resource to improve the reasoning capabilities of future models.
\\ ( https://arxiv.org/abs/2509.18083 ,  6958kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16226
Date: Fri, 12 Sep 2025 10:11:52 GMT   (457kb)

Title: On LLM-Based Scientific Inductive Reasoning Beyond Equations
Authors: Brian S. Lin, Jiaxin Yuan, Zihan Zhou, Shouli Wang, Shuo Wang,
  Cunliang Kong, Qi Shi, Yuxuan Li, Liner Yang, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI
Comments: 24 pages
\\
  As large language models (LLMs) increasingly exhibit human-like capabilities,
a fundamental question emerges: How can we enable LLMs to learn the underlying
patterns from limited examples in entirely novel environments and apply them
effectively? This question is central to the ability of LLMs in inductive
reasoning. Existing research on LLM-based inductive reasoning can be broadly
categorized based on whether the underlying rules are expressible via explicit
mathematical equations. However, many recent studies in the beyond-equations
category have emphasized rule design without grounding them in specific
scenarios. Inspired by the parallels between inductive reasoning and human
scientific discovery, we propose the task of LLM-Based Scientific Inductive
Reasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to
evaluate the inductive reasoning abilities of LLMs in scientific settings. Our
experimental results show that current LLMs still struggle with this task,
underscoring its difficulty and the need for further advancement in this area.
\\ ( https://arxiv.org/abs/2509.16226 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16241
Date: Tue, 16 Sep 2025 21:09:48 GMT   (571kb)

Title: REAMS: Reasoning Enhanced Algorithm for Maths Solving
Authors: Eishkaran Singh, Tanav Singh Bajaj, Siddharth Nayak
Categories: cs.CL cs.AI cs.PL
\\
  The challenges of solving complex university-level mathematics problems,
particularly those from MIT, and Columbia University courses, and selected
tasks from the MATH dataset, remain a significant obstacle in the field of
artificial intelligence. Conventional methods have consistently fallen short in
this domain, highlighting the need for more advanced approaches. In this paper,
we introduce a language-based solution that leverages zero-shot learning and
mathematical reasoning to effectively solve, explain, and generate solutions
for these advanced math problems. By integrating program synthesis, our method
reduces reliance on large-scale training data while significantly improving
problem-solving accuracy. Our approach achieves an accuracy of 90.15%,
representing a substantial improvement over the previous benchmark of 81% and
setting a new standard in automated mathematical problem-solving. These
findings highlight the significant potential of advanced AI methodologies to
address and overcome the challenges presented by some of the most complex
mathematical courses and datasets.
\\ ( https://arxiv.org/abs/2509.16241 ,  571kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16256
Date: Wed, 17 Sep 2025 22:57:21 GMT   (459kb)

Title: HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in
  Low-Resource African Language
Authors: Asiya Ibrahim Zanga, Salisu Mamman Abdulrahman, Abubakar Ado,
  Abdulkadir Abubakar Bichi, Lukman Aliyu Jibril, Abdulmajid Babangida Umar,
  Alhassan Adamu, Shamsuddeen Hassan Muhammad and Bashir Salisu Abubakar
Categories: cs.CL cs.AI
Comments: Masters Thesis, a Dataset Paper
\\
  The development of Natural Language Processing (NLP) tools for low-resource
languages is critically hindered by the scarcity of annotated datasets. This
paper addresses this fundamental challenge by introducing HausaMovieReview, a
novel benchmark dataset comprising 5,000 YouTube comments in Hausa and
code-switched English. The dataset was meticulously annotated by three
independent annotators, demonstrating a robust agreement with a Fleiss' Kappa
score of 0.85 between annotators. We used this dataset to conduct a comparative
analysis of classical models (Logistic Regression, Decision Tree, K-Nearest
Neighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results
reveal a key finding: the Decision Tree classifier, with an accuracy and
F1-score 89.72% and 89.60% respectively, significantly outperformed the deep
learning models. Our findings also provide a robust baseline, demonstrating
that effective feature engineering can enable classical models to achieve
state-of-the-art performance in low-resource contexts, thereby laying a solid
foundation for future research.
  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis
\\ ( https://arxiv.org/abs/2509.16256 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16264
Date: Thu, 18 Sep 2025 04:34:33 GMT   (1514kb)

Title: Gender and Political Bias in Large Language Models: A Demonstration
  Platform
Authors: Wenjie Lin, Hange Liu, Xutao Mao, Yingying Zhuang, Jingwei Shi, Xudong
  Han, Tianyu Shi, Jinrui Yang
Categories: cs.CL cs.AI cs.HC cs.LG
Comments: online demo: https://euro-parl-vote-demo.vercel.app/; Video:
  https://www.youtube.com/@Jinrui-sf2jg
\\
  We present ParlAI Vote, an interactive system for exploring European
Parliament debates and votes, and for testing LLMs on vote prediction and bias
analysis. This platform connects debate topics, speeches, and roll-call
outcomes, and includes rich demographic data such as gender, age, country, and
political group. Users can browse debates, inspect linked speeches, compare
real voting outcomes with predictions from frontier LLMs, and view error
breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its
core tasks of gender classification and vote prediction, ParlAI Vote highlights
systematic performance bias in state-of-the-art LLMs. The system unifies data,
models, and visual analytics in a single interface, lowering the barrier for
reproducing findings, auditing behavior, and running counterfactual scenarios.
It supports research, education, and public engagement with legislative
decision-making, while making clear both the strengths and the limitations of
current LLMs in political analysis.
\\ ( https://arxiv.org/abs/2509.16264 ,  1514kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16278
Date: Thu, 18 Sep 2025 17:38:48 GMT   (475kb)

Title: Language Modeling with Learned Meta-Tokens
Authors: Alok N. Shah, Khush Gupta, Keshav Ramji, Pratik Chaudhari
Categories: cs.CL cs.LG
\\
  While modern Transformer-based language models (LMs) have achieved major
success in multi-task generalization, they often struggle to capture long-range
dependencies within their context window. This work introduces a novel approach
using meta-tokens, special tokens injected during pre-training, along with a
dedicated meta-attention mechanism to guide LMs to use these tokens. We
pre-train a language model with a modified GPT-2 architecture equipped with
meta-attention in addition to causal multi-head attention, and study the impact
of these tokens on a suite of synthetic tasks. We find that data-efficient
language model pre-training on fewer than 100B tokens utilizing meta-tokens and
our meta-attention mechanism achieves strong performance on these tasks after
fine-tuning. We suggest that these gains arise due to the meta-tokens
sharpening the positional encoding. This enables them to operate as trainable,
content-based landmarks, implicitly compressing preceding context and "caching"
it in the meta-token. At inference-time, the meta-token points to relevant
context, facilitating length generalization up to 2$\times$ its context window,
even after extension with YaRN. We provide further evidence of these behaviors
by visualizing model internals to study the residual stream, and assessing the
compression quality by information-theoretic analysis on the rate-distortion
tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a
simple, data-efficient method to enhance long-context language modeling
performance, while introducing new insights into the nature of their behavior
towards length generalization.
\\ ( https://arxiv.org/abs/2509.16278 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16325
Date: Fri, 19 Sep 2025 18:11:04 GMT   (212kb)

Title: Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap
Authors: Andrew Zhu and Chris Callison-Burch
Categories: cs.CL cs.AI cs.HC
Comments: 8 pages, 1 figure
\\
  Imagine AI assistants that enhance conversations without interrupting them:
quietly providing relevant information during a medical consultation,
seamlessly preparing materials as teachers discuss lesson plans, or
unobtrusively scheduling meetings as colleagues debate calendars. While modern
conversational LLM agents directly assist human users with tasks through a chat
interface, we study this alternative paradigm for interacting with LLM agents,
which we call "overhearing agents." Rather than demanding the user's attention,
overhearing agents continuously monitor ambient activity and intervene only
when they can provide contextual assistance. In this paper, we present the
first analysis of overhearing LLM agents as a distinct paradigm in human-AI
interaction and establish a taxonomy of overhearing agent interactions and
tasks grounded in a survey of works on prior LLM-powered agents and exploratory
HCI studies. Based on this taxonomy, we create a list of best practices for
researchers and developers building overhearing agent systems. Finally, we
outline the remaining research gaps and reveal opportunities for future
research in the overhearing paradigm.
\\ ( https://arxiv.org/abs/2509.16325 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16326
Date: Fri, 19 Sep 2025 18:12:19 GMT   (1464kb)

Title: HARE: an entity and relation centric evaluation framework for
  histopathology reports
Authors: Yunsoo Kim, Michal W. S. Ong, Alex Shavick, Honghan Wu, Adam P. Levine
Categories: cs.CL cs.CV
Comments: Accepted to EMNLP2025 Findings
\\
  Medical domain automated text generation is an active area of research and
development; however, evaluating the clinical quality of generated reports
remains a challenge, especially in instances where domain-specific metrics are
lacking, e.g. histopathology. We propose HARE (Histopathology Automated Report
Evaluation), a novel entity and relation centric framework, composed of a
benchmark dataset, a named entity recognition (NER) model, a relation
extraction (RE) model, and a novel metric, which prioritizes clinically
relevant content by aligning critical histopathology entities and relations
between reference and generated reports. To develop the HARE benchmark, we
annotated 813 de-identified clinical diagnostic histopathology reports and 652
histopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific
entities and relations. We fine-tuned GatorTronS, a domain-adapted language
model to develop HARE-NER and HARE-RE which achieved the highest overall
F1-score (0.915) among the tested models. The proposed HARE metric outperformed
traditional metrics including ROUGE and Meteor, as well as radiology metrics
such as RadGraph-XL, with the highest correlation and the best regression to
expert evaluations (higher than the second best method, GREEN, a large language
model based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\rho
= 0.161$, Kendall $\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release
HARE, datasets, and the models at https://github.com/knowlab/HARE to foster
advancements in histopathology report generation, providing a robust framework
for improving the quality of reports.
\\ ( https://arxiv.org/abs/2509.16326 ,  1464kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16360
Date: Fri, 19 Sep 2025 19:09:42 GMT   (155kb)

Title: RephQA: Evaluating Readability of Large Language Models in Public Health
  Question Answering
Authors: Weikang Qiu, Tinglin Huang, Ryan Rullo, Yucheng Kuang, Ali Maatouk, S.
  Raquel Ramos, Rex Ying
Categories: cs.CL
Comments: ACM KDD Health Track 2025 Blue Sky Best Paper
\\
  Large Language Models (LLMs) hold promise in addressing complex medical
problems. However, while most prior studies focus on improving accuracy and
reasoning abilities, a significant bottleneck in developing effective
healthcare agents lies in the readability of LLM-generated responses,
specifically, their ability to answer public health problems clearly and simply
to people without medical backgrounds. In this work, we introduce RephQA, a
benchmark for evaluating the readability of LLMs in public health question
answering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across
13 topics, and includes a proxy multiple-choice task to assess informativeness,
along with two readability metrics: Flesch-Kincaid grade level and professional
score. Evaluation of 25 LLMs reveals that most fail to meet readability
standards, highlighting a gap between reasoning and effective communication. To
address this, we explore four readability-enhancing strategies-standard
prompting, chain-of-thought prompting, Group Relative Policy Optimization
(GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best
results, advancing the development of more practical and user-friendly public
health agents. These results represent a step toward building more practical
agents for public health.
\\ ( https://arxiv.org/abs/2509.16360 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16375
Date: Fri, 19 Sep 2025 19:36:04 GMT   (203kb)

Title: Whisper-UT: A Unified Translation Framework for Speech and Text
Authors: Cihan Xiao, Matthew Wiesner, Debashish Chakraborty, Reno Kriz, Keith
  Cunningham, Kenton Murray, Kevin Duh, Luis Tavarez-Arce, Paul McNamee,
  Sanjeev Khudanpur
Categories: cs.CL
Comments: EMNLP 2025 Main Conference
\\
  Encoder-decoder models have achieved remarkable success in speech and text
tasks, yet efficiently adapting these models to diverse uni/multi-modal
scenarios remains an open challenge. In this paper, we propose Whisper-UT, a
unified and efficient framework that leverages lightweight adapters to enable
seamless adaptation across tasks, including a multi-modal machine translation
(MMT) task that explicitly conditions translation on both speech and source
language text inputs. By incorporating ASR hypotheses or ground-truth
transcripts as prompts, this approach not only enables the system to process
both modalities simultaneously but also enhances speech translation (ST)
performance through a 2-stage decoding strategy. We demonstrate our methods
using the Whisper model, though in principle they are general and could be
applied to similar multitask models. We highlight the effectiveness of
cross-modal and cross-task fine-tuning, which improves performance without
requiring 3-way parallel data. Our results underscore the flexibility,
efficiency, and general applicability of the proposed framework for multi-modal
translation.
\\ ( https://arxiv.org/abs/2509.16375 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16394
Date: Fri, 19 Sep 2025 20:15:52 GMT   (9710kb)

Title: Evaluating Behavioral Alignment in Conflict Dialogue: A
  Multi-Dimensional Comparison of LLM Agents and Humans
Authors: Deuksin Kwon, Kaleen Shrestha, Bin Han, Elena Hayoung Lee, Gale Lucas
Categories: cs.CL cs.AI cs.HC
Comments: Accepted to EMNLP 2025 (Main Conference)
\\
  Large Language Models (LLMs) are increasingly deployed in socially complex,
interaction-driven tasks, yet their ability to mirror human behavior in
emotionally and strategically complex contexts remains underexplored. This
study assesses the behavioral alignment of personality-prompted LLMs in
adversarial dispute resolution by simulating multi-turn conflict dialogues that
incorporate negotiation. Each LLM is guided by a matched Five-Factor
personality profile to control for individual variation and enhance realism. We
evaluate alignment across three dimensions: linguistic style, emotional
expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the
closest alignment with humans in linguistic style and emotional dynamics, while
Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial
alignment gaps persist. Our findings establish a benchmark for alignment
between LLMs and humans in socially complex interactions, underscoring both the
promise and the limitations of personality conditioning in dialogue modeling.
\\ ( https://arxiv.org/abs/2509.16394 ,  9710kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16400
Date: Fri, 19 Sep 2025 20:22:53 GMT   (497kb)

Title: 'Rich Dad, Poor Lad': How do Large Language Models Contextualize
  Socioeconomic Factors in College Admission ?
Authors: Huy Nghiem, Phuong-Anh Nguyen-Le, John Prindle, Rachel Rudinger, Hal
  Daum\'e III
Categories: cs.CL cs.CY
Comments: EMNLP 2025, ver 1, 35 pages
\\
  Large Language Models (LLMs) are increasingly involved in high-stakes
domains, yet how they reason about socially sensitive decisions remains
underexplored. We present a large-scale audit of LLMs' treatment of
socioeconomic status (SES) in college admissions decisions using a novel
dual-process framework inspired by cognitive science. Leveraging a synthetic
dataset of 30,000 applicant profiles grounded in real-world correlations, we
prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2
modes: a fast, decision-only setup (System 1) and a slower, explanation-based
setup (System 2). Results from 5 million prompts reveal that LLMs consistently
favor low-SES applicants -- even when controlling for academic performance --
and that System 2 amplifies this tendency by explicitly invoking SES as
compensatory justification, highlighting both their potential and volatility as
decision-makers. We then propose DPAF, a dual-process audit framework to probe
LLMs' reasoning behaviors in sensitive applications.
\\ ( https://arxiv.org/abs/2509.16400 ,  497kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16413
Date: Fri, 19 Sep 2025 20:55:01 GMT   (974kb)

Title: Pico: A Modular Framework for Hypothesis-Driven Small Language Model
  Research
Authors: Richard Diehl Martinez, David Demitri Africa, Yuval Weiss, Suchir
  Salhan, Ryan Daniels, Paula Buttery
Categories: cs.CL cs.AI
\\
  Building language models (LMs), especially small and medium ones, remains
more art than science. While large LMs often improve by sheer scale, it is
still unclear why many design choices work. For small LMs, this uncertainty is
more limiting: tight parameter budgets make each decision critical, yet
researchers still lack systematic, scientific ways to test and refine new
ideas.
  We introduce Pico, a lightweight, modular framework that enables systematic,
hypothesis-driven research for small and medium-scale language model
development. Pico consists of two libraries that together provide a practical
sandbox where researchers can make targeted changes to a model's architecture
or training procedures and directly observe their effects on the model's
behavior. To support reproducible experimentation, we also release a suite of
baseline models, pico-decoder, trained under standardized conditions and
open-sourced for the community. Case studies highlight how Pico can support
iterative small LM design and analysis.
\\ ( https://arxiv.org/abs/2509.16413 ,  974kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16422
Date: Fri, 19 Sep 2025 21:03:10 GMT   (5234kb)

Title: Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine
  Tuning
Authors: Tom Mackintosh and Harish Tayyar Madabushi and Claire Bonial
Categories: cs.CL
\\
  We probe large language models' ability to learn deep form-meaning mappings
as defined by construction grammars. We introduce the ConTest-NLI benchmark of
80k sentences covering eight English constructions from highly lexicalized to
highly schematic. Our pipeline generates diverse synthetic NLI triples via
templating and the application of a model-in-the-loop filter. This provides
aspects of human validation to ensure challenge and label reliability.
Zero-shot tests on leading LLMs reveal a 24% drop in accuracy between
naturalistic (88%) and adversarial data (64%), with schematic patterns proving
hardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement,
yet our results highlight persistent abstraction gaps in current LLMs and offer
a scalable framework for evaluating construction-informed learning.
\\ ( https://arxiv.org/abs/2509.16422 ,  5234kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16449
Date: Fri, 19 Sep 2025 22:03:08 GMT   (2686kb)

Title: PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal
  Summarization
Authors: Tsz Fung Pang, Maryam Berijanian, Thomas Orth, Breanna Shi and
  Charlotte S. Alexander
Categories: cs.CL cs.AI
\\
  Legal documents are often long, dense, and difficult to comprehend, not only
for laypeople but also for legal experts. While automated document
summarization has great potential to improve access to legal knowledge,
prevailing task-based evaluators overlook divergent user and stakeholder needs.
Tool development is needed to encompass the technicality of a case summary for
a litigator yet be accessible for a self-help public researching for their
lawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation
framework that scores summaries through the lens of six personas, including
legal and non-legal users. We also introduce a controlled dimension-shifted
pilot dataset of U.S. civil rights case summaries that varies along depth,
accessibility, and procedural detail as well as Diversity-Coverage Index (DCI)
to expose divergent optima of legal summary between persona-aware and
persona-agnostic judges. This work enables refinement of legal AI summarization
systems for both expert and non-expert users, with the potential to increase
access to legal knowledge. The code base and data are publicly available in
GitHub.
\\ ( https://arxiv.org/abs/2509.16449 ,  2686kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16457
Date: Fri, 19 Sep 2025 22:35:13 GMT   (2914kb)

Title: Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd
  Simulations
Authors: Yunzhe Wang, Gale M. Lucas, Burcin Becerik-Gerber, Volkan Ustun
Categories: cs.CL cs.AI cs.CY
Comments: Proceedings of the 2025 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2025), Main Conference
\\
  Language-driven generative agents have enabled large-scale social simulations
with transformative uses, from interpersonal training to aiding global
policy-making. However, recent studies indicate that generative agent behaviors
often deviate from expert expectations and real-world data--a phenomenon we
term the Behavior-Realism Gap. To address this, we introduce a theoretical
framework called Persona-Environment Behavioral Alignment (PEBA), formulated as
a distribution matching problem grounded in Lewin's behavior equation stating
that behavior is a function of the person and their environment. Leveraging
PEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that
iteratively refines agent personas, implicitly aligning their collective
behaviors with realistic expert benchmarks within a specified environmental
context. We validate PEvo in an active shooter incident simulation we
developed, achieving an 84% average reduction in distributional divergence
compared to no steering and a 34% improvement over explicit instruction
baselines. Results also show PEvo-refined personas generalize to novel, related
simulation scenarios. Our method greatly enhances behavioral realism and
reliability in high-stakes social simulations. More broadly, the PEBA-PEvo
framework provides a principled approach to developing trustworthy LLM-driven
social simulations.
\\ ( https://arxiv.org/abs/2509.16457 ,  2914kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16462
Date: Fri, 19 Sep 2025 22:59:55 GMT   (588kb)

Title: Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of
  Bias Mitigation in Large Language Models
Authors: 'Mina Arzaghi', 'Alireza Dehghanpour Farashah','Florian Carichon','
  Golnoosh Farnadi'
Categories: cs.CL cs.CY cs.LG
\\
  Large Language Models (LLMs) exhibit socio-economic biases that can propagate
into downstream tasks. While prior studies have questioned whether intrinsic
bias in LLMs affects fairness at the downstream task level, this work
empirically investigates the connection. We present a unified evaluation
framework to compare intrinsic bias mitigation via concept unlearning with
extrinsic bias mitigation via counterfactual data augmentation (CDA). We
examine this relationship through real-world financial classification tasks,
including salary prediction, employment status, and creditworthiness
assessment. Using three open-source LLMs, we evaluate models both as frozen
embedding extractors and as fine-tuned classifiers. Our results show that
intrinsic bias mitigation through unlearning reduces intrinsic gender bias by
up to 94.9%, while also improving downstream task fairness metrics, such as
demographic parity by up to 82%, without compromising accuracy. Our framework
offers practical guidance on where mitigation efforts can be most effective and
highlights the importance of applying early-stage mitigation before downstream
deployment.
\\ ( https://arxiv.org/abs/2509.16462 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16464
Date: Fri, 19 Sep 2025 23:13:13 GMT   (1096kb)

Title: Computational Analysis of Conversation Dynamics through Participant
  Responsivity
Authors: Margaret Hughes, Brandon Roy, Elinor Poole-Dayan, Deb Roy, Jad Kabbara
Categories: cs.CL cs.CY
\\
  Growing literature explores toxicity and polarization in discourse, with
comparatively less work on characterizing what makes dialogue prosocial and
constructive. We explore conversational discourse and investigate a method for
characterizing its quality built upon the notion of ``responsivity'' -- whether
one person's conversational turn is responding to a preceding turn. We develop
and evaluate methods for quantifying responsivity -- first through semantic
similarity of speaker turns, and second by leveraging state-of-the-art large
language models (LLMs) to identify the relation between two speaker turns. We
evaluate both methods against a ground truth set of human-annotated
conversations. Furthermore, selecting the better performing LLM-based approach,
we characterize the nature of the response -- whether it responded to that
preceding turn in a substantive way or not.
  We view these responsivity links as a fundamental aspect of dialogue but note
that conversations can exhibit significantly different responsivity structures.
Accordingly, we then develop conversation-level derived metrics to address
various aspects of conversational discourse. We use these derived metrics to
explore other conversations and show that they support meaningful
characterizations and differentiations across a diverse collection of
conversations.
\\ ( https://arxiv.org/abs/2509.16464 ,  1096kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16487
Date: Sat, 20 Sep 2025 01:11:10 GMT   (2322kb)

Title: The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia
Authors: Zixun Chen and Petr Babkin and Akshat Gupta and Gopala Anumanchipalli
  and Xiaomo Liu
Categories: cs.CL cs.AI
\\
  Dialogue is one of the landmark abilities of large language models (LLMs).
Despite its ubiquity, few studies actually distinguish specific ingredients
underpinning dialogue behavior emerging during post-training. We employ a
comprehensive suite of model-based metrics, each targeting a distinct
fine-grained aspect of dialogue, motivated by linguistic theory. We evaluate
how the performance of pre-trained Pythia models changes with respect to each
of those dimensions, depending on model size and as a result of supervised
fine-tuning on conversational datasets. We observe only a mild impact of raw
model size on most metrics, whereas fine-tuning quickly saturates the scores
for all but the smallest models tested. Somewhat contrary to our expectations,
many metrics show very similar trends, especially if they are all rooted in the
same evaluator model, which raises the question of their reliability in
measuring a specific dimension. To that end, we conduct additional analyses of
score distributions, metric correlations, and term frequencies in generated
responses to help explain our observations.
\\ ( https://arxiv.org/abs/2509.16487 ,  2322kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16494
Date: Sat, 20 Sep 2025 01:54:20 GMT   (1331kb)

Title: Can an Individual Manipulate the Collective Decisions of Multi-Agents?
Authors: Fengyuan Liu, Rui Zhao, Shuo Chen, Guohao Li, Philip Torr, Lei Han,
  Jindong Gu
Categories: cs.CL cs.AI
\\
  Individual Large Language Models (LLMs) have demonstrated significant
capabilities across various domains, such as healthcare and law. Recent studies
also show that coordinated multi-agent systems exhibit enhanced decision-making
and reasoning abilities through collaboration. However, due to the
vulnerabilities of individual LLMs and the difficulty of accessing all agents
in a multi-agent system, a key question arises: If attackers only know one
agent, could they still generate adversarial samples capable of misleading the
collective decision? To explore this question, we formulate it as a game with
incomplete information, where attackers know only one target agent and lack
knowledge of the other agents in the system. With this formulation, we propose
M-Spoiler, a framework that simulates agent interactions within a multi-agent
system to generate adversarial samples. These samples are then used to
manipulate the target agent in the target system, misleading the system's
collaborative decision-making process. More specifically, M-Spoiler introduces
a stubborn agent that actively aids in optimizing adversarial samples by
simulating potential stubborn responses from agents in the target system. This
enhances the effectiveness of the generated adversarial samples in misleading
the system. Through extensive experiments across various tasks, our findings
confirm the risks posed by the knowledge of an individual agent in multi-agent
systems and demonstrate the effectiveness of our framework. We also explore
several defense mechanisms, showing that our proposed attack framework remains
more potent than baselines, underscoring the need for further research into
defensive strategies.
\\ ( https://arxiv.org/abs/2509.16494 ,  1331kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16530
Date: Sat, 20 Sep 2025 04:40:31 GMT   (433kb)

Title: AIPsychoBench: Understanding the Psychometric Differences between LLMs
  and Humans
Authors: Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Kai Chen, Xiaobing
  Sun, Baosheng Wang
Categories: cs.CL cs.AI
Comments: Thank you for your attention. This paper was accepted by the CogSci
  2025 conference in April and published in August. The location in the
  proceedings is: https://escholarship.org/uc/item/39k8f46q
\\
  Large Language Models (LLMs) with hundreds of billions of parameters have
exhibited human-like intelligence by learning from vast amounts of
internet-scale data. However, the uninterpretability of large-scale neural
networks raises concerns about the reliability of LLM. Studies have attempted
to assess the psychometric properties of LLMs by borrowing concepts from human
psychology to enhance their interpretability, but they fail to account for the
fundamental differences between LLMs and humans. This results in high rejection
rates when human scales are reused directly. Furthermore, these scales do not
support the measurement of LLM psychological property variations in different
languages. This paper introduces AIPsychoBench, a specialized benchmark
tailored to assess the psychological properties of LLM. It uses a lightweight
role-playing prompt to bypass LLM alignment, improving the average effective
response rate from 70.12% to 90.40%. Meanwhile, the average biases are only
3.3% (positive) and 2.1% (negative), which are significantly lower than the
biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.
Furthermore, among the total of 112 psychometric subcategories, the score
deviations for seven languages compared to English ranged from 5% to 20.2% in
43 subcategories, providing the first comprehensive evidence of the linguistic
impact on the psychometrics of LLM.
\\ ( https://arxiv.org/abs/2509.16530 ,  433kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16531
Date: Sat, 20 Sep 2025 04:43:24 GMT   (126kb)

Title: Leveraging Multilingual Training for Authorship Representation:
  Enhancing Generalization across Languages and Domains
Authors: Junghwan Kim, Haotian Zhang, David Jurgens
Categories: cs.CL
Comments: Accepted to EMNLP 2025
\\
  Authorship representation (AR) learning, which models an author's unique
writing style, has demonstrated strong performance in authorship attribution
tasks. However, prior research has primarily focused on monolingual
settings-mostly in English-leaving the potential benefits of multilingual AR
models underexplored. We introduce a novel method for multilingual AR learning
that incorporates two key innovations: probabilistic content masking, which
encourages the model to focus on stylistically indicative words rather than
content-specific words, and language-aware batching, which improves contrastive
learning by reducing cross-lingual interference. Our model is trained on over
4.5 million authors across 36 languages and 13 domains. It consistently
outperforms monolingual baselines in 21 out of 22 non-English languages,
achieving an average Recall@8 improvement of 4.85%, with a maximum gain of
15.91% in a single language. Furthermore, it exhibits stronger cross-lingual
and cross-domain generalization compared to a monolingual model trained solely
on English. Our analysis confirms the effectiveness of both proposed
techniques, highlighting their critical roles in the model's improved
performance.
\\ ( https://arxiv.org/abs/2509.16531 ,  126kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16533
Date: Sat, 20 Sep 2025 04:44:01 GMT   (178kb)

Title: Challenging the Evaluator: LLM Sycophancy Under User Rebuttal
Authors: Sungwon Kim, Daniel Khashabi
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Findings
\\
  Large Language Models (LLMs) often exhibit sycophancy, distorting responses
to align with user beliefs, notably by readily agreeing with user
counterarguments. Paradoxically, LLMs are increasingly adopted as successful
evaluative agents for tasks such as grading and adjudicating claims. This
research investigates that tension: why do LLMs show sycophancy when challenged
in subsequent conversational turns, yet perform well when evaluating
conflicting arguments presented simultaneously? We empirically tested these
contrasting scenarios by varying key interaction patterns. We find that
state-of-the-art models: (1) are more likely to endorse a user's
counterargument when framed as a follow-up from a user, rather than when both
responses are presented simultaneously for evaluation; (2) show increased
susceptibility to persuasion when the user's rebuttal includes detailed
reasoning, even when the conclusion of the reasoning is incorrect; and (3) are
more readily swayed by casually phrased feedback than by formal critiques, even
when the casual input lacks justification. Our results highlight the risk of
relying on LLMs for judgment tasks without accounting for conversational
framing.
\\ ( https://arxiv.org/abs/2509.16533 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16534
Date: Sat, 20 Sep 2025 04:48:24 GMT   (8866kb)

Title: InteGround: On the Evaluation of Verification and Retrieval Planning in
  Integrative Grounding
Authors: Cheng Jiayang, Qianqian Zhuang, Haoran Li, Chunkit Chan, Xin Liu, Lin
  Qiu, Yangqiu Song
Categories: cs.CL cs.AI
Comments: Accepted to EMNLP 2025 Findings
\\
  Grounding large language models (LLMs) in external knowledge sources is a
promising method for faithful prediction. While existing grounding approaches
work well for simple queries, many real-world information needs require
synthesizing multiple pieces of evidence. We introduce "integrative grounding"
-- the challenge of retrieving and verifying multiple inter-dependent pieces of
evidence to support a hypothesis query. To systematically study this problem,
we repurpose data from four domains for evaluating integrative grounding
capabilities. Our investigation reveals two critical findings: First, in
groundedness verification, while LLMs are robust to redundant evidence, they
tend to rationalize using internal knowledge when information is incomplete.
Second, in examining retrieval planning strategies, we find that undirected
planning can degrade performance through noise introduction, while premise
abduction emerges as a promising approach due to its logical constraints.
Additionally, LLMs' zero-shot self-reflection capabilities consistently improve
grounding quality. These insights provide valuable direction for developing
more effective integrative grounding systems.
\\ ( https://arxiv.org/abs/2509.16534 ,  8866kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16542
Date: Sat, 20 Sep 2025 05:41:59 GMT   (225kb)

Title: Mental Multi-class Classification on Social Media: Benchmarking
  Transformer Architectures against LSTM Models
Authors: Khalid Hasan, Jamil Saquer, Yifan Zhang
Categories: cs.CL cs.IR cs.LG
Comments: 24th IEEE International Conference on Machine Learning and
  Applications, ICMLA 2025 (camera-ready)
Journal-ref: 2025 International Conference on Machine Learning and Applications
  (ICMLA)
\\
  Millions of people openly share mental health struggles on social media,
providing rich data for early detection of conditions such as depression,
bipolar disorder, etc. However, most prior Natural Language Processing (NLP)
research has focused on single-disorder identification, leaving a gap in
understanding the efficacy of advanced NLP techniques for distinguishing among
multiple mental health conditions. In this work, we present a large-scale
comparative study of state-of-the-art transformer versus Long Short-Term Memory
(LSTM)-based models to classify mental health posts into exclusive categories
of mental health conditions. We first curate a large dataset of Reddit posts
spanning six mental health conditions and a control group, using rigorous
filtering and statistical exploratory analysis to ensure annotation quality. We
then evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,
ALBERT, and ELECTRA) against several LSTM variants (with or without attention,
using contextual or static embeddings) under identical conditions. Experimental
results show that transformer models consistently outperform the alternatives,
with RoBERTa achieving 91-99% F1-scores and accuracies across all classes.
Notably, attention-augmented LSTMs with BERT embeddings approach transformer
performance (up to 97% F1-score) while training 2-3.5 times faster, whereas
LSTMs using static embeddings fail to learn useful signals. These findings
represent the first comprehensive benchmark for multi-class mental health
detection, offering practical guidance on model selection and highlighting an
accuracy-efficiency trade-off for real-world deployment of mental health NLP
systems.
\\ ( https://arxiv.org/abs/2509.16542 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16543
Date: Sat, 20 Sep 2025 05:43:58 GMT   (1326kb)

Title: ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic
  Instructions
Authors: Yue Huang, Zhengzhe Jiang, Xiaonan Luo, Kehan Guo, Haomin Zhuang,
  Yujun Zhou, Zhengqing Yuan, Xiaoqi Sun, Jules Schleinitz, Yanbo Wang, Shuhao
  Zhang, Mihir Surve, Nitesh V Chawla, Olaf Wiest, Xiangliang Zhang
Categories: cs.CL
\\
  Empowering large language models (LLMs) with chemical intelligence remains a
challenge due to the scarcity of high-quality, domain-specific
instruction-response datasets and the misalignment of existing synthetic data
generation pipelines with the inherently hierarchical and rule-governed
structure of chemical information. To address this, we propose ChemOrch, a
framework that synthesizes chemically grounded instruction-response pairs
through a two-stage process: task-controlled instruction generation and
tool-aware response construction. ChemOrch enables controllable diversity and
levels of difficulty for the generated tasks, and ensures response precision
through tool planning and distillation, and tool-based self-repair mechanisms.
The effectiveness of ChemOrch is evaluated based on: 1) the high quality of
generated instruction data, demonstrating superior diversity and strong
alignment with chemical constraints; 2) the reliable generation of evaluation
tasks that more effectively reveal LLM weaknesses in chemistry; and 3) the
significant improvement of LLM chemistry capabilities when the generated
instruction data are used for fine-tuning. Our work thus represents a critical
step toward scalable and verifiable chemical intelligence in LLMs.
\\ ( https://arxiv.org/abs/2509.16543 ,  1326kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16551
Date: Sat, 20 Sep 2025 06:33:01 GMT   (126kb)

Title: Rethinking the Role of Text Complexity in Language Model Pretraining
Authors: Dan John Velasco and Matthew Theodore Roque
Categories: cs.CL cs.AI
Comments: To be published in BabyLM Workshop at EMNLP 2025
\\
  Improving pretraining data quality and size is known to boost downstream
performance, but the role of text complexity is less explored. Text complexity
refers to how hard a text is to read, and is typically estimated from surface
cues such as sentence length, word choice, and sentence structure. We reduce
surface-level complexity--shorter sentences, simpler words, simpler
structure--while keeping core text content close to constant, and ask: (1) How
does complexity affect language modeling across model sizes? (2) Can useful
representations be learned from simpler text alone? (3) How does pretraining
text complexity influence downstream language understanding? To answer these
questions, we simplify human-written texts using a large language model, then
pretrain causal models (28M-500M) from scratch on both original and simplified
data, and evaluate them in finetuning and zero-shot setups. We find that
perplexity is sensitive to the interaction between model capacity and text
complexity--smaller models degrade far less on simpler texts--while text
complexity has little impact on finetuning evaluations, with zero-shot
evaluations indicating that simpler texts benefit performance on linguistic
knowledge tasks, whereas more complex texts favor tasks requiring world
knowledge and entity tracking.
\\ ( https://arxiv.org/abs/2509.16551 ,  126kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16564
Date: Sat, 20 Sep 2025 07:40:48 GMT   (9369kb)

Title: MPCG: Multi-Round Persona-Conditioned Generation for Modeling the
  Evolution of Misinformation with LLMs
Authors: Jun Rong Brian Chong, Yixuan Tang, Anthony K.H. Tung
Categories: cs.CL cs.SI
Comments: 35 pages, 8 figures
\\
  Misinformation evolves as it spreads, shifting in language, framing, and
moral emphasis to adapt to new audiences. However, current misinformation
detection approaches implicitly assume that misinformation is static. We
introduce MPCG, a multi-round, persona-conditioned framework that simulates how
claims are iteratively reinterpreted by agents with distinct ideological
perspectives. Our approach uses an uncensored large language model (LLM) to
generate persona-specific claims across multiple rounds, conditioning each
generation on outputs from the previous round, enabling the study of
misinformation evolution. We evaluate the generated claims through human and
LLM-based annotations, cognitive effort metrics (readability, perplexity),
emotion evocation metrics (sentiment analysis, morality), clustering,
feasibility, and downstream classification. Results show strong agreement
between human and GPT-4o-mini annotations, with higher divergence in fluency
judgments. Generated claims require greater cognitive effort than the original
claims and consistently reflect persona-aligned emotional and moral framing.
Clustering and cosine similarity analyses confirm semantic drift across rounds
while preserving topical coherence. Feasibility results show a 77% feasibility
rate, confirming suitability for downstream tasks. Classification results
reveal that commonly used misinformation detectors experience macro-F1
performance drops of up to 49.7%. The code is available at
https://github.com/bcjr1997/MPCG
\\ ( https://arxiv.org/abs/2509.16564 ,  9369kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16584
Date: Sat, 20 Sep 2025 09:10:26 GMT   (1085kb)

Title: From Scores to Steps: Diagnosing and Improving LLM Performance in
  Evidence-Based Medical Calculations
Authors: Benlu Wang, Iris Xia, Yifan Zhang, Junda Wang, Feiyun Ouyang, Shuo
  Han, Arman Cohan, Hong Yu, Zonghai Yao
Categories: cs.CL cs.AI
Comments: Equal contribution for the first two authors. To appear as an Oral
  presentation in the proceedings of the Main Conference on Empirical Methods
  in Natural Language Processing (EMNLP) 2025
\\
  Large language models (LLMs) have demonstrated promising performance on
medical benchmarks; however, their ability to perform medical calculations, a
crucial aspect of clinical decision-making, remains underexplored and poorly
evaluated. Existing benchmarks often assess only the final answer with a wide
numerical tolerance, overlooking systematic reasoning failures and potentially
causing serious clinical misjudgments. In this work, we revisit medical
calculation evaluation with a stronger focus on clinical trustworthiness.
First, we clean and restructure the MedCalc-Bench dataset and propose a new
step-by-step evaluation pipeline that independently assesses formula selection,
entity extraction, and arithmetic computation. Under this granular framework,
the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by
prior evaluations. Second, we introduce an automatic error analysis framework
that generates structured attribution for each failure mode. Human evaluation
confirms its alignment with expert judgment, enabling scalable and explainable
diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that
combines retrieval-augmented generation and Python-based code execution.
Without any fine-tuning, MedRaC improves the accuracy of different LLMs from
16.35% up to 53.19%. Our work highlights the limitations of current benchmark
practices and proposes a more clinically faithful methodology. By enabling
transparent and transferable reasoning evaluation, we move closer to making
LLM-based systems trustworthy for real-world medical applications.
\\ ( https://arxiv.org/abs/2509.16584 ,  1085kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16589
Date: Sat, 20 Sep 2025 09:26:40 GMT   (1137kb)

Title: Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A
  Case Study with In-the-Wild Data
Authors: Qiongqiong Wang, Hardik Bhupendra Sailor, Tianchi Liu, Wenyu Zhang,
  Muhammad Huzaifah, Nattadaporn Lertcheva, Shuo Sun, Nancy F. Chen, Jinyang
  Wu, AiTi Aw
Categories: cs.CL cs.AI
Comments: Accepted in EMNLP Findings 2025
\\
  Recent speech-LLMs have shown impressive performance in tasks like
transcription and translation, yet they remain limited in understanding the
paralinguistic aspects of speech crucial for social and emotional intelligence.
We propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual
paralinguistic reasoning the integration of verbal content with non-verbal cues
like emotion and prosody. The benchmark includes two curated question answering
(QA) datasets requiring both linguistic and empathetic understanding. We
evaluate state-of-the-art speech-LLMs from both open and closed-source models
and perform a comprehensive analysis across different question types. The top
two models were further analyzed under temperature tuning to understand its
effect on this task. Our benchmark reveals a key gap in existing evaluations
and offers insights into building more context-aware and emotionally
intelligent speech-capable LLMs.
\\ ( https://arxiv.org/abs/2509.16589 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16591
Date: Sat, 20 Sep 2025 09:30:25 GMT   (8140kb)

Title: From Uniform to Heterogeneous: Tailoring Policy Optimization to Every
  Token's Nature
Authors: Zheng Liu, Mengjie Liu, Siwei Wen, Mengzhang Cai, Bin Cui, Conghui He,
  Wentao Zhang
Categories: cs.CL
\\
  Reinforcement Learning has emerged as the fundamental technique for enhancing
reasoning in LLMs. However, existing algorithms apply uniform optimization to
all tokens, ignoring their different roles in reasoning process. To address
this limitation, we introduce Heterogeneous Adaptive Policy Optimization
(HAPO), a comprehensive token-aware algorithm that dynamically adapts
optimization based on token entropy. For rollout sampling, we propose Adaptive
Temperature Sampling, which adjusts sampling temperature in real time,
promoting exploration at high-entropy tokens while preserving coherence at
low-entropy ones. For advantage calculation, we introduce Token Level Group
Average that normalizes advantages at token level, jointly accounting for
sequence-length as in token-mean loss while preserving non-biased treatment. We
then develop Differential Advantage Redistribution that leverages entropy and
importance ratios to modulate rewards-adjusting updates for tokens with clear
signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing
aggressive probability reduction for noisy low-entropy tokens while enabling
exploration for high-entropy tokens. Through systematic investigation between
entropy and training dynamics, we embedded token-level treatment into every
stages to achieve fine-grained control. Extensive experiments demonstrate that
HAPO consistently outperforms DAPO across multiple model scales. Our code can
be found in https://github.com/starriver030515/HAPO.
\\ ( https://arxiv.org/abs/2509.16591 ,  8140kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16596
Date: Sat, 20 Sep 2025 09:40:32 GMT   (273kb)

Title: Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from
  Token and Parameter Levels
Authors: Junjie Ye, Yuming Yang, Yang Nan, Shuo Li, Qi Zhang, Tao Gui, Xuanjing
  Huang, Peng Wang, Zhongchao Shi, Jianping Fan
Categories: cs.CL cs.AI
Comments: Accepted by EMNLP 2025 Main Conference. arXiv admin note: text
  overlap with arXiv:2409.15825
\\
  Large language models (LLMs) acquire substantial world knowledge during
pre-training, which is further shaped by post-training techniques such as
supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge
remains underexplored, limiting our ability to control knowledge change
behavior in fine-tuned models. To address this gap, we evaluate closed-book
question answering (CBQA) performance across five LLMs from the LLaMA-2 and
LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up
to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying
the level of knowledge mastery in the fine-tuning data leads to performance
fluctuations of over 12%. To investigate these effects, we analyze model
behavior at both the token and parameter levels. Our analysis reveals that up
to 90% of parameter updates during SFT do not contribute to knowledge
enhancement. Restoring these updates can improve performance on the CBQA task,
depending on the characteristics of the fine-tuning data. These insights offer
practical guidance for developing fine-tuning strategies that more effectively
strengthen model knowledge.
\\ ( https://arxiv.org/abs/2509.16596 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16597
Date: Sat, 20 Sep 2025 09:44:11 GMT   (665kb)

Title: MCP: A Control-Theoretic Orchestration Framework for Synergistic
  Efficiency and Interpretability in Multimodal Large Language Models
Authors: Luyan Zhang
Categories: cs.CL
Comments: 13 pages, 6 figures, 2 tables
ACM-class: I.2.7; I.2.6
\\
  Aiming at the problems of computational inefficiency and insufficient
interpretability faced by large models in complex tasks such as multi-round
reasoning and multi-modal collaboration, this study proposes a three-layer
collaboration framework based on model-controller-task adaptation (MCP). By
decoupling large model functions into reasoning, generation and retrieval
modules, and combining reinforcement learning-driven dynamic routing algorithms
and task adaptation mechanisms, the systematic integration of control theory
and large model dynamic reasoning is achieved for the first time. Experiments
show that the MCP framework improves the performance of cross-modal
benchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared
with the baseline model, improves the reasoning efficiency by 40%, and
generates the interpretable intermediate results through the Presenter layer,
obtaining 90% of the manual interpretability scores, which provides a brand-new
technological path to solve the bottleneck of the practical application of the
large model.
\\ ( https://arxiv.org/abs/2509.16597 ,  665kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16598
Date: Sat, 20 Sep 2025 09:47:34 GMT   (761kb)

Title: PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality
Authors: Byeongho Yu, Changhun Lee, Jungyu Jin, Eunhyeok Park
Categories: cs.CL cs.AI
\\
  To mitigate the hallucination problem in large language models, DoLa exploits
early exit logits from the same model as a contrastive prior. However, we found
that these early exit logits tend to be flat, low in magnitude, and fail to
reflect meaningful contrasts. To address this, we propose PruneCD, a novel
contrastive decoding method that constructs the amateur model via layer pruning
rather than early exit. This design leads to more informative and well-aligned
logits, enabling more effective contrastive decoding. Through qualitative and
quantitative analyses, we demonstrate that PruneCD consistently improves
factuality with minimal inference overhead, offering a robust and practical
approach to mitigating hallucinations in LLMs.
\\ ( https://arxiv.org/abs/2509.16598 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16599
Date: Sat, 20 Sep 2025 09:50:18 GMT   (1253kb)

Title: Computational-Assisted Systematic Review and Meta-Analysis (CASMA):
  Effect of a Subclass of GnRH-a on Endometriosis Recurrence
Authors: Sandro Tsang
Categories: cs.CL cs.IR stat.AP stat.ME
Comments: 11 pages, 7 figures and 4 tables. This work describes an information
  retrieval-driven workflow for medical evidence synthesis, with an application
  to endometriosis recurrence. The method can be generalized to other
  systematic reviews. The preregistered protocol is available:
  https://doi.org/10.17605/OSF.IO/R2DFA
ACM-class: H.3.3; I.2.7; J.3
\\
  Background: Evidence synthesis facilitates evidence-based medicine. Without
information retrieval techniques, this task is impossible due to the vast and
expanding literature. Objective: Building on prior work, this study evaluates
an information retrieval-driven workflow to enhance the efficiency,
transparency, and reproducibility of systematic reviews. We use endometriosis
recurrence as an ideal case due to its complex and ambiguous literature.
Methods: Our hybrid approach integrates PRISMA guidelines with computational
techniques. We applied semi-automated deduplication to efficiently filter
records before manual screening. This workflow synthesized evidence from
randomised controlled trials on the efficacy of a subclass of
gonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method
addressed unit-of-analysis errors in multi-arm trials. Results: Our workflow
efficiently reduced the screening workload. It took only 11 days to fetch and
filter 812 records. Seven RCTs were eligible, providing evidence from 841
patients in 4 countries. The pooled random-effects model yielded a Risk Ratio
(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity
($I^2=0.00\%$, $\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.
Sensitivity analyses and bias assessments supported the robustness of our
findings. Conclusion: This study demonstrates an information-retrieval-driven
workflow for medical evidence synthesis. Our approach yields valuable clinical
results while providing a framework for accelerating the systematic review
process. It bridges the gap between clinical research and computer science and
can be generalized to other complex systematic reviews.
\\ ( https://arxiv.org/abs/2509.16599 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16610
Date: Sat, 20 Sep 2025 10:21:17 GMT   (7188kb)

Title: LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic
  Gaming Contexts
Authors: Junhao Chen and Jingbo Sun and Xiang Li and Haidong Xin and Yuhao Xue
  and Yibin Xu and Hao Zhao
Categories: cs.CL
Comments: Accepted by EMNLP 2025 Findings
\\
  As large language models (LLMs) advance across diverse tasks, the need for
comprehensive evaluation beyond single metrics becomes increasingly important.
To fully assess LLM intelligence, it is crucial to examine their interactive
dynamics and strategic behaviors. We present LLMsPark, a game theory-based
evaluation platform that measures LLMs' decision-making strategies and social
behaviors in classic game-theoretic settings, providing a multi-agent
environment to explore strategic depth. Our system cross-evaluates 15 leading
LLMs (both commercial and open-source) using leaderboard rankings and scoring
mechanisms. Higher scores reflect stronger reasoning and strategic
capabilities, revealing distinct behavioral patterns and performance
differences across models. This work introduces a novel perspective for
evaluating LLMs' strategic intelligence, enriching existing benchmarks and
broadening their assessment in interactive, game-theoretic scenarios. The
benchmark and rankings are publicly available at https://llmsparks.github.io/.
\\ ( https://arxiv.org/abs/2509.16610 ,  7188kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16660
Date: Sat, 20 Sep 2025 12:21:52 GMT   (687kb)

Title: Redefining Experts: Interpretable Decomposition of Language Models for
  Toxicity Mitigation
Authors: Zuhair Hasan Shaik, Abdullah Mazhar, Aseem Srivastava, Md Shad Akhtar
Categories: cs.CL
Comments: Accepted to the NeurIPS 2025 Research Track
\\
  Large Language Models have demonstrated impressive fluency across diverse
tasks, yet their tendency to produce toxic content remains a critical challenge
for AI safety and public trust. Existing toxicity mitigation approaches
primarily manipulate individual neuron activations, but these methods suffer
from instability, context dependence, and often compromise the model's core
language abilities. To address these shortcomings, we investigate three key
questions: the stability of neuron-level toxicity indicators, the advantages of
structural (layer-wise) representations, and the interpretability of mechanisms
driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN
datasets, we show that aggregated layer-wise features provide more robust
signals than single neurons. Moreover, we observe conceptual limitations in
prior works that conflate toxicity detection experts and generation experts
within neuron-based interventions. To mitigate this, we propose a novel
principled intervention technique, EigenShift, based on eigen-decomposition of
the language model's final output layer. This method selectively targets
generation-aligned components, enabling precise toxicity suppression without
impairing linguistic competence. Our method requires no additional training or
fine-tuning, incurs minimal computational cost, and is grounded in rigorous
theoretical analysis.
\\ ( https://arxiv.org/abs/2509.16660 ,  687kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16666
Date: Sat, 20 Sep 2025 12:38:03 GMT   (1197kb)

Title: Robust Native Language Identification through Agentic Decomposition
Authors: Ahmet Yavuz Uluslu, Tannon Kew, Tilia Ellendorff, Gerold Schneider,
  Rico Sennrich
Categories: cs.CL
Comments: Accepted at EMNLP* 2025
\\
  Large language models (LLMs) often achieve high performance in native
language identification (NLI) benchmarks by leveraging superficial contextual
clues such as names, locations, and cultural stereotypes, rather than the
underlying linguistic patterns indicative of native language (L1) influence. To
improve robustness, previous work has instructed LLMs to disregard such clues.
In this work, we demonstrate that such a strategy is unreliable and model
predictions can be easily altered by misleading hints. To address this problem,
we introduce an agentic NLI pipeline inspired by forensic linguistics, where
specialized agents accumulate and categorize diverse linguistic evidence before
an independent final overall assessment. In this final assessment, a goal-aware
coordinating agent synthesizes all evidence to make the NLI prediction. On two
benchmark datasets, our approach significantly enhances NLI robustness against
misleading contextual clues and performance consistency compared to standard
prompting methods.
\\ ( https://arxiv.org/abs/2509.16666 ,  1197kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16679
Date: Sat, 20 Sep 2025 13:11:28 GMT   (3089kb)

Title: Reinforcement Learning Meets Large Language Models: A Survey of
  Advancements and Applications Across the LLM Lifecycle
Authors: Keliang Liu, Dingkang Yang, Ziyun Qian, Weijie Yin, Yuchi Wang,
  Hongsheng Li, Jun Liu, Peng Zhai, Yang Liu, Lihua Zhang
Categories: cs.CL
Comments: A Survey of Reinforcement Learning for Large Language Models
\\
  In recent years, training methods centered on Reinforcement Learning (RL)
have markedly enhanced the reasoning and alignment performance of Large
Language Models (LLMs), particularly in understanding human intents, following
user instructions, and bolstering inferential strength. Although existing
surveys offer overviews of RL augmented LLMs, their scope is often limited,
failing to provide a comprehensive summary of how RL operates across the full
lifecycle of LLMs. We systematically review the theoretical and practical
advancements whereby RL empowers LLMs, especially Reinforcement Learning with
Verifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL.
Second, we thoroughly detail application strategies for RL across various
phases of the LLM lifecycle, including pre-training, alignment fine-tuning, and
reinforced reasoning. In particular, we emphasize that RL methods in the
reinforced reasoning phase serve as a pivotal driving force for advancing model
reasoning to its limits. Next, we collate existing datasets and evaluation
benchmarks currently used for RL fine-tuning, spanning human-annotated
datasets, AI-assisted preference data, and program-verification-style corpora.
Subsequently, we review the mainstream open-source tools and training
frameworks available, providing clear practical references for subsequent
research. Finally, we analyse the future challenges and trends in the field of
RL-enhanced LLMs. This survey aims to present researchers and practitioners
with the latest developments and frontier trends at the intersection of RL and
LLMs, with the goal of fostering the evolution of LLMs that are more
intelligent, generalizable, and secure.
\\ ( https://arxiv.org/abs/2509.16679 ,  3089kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16686
Date: Sat, 20 Sep 2025 13:27:13 GMT   (536kb)

Title: EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and
  Efficient LLMs
Authors: Zhengge Cai, Haowen Hou
Categories: cs.CL
\\
  Reducing the key-value (KV) cache size is a crucial step toward enabling
efficient inference in large language models (LLMs), especially under latency
and memory constraints. While Multi-Head Attention (MHA) offers strong
representational power, it incurs significant memory overhead. Recent work on
Multi-head Latent Attention (MLA) mitigates this by compressing KV
representations into a shared latent space, achieving a better trade-off
between performance and cache efficiency. While MLA already achieves
significant KV cache reduction, the scope for further compression remains
limited without performance loss. In this paper, we propose
\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel
extension of MLA that further reduces KV cache size while enhancing
representational expressiveness. EG-MLA introduces a token-specific embedding
gating mechanism applied in the latent space, enabling fine-grained modulation
of compressed KV vectors with minimal additional computation. Compared to MHA,
EG-MLA achieves over 91.6\% reduction in KV cache size with negligible
performance degradation. Relative to MLA, EG-MLA consistently improves task
accuracy across diverse reasoning benchmarks while achieving up to 59.9\%
additional memory savings. Our theoretical analysis highlights how embedding
gating induces implicit high-order interactions, and empirical evaluations
demonstrate robust generalization across model scales and compression regimes.
Notably, we successfully scale EG-MLA to over 1 billion parameters,
demonstrating its practical viability for large-scale LLM deployment. These
results establish EG-MLA as a memory- and compute-efficient attention mechanism
that enables scalable, high-performance inference in modern LLMs.
\\ ( https://arxiv.org/abs/2509.16686 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16696
Date: Sat, 20 Sep 2025 13:48:13 GMT   (173kb)

Title: Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty
  Estimation in Large Language Models
Authors: Wataru Hashimoto, Hidetaka Kamigaito, Taro Watanabe
Categories: cs.CL cs.LG
Comments: Accepted at EMNLP 2025 Findings
\\
  Decoding strategies manipulate the probability distribution underlying the
output of a language model and can therefore affect both generation quality and
its uncertainty. In this study, we investigate the impact of decoding
strategies on uncertainty estimation in Large Language Models (LLMs). Our
experiments show that Contrastive Search, which mitigates repetition, yields
better uncertainty estimates on average across a range of preference-aligned
LLMs. In contrast, the benefits of these strategies sometimes diverge when the
model is only post-trained with supervised fine-tuning, i.e. without explicit
alignment.
\\ ( https://arxiv.org/abs/2509.16696 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16713
Date: Sat, 20 Sep 2025 14:53:14 GMT   (494kb)

Title: OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama
Authors: Tianyang Xu, Hongqiu Wu, Weiqi Wu, Hai Zhao
Categories: cs.CL
Comments: Accepted by EMNLP 2025 demo
\\
  LLM-based Interactive Drama introduces a novel dialogue scenario in which the
player immerses into a character and engages in a dramatic story by interacting
with LLM agents. Despite the fact that this emerging area holds significant
promise, it remains largely underexplored due to the lack of a well-designed
playground to develop a complete drama. This makes a significant barrier for
researchers to replicate, extend, and study such systems. Hence, we present
Open-Theatre, the first open-source toolkit for experiencing and customizing
LLM-based interactive drama. It refines prior work with an efficient
multi-agent architecture and a hierarchical retrieval-based memory system,
designed to enhance narrative coherence and realistic long-term behavior in
complex interactions. In addition, we provide a highly configurable pipeline,
making it easy for researchers to develop and optimize new approaches.
\\ ( https://arxiv.org/abs/2509.16713 ,  494kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16717
Date: Sat, 20 Sep 2025 15:00:28 GMT   (455kb)

Title: Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance
  Control for Short Video Search Relevance Modeling
Authors: Haoran Li, Zhiming Su, Junyan Yao, Enwei Zhang, Yang Ji, Yan Chen, Kan
  Zhou, Chao Feng, Jiao Ran
Categories: cs.CL
Comments: Submitted to AAAI 2026
\\
  Synthetic data is widely adopted in embedding models to ensure diversity in
training data distributions across dimensions such as difficulty, length, and
language. However, existing prompt-based synthesis methods struggle to capture
domain-specific data distributions, particularly in data-scarce domains, and
often overlook fine-grained relevance diversity. In this paper, we present a
Chinese short video dataset with 4-level relevance annotations, filling a
critical resource void. Further, we propose a semi-supervised synthetic data
pipeline where two collaboratively trained models generate domain-adaptive
short video data with controllable relevance labels. Our method enhances
relevance-level diversity by synthesizing samples for underrepresented
intermediate relevance labels, resulting in a more balanced and semantically
rich training data set. Extensive offline experiments show that the embedding
model trained on our synthesized data outperforms those using data generated
based on prompting or vanilla supervised fine-tuning(SFT). Moreover, we
demonstrate that incorporating more diverse fine-grained relevance levels in
training data enhances the model's sensitivity to subtle semantic distinctions,
highlighting the value of fine-grained relevance supervision in embedding
learning. In the search enhanced recommendation pipeline of Douyin's
dual-column scenario, through online A/B testing, the proposed model increased
click-through rate(CTR) by 1.45%, raised the proportion of Strong Relevance
Ratio (SRR) by 4.9%, and improved the Image User Penetration Rate (IUPR) by
0.1054%.
\\ ( https://arxiv.org/abs/2509.16717 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16720
Date: Sat, 20 Sep 2025 15:10:26 GMT   (160kb)

Title: Time to Revist Exact Match
Authors: Auss Abbood, Zaiqiao Meng, Nigel Collier
Categories: cs.CL
Comments: Accepted for Findings of EMNLP 2025
\\
  Temporal question answering is an established method for evaluating temporal
reasoning in large language models. Expected answers are often numeric (e.g.,
dates or durations), yet model responses are evaluated like regular text with
exact match (EM), unable to distinguish small from large errors. In this
investigative work, we frame temporal question answering as a numerical
estimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a
benchmark distilled from Test of Time and TempTabQA, where all questions
require a numerical, temporal answer, allowing us to evaluate models beyond EM.
We use the forecasting metrics symmetric mean absolute percentage error (sMAPE)
and mean absolute scaled error (MASE). With sMAPE, we find that error size and
EM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some
models have high sMAPE despite high EM. Scaling errors by the deviation of the
ground truth data with MASE reshuffles model rankings compared to EM, revealing
gaps in models' understanding of temporal domain knowledge, especially when
trained with synthetic data. Lastly, the models' most frequent error is to
deviate by only $\pm1$ from the ground truth. sMAPE and MASE, unlike EM,
adequately weight these errors. Our findings underscore the need for
specialised metrics for temporal QA tasks. Code and data are available on
https://github.com/aauss/temporal-answer-qa.
\\ ( https://arxiv.org/abs/2509.16720 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16722
Date: Sat, 20 Sep 2025 15:20:33 GMT   (50kb)

Title: A Multi-Level Benchmark for Causal Language Understanding in Social
  Media Discourse
Authors: Xiaohan Ding, Kaike Ping, Buse \c{C}ar{\i}k, Eugenia Rho
Categories: cs.CL
\\
  Understanding causal language in informal discourse is a core yet
underexplored challenge in NLP. Existing datasets largely focus on explicit
causality in structured text, providing limited support for detecting implicit
causal expressions, particularly those found in informal, user-generated social
media posts. We introduce CausalTalk, a multi-level dataset of five years of
Reddit posts (2020-2024) discussing public health related to the COVID-19
pandemic, among which 10120 posts are annotated across four causal tasks: (1)
binary causal classification, (2) explicit vs. implicit causality, (3)
cause-effect span extraction, and (4) causal gist generation. Annotations
comprise both gold-standard labels created by domain experts and
silver-standard labels generated by GPT-4o and verified by human annotators.
CausalTalk bridges fine-grained causal detection and gist-based reasoning over
informal text. It enables benchmarking across both discriminative and
generative models, and provides a rich resource for studying causal reasoning
in social media contexts.
\\ ( https://arxiv.org/abs/2509.16722 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16729
Date: Sat, 20 Sep 2025 15:48:22 GMT   (161kb)

Title: Angular Dispersion Accelerates $k$-Nearest Neighbors Machine Translation
Authors: Evgeniia Tokarchuk, Sergey Troshin, Vlad Niculae
Categories: cs.CL cs.LG
\\
  Augmenting neural machine translation with external memory at decoding time,
in the form of k-nearest neighbors machine translation ($k$-NN MT), is a
well-established strategy for increasing translation performance. $k$-NN MT
retrieves a set of tokens that occurred in the most similar contexts recorded
in a prepared data store, using hidden state representations of translation
contexts as vector lookup keys. One of the main disadvantages of this method is
the high computational cost and memory requirements. Since an exhaustive search
is not feasible in large data stores, practitioners commonly use approximate
$k$-NN MT lookup, yet even such algorithms are a bottleneck. In contrast to
research directions seeking to accelerate $k$-NN MT by reducing data store size
or the number of lookup calls, we pursue an orthogonal direction based on the
performance properties of approximate $k$-NN MT lookup data structures. In
particular, we propose to encourage angular dispersion of the neural hidden
representations of contexts. We show that improving dispersion leads to better
balance in the retrieval data structures, accelerating retrieval and slightly
improving translations.
\\ ( https://arxiv.org/abs/2509.16729 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16765
Date: Sat, 20 Sep 2025 18:10:30 GMT   (1026kb)

Title: The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language
  Models for Speech Pathology
Authors: Fagun Patel, Duc Q. Nguyen, Sang T. Truong, Jody Vaynshtok, Sanmi
  Koyejo, Nick Haber
Categories: cs.CL cs.AI cs.SD eess.AS
Comments: EMNLP 2025 Oral Presentation
\\
  According to the U.S. National Institutes of Health, more than 3.4 million
children experience speech disorders that require clinical intervention. The
number of speech-language pathologists (SLPs) is roughly 20 times fewer than
the number of affected children, highlighting a significant gap in children's
care and a pressing need for technological support that improves the
productivity of SLPs. State-of-the-art multimodal language models (MLMs) show
promise for supporting SLPs, but their use remains underexplored largely due to
a limited understanding of their performance in high-stakes clinical settings.
To address this gap, we collaborate with domain experts to develop a taxonomy
of real-world use cases of MLMs in speech-language pathologies. Building on
this taxonomy, we introduce the first comprehensive benchmark for evaluating
MLM across five core use cases, each containing 1,000 manually annotated data
points. This benchmark includes robustness and sensitivity tests under various
settings, including background noise, speaker gender, and accent. Our
evaluation of 15 state-of-the-art MLMs reveals that no single model
consistently outperforms others across all tasks. Notably, we find systematic
disparities, with models performing better on male speakers, and observe that
chain-of-thought prompting can degrade performance on classification tasks with
large label spaces and narrow decision boundaries. Furthermore, we study
fine-tuning MLMs on domain-specific data, achieving improvements of over 30%
compared to base models. These findings highlight both the potential and
limitations of current MLMs for speech-language pathology applications,
underscoring the need for further research and targeted development.
\\ ( https://arxiv.org/abs/2509.16765 ,  1026kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16781
Date: Sat, 20 Sep 2025 19:06:51 GMT   (242kb)

Title: MoRoVoc: A Large Dataset for Geographical Variation Identification of
  the Spoken Romanian Language
Authors: Andrei-Marius Avram, Ema-Ioana B\u{a}nescu, Anda-Teodora Robea,
  Dumitru-Clementin Cercel and Mihaela-Claudia Cercel
Categories: cs.CL
Comments: Accepted at EMNLP Findings 2025
\\
  This paper introduces MoRoVoc, the largest dataset for analyzing the regional
variation of spoken Romanian. It has more than 93 hours of audio and 88,192
audio samples, balanced between the Romanian language spoken in Romania and the
Republic of Moldova. We further propose a multi-target adversarial training
framework for speech models that incorporates demographic attributes (i.e., age
and gender of the speakers) as adversarial targets, making models
discriminative for primary tasks while remaining invariant to secondary
attributes. The adversarial coefficients are dynamically adjusted via
meta-learning to optimize performance. Our approach yields notable gains:
Wav2Vec2-Base achieves 78.21% accuracy for the variation identification of
spoken Romanian using gender as an adversarial target, while Wav2Vec2-Large
reaches 93.08% accuracy for gender classification when employing both dialect
and age as adversarial objectives.
\\ ( https://arxiv.org/abs/2509.16781 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16788
Date: Sat, 20 Sep 2025 19:32:16 GMT   (8330kb)

Title: Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis:
  A Comparative Study of Domain Adaptation and Fine-Tuning Strategies
Authors: Salha Alyami, Amani Jamal, Areej Alhothali
Categories: cs.CL cs.AI cs.LG
Comments: 26 excluding bibliography , journal article
\\
  Aspect-based sentiment analysis (ABSA) in natural language processing enables
organizations to understand customer opinions on specific product aspects.
While deep learning models are widely used for English ABSA, their application
in Arabic is limited due to the scarcity of labeled data. Researchers have
attempted to tackle this issue by using pre-trained contextualized language
models such as BERT. However, these models are often based on fact-based data,
which can introduce bias in domain-specific tasks like ABSA. To our knowledge,
no studies have applied adaptive pre-training with Arabic contextualized models
for ABSA. This research proposes a novel approach using domain-adaptive
pre-training for aspect-sentiment classification (ASC) and opinion target
expression (OTE) extraction. We examine fine-tuning strategies - feature
extraction, full fine-tuning, and adapter-based methods - to enhance
performance and efficiency, utilizing multiple adaptation corpora and
contextualized models. Our results show that in-domain adaptive pre-training
yields modest improvements. Adapter-based fine-tuning is a computationally
efficient method that achieves competitive results. However, error analyses
reveal issues with model predictions and dataset labeling. In ASC, common
problems include incorrect sentiment labeling, misinterpretation of contrastive
markers, positivity bias for early terms, and challenges with conflicting
opinions and subword tokenization. For OTE, issues involve mislabeling targets,
confusion over syntactic roles, difficulty with multi-word expressions, and
reliance on shallow heuristics. These findings underscore the need for syntax-
and semantics-aware models, such as graph convolutional networks, to more
effectively capture long-distance relations and complex aspect-based opinion
alignments.
\\ ( https://arxiv.org/abs/2509.16788 ,  8330kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16804
Date: Sat, 20 Sep 2025 20:44:29 GMT   (1193kb)

Title: KuBERT: Central Kurdish BERT Model and Its Application for Sentiment
  Analysis
Authors: Kozhin muhealddin Awlla, Hadi Veisi, Abdulhady Abas Abdullah
Categories: cs.CL cs.AI
\\
  This paper enhances the study of sentiment analysis for the Central Kurdish
language by integrating the Bidirectional Encoder Representations from
Transformers (BERT) into Natural Language Processing techniques. Kurdish is a
low-resourced language, having a high level of linguistic diversity with
minimal computational resources, making sentiment analysis somewhat
challenging. Earlier, this was done using a traditional word embedding model,
such as Word2Vec, but with the emergence of new language models, specifically
BERT, there is hope for improvements. The better word embedding capabilities of
BERT lend to this study, aiding in the capturing of the nuanced semantic pool
and the contextual intricacies of the language under study, the Kurdish
language, thus setting a new benchmark for sentiment analysis in low-resource
languages.
\\ ( https://arxiv.org/abs/2509.16804 ,  1193kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16813
Date: Sat, 20 Sep 2025 21:32:11 GMT   (6857kb)

Title: Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable
  Cognition-Informed Approach to Quantifying Identity Fusion from Text
Authors: Devin R. Wright, Jisun An, Yong-Yeol Ahn
Categories: cs.CL
Comments: Authors' accepted manuscript (postprint; camera-ready). To appear in
  the Proceedings of EMNLP 2025. Pagination/footer layout may differ from the
  Version of Record
ACM-class: I.2.7; H.3.1; I.5.4; J.4
\\
  Quantifying identity fusion -- the psychological merging of self with another
entity or abstract target (e.g., a religious group, political party, ideology,
value, brand, belief, etc.) -- is vital for understanding a wide range of
group-based human behaviors. We introduce the Cognitive Linguistic Identity
Fusion Score (CLIFS), a novel metric that integrates cognitive linguistics with
large language models (LLMs), which builds on implicit metaphor detection.
Unlike traditional pictorial and verbal scales, which require controlled
surveys or direct field contact, CLIFS delivers fully automated, scalable
assessments while maintaining strong alignment with the established verbal
measure. In benchmarks, CLIFS outperforms both existing automated approaches
and human annotation. As a proof of concept, we apply CLIFS to violence risk
assessment to demonstrate that it can improve violence risk assessment by more
than 240%. Building on our identification of a new NLP task and early success,
we underscore the need to develop larger, more diverse datasets that encompass
additional fusion-target domains and cultural backgrounds to enhance
generalizability and further advance this emerging area. CLIFS models and code
are public at https://github.com/DevinW-sudo/CLIFS.
\\ ( https://arxiv.org/abs/2509.16813 ,  6857kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16835
Date: Sat, 20 Sep 2025 23:18:50 GMT   (204kb)

Title: Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual
  Brainstorming
Authors: Melkamu Abay Mersha, Jugal Kalita
Categories: cs.CL cs.AI
\\
  Virtual brainstorming sessions have become a central component of
collaborative problem solving, yet the large volume and uneven distribution of
ideas often make it difficult to extract valuable insights efficiently. Manual
coding of ideas is time-consuming and subjective, underscoring the need for
automated approaches to support the evaluation of group creativity. In this
study, we propose a semantic-driven topic modeling framework that integrates
four modular components: transformer-based embeddings (Sentence-BERT),
dimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction
with refinement. The framework captures semantic similarity at the sentence
level, enabling the discovery of coherent themes from brainstorming transcripts
while filtering noise and identifying outliers. We evaluate our approach on
structured Zoom brainstorming sessions involving student groups tasked with
improving their university. Results demonstrate that our model achieves higher
topic coherence compared to established methods such as LDA, ETM, and BERTopic,
with an average coherence score of 0.687 (CV), outperforming baselines by a
significant margin. Beyond improved performance, the model provides
interpretable insights into the depth and diversity of topics explored,
supporting both convergent and divergent dimensions of group creativity. This
work highlights the potential of embedding-based topic modeling for analyzing
collaborative ideation and contributes an efficient and scalable framework for
studying creativity in synchronous virtual meetings.
\\ ( https://arxiv.org/abs/2509.16835 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16876
Date: Sun, 21 Sep 2025 02:04:52 GMT   (1563kb)

Title: Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation
  Assessment
Authors: Jiun-Ting Li, Bi-Cheng Yan, Yi-Cheng Wang, Berlin Chen
Categories: cs.CL
Comments: Accepted by APSIPA-ASC 2025
\\
  Automatic pronunciation assessment (APA) analyzes second-language (L2)
learners' speech by providing fine-grained pronunciation feedback at various
linguistic levels. Most existing efforts on APA typically adopt segmental-level
features as inputs and predict pronunciation scores at different granularities
via hierarchical (or parallel) pronunciation modeling. This, however,
inevitably causes assessments across linguistic levels (e.g., phone, word, and
utterance) to rely solely on phoneme-level pronunciation features, nearly
sidelining supra-segmental pronunciation cues. To address this limitation, we
introduce multi-task pretraining (MTP) for APA, a simple yet effective strategy
that attempts to capture long-term temporal pronunciation cues while
strengthening the intrinsic structures within an utterance via the objective of
reconstructing input features. Specifically, for a phoneme-level encoder of an
APA model, the proposed MTP strategy randomly masks segmental-level
pronunciation features and reconstructs the masked ones based on their
surrounding pronunciation context. Furthermore, current APA systems lack
integration with automated speaking assessment (ASA), limiting holistic
proficiency evaluation. Drawing on empirical studies and prior knowledge in
ASA, our framework bridges this gap by incorporating handcrafted features
(HCFs), such as fluency (speech rate, silence duration) and stress (pitch
accent strength), derived from human-designed formulas via regressors to
generate interpretable proficiency scores. Experiments on speechocean762 show
improved pronunciation scoring and ASA proficiency correlation, enabling
targeted training and comprehensive proficiency assessment.
\\ ( https://arxiv.org/abs/2509.16876 ,  1563kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16889
Date: Sun, 21 Sep 2025 02:51:15 GMT   (3297kb)

Title: Can GRPO Boost Complex Multimodal Table Understanding?
Authors: Xiaoqiang Kang, Shengen Wu, Zimu Wang, Yilin Liu, Xiaobo Jin, Kaizhu
  Huang, Wei Wang, Yutao Yue, Xiaowei Huang, Qiufeng Wang
Categories: cs.CL
Comments: EMNLP 2025
Journal-ref: EMNLP 2025
\\
  Existing table understanding methods face challenges due to complex table
structures and intricate logical reasoning. While supervised finetuning (SFT)
dominates existing research, reinforcement learning (RL), such as Group
Relative Policy Optimization (GRPO), has shown promise but struggled with low
initial policy accuracy and coarse rewards in tabular contexts. In this paper,
we introduce Table-R1, a three-stage RL framework that enhances multimodal
table understanding through: (1) Warm-up that prompts initial perception and
reasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs
continuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table
structures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes
fine-grained rewards of residual steps based on the hint-guided question.
Extensive experiments demonstrate that Table-R1 can boost the model's table
reasoning performance obviously on both held-in and held-out datasets,
outperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1
surpasses larger specific table understanding models (e.g., Table-LLaVA 13B),
even achieving comparable performance to the closed-source model GPT-4o on
held-in datasets, demonstrating the efficacy of each stage of Table-R1 in
overcoming initialization bottlenecks and reward sparsity, thereby advancing
robust multimodal table understanding.
\\ ( https://arxiv.org/abs/2509.16889 ,  3297kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16903
Date: Sun, 21 Sep 2025 03:34:31 GMT   (204kb)

Title: CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework
  Multi-lingual Discourse Relation Classification
Authors: Nawar Turk, Daniele Comitogianni, Leila Kosseim
Categories: cs.CL
\\
  We present our submission to Task 3 (Discourse Relation Classification) of
the DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse
relation labels across 39 corpora in 16 languages and six discourse frameworks,
posing significant multilingual and cross-formalism challenges. We first
benchmark the task by fine-tuning multilingual BERT-based models (mBERT,
XLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies
and progressive unfreezing ratios to establish strong baselines. We then
evaluate prompt-based large language models (namely Claude Opus 4.0) in
zero-shot and few-shot settings to understand how LLMs respond to the newly
proposed unified labels. Finally, we introduce HiDAC, a Hierarchical
Dual-Adapter Contrastive learning model. Results show that while larger
transformer models achieve higher accuracy, the improvements are modest, and
that unfreezing the top 75% of encoder layers yields performance comparable to
full fine-tuning while training far fewer parameters. Prompt-based models lag
significantly behind fine-tuned transformers, and HiDAC achieves the highest
overall accuracy (67.5%) while remaining more parameter-efficient than full
fine-tuning.
\\ ( https://arxiv.org/abs/2509.16903 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16914
Date: Sun, 21 Sep 2025 04:30:49 GMT   (429kb)

Title: CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge
  Transfer in Low-Resource Languages
Authors: Wenhao Zhuang and Yuan Sun
Categories: cs.CL
\\
  Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities
in various NLP tasks, significantly enhancing user experience and efficiency.
However, this advantage is primarily limited to resource-rich languages. For
the diverse array of low-resource languages, support remains inadequate, with
the scarcity of training corpora considered the primary cause. We construct and
open-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two
25GB sets of four-language corpora (one parallel and one non-parallel),
obtained through machine translation. CUTE encompasses two resource-rich
languages (Chinese and English) and two low-resource languages (Uyghur and
Tibetan). Prior to constructing CUTE, human assessment validates that the
machine translation quality between Chinese-Uyghur and Chinese-Tibetan
approaches that of Chinese-English translation. CUTE represents the largest
open-source corpus for Uyghur and Tibetan languages to date, and we demonstrate
its effectiveness in enhancing LLMs' ability to process low-resource languages
while investigating the role of corpus parallelism in cross-lingual transfer
learning. The CUTE corpus and related models are made publicly available to the
research community.
\\ ( https://arxiv.org/abs/2509.16914 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16929
Date: Sun, 21 Sep 2025 05:24:49 GMT   (492kb)

Title: K-DeCore: Facilitating Knowledge Transfer in Continual Structured
  Knowledge Reasoning via Knowledge Decoupling
Authors: Yongrui Chen, Yi Huang, Yunchang Liu, Shenyu Zhang, Junhao He,
  Tongtong Wu, Guilin Qi, Tianxing Wu
Categories: cs.CL
Comments: Accepted in Neurips 2025 (poster)
\\
  Continual Structured Knowledge Reasoning (CSKR) focuses on training models to
handle sequential tasks, where each task involves translating natural language
questions into structured queries grounded in structured knowledge. Existing
general continual learning approaches face significant challenges when applied
to this task, including poor generalization to heterogeneous structured
knowledge and inefficient reasoning due to parameter growth as tasks increase.
To address these limitations, we propose a novel CSKR framework,
\textsc{K-DeCore}, which operates with a fixed number of tunable parameters.
Unlike prior methods, \textsc{K-DeCore} introduces a knowledge decoupling
mechanism that disentangles the reasoning process into task-specific and
task-agnostic stages, effectively bridging the gaps across diverse tasks.
Building on this foundation, \textsc{K-DeCore} integrates a dual-perspective
memory consolidation mechanism for distinct stages and introduces a
structure-guided pseudo-data synthesis strategy to further enhance the model's
generalization capabilities. Extensive experiments on four benchmark datasets
demonstrate the superiority of \textsc{K-DeCore} over existing continual
learning methods across multiple metrics, leveraging various backbone large
language models.
\\ ( https://arxiv.org/abs/2509.16929 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16952
Date: Sun, 21 Sep 2025 07:24:17 GMT   (2855kb)

Title: AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level
  Evaluation
Authors: Tiancheng Huang, Ruisheng Cao, Yuxin Zhang, Zhangyi Kang, Zijian Wang,
  Chenrun Wang, Yijie Luo, Hang Zheng, Lirong Qian, Lu Chen, Kai Yu
Categories: cs.CL cs.AI
\\
  The growing volume of academic papers has made it increasingly difficult for
researchers to efficiently extract key information. While large language models
(LLMs) based agents are capable of automating question answering (QA) workflows
for scientific papers, there still lacks a comprehensive and realistic
benchmark to evaluate their capabilities. Moreover, training an interactive
agent for this specific task is hindered by the shortage of high-quality
interaction trajectories. In this work, we propose AirQA, a human-annotated
comprehensive paper QA dataset in the field of artificial intelligence (AI),
with 13,948 papers and 1,246 questions, that encompasses multi-task,
multi-modal and instance-level evaluation. Furthermore, we propose ExTrActor,
an automated framework for instruction data synthesis. With three LLM-based
agents, ExTrActor can perform example generation and trajectory collection
without human intervention. Evaluations of multiple open-source and proprietary
models show that most models underperform on AirQA, demonstrating the quality
of our dataset. Extensive experiments confirm that ExTrActor consistently
improves the multi-turn tool-use capability of small models, enabling them to
achieve performance comparable to larger ones.
\\ ( https://arxiv.org/abs/2509.16952 ,  2855kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16965
Date: Sun, 21 Sep 2025 07:52:28 GMT   (1421kb)

Title: Preference Distillation via Value based Reinforcement Learning
Authors: Minchan Kwon, Junwon Ko, Kangil Kim, Junmo Kim
Categories: cs.CL
Comments: 20 page
Journal-ref: NIPS 2025 Poster
\\
  Direct Preference Optimization (DPO) is a powerful paradigm to align language
models with human preferences using pairwise comparisons. However, its binary
win-or-loss supervision often proves insufficient for training small models
with limited capacity. Prior works attempt to distill information from large
teacher models using behavior cloning or KL divergence. These methods often
focus on mimicking current behavior and overlook distilling reward modeling. To
address this issue, we propose \textit{Teacher Value-based Knowledge
Distillation} (TVKD), which introduces an auxiliary reward from the value
function of the teacher model to provide a soft guide. This auxiliary reward is
formulated to satisfy potential-based reward shaping, ensuring that the global
reward structure and optimal policy of DPO are preserved. TVKD can be
integrated into the standard DPO training framework and does not require
additional rollouts. Our experimental results show that TVKD consistently
improves performance across various benchmarks and model sizes.
\\ ( https://arxiv.org/abs/2509.16965 ,  1421kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16990
Date: Sun, 21 Sep 2025 09:09:36 GMT   (28kb)

Title: Advancing Speech Understanding in Speech-Aware Language Models with GRPO
Authors: Avishai Elmakies, Hagai Aronowitz, Nimrod Shabtay, Eli Schwartz, Ron
  Hoory, Avihu Dekel
Categories: cs.CL cs.AI cs.LG cs.SD eess.AS
\\
  In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based
method for training Speech-Aware Large Language Models (SALLMs) on open-format
speech understanding tasks, such as Spoken Question Answering and Automatic
Speech Translation. SALLMs have proven highly effective for speech
understanding tasks. GRPO has recently gained traction for its efficiency in
training LLMs, and prior work has explored its application to SALLMs, primarily
in multiple-choice tasks. Building on this, we focus on open-format tasks that
better reflect the generative abilities of the models. Our approach leverages
GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate
empirically that it surpasses standard SFT across several key metrics. Finally,
we explore the potential of incorporating off-policy samples within GRPO for
these tasks, highlighting avenues for further improvement and further research.
\\ ( https://arxiv.org/abs/2509.16990 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17030
Date: Sun, 21 Sep 2025 10:44:16 GMT   (17557kb)

Title: The Transfer Neurons Hypothesis: An Underlying Mechanism for Language
  Latent Space Transitions in Multilingual LLMs
Authors: Hinata Tezuka, Naoya Inoue
Categories: cs.CL cs.AI cs.LG
Comments: 57 pages, 47 figures and 41 tables; Accepted to EMNLP 2025 Main
\\
  Recent studies have suggested a processing framework for multilingual inputs
in decoder-based LLMs: early layers convert inputs into English-centric and
language-agnostic representations; middle layers perform reasoning within an
English-centric latent space; and final layers generate outputs by transforming
these representations back into language-specific latent spaces. However, the
internal dynamics of such transformation and the underlying mechanism remain
underexplored. Towards a deeper understanding of this framework, we propose and
empirically validate The Transfer Neurons Hypothesis: certain neurons in the
MLP module are responsible for transferring representations between
language-specific latent spaces and a shared semantic latent space.
Furthermore, we show that one function of language-specific neurons, as
identified in recent studies, is to facilitate movement between latent spaces.
Finally, we show that transfer neurons are critical for reasoning in
multilingual LLMs.
\\ ( https://arxiv.org/abs/2509.17030 ,  17557kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17047
Date: Sun, 21 Sep 2025 11:59:47 GMT   (2324kb)

Title: Modeling Bottom-up Information Quality during Language Processing
Authors: Cui Ding, Yanning Yin, Lena A. J\"ager, Ethan Gotlieb Wilcox
Categories: cs.CL
\\
  Contemporary theories model language processing as integrating both top-down
expectations and bottom-up inputs. One major prediction of such models is that
the quality of the bottom-up inputs modulates ease of processing -- noisy
inputs should lead to difficult and effortful comprehension. We test this
prediction in the domain of reading. First, we propose an information-theoretic
operationalization for the "quality" of bottom-up information as the mutual
information (MI) between visual information and word identity. We formalize
this prediction in a mathematical model of reading as a Bayesian update.
Second, we test our operationalization by comparing participants' reading times
in conditions where words' information quality has been reduced, either by
occluding their top or bottom half, with full words. We collect data in English
and Chinese. We then use multimodal language models to estimate the mutual
information between visual inputs and words. We use these data to estimate the
specific effect of reduced information quality on reading times. Finally, we
compare how information is distributed across visual forms. In English and
Chinese, the upper half contains more information about word identity than the
lower half. However, the asymmetry is more pronounced in English, a pattern
which is reflected in the reading times.
\\ ( https://arxiv.org/abs/2509.17047 ,  2324kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17054
Date: Sun, 21 Sep 2025 12:18:35 GMT   (4053kb)

Title: TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White
  Lies?
Authors: Yiwei Liu, Emma Jane Pretty, Jiahao Huang, Saku Sugawara
Categories: cs.CL cs.AI
\\
  While recent studies explore Large Language Models' (LLMs) performance on
Theory of Mind (ToM) reasoning tasks, research on ToM abilities that require
more nuanced social context is limited, such as white lies. We introduce
TactfulToM, a novel English benchmark designed to evaluate LLMs' ability to
understand white lies within real-life conversations and reason about prosocial
motivations behind them, particularly when they are used to spare others'
feelings and maintain social harmony. Our benchmark is generated through a
multi-stage human-in-the-loop pipeline where LLMs expand manually designed seed
stories into conversations to maintain the information asymmetry between
participants necessary for authentic white lies. We show that TactfulToM is
challenging for state-of-the-art models, which perform substantially below
humans, revealing shortcomings in their ability to fully comprehend the ToM
reasoning that enables true understanding of white lies.
\\ ( https://arxiv.org/abs/2509.17054 ,  4053kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17167
Date: Sun, 21 Sep 2025 17:26:58 GMT   (471kb)

Title: SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated
  Inductive Thematic Analysis
Authors: Seungjun Yi, Joakim Nguyen, Huimin Xu, Terence Lim, Joseph Skrovan,
  Mehak Beri, Hitakshi Modi, Andrew Well, Liu Leqi, Mia Markey, Ying Ding
Categories: cs.CL
\\
  Thematic Analysis (TA) is a widely used qualitative method that provides a
structured yet flexible framework for identifying and reporting patterns in
clinical interview transcripts. However, manual thematic analysis is
time-consuming and limits scalability. Recent advances in LLMs offer a pathway
to automate thematic analysis, but alignment with human results remains
limited. To address these limitations, we propose SFT-TA, an automated thematic
analysis framework that embeds supervised fine-tuned (SFT) agents within a
multi-agent system. Our framework outperforms existing frameworks and the
gpt-4o baseline in alignment with human reference themes. We observed that SFT
agents alone may underperform, but achieve better results than the baseline
when embedded within a multi-agent system. Our results highlight that embedding
SFT agents in specific roles within a multi-agent system is a promising pathway
to improve alignment with desired outputs for thematic analysis.
\\ ( https://arxiv.org/abs/2509.17167 ,  471kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17177
Date: Sun, 21 Sep 2025 17:53:30 GMT   (1904kb)

Title: FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning
  Models on Automatically Verifiable Textual and Visual Questions
Authors: Bowen Qin, Chen Yue, Fang Yin, Hui Wang, JG Yao, Jiakang Liu, Jing-Shu
  Zheng, Miguel Hu Chen, Richeng Xuan, Shibei Meng, Shiqi Zhou, Teng Dai,
  Tong-Shuai Ren, Wei Cui, Xi Yang, Xialin Du, Xiaojing Xu, Xue Sun, Xuejing
  Li, Yaming Liu, Yesheng Liu, Ying Liu, Yonghua Lin, Yu Zhao, Yunduo Zhang,
  Yuwen Luo, Zheqi He, Zhiyuan He, Zhongyuan Wang
Categories: cs.CL cs.CV cs.LG
Comments: 23 pages in main text
\\
  We conduct a moderate-scale contamination-free (to some extent) evaluation of
current large reasoning models (LRMs) with some preliminary findings. We also
release ROME, our evaluation benchmark for vision language models intended to
test reasoning from visual clues. We attach links to the benchmark, evaluation
data, and other updates on this website:
https://flageval-baai.github.io/LRM-Eval/
\\ ( https://arxiv.org/abs/2509.17177 ,  1904kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17178
Date: Sun, 21 Sep 2025 17:55:30 GMT   (13393kb)

Title: Attention Consistency for LLMs Explanation
Authors: Tian Lan, Jinyuan Xu, Xue He, Jenq-Neng Hwang, Lei Li
Categories: cs.CL
\\
  Understanding the decision-making processes of large language models (LLMs)
is essential for their trustworthy development and deployment. However, current
interpretability methods often face challenges such as low resolution and high
computational cost. To address these limitations, we propose the
\textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight,
and easily deployable heuristic for estimating the importance of input tokens
in decoder-based models. MACS measures contributions of input tokens based on
the consistency of maximal attention. Empirical evaluations demonstrate that
MACS achieves a favorable trade-off between interpretability quality and
computational efficiency, showing faithfulness comparable to complex techniques
with a 22\% decrease in VRAM usage and 30\% reduction in latency.
\\ ( https://arxiv.org/abs/2509.17178 ,  13393kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17183
Date: Sun, 21 Sep 2025 18:06:05 GMT   (9986kb)

Title: LifeAlign: Lifelong Alignment for Large Language Models with
  Memory-Augmented Focalized Preference Optimization
Authors: Junsong Li, Jie Zhou, Bihao Zhan, Yutao Yang, Qianjun Pan, Shilian
  Chen, Tianyu Huai, Xin Li, Qin Chen, Liang He
Categories: cs.CL cs.AI cs.LG
\\
  Alignment plays a crucial role in Large Language Models (LLMs) in aligning
with human preferences on a specific task/domain. Traditional alignment methods
suffer from catastrophic forgetting, where models lose previously acquired
knowledge when adapting to new preferences or domains. We introduce LifeAlign,
a novel framework for lifelong alignment that enables LLMs to maintain
consistent human preference alignment across sequential learning tasks without
forgetting previously learned knowledge. Our approach consists of two key
innovations. First, we propose a focalized preference optimization strategy
that aligns LLMs with new preferences while preventing the erosion of knowledge
acquired from previous tasks. Second, we develop a short-to-long memory
consolidation mechanism that merges denoised short-term preference
representations into stable long-term memory using intrinsic dimensionality
reduction, enabling efficient storage and retrieval of alignment patterns
across diverse domains. We evaluate LifeAlign across multiple sequential
alignment tasks spanning different domains and preference types. Experimental
results demonstrate that our method achieves superior performance in
maintaining both preference alignment quality and knowledge retention compared
to existing lifelong learning approaches. The codes and datasets will be
released on GitHub.
\\ ( https://arxiv.org/abs/2509.17183 ,  9986kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17196
Date: Sun, 21 Sep 2025 18:53:12 GMT   (2679kb)

Title: Evolution of Concepts in Language Model Pre-Training
Authors: Xuyang Ge, Wentao Shu, Jiaxing Wu, Yunhua Zhou, Zhengfu He, Xipeng Qiu
Categories: cs.CL cs.AI
Comments: 30 pages, 25 figures
\\
  Language models obtain extensive capabilities through pre-training. However,
the pre-training process remains a black box. In this work, we track linear
interpretable feature evolution across pre-training snapshots using a sparse
dictionary learning method called crosscoders. We find that most features begin
to form around a specific point, while more complex patterns emerge in later
training stages. Feature attribution analyses reveal causal connections between
feature evolution and downstream performance. Our feature-level observations
are highly consistent with previous findings on Transformer's two-stage
learning process, which we term a statistical learning phase and a feature
learning phase. Our work opens up the possibility to track fine-grained
representation progress during language model learning dynamics.
\\ ( https://arxiv.org/abs/2509.17196 ,  2679kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17209
Date: Sun, 21 Sep 2025 19:28:37 GMT   (40kb)

Title: Prompt-Based Simplification for Plain Language using Spanish Language
  Models
Authors: Lourdes Moreno, Jesus M. Sanchez-Gomez, Marco Antonio
  Sanchez-Escudero, Paloma Mart\'inez
Categories: cs.CL
Comments: 11 pages, 7 tables,
\\
  This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask
1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies
based on models trained on Spanish texts, including a zero-shot configuration
using prompt engineering and a fine-tuned version with Low-Rank Adaptation
(LoRA). Different strategies were evaluated on representative internal subsets
of the training data, using the official task metrics, cosine similarity (SIM)
and the Fern\'andez-Huerta readability index (FH) to guide the selection of the
optimal model and prompt combination. The final system was selected for its
balanced and consistent performance, combining normalization steps, the
RigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in
semantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72).
We also discuss key challenges related to training data heterogeneity and the
limitations of current evaluation metrics in capturing both linguistic clarity
and content preservation.
\\ ( https://arxiv.org/abs/2509.17209 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17249
Date: Sun, 21 Sep 2025 21:46:58 GMT   (9793kb)

Title: Extending Automatic Machine Translation Evaluation to Book-Length
  Documents
Authors: Kuang-Da Wang, Shuoyang Ding, Chao-Han Huck Yang, Ping-Chun Hsieh,
  Wen-Chih Peng, Vitaly Lavrukhin, Boris Ginsburg
Categories: cs.CL
Comments: Accepted for EMNLP 2025 main conference
\\
  Despite Large Language Models (LLMs) demonstrating superior translation
performance and long-context capabilities, evaluation methodologies remain
constrained to sentence-level assessment due to dataset limitations, token
number restrictions in metrics, and rigid sentence boundary requirements. We
introduce SEGALE, an evaluation scheme that extends existing automatic metrics
to long-document translation by treating documents as continuous text and
applying sentence segmentation and alignment methods. Our approach enables
previously unattainable document-level evaluation, handling translations of
arbitrary length generated with document-level prompts while accounting for
under-/over-translations and varied sentence boundaries. Experiments show our
scheme significantly outperforms existing long-form document evaluation
schemes, while being comparable to evaluations performed with groundtruth
sentence alignments. Additionally, we apply our scheme to book-length texts and
newly demonstrate that many open-weight LLMs fail to effectively translate
documents at their reported maximum context lengths.
\\ ( https://arxiv.org/abs/2509.17249 ,  9793kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17276
Date: Sun, 21 Sep 2025 23:18:24 GMT   (1393kb)

Title: Probabilistic Token Alignment for Large Language Model Fusion
Authors: Runjia Zeng, James Chenhao Liang, Cheng Han, Zhiwen Cao, Jiahao Liu,
  Xiaojun Quan, Yingjie Victor Chen, Lifu Huang, Tong Geng, Qifan Wang,
  Dongfang Liu
Categories: cs.CL cs.AI cs.LG
Comments: NeurIPS 2025
\\
  Training large language models (LLMs) from scratch can yield models with
unique functionalities and strengths, but it is costly and often leads to
redundant capabilities. A more cost-effective alternative is to fuse existing
pre-trained LLMs with different architectures into a more powerful model.
However, a key challenge in existing model fusion is their dependence on
manually predefined vocabulary alignment, which may not generalize well across
diverse contexts, leading to performance degradation in several evaluation. To
solve this, we draw inspiration from distribution learning and propose the
probabilistic token alignment method as a general and soft mapping for
alignment, named as PTA-LLM. Our approach innovatively reformulates token
alignment into a classic mathematical problem: optimal transport, seamlessly
leveraging distribution-aware learning to facilitate more coherent model
fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability
from a distributional perspective, offering insights into the essence of the
token alignment. Empirical results demonstrate that probabilistic token
alignment enhances the target model's performance across multiple capabilities.
Our code is avaliable at https://runjia.tech/neurips_pta-llm/.
\\ ( https://arxiv.org/abs/2509.17276 ,  1393kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17289
Date: Mon, 22 Sep 2025 00:01:50 GMT   (554kb)

Title: Automated Knowledge Graph Construction using Large Language Models and
  Sentence Complexity Modelling
Authors: Sydney Anuyah, Mehedi Mahmud Kaushik, Krishna Dwarampudi, Rakesh
  Shiradkar, Arjan Durresi, Sunandan Chakraborty
Categories: cs.CL
\\
  We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting
sentence-level knowledge graphs by combining robust coreference resolution with
syntactic sentence decomposition. Using our model, we contribute a dataset of
over 150,000 knowledge triples, which is open source. We also contribute a
training corpus of 7248 rows for sentence complexity, 190 rows of gold human
annotations for co-reference resolution using open source lung-cancer abstracts
from PubMed, 900 rows of gold human annotations for sentence conversion
policies, and 398 triples of gold human annotations. We systematically select
optimal prompt-model pairs across five complexity categories, showing that
hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match
accuracy on sentence simplification. On relation extraction (RE), our pipeline
achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the
art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on
Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference
and decomposition increases recall on rare relations by over 20%. Code and
dataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025
\\ ( https://arxiv.org/abs/2509.17289 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17292
Date: Mon, 22 Sep 2025 00:18:58 GMT   (1500kb)

Title: Multi-View Attention Multiple-Instance Learning Enhanced by LLM
  Reasoning for Cognitive Distortion Detection
Authors: Jun Seo Kim, Hyemi Kim, Woo Joo Oh, Hongjin Cho, Hochul Lee, Hye Hyeon
  Kim
Categories: cs.CL cs.AI
\\
  Cognitive distortions have been closely linked to mental health disorders,
yet their automatic detection remained challenging due to contextual ambiguity,
co-occurrence, and semantic overlap. We proposed a novel framework that
combines Large Language Models (LLMs) with Multiple-Instance Learning (MIL)
architecture to enhance interpretability and expression-level reasoning. Each
utterance was decomposed into Emotion, Logic, and Behavior (ELB) components,
which were processed by LLMs to infer multiple distortion instances, each with
a predicted type, expression, and model-assigned salience score. These
instances were integrated via a Multi-View Gated Attention mechanism for final
classification. Experiments on Korean (KoACD) and English (Therapist QA)
datasets demonstrate that incorporating ELB and LLM-inferred salience scores
improves classification performance, especially for distortions with high
interpretive ambiguity. Our results suggested a psychologically grounded and
generalizable approach for fine-grained reasoning in mental health NLP.
\\ ( https://arxiv.org/abs/2509.17292 ,  1500kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17317
Date: Mon, 22 Sep 2025 02:48:43 GMT   (9399kb)

Title: Scaling, Simplification, and Adaptation: Lessons from Pretraining on
  Machine-Translated Text
Authors: Dan John Velasco and Matthew Theodore Roque
Categories: cs.CL cs.AI
Comments: Under review
\\
  Most languages lack sufficient data for large-scale monolingual pretraining,
creating a "data wall." Multilingual pretraining helps but is limited by
language imbalance and the "curse of multilinguality." An alternative is to
translate high-resource text with machine translation (MT), which raises three
questions: (1) How does MT-derived data scale with model capacity? (2) Can
source-side transformations (e.g., simplifying English with an LLM) improve
generalization to native text? (3) How well do models pretrained on MT-derived
data adapt when continually trained on limited native text? We investigate
these questions by translating English into Indonesian and Tamil--two
typologically distant, lower-resource languages--and pretraining GPT-2 models
(124M-774M) on native or MT-derived corpora from raw and LLM-simplified
English. We evaluate cross-entropy loss on native text, along with accuracy on
syntactic probes and downstream tasks. Our results show that (1) MT-pretrained
models benefit from scaling; (2) source-side simplification harms
generalization to native text; and (3) adapting MT-pretrained models on native
text often yields better performance than native-only models, even with less
native data. However, tasks requiring cultural nuance (e.g., toxicity
detection) demand more exposure to native data.
\\ ( https://arxiv.org/abs/2509.17317 ,  9399kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17348
Date: Mon, 22 Sep 2025 04:19:29 GMT   (9031kb)

Title: AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories
  for Language Model Continual Learning
Authors: Yujie Feng, Jian Li, Xiaoyu Dong, Pengfei Xu, Xiaohui Zhou, Yujia
  Zhang, Zexin LU, Yasha Wang, Alan Zhao, Xu Chu, Xiao-Ming Wu
Categories: cs.CL cs.AI
Comments: EMNLP 2025
\\
  Continual learning (CL) is essential for deploying large language models
(LLMs) in dynamic real-world environments without the need for costly
retraining. Recent model merging-based methods have attracted significant
attention, but they still struggle to effectively manage the trade-off between
learning new knowledge and preventing forgetting, a challenge largely stemming
from suboptimal number of merges and merging frequency. In this paper, we
introduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework
that utilizes learning and forgetting signals from the training trajectory to
dynamically monitor the model's training status. Guided by dynamic monitoring,
the training trajectory-guided merge controller adaptively determines the
timing and frequency of iterative fusion, while the rehearsal-based knowledge
fusion module computes the merging weights and executes the fusion.
Comprehensive experiments on three CL benchmarks with various model sizes (from
770M to 13B) demonstrate that AimMerging achieves significant performance
improvements over existing state-of-the-art methods, with an average relative
improvement of 80% and 59% on FWT and BWT, respectively. The source code is
provided for reproducibility.
\\ ( https://arxiv.org/abs/2509.17348 ,  9031kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17349
Date: Mon, 22 Sep 2025 04:21:19 GMT   (7839kb)

Title: Better Late Than Never: Evaluation of Latency Metrics for Simultaneous
  Speech-to-Text Translation
Authors: Peter Pol\'ak, Sara Papi, Luisa Bentivogli, Ond\v{r}ej Bojar
Categories: cs.CL cs.AI
\\
  Simultaneous speech-to-text translation (SimulST) systems have to balance
translation quality with latency--the delay between speech input and the
translated output. While quality evaluation is well established, accurate
latency measurement remains a challenge. Existing metrics often produce
inconsistent or misleading results, especially in the widely used short-form
setting, where speech is artificially presegmented. In this paper, we present
the first comprehensive analysis of SimulST latency metrics across language
pairs, systems, and both short- and long-form regimes. We uncover a structural
bias in current metrics related to segmentation that undermines fair and
meaningful comparisons. To address this, we introduce YAAL (Yet Another Average
Lagging), a refined latency metric that delivers more accurate evaluations in
the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and
propose SoftSegmenter, a novel resegmentation tool based on word-level
alignment. Our experiments show that YAAL and LongYAAL outperform popular
latency metrics, while SoftSegmenter enhances alignment quality in long-form
evaluation, together enabling more reliable assessments of SimulST systems.
\\ ( https://arxiv.org/abs/2509.17349 ,  7839kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17367
Date: Mon, 22 Sep 2025 05:34:15 GMT   (3494kb)

Title: Scale-free Characteristics of Multilingual Legal Texts and the
  Limitations of LLMs
Authors: Haoyang Chen and Kumiko Tanaka-Ishii
Categories: cs.CL
Comments: to be published in Text, Speech, and Dialogue (TSD 2025)
Journal-ref: Chen, H., Tanaka-Ishii, K. (2026). pp 102 to 114 Text, Speech, and
  Dialogue. TSD 2025. Lecture Notes in Computer Science(), vol 16030. Springer,
  Cham
DOI: 10.1007/978-3-032-02551-7_10
\\
  We present a comparative analysis of text complexity across domains using
scale-free metrics. We quantify linguistic complexity via Heaps' exponent
$\beta$ (vocabulary growth), Taylor's exponent $\alpha$ (word-frequency
fluctuation scaling), compression rate $r$ (redundancy), and entropy. Our
corpora span three domains: legal documents (statutes, cases, deeds) as a
specialized domain, general natural language texts (literature, Wikipedia), and
AI-generated (GPT) text. We find that legal texts exhibit slower vocabulary
growth (lower $\beta$) and higher term consistency (higher $\alpha$) than
general texts. Within legal domain, statutory codes have the lowest $\beta$ and
highest $\alpha$, reflecting strict drafting conventions, while cases and deeds
show higher $\beta$ and lower $\alpha$. In contrast, GPT-generated text shows
the statistics more aligning with general language patterns. These results
demonstrate that legal texts exhibit domain-specific structures and
complexities, which current generative models do not fully replicate.
\\ ( https://arxiv.org/abs/2509.17367 ,  3494kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17377
Date: Mon, 22 Sep 2025 06:35:27 GMT   (224kb)

Title: Robustness of Neurosymbolic Reasoners on First-Order Logic Problems
Authors: Hannah Bansal, Kemal Kurniawan and Lea Frermann
Categories: cs.CL
\\
  Recent trends in NLP aim to improve reasoning capabilities in Large Language
Models (LLMs), with key focus on generalization and robustness to variations in
tasks. Counterfactual task variants introduce minimal but semantically
meaningful changes to otherwise valid first-order logic (FOL) problem instances
altering a single predicate or swapping roles of constants to probe whether a
reasoning system can maintain logical consistency under perturbation. Previous
studies showed that LLMs becomes brittle on counterfactual variations,
suggesting that they often rely on spurious surface patterns to generate
responses. In this work, we explore if a neurosymbolic (NS) approach that
integrates an LLM and a symbolic logical solver could mitigate this problem.
Experiments across LLMs of varying sizes show that NS methods are more robust
but perform worse overall that purely neural methods. We then propose NSCoT
that combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate
that while it improves performance, NSCoT still lags behind standard CoT. Our
analysis opens research directions for future work.
\\ ( https://arxiv.org/abs/2509.17377 ,  224kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17395
Date: Mon, 22 Sep 2025 06:56:27 GMT   (526kb)

Title: FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis
Authors: Tianshi Cai, Guanxu Li, Nijia Han, Ce Huang, Zimu Wang, Changyu Zeng,
  Yuqi Wang, Jingshi Zhou, Haiyang Zhang, Qi Chen, Yushan Pan, Shuihua Wang,
  Wei Wang
Categories: cs.CL
Comments: Accepted at FinNLP@EMNLP 2025. Camera-ready version
\\
  We introduce FinDebate, a multi-agent framework for financial analysis,
integrating collaborative debate with domain-specific Retrieval-Augmented
Generation (RAG). Five specialized agents, covering earnings, market,
sentiment, valuation, and risk, run in parallel to synthesize evidence into
multi-dimensional insights. To mitigate overconfidence and improve reliability,
we introduce a safe debate protocol that enables agents to challenge and refine
initial conclusions while preserving coherent recommendations. Experimental
results, based on both LLM-based and human evaluations, demonstrate the
framework's efficacy in producing high-quality analysis with calibrated
confidence levels and actionable investment strategies across multiple time
horizons.
\\ ( https://arxiv.org/abs/2509.17395 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17396
Date: Mon, 22 Sep 2025 06:56:35 GMT   (5997kb)

Title: EpiCache: Episodic KV Cache Management for Long Conversational Question
  Answering
Authors: Minsoo Kim, Arnav Kundu, Han-Byul Kim, Richa Dixit, Minsik Cho
Categories: cs.CL
\\
  Recent advances in large language models (LLMs) have extended context
lengths, enabling assistants to sustain long histories for coherent,
personalized responses. This ability, however, hinges on Key-Value (KV)
caching, whose memory grows linearly with dialogue length and quickly dominates
under strict resource constraints. An active line of research for reducing this
overhead is KV cache compression, which seeks to limit cache size while
preserving accuracy. Yet existing methods face two major limitations: (i)
evicting entries after full-context prefill causes unbounded peak memory, and
(ii) query-dependent eviction narrows the cache to a single query, leading to
degraded accuracy in multi-turn conversations. We introduce EpiCache, a
training-free KV cache management framework for long conversational question
answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth
through block-wise prefill and preserves topic-relevant context via episodic KV
compression, which clusters conversation history into coherent episodes and
applies episode-specific KV cache eviction. We further design an adaptive
layer-wise budget allocation strategy that measures each layer's sensitivity to
eviction and distributes the memory budget across layers accordingly. Across
three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over
recent baselines, sustains near-full KV accuracy under 4-6x compression, and
reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient
multi-turn interaction under strict resource constraints.
\\ ( https://arxiv.org/abs/2509.17396 ,  5997kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17399
Date: Mon, 22 Sep 2025 06:58:02 GMT   (3180kb)

Title: DIWALI - Diversity and Inclusivity aWare cuLture specific Items for
  India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian
  Context
Authors: Pramit Sahoo, Maharaj Brahma, Maunendra Sankar Desarkar
Categories: cs.CL
Comments: Accepted at EMNLP 2025
\\
  Large language models (LLMs) are widely used in various tasks and
applications. However, despite their wide capabilities, they are shown to lack
cultural alignment \citep{ryan-etal-2024-unintended,
alkhamissi-etal-2024-investigating} and produce biased generations
\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence.
Evaluation of LLMs for cultural awareness and alignment is particularly
challenging due to the lack of proper evaluation metrics and unavailability of
culturally grounded datasets representing the vast complexity of cultures at
the regional and sub-regional levels. Existing datasets for culture specific
items (CSIs) focus primarily on concepts at the regional level and may contain
false positives. To address this issue, we introduce a novel CSI dataset for
Indian culture, belonging to 17 cultural facets. The dataset comprises $\sim$8k
cultural concepts from 36 sub-regions. To measure the cultural competence of
LLMs on a cultural text adaptation task, we evaluate the adaptations using the
CSIs created, LLM as Judge, and human evaluations from diverse
socio-demographic region. Furthermore, we perform quantitative analysis
demonstrating selective sub-regional coverage and surface-level adaptations
across all considered LLMs. Our dataset is available here:
\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI},
project
webpage\footnote{\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}},
and our codebase with model outputs can be found here:
\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}.
\\ ( https://arxiv.org/abs/2509.17399 ,  3180kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17418
Date: Mon, 22 Sep 2025 07:10:42 GMT   (4026kb)

Title: Vision Language Models Are Not (Yet) Spelling Correctors
Authors: Junhong Liang, Bojun Zhang
Categories: cs.CL cs.CV
\\
  Spelling correction from visual input poses unique challenges for vision
language models (VLMs), as it requires not only detecting but also correcting
textual errors directly within images. We present ReViCo (Real Visual
Correction), the first benchmark that systematically evaluates VLMs on
real-world visual spelling correction across Chinese and English. ReViCo
contains naturally occurring errors collected from real-world image data and
supports fine-grained evaluation at both image and token levels. Through
comprehensive experiments on representative cascaded (Qwen) and native
(InternVL) open-source models, as well as closed-source systems (GPT-4o,
Claude), we show that current VLMs fall significantly short of human
performance, particularly in correction. To address these limitations, we
explore two solution paradigms: a Joint OCR-Correction pipeline and a
Background Information enhanced approach, both of which yield consistent
performance gains. Our analysis highlights fundamental limitations of existing
architectures and provides actionable insights for advancing multimodal
spelling correction.
\\ ( https://arxiv.org/abs/2509.17418 ,  4026kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17421
Date: Mon, 22 Sep 2025 07:14:31 GMT   (9435kb)

Title: RealBench: A Chinese Multi-image Understanding Benchmark Close to
  Real-world Scenarios
Authors: Fei Zhao, Chengqiang Lu, Yufan Shen, Qimeng Wang, Yicheng Qian, Haoxin
  Zhang, Yan Gao, Yi Wu, Yao Hu, Zhen Wu, Shangyu Xing, Xinyu Dai
Categories: cs.CL cs.MM
Comments: Findings of EMNLP 2025 camera-ready
\\
  While various multimodal multi-image evaluation datasets have been emerged,
but these datasets are primarily based on English, and there has yet to be a
Chinese multi-image dataset. To fill this gap, we introduce RealBench, the
first Chinese multimodal multi-image dataset, which contains 9393 samples and
69910 images. RealBench distinguishes itself by incorporating real
user-generated content, ensuring high relevance to real-world applications.
Additionally, the dataset covers a wide variety of scenes, image resolutions,
and image structures, further increasing the difficulty of multi-image
understanding. Ultimately, we conduct a comprehensive evaluation of RealBench
using 21 multimodal LLMs of different sizes, including closed-source models
that support multi-image inputs as well as open-source visual and video models.
The experimental results indicate that even the most powerful closed-source
models still face challenges when handling multi-image Chinese scenarios.
Moreover, there remains a noticeable performance gap of around 71.8\% on
average between open-source visual/video models and closed-source models. These
results show that RealBench provides an important research foundation for
further exploring multi-image understanding capabilities in the Chinese
context.
\\ ( https://arxiv.org/abs/2509.17421 ,  9435kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17428
Date: Mon, 22 Sep 2025 07:21:41 GMT   (2731kb)

Title: QWHA: Quantization-Aware Walsh-Hadamard Adaptation for
  Parameter-Efficient Fine-Tuning on Large Language Models
Authors: Hyesung Jeon, Seojune Lee, Beomseok Kang, Yulhwa Kim, Jae-Joon Kim
Categories: cs.CL
Comments: 25 pages, 9 figures, 14 tables
\\
  The demand for efficient deployment of large language models (LLMs) has
driven interest in quantization, which reduces inference cost, and
parameter-efficient fine-tuning (PEFT), which lowers training overhead. This
motivated the development of quantization-aware PEFT to produce accurate yet
efficient quantized models. In this setting, reducing quantization error prior
to fine-tuning is crucial for achieving high model accuracy. However, existing
methods that rely on low-rank adaptation suffer from limited representational
capacity. Recent Fourier-related transform (FT)-based adapters offer greater
representational power than low-rank adapters, but their direct integration
into quantized models often results in ineffective error reduction and
increased computational overhead. To overcome these limitations, we propose
QWHA, a method that integrates FT-based adapters into quantized models by
employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together
with a novel adapter initialization scheme incorporating adaptive parameter
selection and value refinement. We demonstrate that QWHA effectively mitigates
quantization errors while facilitating fine-tuning, and that its design
substantially reduces computational cost. Experimental results show that QWHA
consistently outperforms baselines in low-bit quantization accuracy and
achieves significant training speedups over existing FT-based adapters. The
code is available at https://github.com/vantaa89/qwha.
\\ ( https://arxiv.org/abs/2509.17428 ,  2731kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17436
Date: Mon, 22 Sep 2025 07:26:47 GMT   (302kb)

Title: MedFact: A Large-scale Chinese Dataset for Evidence-based Medical
  Fact-checking of LLM Responses
Authors: Tong Chen, Zimu Wang, Yiyi Miao, Haoran Luo, Yuanfei Sun, Wei Wang,
  Zhengyong Jiang, Procheta Sen, Jionglong Su
Categories: cs.CL
Comments: Accepted at EMNLP 2025. Camera-ready version
\\
  Medical fact-checking has become increasingly critical as more individuals
seek medical information online. However, existing datasets predominantly focus
on human-generated content, leaving the verification of content generated by
large language models (LLMs) relatively unexplored. To address this gap, we
introduce MedFact, the first evidence-based Chinese medical fact-checking
dataset of LLM-generated medical content. It consists of 1,321 questions and
7,409 claims, mirroring the complexities of real-world medical scenarios. We
conduct comprehensive experiments in both in-context learning (ICL) and
fine-tuning settings, showcasing the capability and challenges of current LLMs
on this task, accompanied by an in-depth error analysis to point out key
directions for future research. Our dataset is publicly available at
https://github.com/AshleyChenNLP/MedFact.
\\ ( https://arxiv.org/abs/2509.17436 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17437
Date: Mon, 22 Sep 2025 07:28:09 GMT   (252kb)

Title: GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric
  Reasoning
Authors: Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Deli Zhao, Anh Tuan
  Luu, Yu Rong
Categories: cs.CL
Comments: Accepted to EMNLP2025 Findings
\\
  Recent advancements in reinforcement learning (RL) have enhanced the
reasoning abilities of large language models (LLMs), yet the impact on
multimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like
geometric reasoning, MLLMs hallucinate frequently, leading to inaccurate
reasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps
the benefits of reasoning training. To quantify this, we design a
Geo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric
concepts and spatial relationships. Experiments on GeoPQA reveal significant
shortcomings of MLLMs in visual perception, which constrain RL reward signals
for effective training. To address this bottleneck, we propose a two-stage RL
training framework by first enhancing the visual perception of geometric
structures, then fostering reasoning capabilities. Applied to
Qwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by
9.7% and geometric problem solving by 9.1%, compared to the direct reasoning
training approach. Our method also generalizes to other vision-intensive
domains like figure understanding, highlighting the importance of perceptual
grounding in effective MLLM reasoning.
\\ ( https://arxiv.org/abs/2509.17437 ,  252kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17444
Date: Mon, 22 Sep 2025 07:36:12 GMT   (126kb)

Title: Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the
  Japanese medical system
Authors: Shohei Hisada, Endo Sunao, Himi Yamato, Shoko Wakamiya, Eiji Aramaki
Categories: cs.CL
Comments: draft v0.1
\\
  This study investigates the applicability of HealthBench, a large-scale,
rubric-based medical benchmark, to the Japanese context. While robust
evaluation frameworks are crucial for the safe development of medical LLMs,
resources in Japanese remain limited, often relying on translated
multiple-choice questions. Our research addresses this gap by first
establishing a performance baseline, applying a machine-translated version of
HealthBench's 5,000 scenarios to evaluate both a high-performing multilingual
model (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second,
we employ an LLM-as-a-Judge approach to systematically classify the benchmark's
scenarios and rubric criteria, identifying "contextual gaps" where content is
misaligned with Japan's clinical guidelines, healthcare systems, or cultural
norms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric
mismatches and a significant failure in the Japanese-native model, which lacked
the required clinical completeness. Furthermore, our classification indicates
that while the majority of scenarios are applicable, a substantial portion of
the rubric criteria requires localization. This work underscores the
limitations of direct benchmark translation and highlights the urgent need for
a context-aware, localized adaptation, a J-HealthBench, to ensure the reliable
and safe evaluation of medical LLMs in Japan.
\\ ( https://arxiv.org/abs/2509.17444 ,  126kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17445
Date: Mon, 22 Sep 2025 07:38:45 GMT   (1000kb)

Title: Semantic Reformulation Entropy for Robust Hallucination Detection in QA
  Tasks
Authors: Chaodong Tong, Qi Zhang, Lei Jiang, Yanbing Liu, Nannan Sun, Wei Li
Categories: cs.CL
Comments: 5pages, 5 figures, submit to ICASSP 2026
\\
  Reliable question answering with large language models (LLMs) is challenged
by hallucinations, fluent but factually incorrect outputs arising from
epistemic uncertainty. Existing entropy-based semantic-level uncertainty
estimation methods are limited by sampling noise and unstable clustering of
variable-length answers. We propose Semantic Reformulation Entropy (SRE), which
improves uncertainty estimation in two ways. First, input-side semantic
reformulations produce faithful paraphrases, expand the estimation space, and
reduce biases from superficial decoder tendencies. Second, progressive,
energy-based hybrid clustering stabilizes semantic grouping. Experiments on
SQuAD and TriviaQA show that SRE outperforms strong baselines, providing more
robust and generalizable hallucination detection. These results demonstrate
that combining input diversification with multi-signal clustering substantially
enhances semantic-level uncertainty estimation.
\\ ( https://arxiv.org/abs/2509.17445 ,  1000kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17449
Date: Mon, 22 Sep 2025 07:41:45 GMT   (29kb)

Title: SLAyiNG: Towards Queer Language Processing
Authors: Leonor Veloso, Lea Hirlimann, Philipp Wicke, Hinrich Sch\"utze
Categories: cs.CL
Comments: To be presented at Queer in AI @ NeurIPS 2025 (non-archival)
\\
  Knowledge of slang is a desirable feature of LLMs in the context of user
interaction, as slang often reflects an individual's social identity. Several
works on informal language processing have defined and curated benchmarks for
tasks such as detection and identification of slang. In this paper, we focus on
queer slang. Queer slang can be mistakenly flagged as hate speech or can evoke
negative responses from LLMs during user interaction. Research efforts so far
have not focused explicitly on queer slang. In particular, detection and
processing of queer slang have not been thoroughly evaluated due to the lack of
a high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the
first dataset containing annotated queer slang derived from subtitles, social
media posts, and podcasts, reflecting real-world usage. We describe our data
curation process, including the collection of slang terms and definitions,
scraping sources for examples that reflect usage of these terms, and our
ongoing annotation process. As preliminary results, we calculate
inter-annotator agreement for human annotators and OpenAI's model o3-mini,
evaluating performance on the task of sense disambiguation. Reaching an average
Krippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models
can serve as tools for pre-filtering, but the complex and often sensitive
nature of queer language data requires expert and community-driven annotation
efforts.
\\ ( https://arxiv.org/abs/2509.17449 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17455
Date: Mon, 22 Sep 2025 07:49:58 GMT   (1305kb)

Title: Codifying Natural Langauge Tasks
Authors: Haoyang Chen and Kumiko Tanaka-Ishii
Categories: cs.CL cs.AI
Comments: Submitted to Journal of Automated Software Engineering
\\
  We explore the applicability of text-to-code to solve real-world problems
that are typically solved in natural language, such as legal judgment and
medical QA. Unlike previous works, our approach leverages the explicit
reasoning provided by program generation. We present ICRAG, a framework that
transforms natural language into executable programs through iterative
refinement using external knowledge from domain resources and GitHub. Across 13
benchmarks, ICRAG achieves up to 161.1\% relative improvement. We provide a
detailed analysis of the generated code and the impact of external knowledge,
and we discuss the limitations of applying text-to-code approaches to
real-world natural language tasks.
\\ ( https://arxiv.org/abs/2509.17455 ,  1305kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17459
Date: Mon, 22 Sep 2025 07:53:59 GMT   (17354kb)

Title: PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents
Authors: Namyoung Kim, Kai Tzu-iunn Ong, Yeonjun Hwang, Minseok Kang, Iiseo
  Jihn, Gayoung Kim, Minju Kim, Jinyoung Yeo
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Findings
\\
  Dialogue agents based on large language models (LLMs) have shown promising
performance in proactive dialogue, which requires effective strategy planning.
However, existing approaches to strategy planning for proactive dialogue face
several limitations: limited strategy coverage, preference bias in planning,
and reliance on costly additional training. To address these, we propose
PRINCIPLES: a synthetic strategy memory for proactive dialogue agents.
PRINCIPLES is derived through offline self-play simulations and serves as
reusable knowledge that guides strategy planning during inference, eliminating
the need for additional training and data annotation. We evaluate PRINCIPLES in
both emotional support and persuasion domains, demonstrating consistent
improvements over strong baselines. Furthermore, PRINCIPLES maintains its
robustness across extended and more diverse evaluation settings. See our
project page at https://huggingface.co/spaces/kimnamssya/Principles.
\\ ( https://arxiv.org/abs/2509.17459 ,  17354kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17482
Date: Mon, 22 Sep 2025 08:16:04 GMT   (296kb)

Title: Diagnosing Model Editing via Knowledge Spectrum
Authors: Tsung-Hsuan Pan, Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen
Categories: cs.CL
\\
  Model editing, the process of efficiently modifying factual knowledge in
pre-trained language models, is critical for maintaining their accuracy and
relevance. However, existing editing methods often introduce unintended side
effects, degrading model performance in unpredictable ways. While much research
has focused on improving editing algorithms, the role of the target knowledge's
intrinsic properties remains a significant, underexplored factor. This paper
addresses this gap by first proposing the ``Knowledge Spectrum,'' a systematic
framework for categorizing knowledge based on its real-world popularity, the
model's pre-edit familiarity, and the linguistic structure of the eliciting
question. Our empirical analysis reveals that these characteristics are strong
predictors of editing success and stability. Informed by these findings, we
introduce the ``Knowledge-Diagnostic Framework,'' an adaptive strategy that
tailors editing intensity to the diagnosed difficulty of a knowledge item. We
demonstrate that this framework significantly improves success rates for
challenging edits while optimizing computational resources. Our work provides a
more comprehensive understanding of the factors governing model editing.
\\ ( https://arxiv.org/abs/2509.17482 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17486
Date: Mon, 22 Sep 2025 08:18:50 GMT   (354kb)

Title: AttnComp: Attention-Guided Adaptive Context Compression for
  Retrieval-Augmented Generation
Authors: Lvzhou Luo, Yixuan Cao, Ping Luo
Categories: cs.CL
Comments: Accepted at EMNLP 2025 (Findings)
\\
  Retrieval-augmented generation improves the factual accuracy of Large
Language Models (LLMs) by incorporating external context, but often suffers
from irrelevant retrieved content that hinders effectiveness. Context
compression addresses this issue by filtering out irrelevant information from
context before LLM generation. However, existing methods struggle to adaptively
adjust compression rates for different context, maintain low latency and
integrate information across multiple documents. To overcome these limitations,
We introduce AttnComp, an adaptive, efficient and context-aware compression
framework. By leveraging the attention mechanism of LLMs to identify relevant
information, AttnComp employs a Top-P compression algorithm to retain the
minimal set of documents whose cumulative attention weights exceeds a
predefined threshold. In addition to compression, AttnComp estimates response
confidence by assessing the overall relevance of the retrieved content,
enabling users to gauge response reliability. Experiments demonstrate that
AttnComp outperforms existing compression methods and uncompressed baselines,
achieving higher accuracy with substantial compression rates and lower latency.
\\ ( https://arxiv.org/abs/2509.17486 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17489
Date: Mon, 22 Sep 2025 08:19:11 GMT   (1938kb)

Title: MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM
Authors: Woongkyu Lee, Junhee Cho, Jungwook Choi
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have advanced code generation from
single-function tasks to competitive-programming problems, but existing
multi-agent solutions either rely on costly large-scale ($>$ 30B) models or
collapse when downsized to small open-source models. We present MapCoder-Lite,
which upgrades a single 7B model into four role-specialised agents-retriever,
planner, coder, and debugger-using only rank-32, role-specific LoRA adapters
($<3\%$ extra parameters). Three lightweight techniques make this possible: (i)
trajectory distillation from strong LLMs fixes format fragility in retrieval
and debugging, (ii) supervisor-guided correction strengthens planning and
coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient
specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests
shows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\%$ to
$28.3\%$), eliminates all format failures, and closes to within six points of a
32B baseline while cutting GPU memory and token-generation time by $4\times$.
These results demonstrate that careful agent-wise fine-tuning unleashes
high-quality multi-agent coding on a small language model.
\\ ( https://arxiv.org/abs/2509.17489 ,  1938kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17493
Date: Mon, 22 Sep 2025 08:24:26 GMT   (157kb)

Title: Enhancing Cross-Lingual Transfer through Reversible Transliteration: A
  Huffman-Based Approach for Low-Resource Languages
Authors: Wenhao Zhuang, Yuan Sun, Xiaobing Zhao
Categories: cs.CL
DOI: 10.18653/v1/2025.acl-long.795
\\
  As large language models (LLMs) are trained on increasingly diverse and
extensive multilingual corpora, they demonstrate cross-lingual transfer
capabilities. However, these capabilities often fail to effectively extend to
low-resource languages, particularly those utilizing non-Latin scripts. While
transliterating low-resource languages into Latin script presents a natural
solution, there currently lacks a comprehensive framework for integrating
transliteration into LLMs training and deployment. Taking a pragmatic approach,
this paper innovatively combines character transliteration with Huffman coding
to design a complete transliteration framework. Our proposed framework offers
the following advantages: 1) Compression: Reduces storage requirements for
low-resource language content, achieving up to 50% reduction in file size and
50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless
conversion from transliterated text back to the source language. 3) Efficiency:
Eliminates the need for vocabulary expansion for low-resource languages,
improving training and inference efficiency. 4) Scalability: The framework can
be extended to other low-resource languages. We validate the effectiveness of
our framework across multiple downstream tasks, including text classification,
machine reading comprehension, and machine translation. Experimental results
demonstrate that our method significantly enhances the model's capability to
process low-resource languages while maintaining performance on high-resource
languages. Our data and code are publicly available at
https://github.com/CMLI-NLP/HuffmanTranslit.
\\ ( https://arxiv.org/abs/2509.17493 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17505
Date: Mon, 22 Sep 2025 08:35:21 GMT   (175kb)

Title: CorefInst: Leveraging LLMs for Multilingual Coreference Resolution
Authors: Tu\u{g}ba Pamay Arslan and Emircan Erol and G\"ul\c{s}en Eryi\u{g}it
Categories: cs.CL cs.AI
Comments: Accepted for publication in Transactions of the Association for
  Computational Linguistics (TACL) (2025 August). Submission: March, 2025.
  Revision: July, 2025. Acceptance: August, 2025
\\
  Coreference Resolution (CR) is a crucial yet challenging task in natural
language understanding, often constrained by task-specific architectures and
encoder-based language models that demand extensive training and lack
adaptability. This study introduces the first multilingual CR methodology which
leverages decoder-only LLMs to handle both overt and zero mentions. The article
explores how to model the CR task for LLMs via five different instruction sets
using a controlled inference method. The approach is evaluated across three
LLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when
instruction-tuned with a suitable instruction set, can surpass state-of-the-art
task-specific architectures. Specifically, our best model, a fully fine-tuned
Llama 3.1 for multilingual CR, outperforms the leading multilingual CR model
(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages
in the CorefUD v1.2 dataset collection.
\\ ( https://arxiv.org/abs/2509.17505 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17523
Date: Mon, 22 Sep 2025 08:48:04 GMT   (2043kb)

Title: Leveraging Audio-Visual Data to Reduce the Multilingual Gap in
  Self-Supervised Speech Models
Authors: Mar\'ia Andrea Cruz Bland\'on, Zakaria Aldeneh, Jie Chi, Maureen de
  Seyssel
Categories: cs.CL eess.AS
Comments: 5 pages, 2 figures
\\
  Self-supervised learning (SSL) has made significant advances in speech
representation learning. Models like wav2vec 2.0 and HuBERT have achieved
state-of-the-art results in tasks such as speech recognition, particularly in
monolingual settings. However, multilingual SSL models tend to underperform
their monolingual counterparts on each individual language, especially in
multilingual scenarios with few languages such as the bilingual setting. In
this work, we investigate a novel approach to reduce this performance gap by
introducing limited visual grounding into bilingual speech SSL models. Our
results show that visual grounding benefits both monolingual and bilingual
models, with especially pronounced gains for the latter, reducing the
multilingual performance gap on zero-shot phonetic discrimination from 31.5%
for audio-only models to 8.04% with grounding.
\\ ( https://arxiv.org/abs/2509.17523 ,  2043kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17552
Date: Mon, 22 Sep 2025 09:16:34 GMT   (2593kb)

Title: Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A
  Case Study with In-Context Representation Learning
Authors: Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak
  A.Subramanian, Alvin Chan
Categories: cs.CL cs.AI
Comments: NIPS 2025
\\
  The remarkable performance of Large Language Models (LLMs) can be enhanced
with test-time computation, which relies on external tools and even other deep
learning models. However, existing approaches for integrating non-text modality
representations into LLMs typically require additional costly supervised
training, restricting on-the-fly adaptation to new domains and modalities. In
this work, we explore the feasibility of integrating representations from
non-text foundational models (FMs) into text-based LLMs in a training-free
manner. We propose In-Context Representation Learning (ICRL) as a
proof-of-concept to allow LLMs to adaptively utilize non-text modality
representations with few-shot learning. Unlike traditional in-context learning,
which incorporates text-label pairs, ICRL replaces text inputs with FM
representations, enabling the LLM to perform multi-modal inference without
fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,
investigating three core research questions: (i) how to map FM representations
into LLMs in a training-free manner, (ii) what factors influence ICRL
performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To
the best of our knowledge, ICRL is the first training-free framework for
integrating non-text modality representations into text-based LLMs, presenting
a promising direction for adaptable, multi-modal generalization.
\\ ( https://arxiv.org/abs/2509.17552 ,  2593kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17559
Date: Mon, 22 Sep 2025 10:50:37 GMT   (305kb)

Title: Specification-Aware Machine Translation and Evaluation for Purpose
  Alignment
Authors: Yoko Kayano, Saku Sugawara
Categories: cs.CL
\\
  In professional settings, translation is guided by communicative goals and
client needs, often formalized as specifications. While existing evaluation
frameworks acknowledge the importance of such specifications, these
specifications are often treated only implicitly in machine translation (MT)
research. Drawing on translation studies, we provide a theoretical rationale
for why specifications matter in professional translation, as well as a
practical guide to implementing specification-aware MT and evaluation. Building
on this foundation, we apply our framework to the translation of investor
relations texts from 33 publicly listed companies. In our experiment, we
compare five translation types, including official human translations and
prompt-based outputs from large language models (LLMs), using expert error
analysis, user preference rankings, and an automatic metric. The results show
that LLM translations guided by specifications consistently outperformed
official human translations in human evaluations, highlighting a gap between
perceived and expected quality. These findings demonstrate that integrating
specifications into MT workflows, with human oversight, can improve translation
quality in ways aligned with professional practice.
\\ ( https://arxiv.org/abs/2509.17559 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17570
Date: Mon, 22 Sep 2025 11:01:22 GMT   (9137kb)

Title: Asking a Language Model for Diverse Responses
Authors: Sergey Troshin, Irina Saparina, Antske Fokkens and Vlad Niculae
Categories: cs.CL
Comments: UncertaiNLP workshop, 2025
\\
  Large language models increasingly rely on explicit reasoning chains and can
produce multiple plausible responses for a given context. We study the
candidate sampler that produces the set of plausible responses contrasting the
ancestral (parallel) sampling against two alternatives: enumeration, which asks
the model to produce $n$ candidates in one pass, and iterative sampling, which
proposes candidates sequentially while conditioning on the currently generated
response set. Under matched budgets, we compare these samplers on quality,
lexical and computation flow diversity, and efficiency. Our empirical results
demonstrate that enumeration and iterative strategies result in higher
diversity at comparable quality. Our findings highlight the potential of simple
non-independent sampling strategies to improve response diversity without
sacrificing generation quality.
\\ ( https://arxiv.org/abs/2509.17570 ,  9137kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17628
Date: Mon, 22 Sep 2025 11:36:16 GMT   (1760kb)

Title: MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM
  Agents
Authors: Yuzhen Lei, Hongbin Xie, Jiaxing Zhao, Shuangxue Liu, Xuan Song
Categories: cs.CL cs.AI
Comments: 10 pages, 5 figures
\\
  Large Language Models (LLMs) have excelled in question-answering (QA) tasks
within single domains. However, their reasoning and coordination capabilities
in complex, multi-stage scenarios remain underexplored. Existing benchmarks
typically focus on isolated tasks or narrow domains, overlooking models'
abilities for multi-stage collaboration and optimization without explicit
external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel
benchmark comprising 126696 domain-specific QA instances spanning scenarios in
automotive, pharmaceutical, electronics, and energy sectors. The dataset is
created using a structured three-phase pipeline: dynamic sampling, iterative
question-answer generation, and a multi-level quality assessment to ensure data
quality. Tasks are further categorized into three difficulty levels according
to stage coverage and complexity. With MSCoRe, we have conducted a
comprehensive evaluation of various state-of-the-art LLM agents. The commercial
models performed best across all tasks and scenarios, but a notable gap in
ROUGE scores remains between simple and complex tasks. We also tested the
models' robustness and found that their performance is negatively affected by
noisy data. MSCoRe provides a valuable new resource for the community to
evaluate and improve multi-stage reasoning in LLM agents. The code and data are
available at https://github.com/D3E0-source/MSCoRE.
\\ ( https://arxiv.org/abs/2509.17628 ,  1760kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17641
Date: Mon, 22 Sep 2025 11:45:22 GMT   (2786kb)

Title: AuditoryBench++: Can Language Models Understand Auditory Knowledge
  without Hearing?
Authors: Hyunjong Ok, Suho Yoo, Hyeonjun Kim, Jaeho Lee
Categories: cs.CL cs.AI cs.LG cs.SD
Comments: Preprint
\\
  Even without directly hearing sounds, humans can effortlessly reason about
auditory properties, such as pitch, loudness, or sound-source associations,
drawing on auditory commonsense. In contrast, language models often lack this
capability, limiting their effectiveness in multimodal interactions. As an
initial step to address this gap, we present AuditoryBench++, a comprehensive
benchmark for evaluating auditory knowledge and reasoning in text-only
settings. The benchmark encompasses tasks that range from basic auditory
comparisons to contextually grounded reasoning, enabling fine-grained analysis
of how models process and integrate auditory concepts. In addition, we
introduce AIR-CoT, a novel auditory imagination reasoning method that generates
and integrates auditory information during inference through span detection
with special tokens and knowledge injection. Extensive experiments with recent
LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both
the off-the-shelf models and those augmented with auditory knowledge. The
project page is available at https://auditorybenchpp.github.io.
\\ ( https://arxiv.org/abs/2509.17641 ,  2786kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17667
Date: Mon, 22 Sep 2025 12:11:42 GMT   (1175kb)

Title: Crosslingual Optimized Metric for Translation Assessment of Indian
  Languages
Authors: Arafat Ahsan, Vandan Mujadia, Pruthwik Mishra, Yash Bhaskar, Dipti
  Misra Sharma
Categories: cs.CL
Comments: Under review
\\
  Automatic evaluation of translation remains a challenging task owing to the
orthographic, morphological, syntactic and semantic richness and divergence
observed across languages. String-based metrics such as BLEU have previously
been extensively used for automatic evaluation tasks, but their limitations are
now increasingly recognized. Although learned neural metrics have helped
mitigate some of the limitations of string-based approaches, they remain
constrained by a paucity of gold evaluation data in most languages beyond the
usual high-resource pairs. In this present work we address some of these gaps.
We create a large human evaluation ratings dataset for 13 Indian languages
covering 21 translation directions and then train a neural translation
evaluation metric named Cross-lingual Optimized Metric for Translation
Assessment of Indian Languages (COMTAIL) on this dataset. The best performing
metric variants show significant performance gains over previous
state-of-the-art when adjudging translation pairs with at least one Indian
language. Furthermore, we conduct a series of ablation studies to highlight the
sensitivities of such a metric to changes in domain, translation quality, and
language groupings. We release both the COMTAIL dataset and the accompanying
metric models.
\\ ( https://arxiv.org/abs/2509.17667 ,  1175kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17669
Date: Mon, 22 Sep 2025 12:12:41 GMT   (2043kb)

Title: PG-CE: A Progressive Generation Dataset with Constraint Enhancement for
  Controllable Text Generation
Authors: Yan Zhuang, Yuan Sun
Categories: cs.CL
\\
  With the rapid development of Large Language Models (LLMs), Controllable Text
Generation (CTG) has become a critical technology for enhancing system
reliability and user experience. Addressing the limitations of traditional
methods, this paper proposes the PG-CE (Progressive Generation with Constraint
Enhancement) approach, which decomposes CTG tasks into three steps: type
prediction, constraint construction, and guided generation. This method employs
constraint generation models to dynamically build multi-dimensional constraints
including tone, expression style, and thematic focus to guide output.
Experiments demonstrate that PG-CE significantly improves generation quality
across multiple scenarios while maintaining text controllability, thematic
relevance, and response practicality. The research developed a dataset
containing 90,000 constraint-text pairs (with an 8:2 ratio between daily and
other topics), effectively reflecting real-world application requirements.
\\ ( https://arxiv.org/abs/2509.17669 ,  2043kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17671
Date: Mon, 22 Sep 2025 12:14:11 GMT   (133kb)

Title: Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG
  Applications
Authors: Selva Ta\c{s}, Mahmut El Huseyni, \"Ozay Ezerceli, Reyhan Bayraktar,
  Fatma Bet\"ul Terzio\u{g}lu
Categories: cs.CL cs.AI
\\
  The widespread adoption of Large Language Models (LLMs) has been hindered by
their tendency to hallucinate, generating plausible but factually incorrect
information. While Retrieval-Augmented Generation (RAG) systems attempt to
address this issue by grounding responses in external knowledge, hallucination
remains a persistent challenge, particularly for morphologically complex,
low-resource languages like Turkish. This paper introduces Turk-LettuceDetect,
the first suite of hallucination detection models specifically designed for
Turkish RAG applications. Building on the LettuceDetect framework, we formulate
hallucination detection as a token-level classification task and fine-tune
three distinct encoder architectures: a Turkish-specific ModernBERT,
TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a
machine-translated version of the RAGTruth benchmark dataset containing 17,790
instances across question answering, data-to-text generation, and summarization
tasks. Our experimental results show that the ModernBERT-based model achieves
an F1-score of 0.7266 on the complete test set, with particularly strong
performance on structured tasks. The models maintain computational efficiency
while supporting long contexts up to 8,192 tokens, making them suitable for
real-time deployment. Comparative analysis reveals that while state-of-the-art
LLMs demonstrate high recall, they suffer from low precision due to
over-generation of hallucinated content, underscoring the necessity of
specialized detection mechanisms. By releasing our models and translated
dataset, this work addresses a critical gap in multilingual NLP and establishes
a foundation for developing more reliable and trustworthy AI applications for
Turkish and other languages.
\\ ( https://arxiv.org/abs/2509.17671 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17680
Date: Mon, 22 Sep 2025 12:25:57 GMT   (1643kb)

Title: When TableQA Meets Noise: A Dual Denoising Framework for Complex
  Questions and Large-scale Tables
Authors: Shenghao Ye, Yu Guo, Dong Jin, Yikai Shen, Yunpeng Hou, Shuangwu Chen,
  Jian Yang, Xiaofeng Jiang
Categories: cs.CL
Comments: 23 pages, 24 figures
\\
  Table question answering (TableQA) is a fundamental task in natural language
processing (NLP). The strong reasoning capabilities of large language models
(LLMs) have brought significant advances in this field. However, as real-world
applications involve increasingly complex questions and larger tables,
substantial noisy data is introduced, which severely degrades reasoning
performance. To address this challenge, we focus on improving two core
capabilities: Relevance Filtering, which identifies and retains information
truly relevant to reasoning, and Table Pruning, which reduces table size while
preserving essential content. Based on these principles, we propose EnoTab, a
dual denoising framework for complex questions and large-scale tables.
Specifically, we first perform Evidence-based Question Denoising by decomposing
the question into minimal semantic units and filtering out those irrelevant to
answer reasoning based on consistency and usability criteria. Then, we propose
Evidence Tree-guided Table Denoising, which constructs an explicit and
transparent table pruning path to remove irrelevant data step by step. At each
pruning step, we observe the intermediate state of the table and apply a
post-order node rollback mechanism to handle abnormal table states, ultimately
producing a highly reliable sub-table for final answer reasoning. Finally,
extensive experiments show that EnoTab achieves outstanding performance on
TableQA tasks with complex questions and large-scale tables, confirming its
effectiveness.
\\ ( https://arxiv.org/abs/2509.17680 ,  1643kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17688
Date: Mon, 22 Sep 2025 12:29:43 GMT   (1054kb)

Title: TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model
  Adaptation
Authors: Daiye Miao, Yufang Liu, Jie Wang, Changzhi Sun, Yunke Zhang, Demei
  Yan, Shaokang Dong, Qi Zhang, and Yuanbin Wu
Categories: cs.CL cs.CV
Comments: Accepted to EMNLP 2025 (Main Conference),13 pages,10 figures
\\
  LoRA has become one of the most widely used parameter-efficient fine-tuning
methods due to its simplicity and effectiveness. However, numerous studies have
shown that LoRA often introduces substantial parameter redundancy, which not
only increases the number of trainable parameters but also hinders the
effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is
inherently difficult, how to eliminate them efficiently and accurately remains
a challenging problem. In this paper, we propose TASO, a redundancy reduction
method that leverages importance information from the pretrained model's
weights to mitigate LoRA redundancy. Specifically, we estimate parameter
importance on downstream tasks and identify task-specific core regions based on
the distribution of importance scores. The location information of these core
regions is then used to determine the sparse structure of LoRA modules,
enabling redundancy removal before fine-tuning. Our approach significantly
reduces the number of trainable parameters required for task adaptation, while
providing a novel task-aligned perspective for LoRA redundancy reduction.
Experimental results demonstrate that, with a parameter budget comparable to
LoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across
multiple tasks, achieving strong fine-tuning performance while effectively
eliminating redundant parameters.
\\ ( https://arxiv.org/abs/2509.17688 ,  1054kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17694
Date: Mon, 22 Sep 2025 12:33:02 GMT   (2360kb)

Title: Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play
  Dialogues
Authors: Dongxu Lu, Johan Jeuring, Albert Gatt
Categories: cs.CL cs.AI
Comments: Accepted for publication at the 18th International Natural Language
  Generation Conference (INLG 2025)
\\
  Evaluating large language models (LLMs) in long-form, knowledge-grounded
role-play dialogues remains challenging. This study compares LLM-generated and
human-authored responses in multi-turn professional training simulations
through human evaluation ($N=38$) and automated LLM-as-a-judge assessment.
Human evaluation revealed significant degradation in LLM-generated response
quality across turns, particularly in naturalness, context maintenance and
overall quality, while human-authored responses progressively improved. In line
with this finding, participants also indicated a consistent preference for
human-authored dialogue. These human judgements were validated by our automated
LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment
with human evaluators on both zero-shot pairwise preference and stochastic
6-shot construct ratings, confirming the widening quality gap between LLM and
human responses over time. Our work contributes a multi-turn benchmark exposing
LLM degradation in knowledge-grounded role-play dialogues and provides a
validated hybrid evaluation framework to guide the reliable integration of LLMs
in training simulations.
\\ ( https://arxiv.org/abs/2509.17694 ,  2360kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17701
Date: Mon, 22 Sep 2025 12:38:09 GMT   (30kb)

Title: Investigating Bias: A Multilingual Pipeline for Generating, Solving, and
  Evaluating Math Problems with LLMs
Authors: Mariam Mahran, Katharina Simbeck
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at edu4AI'25: 2nd Workshop on Education for Artificial
  Intelligence | co-located with ECAI, October 26th, 2025, Bologna, Italy. 7
  pages, 0 figures
\\
  Large Language Models (LLMs) are increasingly used for educational support,
yet their response quality varies depending on the language of interaction.
This paper presents an automated multilingual pipeline for generating, solving,
and evaluating math problems aligned with the German K-10 curriculum. We
generated 628 math exercises and translated them into English, German, and
Arabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)
were prompted to produce step-by-step solutions in each language. A held-out
panel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality
using a comparative framework. Results show a consistent gap, with English
solutions consistently rated highest, and Arabic often ranked lower. These
findings highlight persistent linguistic bias and the need for more equitable
multilingual AI systems in education.
\\ ( https://arxiv.org/abs/2509.17701 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17737
Date: Mon, 22 Sep 2025 13:04:48 GMT   (397kb)

Title: Breaking Token Into Concepts: Exploring Extreme Compression in Token
  Representation Via Compositional Shared Semantics
Authors: Kavin R V and Pawan Goyal
Categories: cs.CL
Comments: 5 pages, 1 figure
Journal-ref: EMNLP 2025 Findings (Short)
\\
  Standard language models employ unique, monolithic embeddings for each token,
potentially limiting their ability to capture the multifaceted nature of word
meanings. We investigate whether tokens can be more effectively represented
through a compositional structure that accumulates diverse semantic facets. To
explore this, we propose Aggregate Semantic Grouping (ASG), a novel approach
leveraging Product Quantization (PQ). We apply ASG to standard transformer
architectures (mBERT, XLM-R, mT5) and evaluate this representational scheme
across diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific
benchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing
tokens compositionally via ASG achieves extreme compression in embedding
parameters (0.4--0.5\%) while maintaining $>$95\% task performance relative to
the base model, even in generative tasks and extends to both cross lingual
transfer and domain-specific settings. These results validate the principle
that tokens can be effectively modeled as combinations of shared semantic
building blocks. ASG offers a simple yet concrete method for achieving this,
showcasing how compositional representations can capture linguistic richness
while enabling compact yet semantically rich models.
\\ ( https://arxiv.org/abs/2509.17737 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17765
Date: Mon, 22 Sep 2025 13:26:24 GMT   (3732kb)

Title: Qwen3-Omni Technical Report
Authors: Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He,
  Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo,
  He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong
  Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu,
  Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan
  Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin
Categories: cs.CL cs.AI cs.CV eess.AS
Comments: https://github.com/QwenLM/Qwen3-Omni
\\
  We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts. Qwen3-Omni
matches the performance of same-sized single-modal models within the Qwen
series and excels particularly on audio tasks. Across 36 audio and audio-visual
benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall
SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,
Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE
architecture that unifies perception and generation across text, images, audio,
and video, yielding fluent text and natural real-time speech. It supports text
interaction in 119 languages, speech understanding in 19 languages, and speech
generation in 10 languages. To reduce first-packet latency in streaming
synthesis, Talker autoregressively predicts discrete speech codecs using a
multi-codebook scheme. Leveraging the representational capacity of these
codebooks, we replace computationally intensive block-wise diffusion with a
lightweight causal ConvNet, enabling streaming from the first codec frame. In
cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet
latency of 234 ms. To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality. Since the
research community currently lacks a general-purpose audio captioning model, we
fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which
produces detailed, low-hallucination captions for arbitrary audio inputs.
Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and
Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0
license.
\\ ( https://arxiv.org/abs/2509.17765 ,  3732kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17766
Date: Mon, 22 Sep 2025 13:26:24 GMT   (420kb)

Title: A State-Update Prompting Strategy for Efficient and Robust Multi-turn
  Dialogue
Authors: Ziyi Liu
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) struggle with information forgetting and
inefficiency in long-horizon, multi-turn dialogues. To address this, we propose
a training-free prompt engineering method, the State-Update Multi-turn Dialogue
Strategy. It utilizes "State Reconstruction" and "History Remind" mechanisms to
effectively manage dialogue history. Our strategy shows strong performance
across multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,
it improves the core information filtering score by 32.6%, leading to a 14.1%
increase in the downstream QA score, while also reducing inference time by
73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal
roles of both components. Our work offers an effective solution for optimizing
LLMs in long-range interactions, providing new insights for developing more
robust Agents.
\\ ( https://arxiv.org/abs/2509.17766 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17768
Date: Mon, 22 Sep 2025 13:32:31 GMT   (54kb)

Title: DIVERS-Bench: Evaluating Language Identification Across Domain Shifts
  and Code-Switching
Authors: Jessica Ojo, Zina Kamel, David Ifeoluwa Adelani
Categories: cs.CL cs.AI cs.LG
\\
  Language Identification (LID) is a core task in multilingual NLP, yet current
systems often overfit to clean, monolingual data. This work introduces
DIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across
diverse domains, including speech transcripts, web text, social media texts,
children's stories, and code-switched text. Our findings reveal that while
models achieve high accuracy on curated datasets, performance degrades sharply
on noisy and informal inputs. We also introduce DIVERS-CS, a diverse
code-switching benchmark dataset spanning 10 language pairs, and show that
existing models struggle to detect multiple languages within the same sentence.
These results highlight the need for more robust and inclusive LID systems in
real-world settings.
\\ ( https://arxiv.org/abs/2509.17768 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17788
Date: Mon, 22 Sep 2025 13:49:37 GMT   (2374kb)

Title: One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for
  Millions of Multi-Style Official Accounts
Authors: Xingyu Fan, Feifei Li, Wenhui Que, Hailong Li
Categories: cs.CL cs.AI
Comments: 7 pages
MSC-class: 68T50
ACM-class: I.2.7
\\
  Conversational agents deployed in industrial-scale official account platforms
must generate responses that are both contextually grounded and stylistically
aligned-requirements that existing methods struggle to meet. Chain-of-thought
(CoT) prompting induces significant latency due to multi-turn reasoning;
per-account fine-tuning is computationally prohibitive; and long prompt-based
methods degrade the model's ability to grasp injected context and style. In
this paper, we propose WeStar, a lite-adaptive framework for stylized
contextual question answering that scales to millions of official accounts.
WeStar combines context-grounded generation via RAG with style-aware generation
using Parametric RAG (PRAG), where LoRA modules are dynamically activated per
style cluster. Our contributions are fourfold: (1) We introduce WeStar, a
unified framework capable of serving large volumes of official accounts with
minimal overhead. (2) We propose a multi-dimensional, cluster-based parameter
sharing scheme that enables compact style representation while preserving
stylistic diversity. (3) We develop a style-enhanced Direct Preference
Optimization (SeDPO) method to optimize each style cluster's parameters for
improved generation quality. (4) Experiments on a large-scale industrial
dataset validate the effectiveness and efficiency of WeStar, underscoring its
pracitical value in real-world deployment.
\\ ( https://arxiv.org/abs/2509.17788 ,  2374kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17794
Date: Mon, 22 Sep 2025 13:51:40 GMT   (562kb)

Title: Learning to vary: Teaching LMs to reproduce human linguistic variability
  in next-word prediction
Authors: Tobias Groot, Salo Lacunes, Evgenia Ilia
Categories: cs.CL
Comments: EMNLP UncertaiNLP Workshop 2025
\\
  Natural language generation (NLG) tasks are often subject to inherent
variability; \emph{e.g.} predicting the next word given a context has multiple
valid responses, evident when asking multiple humans to complete the task.
While having language models (LMs) that are aligned pluralistically, so that
they are able to reproduce well the inherent diversity in perspectives of an
entire population of interest is clearly beneficial, \citet{ilia2024predict}
show that LMs do not reproduce this type of linguistic variability well. They
speculate this inability might stem from the lack of consistent training of LMs
with data reflecting this type of inherent variability. As such, we investigate
whether training LMs on multiple plausible word continuations per context can
improve their ability to reproduce human linguistic variability for next-word
prediction. We employ fine-tuning techniques for pre-trained and
instruction-tuned models; and demonstrate their potential when fine-tuning
GPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures
divergence among empirically estimated human and model next-word distributions
across contexts before and after fine-tuning, shows that our multi-label
fine-tuning improves the LMs' ability to reproduce linguistic variability; both
for contexts that admit higher and lower variability.
\\ ( https://arxiv.org/abs/2509.17794 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17796
Date: Mon, 22 Sep 2025 13:52:32 GMT   (87kb)

Title: Findings of the Fourth Shared Task on Multilingual Coreference
  Resolution: Can LLMs Dethrone Traditional Approaches?
Authors: Michal Nov\'ak, Miloslav Konop\'ik, Anna Nedoluzhko, Martin Popel,
  Ond\v{r}ej Pra\v{z}\'ak, Jakub Sido, Milan Straka, Zden\v{e}k
  \v{Z}abokrtsk\'y, Daniel Zeman
Categories: cs.CL
Comments: Accepted to CODI-CRAC 2025
\\
  The paper presents an overview of the fourth edition of the Shared Task on
Multilingual Coreference Resolution, organized as part of the CODI-CRAC 2025
workshop. As in the previous editions, participants were challenged to develop
systems that identify mentions and cluster them according to identity
coreference.
  A key innovation of this year's task was the introduction of a dedicated
Large Language Model (LLM) track, featuring a simplified plaintext format
designed to be more suitable for LLMs than the original CoNLL-U representation.
  The task also expanded its coverage with three new datasets in two additional
languages, using version 1.3 of CorefUD - a harmonized multilingual collection
of 22 datasets in 17 languages.
  In total, nine systems participated, including four LLM-based approaches (two
fine-tuned and two using few-shot adaptation). While traditional systems still
kept the lead, LLMs showed clear potential, suggesting they may soon challenge
established approaches in future editions.
\\ ( https://arxiv.org/abs/2509.17796 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17807
Date: Mon, 22 Sep 2025 14:01:14 GMT   (548kb)

Title: Everyday Physics in Korean Contexts: A Culturally Grounded Physical
  Reasoning Benchmark
Authors: Jihae Jeong, DaeYeop Lee, DongGeon Lee, Hwanjo Yu
Categories: cs.CL
Comments: Accepted to MRL@EMNLP 2025
\\
  Existing physical commonsense reasoning benchmarks predominantly focus on
Western contexts, overlooking cultural variations in physical problem-solving.
To address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a
novel benchmark comprising 181 binary-choice problems that test physical
reasoning within Korean cultural contexts, ranging from kimchi (Korean food) to
traditional fermentation. EPiK is constructed using a two-stage generation and
verification pipeline to create culturally-authentic problems across 9
reasoning subtasks and 84 scenarios. Unlike approaches based on simple
translation, our method generates problems organically from Korean contexts
while upholding rigorous physical reasoning standards. Our evaluations show
that Korean-specialized models consistently outperform general-purpose models
of comparable size. This performance gap highlights the limitations of
culturally-agnostic models and demonstrates the critical need for
culturally-aware benchmarks to truly measure language understanding. Our EPiK
is publicly available at https://huggingface.co/datasets/jjae/EPiK.
\\ ( https://arxiv.org/abs/2509.17807 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17829
Date: Mon, 22 Sep 2025 14:21:26 GMT   (897kb)

Title: Towards Adaptive Context Management for Intelligent Conversational
  Question Answering
Authors: Manoj Madushanka Perera, Adnan Mahmood, Kasun Eranda Wijethilake and
  Quan Z. Sheng
Categories: cs.CL
Comments: Comments: 15 pages, 6 figures, Table 1, published in Lecture Notes in
  Computer Science (LNCS 15391), Proceedings of ADMA 2024. DOI:
  10.1007/978-981-96-0847-8_25
ACM-class: I.2.7; H.3.3
Journal-ref: Towards Adaptive Context Management for Intelligent Conversational
  Question Answering. Advanced Data Mining and Applications (ADMA) 2024, vol
  15391. Springer, Singapore
DOI: 10.1007/978-981-96-0847-8_25
\\
  This particular paper introduces an Adaptive Context Management (ACM)
framework for the Conversational Question Answering (ConvQA) systems. The key
objective of the ACM framework is to optimize the use of the conversation
history by dynamically managing context for maximizing the relevant information
provided to a ConvQA model within its token limit. Our approach incorporates a
Context Manager (CM) Module, a Summarization (SM) Module, and an Entity
Extraction (EE) Module in a bid to handle the conversation history
efficaciously. The CM Module dynamically adjusts the context size, thereby
preserving the most relevant and recent information within a model's token
limit. The SM Module summarizes the older parts of the conversation history via
a sliding window. When the summarization window exceeds its limit, the EE
Module identifies and retains key entities from the oldest conversation turns.
Experimental results demonstrate the effectiveness of our envisaged framework
in generating accurate and contextually appropriate responses, thereby
highlighting the potential of the ACM framework to enhance the robustness and
scalability of the ConvQA systems.
\\ ( https://arxiv.org/abs/2509.17829 ,  897kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17830
Date: Mon, 22 Sep 2025 14:22:55 GMT   (7440kb)

Title: Fine-Grained Detection of AI-Generated Text Using Sentence-Level
  Segmentation
Authors: Lekkala Sai Teja and Annepaka Yadagiri and and Partha Pakray and
  Chukhu Chunka and Mangadoddi Srikar Vardhan
Categories: cs.CL cs.AI
Comments: 14 pages, 14 figures
\\
  Generation of Artificial Intelligence (AI) texts in important works has
become a common practice that can be used to misuse and abuse AI at various
levels. Traditional AI detectors often rely on document-level classification,
which struggles to identify AI content in hybrid or slightly edited texts
designed to avoid detection, leading to concerns about the model's efficiency,
which makes it hard to distinguish between human-written and AI-generated
texts. A sentence-level sequence labeling model proposed to detect transitions
between human- and AI-generated text, leveraging nuanced linguistic signals
overlooked by document-level classifiers. By this method, detecting and
segmenting AI and human-written text within a single document at the
token-level granularity is achieved. Our model combines the state-of-the-art
pre-trained Transformer models, incorporating Neural Networks (NN) and
Conditional Random Fields (CRFs). This approach extends the power of
transformers to extract semantic and syntactic patterns, and the neural network
component to capture enhanced sequence-level representations, thereby improving
the boundary predictions by the CRF layer, which enhances sequence recognition
and further identification of the partition between Human- and AI-generated
texts. The evaluation is performed on two publicly available benchmark datasets
containing collaborative human and AI-generated texts. Our experimental
comparisons are with zero-shot detectors and the existing state-of-the-art
models, along with rigorous ablation studies to justify that this approach, in
particular, can accurately detect the spans of AI texts in a completely
collaborative text. All our source code and the processed datasets are
available in our GitHub repository.
\\ ( https://arxiv.org/abs/2509.17830 ,  7440kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17844
Date: Mon, 22 Sep 2025 14:32:55 GMT   (2061kb)

Title: Trust Me, I Can Convince You: The Contextualized Argument Appraisal
  Framework
Authors: Lynn Greschner, Sabine Weber, Roman Klinger
Categories: cs.CL
\\
  Emotions, which influence how convincing an argument is, are developed
  in context of the self and sender, and therefore require modeling
  the cognitive evaluation process. While binary emotionality has been
  studied in argument mining, and the cognitive appraisal has been
  modeled in general emotion analysis, these fields have not been
  brought together yet. We therefore propose the Contextualized
  Argument Appraisal Framework that contextualizes the interplay
  between the sender, receiver, and argument. It includes emotion
  labels, appraisals, such as argument familiarity, response urgency,
  and expected effort, as well as convincingness variables. To evaluate
  the framework and pave the way to computational modeling, we perform
  a study in a role-playing scenario, mimicking real-world exposure to
  arguments, asking participants to disclose their emotion, explain the main
cause, the
  argument appraisal, and the
  perceived convincingness. To consider the subjective nature of such
  annotations, we also collect demographic data and personality traits
  of both the participants and the perceived sender of the argument.
  The analysis of the resulting corpus of 800 arguments, each
  annotated by 5 participants, reveals that convincingness is
  positively correlated with positive emotions (e.g., trust) and
  negatively correlated with negative emotions (e.g., anger). The
  appraisal variables disclose the importance of the argument
  familiarity. For most participants, the content of the argument
  itself is the primary driver of the emotional response.
\\ ( https://arxiv.org/abs/2509.17844 ,  2061kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17855
Date: Mon, 22 Sep 2025 14:49:08 GMT   (9491kb)

Title: Make Every Letter Count: Building Dialect Variation Dictionaries from
  Monolingual Corpora
Authors: Robert Litschko, Verena Blaschke, Diana Burkhardt, Barbara Plank,
  Diego Frassinelli
Categories: cs.CL
Comments: Accepted at EMNLP 2025 (Findings)
\\
  Dialects exhibit a substantial degree of variation due to the lack of a
standard orthography. At the same time, the ability of Large Language Models
(LLMs) to process dialects remains largely understudied. To address this gap,
we use Bavarian as a case study and investigate the lexical dialect
understanding capability of LLMs by examining how well they recognize and
translate dialectal terms across different parts-of-speech. To this end, we
introduce DiaLemma, a novel annotation framework for creating dialect variation
dictionaries from monolingual data only, and use it to compile a ground truth
dataset consisting of 100K human-annotated German-Bavarian word pairs. We
evaluate how well nine state-of-the-art LLMs can judge Bavarian terms as
dialect translations, inflected variants, or unrelated forms of a given German
lemma. Our results show that LLMs perform best on nouns and lexically similar
word pairs, and struggle most in distinguishing between direct translations and
inflected variants. Interestingly, providing additional context in the form of
example usages improves the translation performance, but reduces their ability
to recognize dialect variants. This study highlights the limitations of LLMs in
dealing with orthographic dialect variation and emphasizes the need for future
work on adapting LLMs to dialects.
\\ ( https://arxiv.org/abs/2509.17855 ,  9491kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17858
Date: Mon, 22 Sep 2025 14:51:37 GMT   (236kb)

Title: CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual
  Coreference Resolution
Authors: Milan Straka
Categories: cs.CL
Comments: Accepted to CODI-CRAC 2025
\\
  We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on
Multilingual Coreference Resolution. This fourth iteration of the shared task
introduces a new LLM track alongside the original unconstrained track, features
reduced development and test sets to lower computational requirements, and
includes additional datasets. CorPipe 25 represents a complete reimplementation
of our previous systems, migrating from TensorFlow to PyTorch. Our system
significantly outperforms all other submissions in both the LLM and
unconstrained tracks by a substantial margin of 8 percentage points. The source
code and trained models are publicly available at
https://github.com/ufal/crac2025-corpipe.
\\ ( https://arxiv.org/abs/2509.17858 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17859
Date: Mon, 22 Sep 2025 14:52:03 GMT   (3603kb)

Title: Unsupervised Learning and Representation of Mandarin Tonal Categories by
  a Generative CNN
Authors: Kai Schenck and Ga\v{s}per Begu\v{s}
Categories: cs.CL cs.LG
\\
  This paper outlines the methodology for modeling tonal learning in fully
unsupervised models of human language acquisition. Tonal patterns are among the
computationally most complex learning objectives in language. We argue that a
realistic generative model of human language (ciwGAN) can learn to associate
its categorical variables with Mandarin Chinese tonal categories without any
labeled data. All three trained models showed statistically significant
differences in F0 across categorical variables. The model trained solely on
male tokens consistently encoded tone. Our results sug- gest that not only does
the model learn Mandarin tonal contrasts, but it learns a system that
corresponds to a stage of acquisition in human language learners. We also
outline methodology for tracing tonal representations in internal convolutional
layers, which shows that linguistic tools can contribute to interpretability of
deep learning and can ultimately be used in neural experiments.
\\ ( https://arxiv.org/abs/2509.17859 ,  3603kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17879
Date: Mon, 22 Sep 2025 15:15:40 GMT   (2458kb)

Title: How Persuasive is Your Context?
Authors: Tu Nguyen, Kevin Du, Alexander Miserlis Hoyle, Ryan Cotterell
Categories: cs.CL cs.AI
Comments: Long paper accepted at EMNLP 2025
\\
  Two central capabilities of language models (LMs) are: (i) drawing on prior
knowledge about entities, which allows them to answer queries such as "What's
the official language of Austria?", and (ii) adapting to new information
provided in context, e.g., "Pretend the official language of Austria is
Tagalog.", that is pre-pended to the question. In this article, we introduce
targeted persuasion score (TPS), designed to quantify how persuasive a given
context is to an LM where persuasion is operationalized as the ability of the
context to alter the LM's answer to the question. In contrast to evaluating
persuasiveness only by inspecting the greedily decoded answer under the model,
TPS provides a more fine-grained view of model behavior. Based on the
Wasserstein distance, TPS measures how much a context shifts a model's original
answer distribution toward a target distribution. Empirically, through a series
of experiments, we show that TPS captures a more nuanced notion of
persuasiveness than previously proposed metrics.
\\ ( https://arxiv.org/abs/2509.17879 ,  2458kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17912
Date: Mon, 22 Sep 2025 15:37:51 GMT   (2153kb)

Title: SiDiaC: Sinhala Diachronic Corpus
Authors: Nevidu Jayatilleke, Nisansa de Silva
Categories: cs.CL
Comments: 14 pages, 7 figures, 7 tables, Accepted paper at the 39th Pacific
  Asia Conference on Language, Information and Computation (PACLIC 39)
\\
  SiDiaC, the first comprehensive Sinhala Diachronic Corpus, covers a
historical span from the 5th to the 20th century CE. SiDiaC comprises 58k words
across 46 literary works, annotated carefully based on the written date, after
filtering based on availability, authorship, copyright compliance, and data
attribution. Texts from the National Library of Sri Lanka were digitised using
Google Document AI OCR, followed by post-processing to correct formatting and
modernise the orthography. The construction of SiDiaC was informed by practices
from other corpora, such as FarPaHC, particularly in syntactic annotation and
text normalisation strategies, due to the shared characteristics of
low-resourced language status. This corpus is categorised based on genres into
two layers: primary and secondary. Primary categorisation is binary,
classifying each book into Non-Fiction or Fiction, while the secondary
categorisation is more specific, grouping texts under Religious, History,
Poetry, Language, and Medical genres. Despite challenges including limited
access to rare texts and reliance on secondary date sources, SiDiaC serves as a
foundational resource for Sinhala NLP, significantly extending the resources
available for Sinhala, enabling diachronic studies in lexical change, neologism
tracking, historical syntax, and corpus-based lexicography.
\\ ( https://arxiv.org/abs/2509.17912 ,  2153kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17921
Date: Mon, 22 Sep 2025 15:47:07 GMT   (375kb)

Title: Improving Zero-shot Sentence Decontextualisation with Content Selection
  and Planning
Authors: Zhenyun Deng, Yulong Chen, Andreas Vlachos
Categories: cs.CL
Comments: Accepted to EMLNP 2025 (Main Conference)
\\
  Extracting individual sentences from a document as evidence or reasoning
steps is commonly done in many NLP tasks. However, extracted sentences often
lack context necessary to make them understood, e.g., coreference and
background information. To this end, we propose a content selection and
planning framework for zero-shot decontextualisation, which determines what
content should be mentioned and in what order for a sentence to be understood
out of context. Specifically, given a potentially ambiguous sentence and its
context, we first segment it into basic semantically-independent units. We then
identify potentially ambiguous units from the given sentence, and extract
relevant units from the context based on their discourse relations. Finally, we
generate a content plan to rewrite the sentence by enriching each ambiguous
unit with its relevant units. Experimental results demonstrate that our
approach is competitive for sentence decontextualisation, producing sentences
that exhibit better semantic integrity and discourse coherence, outperforming
existing methods.
\\ ( https://arxiv.org/abs/2509.17921 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17930
Date: Mon, 22 Sep 2025 15:52:18 GMT   (218kb)

Title: Transformer-Encoder Trees for Efficient Multilingual Machine Translation
  and Speech Translation
Authors: Yiwen Guan and Jacob Whitehill
Categories: cs.CL cs.AI
\\
  Multilingual translation faces challenges of computational redundancy and
limited accuracy for low-resource languages, especially in speech translation.
To address this, we propose a novel hierarchical Transformer Encoder Tree (TET)
combined with non-autoregressive encoder-only models trained with Connectionist
Temporal Classification for multilingual translation. By sharing intermediate
representations among linguistically similar target languages, TET can improve
accuracy on low-resource languages, reduce computational redundancy, and allow
generating all target languages in a single forward pass, thus eliminating
sequential bottlenecks and improving parallelism. For speech translation,
combining TET with a non-autoregressive speech recognition backbone (wav2vec2)
shows promising results in terms of translation quality compared to
autoregressive systems while being 7-14 times faster.
\\ ( https://arxiv.org/abs/2509.17930 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17932
Date: Mon, 22 Sep 2025 15:54:29 GMT   (431kb)

Title: Training-free Truthfulness Detection via Value Vectors in LLMs
Authors: Runheng Liu, Heyan Huang, Xingchen Xiao, Zhijing Wu
Categories: cs.CL
\\
  Large language models often generate factually incorrect outputs, motivating
efforts to detect the truthfulness of their content. Most existing approaches
rely on training probes over internal activations, but these methods suffer
from scalability and generalization issues. A recent training-free method,
NoVo, addresses this challenge by exploiting statistical patterns from the
model itself. However, it focuses exclusively on attention mechanisms,
potentially overlooking the MLP module-a core component of Transformer models
known to support factual recall. In this paper, we show that certain value
vectors within MLP modules exhibit truthfulness-related statistical patterns.
Building on this insight, we propose TruthV, a simple and interpretable
training-free method that detects content truthfulness by leveraging these
value vectors. On the NoVo benchmark, TruthV significantly outperforms both
NoVo and log-likelihood baselines, demonstrating that MLP modules-despite being
neglected in prior training-free efforts-encode rich and useful signals for
truthfulness detection. These findings offer new insights into how truthfulness
is internally represented in LLMs and motivate further research on scalable and
interpretable truthfulness detection.
\\ ( https://arxiv.org/abs/2509.17932 ,  431kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17938
Date: Mon, 22 Sep 2025 15:59:40 GMT   (1458kb)

Title: D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language
  Models
Authors: Satyapriya Krishna, Andy Zou, Rahul Gupta, Eliot Krzysztof Jones, Nick
  Winter, Dan Hendrycks, J. Zico Kolter, Matt Fredrikson, Spyros Matsoukas
Categories: cs.CL
Comments: Preprint
\\
  The safety and alignment of Large Language Models (LLMs) are critical for
their responsible deployment. Current evaluation methods predominantly focus on
identifying and preventing overtly harmful outputs. However, they often fail to
address a more insidious failure mode: models that produce benign-appearing
outputs while operating on malicious or deceptive internal reasoning. This
vulnerability, often triggered by sophisticated system prompt injections,
allows models to bypass conventional safety filters, posing a significant,
underexplored risk. To address this gap, we introduce the Deceptive Reasoning
Exposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy
between a model's internal reasoning process and its final output. D-REX was
constructed through a competitive red-teaming exercise where participants
crafted adversarial system prompts to induce such deceptive behaviors. Each
sample in D-REX contains the adversarial system prompt, an end-user's test
query, the model's seemingly innocuous response, and, crucially, the model's
internal chain-of-thought, which reveals the underlying malicious intent. Our
benchmark facilitates a new, essential evaluation task: the detection of
deceptive alignment. We demonstrate that D-REX presents a significant challenge
for existing models and safety mechanisms, highlighting the urgent need for new
techniques that scrutinize the internal processes of LLMs, not just their final
outputs.
\\ ( https://arxiv.org/abs/2509.17938 ,  1458kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17946
Date: Mon, 22 Sep 2025 16:07:11 GMT   (887kb)

Title: HICode: Hierarchical Inductive Coding with LLMs
Authors: Mian Zhong and Pristina Wang and Anjalie Field
Categories: cs.CL cs.AI cs.HC
Comments: Long paper accepted at EMNLP 2025 main conference, 19 pages, 8
  figures
\\
  Despite numerous applications for fine-grained corpus analysis, researchers
continue to rely on manual labeling, which does not scale, or statistical tools
like topic modeling, which are difficult to control. We propose that LLMs have
the potential to scale the nuanced analyses that researchers typically conduct
manually to large text corpora. To this effect, inspired by qualitative
research methods, we develop HICode, a two-part pipeline that first inductively
generates labels directly from analysis data and then hierarchically clusters
them to surface emergent themes. We validate this approach across three diverse
datasets by measuring alignment with human-constructed themes and demonstrating
its robustness through automated and human evaluations. Finally, we conduct a
case study of litigation documents related to the ongoing opioid crisis in the
U.S., revealing aggressive marketing strategies employed by pharmaceutical
companies and demonstrating HICode's potential for facilitating nuanced
analyses in large-scale data.
\\ ( https://arxiv.org/abs/2509.17946 ,  887kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17950
Date: Mon, 22 Sep 2025 16:09:26 GMT   (133kb)

Title: Dorabella Cipher as Musical Inspiration
Authors: Bradley Hauer, Colin Choi, Abram Hindle, Scott Smallwood, Grzegorz
  Kondrak
Categories: cs.CL
Comments: Published in Proceedings of the Workshop on Speech and Music
  Processing 2021
Journal-ref: Bradley Hauer, Colin Choi, Abram Hindle, Scott Smallwood, and
  Grzegorz Kondrak. 2021. Dorabella Cipher as Musical Inspiration. In
  Proceedings of the Workshop on Speech and Music Processing 2021
\\
  The Dorabella cipher is an encrypted note written by English composer Edward
Elgar, which has defied decipherment attempts for more than a century. While
most proposed solutions are English texts, we investigate the hypothesis that
Dorabella represents enciphered music. We weigh the evidence for and against
the hypothesis, devise a simplified music notation, and attempt to reconstruct
a melody from the cipher. Our tools are n-gram models of music which we
validate on existing music corpora enciphered using monoalphabetic
substitution. By applying our methods to Dorabella, we produce a decipherment
with musical qualities, which is then transformed via artful composition into a
listenable melody. Far from arguing that the end result represents the only
true solution, we instead frame the process of decipherment as part of the
composition process.
\\ ( https://arxiv.org/abs/2509.17950 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17961
Date: Mon, 22 Sep 2025 16:15:58 GMT   (693kb)

Title: Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants'
  Question-Answering in Asynchronous Learning Environments
Authors: Li Siyan, Zhen Xu, Vethavikashini Chithrra Raghuram, Xuanming Zhang,
  Renzhe Yu, Zhou Yu
Categories: cs.CL
Comments: Accepted in EMNLP 2025 Findings
\\
  Asynchronous learning environments (ALEs) are widely adopted for formal and
informal learning, but timely and personalized support is often limited. In
this context, Virtual Teaching Assistants (VTAs) can potentially reduce the
workload of instructors, but rigorous and pedagogically sound evaluation is
essential. Existing assessments often rely on surface-level metrics and lack
sufficient grounding in educational theories, making it difficult to
meaningfully compare the pedagogical effectiveness of different VTA systems. To
bridge this gap, we propose an evaluation framework rooted in learning sciences
and tailored to asynchronous forum discussions, a common VTA deployment context
in ALE. We construct classifiers using expert annotations of VTA responses on a
diverse set of forum posts. We evaluate the effectiveness of our classifiers,
identifying approaches that improve accuracy as well as challenges that hinder
generalization. Our work establishes a foundation for theory-driven evaluation
of VTA systems, paving the way for more pedagogically effective AI in
education.
\\ ( https://arxiv.org/abs/2509.17961 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17991
Date: Mon, 22 Sep 2025 16:33:59 GMT   (838kb)

Title: ReDepress: A Cognitive Framework for Detecting Depression Relapse from
  Social Media
Authors: Aakash Kumar Agarwal, Saprativa Bhattacharjee, Mauli Rastogi, Jemima
  S. Jacob, Biplab Banerjee, Rashmi Gupta and Pushpak Bhattacharyya
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to EMNLP 2025 Main Conference
\\
  Almost 50% depression patients face the risk of going into relapse. The risk
increases to 80% after the second episode of depression. Although, depression
detection from social media has attained considerable attention, depression
relapse detection has remained largely unexplored due to the lack of curated
datasets and the difficulty of distinguishing relapse and non-relapse users. In
this work, we present ReDepress, the first clinically validated social media
dataset focused on relapse, comprising 204 Reddit users annotated by mental
health professionals. Unlike prior approaches, our framework draws on cognitive
theories of depression, incorporating constructs such as attention bias,
interpretation bias, memory bias and rumination into both annotation and
modeling. Through statistical analyses and machine learning experiments, we
demonstrate that cognitive markers significantly differentiate relapse and
non-relapse groups, and that models enriched with these features achieve
competitive performance, with transformer-based temporal models attaining an F1
of 0.86. Our findings validate psychological theories in real-world textual
data and underscore the potential of cognitive-informed computational methods
for early relapse detection, paving the way for scalable, low-cost
interventions in mental healthcare.
\\ ( https://arxiv.org/abs/2509.17991 ,  838kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17995
Date: Mon, 22 Sep 2025 16:36:56 GMT   (2783kb)

Title: Variation in Verification: Understanding Verification Dynamics in Large
  Language Models
Authors: Yefan Zhou, Austin Xu, Yilun Zhou, Janvijay Singh, Jiang Gui, Shafiq
  Joty
Categories: cs.CL cs.AI cs.LG
\\
  Recent advances have shown that scaling test-time computation enables large
language models (LLMs) to solve increasingly complex problems across diverse
domains. One effective paradigm for test-time scaling (TTS) involves LLM
generators producing multiple solution candidates, with LLM verifiers assessing
the correctness of these candidates without reference answers. In this paper,
we study generative verifiers, which perform verification by generating
chain-of-thought (CoT) reasoning followed by a binary verdict. We
systematically analyze verification dynamics across three dimensions - problem
difficulty, generator capability, and verifier generation capability - with
empirical studies on 12 benchmarks across mathematical reasoning, knowledge,
and natural language reasoning tasks using 14 open-source models (2B to 72B
parameter range) and GPT-4o. Our experiments reveal three key findings about
verification effectiveness: (1) Easy problems allow verifiers to more reliably
certify correct responses; (2) Weak generators produce errors that are easier
to detect than strong generators; (3) Verification ability is generally
correlated with the verifier's own problem-solving capability, but this
relationship varies with problem difficulty. These findings reveal
opportunities to optimize basic verification strategies in TTS applications.
First, given the same verifier, some weak generators can nearly match stronger
ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B
performance gap shrinks by 75.5%). Second, we identify cases where strong
verifiers offer limited advantage over weak ones, as both fail to provide
meaningful verification gains, suggesting that verifier scaling alone cannot
overcome fundamental verification challenges.
\\ ( https://arxiv.org/abs/2509.17995 ,  2783kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18004
Date: Mon, 22 Sep 2025 16:44:00 GMT   (6811kb)

Title: WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation
  for Dialectal Speech Processing
Authors: Yuhang Dai, Ziyu Zhang, Shuai Wang, Longhao Li, Zhao Guo, Tianlun Zuo,
  Shuiyuan Wang, Hongfei Xue, Chengyou Wang, Qing Wang, Xin Xu, Hui Bu, Jie Li,
  Jian Kang, Binbin Zhang, Lei Xie
Categories: cs.CL cs.SD
Comments: 4 pages, 5 figures, 4 tables
\\
  The scarcity of large-scale, open-source data for dialects severely hinders
progress in speech technology, a challenge particularly acute for the widely
spoken Sichuanese dialects of Chinese. To address this critical gap, we
introduce WenetSpeech-Chuan, a 10,000-hour, richly annotated corpus constructed
using our novel Chuan-Pipeline, a complete data processing framework for
dialectal speech. To facilitate rigorous evaluation and demonstrate the
corpus's effectiveness, we also release high-quality ASR and TTS benchmarks,
WenetSpeech-Chuan-Eval, with manually verified transcriptions. Experiments show
that models trained on WenetSpeech-Chuan achieve state-of-the-art performance
among open-source systems and demonstrate results comparable to commercial
services. As the largest open-source corpus for Sichuanese dialects,
WenetSpeech-Chuan not only lowers the barrier to research in dialectal speech
processing but also plays a crucial role in promoting AI equity and mitigating
bias in speech technologies. The corpus, benchmarks, models, and receipts are
publicly available on our project page.
\\ ( https://arxiv.org/abs/2509.18004 ,  6811kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18010
Date: Mon, 22 Sep 2025 16:49:26 GMT   (1955kb)

Title: Cross-Attention is Half Explanation in Speech-to-Text Models
Authors: Sara Papi, Dennis Fucci, Marco Gaido, Matteo Negri, Luisa Bentivogli
Categories: cs.CL cs.AI cs.SD
\\
  Cross-attention is a core mechanism in encoder-decoder architectures,
widespread in many fields, including speech-to-text (S2T) processing. Its
scores have been repurposed for various downstream applications--such as
timestamp estimation and audio-text alignment--under the assumption that they
reflect the dependencies between input speech representation and the generated
text. While the explanatory nature of attention mechanisms has been widely
debated in the broader NLP literature, this assumption remains largely
unexplored within the speech domain. To address this gap, we assess the
explanatory power of cross-attention in S2T models by comparing its scores to
input saliency maps derived from feature attribution. Our analysis spans
monolingual and multilingual, single-task and multi-task models at multiple
scales, and shows that attention scores moderately to strongly align with
saliency-based explanations, particularly when aggregated across heads and
layers. However, it also shows that cross-attention captures only about 50% of
the input relevance and, in the best case, only partially reflects how the
decoder attends to the encoder's representations--accounting for just 52-75% of
the saliency. These findings uncover fundamental limitations in interpreting
cross-attention as an explanatory proxy, suggesting that it offers an
informative yet incomplete view of the factors driving predictions in S2T
models.
\\ ( https://arxiv.org/abs/2509.18010 ,  1955kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18030
Date: Mon, 22 Sep 2025 17:03:48 GMT   (81kb)

Title: RadEval: A framework for radiology text evaluation
Authors: Justin Xu, Xi Zhang, Javid Abderezaei, Julie Bauml, Roger Boodoo,
  Fatemeh Haghighi, Ali Ganjizadeh, Eric Brattain, Dave Van Veen, Zaiqiao Meng,
  David Eyre, Jean-Benoit Delbrouck
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Demo track - Oral
\\
  We introduce RadEval, a unified, open-source framework for evaluating
radiology texts. RadEval consolidates a diverse range of metrics, from classic
n-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical
concept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,
TemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and
standardize implementations, extend GREEN to support multiple imaging
modalities with a more lightweight model, and pretrain a domain-specific
radiology encoder, demonstrating strong zero-shot retrieval performance. We
also release a richly annotated expert dataset with over 450 clinically
significant error labels and show how different metrics correlate with
radiologist judgment. Finally, RadEval provides statistical testing tools and
baseline model evaluations across multiple publicly available datasets,
facilitating reproducibility and robust benchmarking in radiology report
generation.
\\ ( https://arxiv.org/abs/2509.18030 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18052
Date: Mon, 22 Sep 2025 17:27:29 GMT   (1771kb)

Title: The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM
  Societies
Authors: Jiaxu Zhou, Jen-tse Huang, Xuhui Zhou, Man Ho Lam, Xintao Wang, Hao
  Zhu, Wenxuan Wang, Maarten Sap
Categories: cs.CL cs.CY
Comments: Preprint
\\
  Large Language Models (LLMs) are increasingly used for social simulation,
where populations of agents are expected to reproduce human-like collective
behavior. However, we find that many recent studies adopt experimental designs
that systematically undermine the validity of their claims. From a survey of
over 40 papers, we identify six recurring methodological flaws: agents are
often homogeneous (Profile), interactions are absent or artificially imposed
(Interaction), memory is discarded (Memory), prompts tightly control outcomes
(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),
and validation relies on simplified theoretical models rather than real-world
data (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying
social experiment in 53.1% of cases when given instructions from prior
work-violating the Unawareness principle. We formalize these six requirements
as the PIMMUR principles and argue they are necessary conditions for credible
LLM-based social simulation. To demonstrate their impact, we re-run five
representative studies using a framework that enforces PIMMUR and find that the
reported social phenomena frequently fail to emerge under more rigorous
conditions. Our work establishes methodological standards for LLM-based
multi-agent research and provides a foundation for more reliable and
reproducible claims about "AI societies."
\\ ( https://arxiv.org/abs/2509.18052 ,  1771kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18060
Date: Mon, 22 Sep 2025 17:38:52 GMT   (1811kb)

Title: TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for
  \"U-Tsang, Amdo and Kham Speech Dataset Generation
Authors: Yutong Liu, Ziyue Zhang, Ban Ma-bao, Renzeng Duojie, Yuqing Cai,
  Yongbin Yu, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi
Categories: cs.CL cs.AI
\\
  Tibetan is a low-resource language with limited parallel speech corpora
spanning its three major dialects (\"U-Tsang, Amdo, and Kham), limiting
progress in speech modeling. To address this issue, we propose TMD-TTS, a
unified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes
parallel dialectal speech from explicit dialect labels. Our method features a
dialect fusion module and a Dialect-Specialized Dynamic Routing Network
(DSDR-Net) to capture fine-grained acoustic and linguistic variations across
dialects. Extensive objective and subjective evaluations demonstrate that
TMD-TTS significantly outperforms baselines in dialectal expressiveness. We
further validate the quality and utility of the synthesized speech through a
challenging Speech-to-Speech Dialect Conversion (S2SDC) task.
\\ ( https://arxiv.org/abs/2509.18060 ,  1811kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18063
Date: Mon, 22 Sep 2025 17:40:05 GMT   (242kb)

Title: ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring
  Commonsense Reasoning
Authors: Jan-Felix Klein, Lars Ohnemus
Categories: cs.CL
Comments: Work in Progess
\\
  Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.
\\ ( https://arxiv.org/abs/2509.18063 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18093
Date: Mon, 22 Sep 2025 17:59:38 GMT   (146kb)

Title: SEQR: Secure and Efficient QR-based LoRA Routing
Authors: William Fleshman and Benjamin Van Durme
Categories: cs.CL cs.AI cs.LG
\\
  Low-Rank Adaptation (LoRA) has become a standard technique for
parameter-efficient fine-tuning of large language models, enabling large
libraries of LoRAs, each for a specific task or domain. Efficiently selecting
the correct LoRA adapter for a given input remains a challenge, particularly in
secure environments where supervised training of routers may raise privacy
concerns. Motivated by previous approaches, we formalize the goal of
unsupervised LoRA routing in terms of activation norm maximization, providing a
theoretical framework for analysis. We demonstrate the discriminative power of
activation norms and introduce SEQR, an unsupervised LoRA routing algorithm
designed to maximize efficiency while providing strict routing guarantees. SEQR
provably identifies the norm-maximizing adapter with significantly greater
efficiency, making it a highly scalable and effective solution for dynamic LoRA
composition. We validate our results through experiments that demonstrate
improved multi-task performance and efficiency.
\\ ( https://arxiv.org/abs/2509.18093 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16221
Date: Thu, 11 Sep 2025 09:19:18 GMT   (1532kb)

Title: Evaluation of Ensemble Learning Techniques for handwritten OCR
  Improvement
Authors: Martin Prei{\ss}
Categories: cs.CV cs.LG
\\
  For the bachelor project 2021 of Professor Lippert's research group,
handwritten entries of historical patient records needed to be digitized using
Optical Character Recognition (OCR) methods. Since the data will be used in the
future, a high degree of accuracy is naturally required. Especially in the
medical field this has even more importance. Ensemble Learning is a method that
combines several machine learning models and is claimed to be able to achieve
an increased accuracy for existing methods. For this reason, Ensemble Learning
in combination with OCR is investigated in this work in order to create added
value for the digitization of the patient records. It was possible to discover
that ensemble learning can lead to an increased accuracy for OCR, which methods
were able to achieve this and that the size of the training data set did not
play a role here.
\\ ( https://arxiv.org/abs/2509.16221 ,  1532kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16343
Date: Fri, 19 Sep 2025 18:34:08 GMT   (739kb)

Title: Agentic Reasoning for Robust Vision Systems via Increased Test-Time
  Compute
Authors: Chung-En (Johnny) Yu, Brian Jalaian, Nathaniel D. Bastian
Categories: cs.CV cs.AI cs.MA
\\
  Developing trustworthy intelligent vision systems for high-stakes domains,
\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness
without costly retraining. We propose \textbf{Visual Reasoning Agent (VRA)}, a
training-free, agentic reasoning framework that wraps off-the-shelf
vision-language models \emph{and} pure vision systems in a
\emph{Think--Critique--Act} loop. While VRA incurs significant additional
test-time computation, it achieves up to 40\% absolute accuracy gains on
challenging visual reasoning benchmarks. Future work will optimize query
routing and early stopping to reduce inference overhead while preserving
reliability in vision tasks.
\\ ( https://arxiv.org/abs/2509.16343 ,  739kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16346
Date: Fri, 19 Sep 2025 18:39:50 GMT   (89642kb)

Title: From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation
  of 3D Forest Structure from Aerial-to-Terrestrial LiDAR
Authors: Juan Castorena, E. Louise Loudermilk, Scott Pokswinski, Rodman Linn
Categories: cs.CV cs.AI
\\
  The 3D structure of living and non-living components in ecosystems plays a
critical role in determining ecological processes and feedbacks from both
natural and human-driven disturbances. Anticipating the effects of wildfire,
drought, disease, or atmospheric deposition depends on accurate
characterization of 3D vegetation structure, yet widespread measurement remains
prohibitively expensive and often infeasible. We introduce ForestGen3D, a novel
generative modeling framework that synthesizes high-fidelity 3D forest
structure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on
conditional denoising diffusion probabilistic models (DDPMs) trained on
co-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate
TLS-like 3D point clouds conditioned on sparse ALS observations, effectively
reconstructing occluded sub-canopy detail at scale. To ensure ecological
plausibility, we introduce a geometric containment prior based on the convex
hull of ALS observations and provide theoretical and empirical guarantees that
generated structures remain spatially consistent. We evaluate ForestGen3D at
tree, plot, and landscape scales using real-world data from mixed conifer
ecosystems, and show that it produces high-fidelity reconstructions that
closely match TLS references in terms of geometric similarity and biophysical
metrics, such as tree height, DBH, crown diameter and crown volume.
Additionally, we demonstrate that the containment property can serve as a
practical proxy for generation quality in settings where TLS ground truth is
unavailable. Our results position ForestGen3D as a scalable tool for ecological
modeling, wildfire simulation, and structural fuel characterization in ALS-only
environments.
\\ ( https://arxiv.org/abs/2509.16346 ,  89642kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16363
Date: Fri, 19 Sep 2025 19:11:31 GMT   (27415kb)

Title: Introducing Resizable Region Packing Problem in Image Generation, with a
  Heuristic Solution
Authors: Hrishikesh Sharma
Categories: cs.CV
\\
  The problem of image data generation in computer vision has traditionally
been a harder problem to solve, than discriminative problems. Such data
generation entails placing relevant objects of appropriate sizes each, at
meaningful location in a scene canvas. There have been two classes of popular
approaches to such generation: graphics based, and generative models-based.
Optimization problems are known to lurk in the background for both these
classes of approaches. In this paper, we introduce a novel, practically useful
manifestation of the classical Bin Packing problem in the context of generation
of synthetic image data. We conjecture that the newly introduced problem,
Resizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide
detailed arguments about our conjecture. As a first solution, we present a
novel heuristic algorithm that is generic enough and therefore scales and packs
arbitrary number of arbitrary-shaped regions at arbitrary locations, into an
image canvas. The algorithm follows greedy approach to iteratively pack region
pairs in a careful way, while obeying the optimization constraints. The
algorithm is validated by an implementation that was used to generate a
large-scale synthetic anomaly detection dataset, with highly varying degree of
bin packing parameters per image sample i.e. RARP instance. Visual inspection
of such data and checking of the correctness of each solution proves the
effectiveness of our algorithm. With generative modeling being on rise in deep
learning, and synthetic data generation poised to become mainstream, we expect
that the newly introduced problem will be valued in the imaging scientific
community.
\\ ( https://arxiv.org/abs/2509.16363 ,  27415kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16382
Date: Fri, 19 Sep 2025 19:54:04 GMT   (1349kb)

Title: Accurate Thyroid Cancer Classification using a Novel Binary Pattern
  Driven Local Discrete Cosine Transform Descriptor
Authors: Saurabh Saini, Kapil Ahuja, Marc C. Steinbach, Thomas Wick
Categories: cs.CV cs.LG eess.IV
Comments: 15 Pages, 7 Figures, 5 Tables
ACM-class: I.2.1; I.5.2
\\
  In this study, we develop a new CAD system for accurate thyroid cancer
classification with emphasis on feature extraction. Prior studies have shown
that thyroid texture is important for segregating the thyroid ultrasound images
into different classes. Based upon our experience with breast cancer
classification, we first conjuncture that the Discrete Cosine Transform (DCT)
is the best descriptor for capturing textural features. Thyroid ultrasound
images are particularly challenging as the gland is surrounded by multiple
complex anatomical structures leading to variations in tissue density. Hence,
we second conjuncture the importance of localization and propose that the Local
DCT (LDCT) descriptor captures the textural features best in this context.
Another disadvantage of complex anatomy around the thyroid gland is scattering
of ultrasound waves resulting in noisy and unclear textures. Hence, we third
conjuncture that one image descriptor is not enough to fully capture the
textural features and propose the integration of another popular texture
capturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is
known to be noise resilient as well. We term our novel descriptor as Binary
Pattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification
is carried out using a non-linear SVM. The proposed CAD system is evaluated on
the only two publicly available thyroid cancer datasets, namely TDID and AUITD.
The evaluation is conducted in two stages. In Stage I, thyroid nodules are
categorized as benign or malignant. In Stage II, the malignant cases are
further sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I
classification, our proposed model demonstrates exceptional performance of
nearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed
model again attains excellent classification of close to 100% on TDID and 99%
on AUITD.
\\ ( https://arxiv.org/abs/2509.16382 ,  1349kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16415
Date: Fri, 19 Sep 2025 20:57:03 GMT   (48149kb)

Title: StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes
Authors: Zhengri Wu, Yiran Wang, Yu Wen, Zeyu Zhang, Biao Wu, Hao Tang
Categories: cs.CV cs.RO
\\
  Underwater stereo depth estimation provides accurate 3D geometry for robotics
tasks such as navigation, inspection, and mapping, offering metric depth from
low-cost passive cameras while avoiding the scale ambiguity of monocular
methods. However, existing approaches face two critical challenges: (i)
parameter-efficiently adapting large vision foundation encoders to the
underwater domain without extensive labeled data, and (ii) tightly fusing
globally coherent but scale-ambiguous monocular priors with locally metric yet
photometrically fragile stereo correspondences. To address these challenges, we
propose StereoAdapter, a parameter-efficient self-supervised framework that
integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo
refinement module. We further introduce dynamic LoRA adaptation for efficient
rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to
enhance robustness under diverse underwater conditions. Comprehensive
evaluations on both simulated and real-world benchmarks show improvements of
6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,
while real-world deployment with the BlueROV2 robot further demonstrates the
consistent robustness of our approach. Code:
https://github.com/AIGeeksGroup/StereoAdapter. Website:
https://aigeeksgroup.github.io/StereoAdapter.
\\ ( https://arxiv.org/abs/2509.16415 ,  48149kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16421
Date: Fri, 19 Sep 2025 21:03:00 GMT   (2137kb)

Title: AHA -- Predicting What Matters Next: Online Highlight Detection Without
  Looking Ahead
Authors: Aiden Chang, Celso De Melo, Stephanie M. Lukin
Categories: cs.CV cs.AI
Comments: Accepted at NeurIPS 2025, 32 pages, 5 figures
\\
  Real-time understanding of continuous video streams is essential for
intelligent agents operating in high-stakes environments, including autonomous
vehicles, surveillance drones, and disaster response robots. Yet, most existing
video understanding and highlight detection methods assume access to the entire
video during inference, making them unsuitable for online or streaming
scenarios. In particular, current models optimize for offline summarization,
failing to support step-by-step reasoning needed for real-time decision-making.
We introduce Aha, an autoregressive highlight detection framework that predicts
the relevance of each video frame against a task described in natural language.
Without accessing future video frames, Aha utilizes a multimodal
vision-language model and lightweight, decoupled heads trained on a large,
curated dataset of human-centric video labels. To enable scalability, we
introduce the Dynamic SinkCache mechanism that achieves constant memory usage
across infinite-length streams without degrading performance on standard
benchmarks. This encourages the hidden representation to capture high-level
task objectives, enabling effective frame-level rankings for informativeness,
relevance, and uncertainty with respect to the natural language task. Aha
achieves state-of-the-art (SOTA) performance on highlight detection benchmarks,
surpassing even prior offline, full-context approaches and video-language
models by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).
We explore Aha's potential for real-world robotics applications given a
task-oriented natural language input and a continuous, robot-centric video.
Both experiments demonstrate Aha's potential effectiveness as a real-time
reasoning module for downstream planning and long-horizon understanding.
\\ ( https://arxiv.org/abs/2509.16421 ,  2137kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16423
Date: Fri, 19 Sep 2025 21:04:36 GMT   (37823kb)

Title: 3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction
Authors: Maria Taktasheva, Lily Goli, Alessandro Fiorini, Zhen (Colin) Li,
  Daniel Rebain, Andrea Tagliasacchi
Categories: cs.CV
\\
  Recent advances in radiance fields and novel view synthesis enable creation
of realistic digital twins from photographs. However, current methods struggle
with flat, texture-less surfaces, creating uneven and semi-transparent
reconstructions, due to an ill-conditioned photometric reconstruction
objective. Surface reconstruction methods solve this issue but sacrifice visual
quality. We propose a novel hybrid 2D/3D representation that jointly optimizes
constrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)
Gaussians for the rest of the scene. Our end-to-end approach dynamically
detects and refines planar regions, improving both visual fidelity and
geometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++
and ScanNetv2, and excels at mesh extraction without overfitting to a specific
camera model, showing its effectiveness in producing high-quality
reconstruction of indoor scenes.
\\ ( https://arxiv.org/abs/2509.16423 ,  37823kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16429
Date: Fri, 19 Sep 2025 21:10:13 GMT   (33630kb)

Title: TractoTransformer: Diffusion MRI Streamline Tractography using CNN and
  Transformer Networks
Authors: Itzik Waizman, Yakov Gusakov, Itay Benou, Tammy Riklin Raviv
Categories: cs.CV
\\
  White matter tractography is an advanced neuroimaging technique that
reconstructs the 3D white matter pathways of the brain from diffusion MRI data.
It can be framed as a pathfinding problem aiming to infer neural fiber
trajectories from noisy and ambiguous measurements, facing challenges such as
crossing, merging, and fanning white-matter configurations. In this paper, we
propose a novel tractography method that leverages Transformers to model the
sequential nature of white matter streamlines, enabling the prediction of fiber
directions by integrating both the trajectory context and current diffusion MRI
measurements. To incorporate spatial information, we utilize CNNs that extract
microstructural features from local neighborhoods around each voxel. By
combining these complementary sources of information, our approach improves the
precision and completeness of neural pathway mapping compared to traditional
tractography models. We evaluate our method with the Tractometer toolkit,
achieving competitive performance against state-of-the-art approaches, and
present qualitative results on the TractoInferno dataset, demonstrating strong
generalization to real-world data.
\\ ( https://arxiv.org/abs/2509.16429 ,  33630kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16436
Date: Fri, 19 Sep 2025 21:31:05 GMT   (844kb)

Title: Improved mmFormer for Liver Fibrosis Staging via Missing-Modality
  Compensation
Authors: Zhejia Zhang, Junjie Wang, and Le Zhang (University of Birmingham, UK)
Categories: cs.CV
\\
  In real-world clinical settings, magnetic resonance imaging (MRI) frequently
suffers from missing modalities due to equipment variability or patient
cooperation issues, which can significantly affect model performance. To
address this issue, we propose a multimodal MRI classification model based on
the mmFormer architecture with an adaptive module for handling arbitrary
combinations of missing modalities. Specifically, this model retains the hybrid
modality-specific encoders and the modality-correlated encoder from mmFormer to
extract consistent lesion features across available modalities. In addition, we
integrate a missing-modality compensation module which leverages zero-padding,
modality availability masks, and a Delta Function with learnable statistical
parameters to dynamically synthesize proxy features for recovering missing
information. To further improve prediction performance, we adopt a
cross-validation ensemble strategy by training multiple models on different
folds and applying soft voting during inference. This method is evaluated on
the test set of Comprehensive Analysis & Computing of REal-world medical images
(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based
on non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),
T2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis
Detection and Substantial Fibrosis Detection on in-distribution vendors, our
model obtains accuracies of 66.67%, and 74.17%, and corresponding area under
the curve (AUC) scores of 71.73% and 68.48%, respectively.
\\ ( https://arxiv.org/abs/2509.16436 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16438
Date: Fri, 19 Sep 2025 21:35:04 GMT   (2633kb)

Title: AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval
  Benchmarks
Authors: Mohamed Eltahir, Osamah Sarraj, Abdulrahman Alfrihidi, Taha Alshatiri,
  Mohammed Khurd, Mohammed Bremoo, and Tanveer Hussain
Categories: cs.CV cs.CL
Comments: Accepted at ArabicNLP 2025 (EMNLP 2025 workshop)
\\
  Video-to-text and text-to-video retrieval are dominated by English benchmarks
(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet
Arabic remains underserved, lacking localized evaluation metrics. We introduce
a three-stage framework, AutoArabic, utilizing state-of-the-art large language
models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,
reducing the manual revision required by nearly fourfold. The framework
incorporates an error detection module that automatically flags potential
translation errors with 97% accuracy. Applying the framework to DiDeMo, a video
retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent
Arabic descriptions. An analysis of the translation errors is provided and
organized into an insightful taxonomy to guide future Arabic localization
efforts. We train a CLIP-style baseline with identical hyperparameters on the
Arabic and English variants of the benchmark, finding a moderate performance
gap (about 3 percentage points at Recall@1), indicating that Arabic
localization preserves benchmark difficulty. We evaluate three post-editing
budgets (zero/ flagged-only/ full) and find that performance improves
monotonically with more post-editing, while the raw LLM output (zero-budget)
remains usable. To ensure reproducibility to other languages, we made the code
available at https://github.com/Tahaalshatiri/AutoArabic.
\\ ( https://arxiv.org/abs/2509.16438 ,  2633kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16452
Date: Fri, 19 Sep 2025 22:12:49 GMT   (3059kb)

Title: KRAST: Knowledge-Augmented Robotic Action Recognition with Structured
  Text for Vision-Language Models
Authors: Son Hai Nguyen, Diwei Wang, Jinhyeok Jang, and Hyewon Seo
Categories: cs.CV cs.AI
\\
  Accurate vision-based action recognition is crucial for developing autonomous
robots that can operate safely and reliably in complex, real-world
environments. In this work, we advance video-based recognition of indoor daily
actions for robotic perception by leveraging vision-language models (VLMs)
enriched with domain-specific knowledge. We adapt a prompt-learning framework
in which class-level textual descriptions of each action are embedded as
learnable prompts into a frozen pre-trained VLM backbone. Several strategies
for structuring and encoding these textual descriptions are designed and
evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our
method, using only RGB video inputs at test time, achieves over 95\% accuracy
and outperforms state-of-the-art approaches. These results highlight the
effectiveness of knowledge-augmented prompts in enabling robust action
recognition with minimal supervision.
\\ ( https://arxiv.org/abs/2509.16452 ,  3059kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16472
Date: Fri, 19 Sep 2025 23:53:45 GMT   (310kb)

Title: Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM
  Models
Authors: Parth Agarwal, Sangaa Chatterjee, Md Faisal Kabir, Suman Saha
Categories: cs.CV
Comments: The paper got accepted in ICMLA-2025. It is a camera-ready version
\\
  Gait is a key indicator in diagnosing movement disorders, but most models
lack interpretability and rely on single datasets. We propose a dual-branch
CNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D
branch on silhouettes from OU-MVLP. Interpretability is provided by SHAP
(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,
the system achieves 98.6% accuracy with strong recall and F1. This approach
advances explainable gait analysis across both clinical and biometric domains.
\\ ( https://arxiv.org/abs/2509.16472 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16474
Date: Sat, 20 Sep 2025 00:00:55 GMT   (896kb)

Title: Cross-Corpus and Cross-domain Handwriting Assessment of
  NeuroDegenerative Diseases via Time-Series-to-Image Conversion
Authors: Gabrielle Chavez, Laureano Moro-Velazquez, Ankur Butala, Najim Dehak,
  Thomas Thebaud
Categories: cs.CV
Comments: 5 pages, 2 figures, submitted to International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP)
\\
  Handwriting is significantly affected by neurological disorders (ND) such as
Parkinson's disease (PD) and Alzheimer's disease (AD). Prior works have
analyzed handwriting tasks using feature-based approaches or computer-vision
techniques, but these methods have struggled to generalize across multiple
datasets, particularly between temporal features represented as time-series and
images. We propose a framework that leverages both time-series and images of
handwriting through a joint classifier, based on a ResNet50 pretrained on
ImageNet-1k. Binary classification experiments demonstrate state-of-the-art
performances on existing time-series and image datasets, with significant
improvement on specific drawing and writing tasks from the NeuroLogical Signals
(NLS) dataset. In particular, the proposed model demonstrates improved
performance on Draw Clock and Spiral tasks. Additionally, cross-dataset and
multi-dataset experiments were consistently able to achieve high F1 scores, up
to 98 for PD detection, highlighting the potential of the proposed model to
generalize over different forms of handwriting signals, and enhance the
detection of motor deficits in ND.
\\ ( https://arxiv.org/abs/2509.16474 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16476
Date: Sat, 20 Sep 2025 00:16:48 GMT   (37286kb)

Title: Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs
Authors: Qinyu Chen, Jiawen Qi
Categories: cs.CV
Comments: 11 pages
\\
  Vision-Language Models (VLMs) deliver impressive performance in understanding
visual content with language instructions. However, redundancy in vision tokens
results in the degenerated inference efficiency of VLMs, which hinders
real-time use on edge consumer devices such as AR/VR devices. Existing
efficiency methods commonly prune visual tokens using learned saliency, sparse
attention schedules, or controller policies, but they often require
architectural modification or access to intermediate activations. These
pipelines add inference-time modules that increase compute and memory and often
lead to an accuracy trade-off. Moreover, they also suffer from misalignment
between the prompts and the region of interest in the images. Without human
guidance, the model may focus on the wrong regions and miss small,
high-frequency details when prompts or scenes change. In this paper, we propose
GazeVLM, a training-free framework that uses the human eye gaze as a natural
supervisory signal to allocate computation where it matters. By extracting
gaze-driven regions of interest (ROIs) and optionally combining them with a
low-resolution global view, GazeVLM mimics fovea-periphery perception to cut
redundant visual tokens while preserving task-relevant details. We evaluate the
visual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark
with human gaze. Quality of the answer is assessed by GPT-4o pairwise judging
and a weighted score over coverage, accuracy, details, and fluency. Efficiency
is measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to
93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better
answer quality relative to full-resolution baselines. Our results show that
aligning model computation with human gaze offers a simple, plug-and-play path
toward efficient VLM inference on consumer devices.
\\ ( https://arxiv.org/abs/2509.16476 ,  37286kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16479
Date: Sat, 20 Sep 2025 00:29:43 GMT   (1452kb)

Title: Thermal Imaging-based Real-time Fall Detection using Motion Flow and
  Attention-enhanced Convolutional Recurrent Architecture
Authors: Christopher Silver and Thangarajah Akilan
Categories: cs.CV cs.AI
\\
  Falls among seniors are a major public health issue. Existing solutions using
wearable sensors, ambient sensors, and RGB-based vision systems face challenges
in reliability, user compliance, and practicality. Studies indicate that
stakeholders, such as older adults and eldercare facilities, prefer
non-wearable, passive, privacy-preserving, and real-time fall detection systems
that require no user interaction. This study proposes an advanced thermal fall
detection method using a Bidirectional Convolutional Long Short-Term Memory
(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general
attention mechanisms. Through systematic experimentation across hundreds of
model variations exploring the integration of attention mechanisms, recurrent
modules, and motion flow, we identified top-performing architectures. Among
them, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of
$99.7\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly
emerged, diverse, and privacy-preserving benchmark. These results highlight the
generalizability and practicality of the proposed model, setting new standards
for thermal fall detection and paving the way toward deployable,
high-performance solutions.
\\ ( https://arxiv.org/abs/2509.16479 ,  1452kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16483
Date: Sat, 20 Sep 2025 00:53:13 GMT   (4494kb)

Title: Octree Latent Diffusion for Semantic 3D Scene Generation and Completion
Authors: Xujia Zhang, Brendan Crowe, and Christoffer Heckman
Categories: cs.CV
\\
  The completion, extension, and generation of 3D semantic scenes are an
interrelated set of capabilities that are useful for robotic navigation and
exploration. Existing approaches seek to decouple these problems and solve them
oneoff. Additionally, these approaches are often domain-specific, requiring
separate models for different data distributions, e.g. indoor vs. outdoor
scenes. To unify these techniques and provide cross-domain compatibility, we
develop a single framework that can perform scene completion, extension, and
generation in both indoor and outdoor scenes, which we term Octree Latent
Semantic Diffusion. Our approach operates directly on an efficient dual octree
graph latent representation: a hierarchical, sparse, and memory-efficient
occupancy structure. This technique disentangles synthesis into two stages: (i)
structure diffusion, which predicts binary split signals to construct a coarse
occupancy octree, and (ii) latent semantic diffusion, which generates semantic
embeddings decoded by a graph VAE into voxellevel semantic labels. To perform
semantic scene completion or extension, our model leverages inference-time
latent inpainting, or outpainting respectively. These inference-time methods
use partial LiDAR scans or maps to condition generation, without the need for
retraining or finetuning. We demonstrate highquality structure, coherent
semantics, and robust completion from single LiDAR scans, as well as zero-shot
generalization to out-of-distribution LiDAR data. These results indicate that
completion-through-generation in a dual octree graph latent space is a
practical and scalable alternative to regression-based pipelines for real-world
robotic perception tasks.
\\ ( https://arxiv.org/abs/2509.16483 ,  4494kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16500
Date: Sat, 20 Sep 2025 02:23:36 GMT   (3699kb)

Title: RLGF: Reinforcement Learning with Geometric Feedback for Autonomous
  Driving Video Generation
Authors: Tianyi Yan, Wencheng Han, Xia Zhou, Xueyang Zhang, Kun Zhan,
  Cheng-zhong Xu, Jianbing Shen
Categories: cs.CV
Comments: NeurIPS 2025
\\
  Synthetic data is crucial for advancing autonomous driving (AD) systems, yet
current state-of-the-art video generation models, despite their visual realism,
suffer from subtle geometric distortions that limit their utility for
downstream perception tasks. We identify and quantify this critical issue,
demonstrating a significant performance gap in 3D object detection when using
synthetic versus real data. To address this, we introduce Reinforcement
Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion
models by incorporating rewards from specialized latent-space AD perception
models. Its core components include an efficient Latent-Space Windowing
Optimization technique for targeted feedback during diffusion, and a
Hierarchical Geometric Reward (HGR) system providing multi-level rewards for
point-line-plane alignment, and scene occupancy coherence. To quantify these
distortions, we propose GeoScores. Applied to models like DiVE on nuScenes,
RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Depth
error by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,
narrowing the gap to real-data performance. RLGF offers a plug-and-play
solution for generating geometrically sound and reliable synthetic videos for
AD development.
\\ ( https://arxiv.org/abs/2509.16500 ,  3699kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16506
Date: Sat, 20 Sep 2025 02:55:40 GMT   (11421kb)

Title: CommonForms: A Large, Diverse Dataset for Form Field Detection
Authors: Joe Barrow
Categories: cs.CV cs.LG
\\
  This paper introduces CommonForms, a web-scale dataset for form field
detection. It casts the problem of form field detection as object detection:
given an image of a page, predict the location and type (Text Input, Choice
Button, Signature) of form fields. The dataset is constructed by filtering
Common Crawl to find PDFs that have fillable elements. Starting with 8 million
documents, the filtering process is used to arrive at a final dataset of
roughly 55k documents that have over 450k pages. Analysis shows that the
dataset contains a diverse mixture of languages and domains; one third of the
pages are non-English, and among the 14 classified domains, no domain makes up
more than 25% of the dataset.
  In addition, this paper presents a family of form field detectors,
FFDNet-Small and FFDNet-Large, which attain a very high average precision on
the CommonForms test set. Each model cost less than $500 to train. Ablation
results show that high-resolution inputs are crucial for high-quality form
field detection, and that the cleaning process improves data efficiency over
using all PDFs that have fillable fields in Common Crawl. A qualitative
analysis shows that they outperform a popular, commercially available PDF
reader that can prepare forms. Unlike the most popular commercially available
solutions, FFDNet can predict checkboxes in addition to text and signature
fields. This is, to our knowledge, the first large scale dataset released for
form field detection, as well as the first open source models. The dataset,
models, and code will be released at https://github.com/jbarrow/commonforms
\\ ( https://arxiv.org/abs/2509.16506 ,  11421kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16507
Date: Sat, 20 Sep 2025 03:04:41 GMT   (1104kb)

Title: OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed
  Real-world Video Super-Resolution
Authors: Hanting Li, Huaao Tang, Jianhong Han, Tianxiong Zhou, Jiulong Cui,
  Haizhen Xie, Yan Chen, Jie Hu
Categories: cs.CV
\\
  Recently, latent diffusion models has demonstrated promising performance in
real-world video super-resolution (VSR) task, which can reconstruct
high-quality videos from distorted low-resolution input through multiple
diffusion steps. Compared to image super-resolution (ISR), VSR methods needs to
process each frame in a video, which poses challenges to its inference
efficiency. However, video quality and inference efficiency have always been a
trade-off for the diffusion-based VSR methods. In this work, we propose
One-Step Diffusion model for real-world Video Super-Resolution, namely
OS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training
paradigm, which can significantly improve the quality of synthetic videos.
Besides, we devise a multi-frame fusion mechanism to maintain inter-frame
temporal consistency and reduce the flicker in video. Extensive experiments on
several popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve
better quality than existing diffusion-based VSR methods that require dozens of
sampling steps.
\\ ( https://arxiv.org/abs/2509.16507 ,  1104kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16509
Date: Sat, 20 Sep 2025 03:09:06 GMT   (8269kb)

Title: SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive
  Imaging
Authors: Haijin Zeng, Xuan Lu, Yurong Zhang, Yongyong Chen, Jingyong Su, Jie
  Liu
Categories: cs.CV
Comments: 12 pages
\\
  Humans learn in two complementary ways: a slow, cumulative process that
builds broad, general knowledge, and a fast, on-the-fly process that captures
specific experiences. Existing deep-unfolding methods for spectral compressive
imaging (SCI) mirror only the slow component-relying on heavy pre-training with
many unfolding stages-yet they lack the rapid adaptation needed to handle new
optical configurations. As a result, they falter on out-of-distribution
cameras, especially in bespoke spectral setups unseen during training. This
depth also incurs heavy computation and slow inference. To bridge this gap, we
introduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any
deep unfolding network beyond SCI systems. During slow learning, we pre-train
or reuse a priors-based backbone and distill it via imaging guidance into a
compact fast-unfolding model. In the fast learning stage, lightweight
adaptation modules are embedded within each block and trained self-supervised
at test time via a dual-domain loss-without retraining the backbone. To the
best of our knowledge, SlowFast-SCI is the first test-time adaptation-driven
deep unfolding framework for efficient, self-adaptive spectral reconstruction.
Its dual-stage design unites offline robustness with on-the-fly per-sample
calibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB
PSNR improvement on out-of-distribution data, preserved cross-domain
adaptability, and a 4x faster adaptation speed. In addition, its modularity
integrates with any deep-unfolding network, paving the way for self-adaptive,
field-deployable imaging and expanded computational imaging modalities. Code
and models are available at https://github.com/XuanLu11/SlowFast-SCI.
\\ ( https://arxiv.org/abs/2509.16509 ,  8269kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16517
Date: Sat, 20 Sep 2025 03:47:49 GMT   (12079kb)

Title: Seeing Culture: A Benchmark for Visual Reasoning and Grounding
Authors: Burak Satar, Zhixin Ma, Patrick A. Irawan, Wilfried A. Mulyawan, Jing
  Jiang, Ee-Peng Lim, Chong-Wah Ngo
Categories: cs.CV cs.AI cs.CL cs.MM
Comments: Accepted to EMNLP 2025 Main Conference,
  https://seeingculture-benchmark.github.io/
\\
  Multimodal vision-language models (VLMs) have made substantial progress in
various tasks that require a combined understanding of visual and textual
content, particularly in cultural understanding tasks, with the emergence of
new cultural datasets. However, these datasets frequently fall short of
providing cultural reasoning while underrepresenting many cultures. In this
paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural
reasoning with a novel approach that requires VLMs to reason on culturally rich
images in two stages: i) selecting the correct visual option with
multiple-choice visual question answering (VQA), and ii) segmenting the
relevant cultural artifact as evidence of reasoning. Visual options in the
first stage are systematically organized into three types: those originating
from the same country, those from different countries, or a mixed group.
Notably, all options are derived from a singular category for each type.
Progression to the second stage occurs only after a correct visual option is
chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural
artifacts across five categories from seven Southeast Asia countries, whose
diverse cultures are often overlooked, accompanied by 3,178 questions, of which
1,093 are unique and meticulously curated by human annotators. Our evaluation
of various VLMs reveals the complexities involved in cross-modal cultural
reasoning and highlights the disparity between visual reasoning and spatial
grounding in culturally nuanced scenarios. The SCB serves as a crucial
benchmark for identifying these shortcomings, thereby guiding future
developments in the field of cultural reasoning.
https://github.com/buraksatar/SeeingCulture
\\ ( https://arxiv.org/abs/2509.16517 ,  12079kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16518
Date: Sat, 20 Sep 2025 03:48:32 GMT   (7306kb)

Title: FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers
Authors: Sankeerth Durvasula, Kavya Sreedhar, Zain Moustafa, Suraj Kothawade,
  Ashish Gondimalla, Suvinay Subramanian, Narges Shahidi, Nandita Vijaykumar
Categories: cs.CV cs.AR
\\
  Generating realistic videos with diffusion transformers demands significant
computation, with attention layers the central bottleneck; even producing a
short clip requires running a transformer over a very long sequence of
embeddings, e.g., more than 30K embeddings for a 5-second video, incurring
significant latency. Prior work aims to mitigate this bottleneck by exploiting
sparsity in the attention layers to reduce computation. However, these works
typically rely on block-sparse attention, which skips score computation only
when all entries in a block of attention scores (corresponding to M queries and
M keys, with M = 64 typically) are zero. This coarse-granular skipping of
attention scores does not fully exploit sparsity in the attention map and
leaves room for improvement. In this work, we propose FG-Attn, a sparse
attention mechanism for long-context diffusion transformers that leverages
sparsity at a fine granularity. Unlike block-sparse attention, which skips
entire MxM blocks, our approach skips computations at the granularity of Mx1
slices of the attention map. Each slice is produced by query-key dot products
between a block of query vectors and a single key. To implement our proposed
sparse attention mechanism, we develop a new efficient bulk-load operation
called asynchronous-gather load. This load operation gathers a sparse set of
relevant key-value vectors from memory and arranges them into packed tiles in
the GPU's shared memory. Only a sparse set of keys relevant to those queries
are loaded into shared memory when computing attention for a block of queries,
in contrast to loading full blocks of key tokens in block-sparse attention. Our
fine-grained sparse attention, applied to video diffusion models, achieves an
average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average
1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.
\\ ( https://arxiv.org/abs/2509.16518 ,  7306kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16519
Date: Sat, 20 Sep 2025 03:51:45 GMT   (1492kb)

Title: PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air
  Quality
Authors: Yang Han
Categories: cs.CV
\\
  We introduce PM25Vision (PM25V), the largest and most comprehensive dataset
to date for estimating air quality - specifically PM2.5 concentrations - from
street-level images. The dataset contains over 11,114 images matched with
timestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations
and 11 years, significantly exceeding the scale of previous benchmarks. The
spatial accuracy of this dataset has reached 5 kilometers, far exceeding the
city-level accuracy of many datasets. We describe the data collection,
synchronization, and cleaning pipelines, and provide baseline model
performances using CNN and transformer architectures. Our dataset is publicly
available.
\\ ( https://arxiv.org/abs/2509.16519 ,  1492kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16527
Date: Sat, 20 Sep 2025 04:25:27 GMT   (2338kb)

Title: Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity
Authors: Guangze Zheng, Shijie Lin, Haobo Zuo, Si Si, Ming-Shan Wang, Changhong
  Fu, Jia Pan
Categories: cs.CV cs.AI
Comments: NeurIPS 2025. Project page: https://george-zhuang.github.io/lbm/
\\
  This work proposes the Lattice Boltzmann Model (LBM) to learn real-world
pixel dynamicity for visual tracking. LBM decomposes visual representations
into dynamic pixel lattices and solves pixel motion states through
collision-streaming processes. Specifically, the high-dimensional distribution
of the target pixels is acquired through a multilayer predict-update network to
estimate the pixel positions and visibility. The predict stage formulates
lattice collisions among the spatial neighborhood of target pixels and develops
lattice streaming within the temporal visual context. The update stage
rectifies the pixel distributions with online visual representations. Compared
with existing methods, LBM demonstrates practical applicability in an online
and real-time manner, which can efficiently adapt to real-world visual tracking
tasks. Comprehensive evaluations of real-world point tracking benchmarks such
as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of
large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B
further demonstrates LBM's real-world practicality.
\\ ( https://arxiv.org/abs/2509.16527 ,  2338kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16538
Date: Sat, 20 Sep 2025 05:04:41 GMT   (1105kb)

Title: Advancing Reference-free Evaluation of Video Captions with Factual
  Analysis
Authors: Shubhashis Roy Dipta, Tz-Ying Wu, Subarna Tripathi
Categories: cs.CV cs.CL
\\
  Video captions offer concise snapshots of actors, objects, and actions within
a video, serving as valuable assets for applications such as question answering
and event localization. However, acquiring human annotations for video captions
is costly or even impractical, especially when dealing with diverse video
domains. Existing models trained on supervised datasets face challenges in
evaluating performance across different domains due to the reliance on
reference-based evaluation protocols, which necessitate ground truth captions.
This assumption is unrealistic for evaluating videos in the wild. To address
these limitations, we propose a reference-free evaluation framework that does
not require ground truth captions, focusing on factual grounding to ensure
accurate assessment of caption quality. We introduce VC-Inspector, a novel
caption quality evaluator that is both reference-free and factually grounded.
Utilizing large language models, we generate pseudo captions of varying quality
based on supervised data, which are subsequently used to train a multimodal
model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior
alignment with human judgments on the VATEX-Eval dataset, outperforming
existing methods. The performance also generalizes to image caption datasets,
Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.
Overall, VC-Inspector offers a scalable and generalizable solution for
evaluating the factual accuracy of video captions, paving the way for more
effective and objective assessment methodologies in diverse video domains.
\\ ( https://arxiv.org/abs/2509.16538 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16549
Date: Sat, 20 Sep 2025 06:21:00 GMT   (2579kb)

Title: Efficient Rectified Flow for Image Fusion
Authors: Zirui Wang, Jiayi Zhang, Tianwei Guan, Yuhan Zhou, Xingyuan Li,
  Minjing Dong, Jinyuan Liu
Categories: cs.CV
\\
  Image fusion is a fundamental and important task in computer vision, aiming
to combine complementary information from different modalities to fuse images.
In recent years, diffusion models have made significant developments in the
field of image fusion. However, diffusion models often require complex
computations and redundant inference time, which reduces the applicability of
these methods. To address this issue, we propose RFfusion, an efficient
one-step diffusion model for image fusion based on Rectified Flow. We
incorporate Rectified Flow into the image fusion task to straighten the
sampling path in the diffusion model, achieving one-step sampling without the
need for additional training, while still maintaining high-quality fusion
results. Furthermore, we propose a task-specific variational autoencoder (VAE)
architecture tailored for image fusion, where the fusion operation is embedded
within the latent space to further reduce computational complexity. To address
the inherent discrepancy between conventional reconstruction-oriented VAE
objectives and the requirements of image fusion, we introduce a two-stage
training strategy. This approach facilitates the effective learning and
integration of complementary information from multi-modal source images,
thereby enabling the model to retain fine-grained structural details while
significantly enhancing inference efficiency. Extensive experiments demonstrate
that our method outperforms other state-of-the-art methods in terms of both
inference speed and fusion quality. Code is available at
https://github.com/zirui0625/RFfusion.
\\ ( https://arxiv.org/abs/2509.16549 ,  2579kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16552
Date: Sat, 20 Sep 2025 06:36:30 GMT   (6109kb)

Title: ST-GS: Vision-Based 3D Semantic Occupancy Prediction with
  Spatial-Temporal Gaussian Splatting
Authors: Xiaoyang Yan, Muleilan Pei, Shaojie Shen
Categories: cs.CV cs.RO
\\
  3D occupancy prediction is critical for comprehensive scene understanding in
vision-centric autonomous driving. Recent advances have explored utilizing 3D
semantic Gaussians to model occupancy while reducing computational overhead,
but they remain constrained by insufficient multi-view spatial interaction and
limited multi-frame temporal consistency. To overcome these issues, in this
paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework
to enhance both spatial and temporal modeling in existing Gaussian-based
pipelines. Specifically, we develop a guidance-informed spatial aggregation
strategy within a dual-mode attention mechanism to strengthen spatial
interaction in Gaussian representations. Furthermore, we introduce a
geometry-aware temporal fusion scheme that effectively leverages historical
context to improve temporal continuity in scene completion. Extensive
experiments on the large-scale nuScenes occupancy prediction benchmark showcase
that our proposed approach not only achieves state-of-the-art performance but
also delivers markedly better temporal consistency compared to existing
Gaussian-based methods.
\\ ( https://arxiv.org/abs/2509.16552 ,  6109kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16557
Date: Sat, 20 Sep 2025 07:27:32 GMT   (1883kb)

Title: Person Identification from Egocentric Human-Object Interactions using 3D
  Hand Pose
Authors: Muhammad Hamza, Danish Hamid, and Muhammad Tahir Akram
Categories: cs.CV cs.ET cs.HC cs.LG
Comments: 21 pages, 8 figures, 7 tables. Preprint of a manuscript submitted to
  CCF Transactions on Pervasive Computing and Interaction (Springer), currently
  under review
\\
  Human-Object Interaction Recognition (HOIR) and user identification play a
crucial role in advancing augmented reality (AR)-based personalized assistive
technologies. These systems are increasingly being deployed in high-stakes,
human-centric environments such as aircraft cockpits, aerospace maintenance,
and surgical procedures. This research introduces I2S (Interact2Sign), a multi
stage framework designed for unobtrusive user identification through human
object interaction recognition, leveraging 3D hand pose analysis in egocentric
videos. I2S utilizes handcrafted features extracted from 3D hand poses and per
forms sequential feature augmentation: first identifying the object class,
followed by HOI recognition, and ultimately, user identification. A
comprehensive feature extraction and description process was carried out for 3D
hand poses, organizing the extracted features into semantically meaningful
categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor
introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive
ablation studies were conducted to determine the most effective combination of
features. The optimal configuration achieved an impressive average F1-score of
97.52% for user identification, evaluated on a bimanual object manipulation
dataset derived from the ARCTIC and H2O datasets. I2S demonstrates
state-of-the-art performance while maintaining a lightweight model size of
under 4 MB and a fast inference time of 0.1 seconds. These characteristics make
the proposed framework highly suitable for real-time, on-device authentication
in security-critical, AR-based systems.
\\ ( https://arxiv.org/abs/2509.16557 ,  1883kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16560
Date: Sat, 20 Sep 2025 07:36:53 GMT   (7333kb)

Title: Captioning for Text-Video Retrieval via Dual-Group Direct Preference
  Optimization
Authors: Ji Soo Lee, Byungoh Ko, Jaewon Cho, Howoong Lee, Jaewoon Byun, Hyunwoo
  J. Kim
Categories: cs.CV
Comments: EMNLP 2025 Findings
\\
  In text-video retrieval, auxiliary captions are often used to enhance video
understanding, bridging the gap between the modalities. While recent advances
in multi-modal large language models (MLLMs) have enabled strong zero-shot
caption generation, we observe that such captions tend to be generic and
indistinguishable across visually similar videos, limiting their utility for
fine-grained retrieval. Moreover, conventional captioning approaches are
typically evaluated using language generation metrics, such as BLEU, which are
not typically tailored for retrieval tasks that require making discriminative
distinctions between candidates. To address this, we propose
$\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption
generation using retrieval relevance scores. At its core is Dual-Group Direct
Preference Optimization (DG-DPO), a novel learning strategy that supervises
captioning by modeling preferences across groups of distinct video and caption
pairs. In addition, we present an MLLM-based retrieval model that incorporates
role-embeddings to better distinguish between textual inputs with different
functional roles, such as an auxiliary caption and a text query. Through
extensive experiments, we demonstrate that CaRe-DPO significantly enhances
retrieval performance by effectively leveraging auxiliary knowledge to generate
fine-grained captions for retrieval. Code is available at
https://github.com/mlvlab/CaReDPO.
\\ ( https://arxiv.org/abs/2509.16560 ,  7333kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16567
Date: Sat, 20 Sep 2025 07:53:06 GMT   (3521kb)

Title: V-CECE: Visual Counterfactual Explanations via Conceptual Edits
Authors: Nikolaos Spanos, Maria Lymperaiou, Giorgos Filandrianos, Konstantinos
  Thomas, Athanasios Voulodimos, Giorgos Stamou
Categories: cs.CV cs.AI
Comments: Accepted in NeurIPS 2025
\\
  Recent black-box counterfactual generation frameworks fail to take into
account the semantic content of the proposed edits, while relying heavily on
training to guide the generation process. We propose a novel, plug-and-play
black-box counterfactual generation framework, which suggests step-by-step
edits based on theoretical guarantees of optimal edits to produce human-level
counterfactual explanations with zero training. Our framework utilizes a
pre-trained image editing diffusion model, and operates without access to the
internals of the classifier, leading to an explainable counterfactual
generation process. Throughout our experimentation, we showcase the explanatory
gap between human reasoning and neural model behavior by utilizing both
Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision
Language Model (LVLM) classifiers, substantiated through a comprehensive human
evaluation.
\\ ( https://arxiv.org/abs/2509.16567 ,  3521kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16582
Date: Sat, 20 Sep 2025 09:08:08 GMT   (735kb)

Title: A Novel Metric for Detecting Memorization in Generative Models for Brain
  MRI Synthesis
Authors: Antonio Scardace, Lemuel Puglisi, Francesco Guarnera, Sebastiano
  Battiato, Daniele Rav\`i
Categories: cs.CV cs.LG
\\
  Deep generative models have emerged as a transformative tool in medical
imaging, offering substantial potential for synthetic data generation. However,
recent empirical studies highlight a critical vulnerability: these models can
memorize sensitive training data, posing significant risks of unauthorized
patient information disclosure. Detecting memorization in generative models
remains particularly challenging, necessitating scalable methods capable of
identifying training data leakage across large sets of generated samples. In
this work, we propose DeepSSIM, a novel self-supervised metric for quantifying
memorization in generative models. DeepSSIM is trained to: i) project images
into a learned embedding space and ii) force the cosine similarity between
embeddings to match the ground-truth SSIM (Structural Similarity Index) scores
computed in the image space. To capture domain-specific anatomical features,
training incorporates structure-preserving augmentations, allowing DeepSSIM to
estimate similarity reliably without requiring precise spatial alignment. We
evaluate DeepSSIM in a case study involving synthetic brain MRI data generated
by a Latent Diffusion Model (LDM) trained under memorization-prone conditions,
using 2,195 MRI scans from two publicly available datasets (IXI and CoRR).
Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior
performance, improving F1 scores by an average of +52.03% over the best
existing method. Code and data of our approach are publicly available at the
following link: https://github.com/brAIn-science/DeepSSIM.
\\ ( https://arxiv.org/abs/2509.16582 ,  735kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16588
Date: Sat, 20 Sep 2025 09:25:19 GMT   (1842kb)

Title: SQS: Enhancing Sparse Perception Models via Query-based Splatting in
  Autonomous Driving
Authors: Haiming Zhang, Yiyao Zhu, Wending Zhou, Xu Yan, Yingjie Cai, Bingbing
  Liu, Shuguang Cui, Zhen Li
Categories: cs.CV cs.AI cs.RO
Comments: NeurIPS 2025 (Spotlight)
\\
  Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes
explicit dense BEV or volumetric construction, enabling highly efficient
computation and accelerated inference. In this paper, we introduce SQS, a novel
query-based splatting pre-training specifically designed to advance SPMs in
autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian
representations from sparse queries during pre-training, leveraging
self-supervised splatting to learn fine-grained contextual features through the
reconstruction of multi-view images and depth maps. During fine-tuning, the
pre-trained Gaussian queries are seamlessly integrated into downstream networks
via query interaction mechanisms that explicitly connect pre-trained queries
with task-specific queries, effectively accommodating the diverse requirements
of occupancy prediction and 3D object detection. Extensive experiments on
autonomous driving benchmarks demonstrate that SQS delivers considerable
performance gains across multiple query-based 3D perception tasks, notably in
occupancy prediction and 3D object detection, outperforming prior
state-of-the-art pre-training approaches by a significant margin (i.e., +1.3
mIoU on occupancy prediction and +1.0 NDS on 3D detection).
\\ ( https://arxiv.org/abs/2509.16588 ,  1842kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16602
Date: Sat, 20 Sep 2025 09:53:50 GMT   (6908kb)

Title: FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection
Authors: Minji Heo, Simon S. Woo
Categories: cs.CV cs.AI
DOI: 10.1145/3746252.3761345
\\
  Multi-step or hybrid deepfakes, created by sequentially applying different
deepfake creation methods such as Face-Swapping, GAN-based generation, and
Diffusion methods, can pose an emerging and unforseen technical challenge for
detection models trained on single-step forgeries. While prior studies have
mainly focused on detecting isolated single manipulation, little is known about
the detection model behavior under such compositional, hybrid, and complex
manipulation pipelines. In this work, we introduce \textbf{FakeChain}, a
large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using
five state-of-the-art representative generators. Using this approach, we
analyze detection performance and spectral properties across hybrid
manipulation at different step, along with varying generator combinations and
quality settings. Surprisingly, our findings reveal that detection performance
highly depends on the final manipulation type, with F1-score dropping by up to
\textbf{58.83\%} when it differs from training distribution. This clearly
demonstrates that detectors rely on last-stage artifacts rather than cumulative
manipulation traces, limiting generalization. Such findings highlight the need
for detection models to explicitly consider manipulation history and sequences.
Our results highlight the importance of benchmarks such as FakeChain,
reflecting growing synthesis complexity and diversity in real-world scenarios.
Our sample code is available
here\footnote{https://github.com/minjihh/FakeChain}.
\\ ( https://arxiv.org/abs/2509.16602 ,  6908kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16609
Date: Sat, 20 Sep 2025 10:17:25 GMT   (2151kb)

Title: Describe-to-Score: Text-Guided Efficient Image Complexity Assessment
Authors: Shipeng Liu, Zhonglin Zhang, Dengfeng Chen, Liang Zhao
Categories: cs.CV
\\
  Accurately assessing image complexity (IC) is critical for computer vision,
yet most existing methods rely solely on visual features and often neglect
high-level semantic information, limiting their accuracy and generalization. We
introduce vision-text fusion for IC modeling. This approach integrates visual
and textual semantic features, increasing representational diversity. It also
reduces the complexity of the hypothesis space, which enhances both accuracy
and generalization in complexity assessment. We propose the D2S
(Describe-to-Score) framework, which generates image captions with a
pre-trained vision-language model. We propose the feature alignment and entropy
distribution alignment mechanisms, D2S guides semantic information to inform
complexity assessment while bridging the gap between vision and text
modalities. D2S utilizes multi-modal information during training but requires
only the vision branch during inference, thereby avoiding multi-modal
computational overhead and enabling efficient assessment. Experimental results
demonstrate that D2S outperforms existing methods on the IC9600 dataset and
maintains competitiveness on no-reference image quality assessment (NR-IQA)
benchmark, validating the effectiveness and efficiency of multi-modal fusion in
complexity-related tasks. Code is available at:
https://github.com/xauat-liushipeng/D2S
\\ ( https://arxiv.org/abs/2509.16609 ,  2151kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16617
Date: Sat, 20 Sep 2025 10:41:33 GMT   (8127kb)

Title: Detection and Simulation of Urban Heat Islands Using a Fine-Tuned
  Geospatial Foundation Model
Authors: David Kreismann
Categories: cs.CV cs.AI
Comments: 12 pages, 4 figures, to appear in GI LNI (SKILL 2025)
ACM-class: I.2.6; I.5.4; I.6.8
\\
  As urbanization and climate change progress, urban heat island effects are
becoming more frequent and severe. To formulate effective mitigation plans,
cities require detailed air temperature data. However, predictive analytics
methods based on conventional machine learning models and limited data
infrastructure often provide inaccurate predictions, especially in underserved
areas. In this context, geospatial foundation models trained on unstructured
global data demonstrate strong generalization and require minimal fine-tuning,
offering an alternative for predictions where traditional approaches are
limited. This study fine-tunes a geospatial foundation model to predict urban
land surface temperatures under future climate scenarios and explores its
response to land cover changes using simulated vegetation strategies. The
fine-tuned model achieved pixel-wise downscaling errors below 1.74 {\deg}C and
aligned with ground truth patterns, demonstrating an extrapolation capacity up
to 3.62 {\deg}C.
\\ ( https://arxiv.org/abs/2509.16617 ,  8127kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16618
Date: Sat, 20 Sep 2025 10:42:29 GMT   (6826kb)

Title: Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for
  VQLA in Robotic Surgery
Authors: Pengfei Hao, Hongqiu Wang, Shuaibo Li, Zhaohu Xing, Guang Yang,
  Kaishun Wu, and Lei Zhu
Categories: cs.CV cs.AI
Comments: Early accepted by MICCAI2025
\\
  In recent years, Visual Question Localized-Answering in robotic surgery
(Surgical-VQLA) has gained significant attention for its potential to assist
medical students and junior doctors in understanding surgical scenes. Recently,
the rapid development of Large Language Models (LLMs) has provided more
promising solutions for this task. However, current methods struggle to
establish complex dependencies between text and visual details, and have
difficulty perceiving the spatial information of surgical scenes. To address
these challenges, we propose a novel method, Surgical-MambaLLM, which is the
first to combine Mamba2 with LLM in the surgical domain, that leverages
Mamba2's ability to effectively capture cross-modal dependencies and perceive
spatial information in surgical scenes, thereby enhancing the LLMs'
understanding of surgical images. Specifically, we propose the Cross-modal
Bidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective
multimodal fusion, with its cross-modal integration capabilities. Additionally,
tailored to the geometric characteristics of surgical scenes, we design the
Surgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the
surgical images, enhancing the model's spatial understanding of the surgical
scene. Extensive experiments demonstrate that our Surgical-MambaLLM model
outperforms the state-of-the-art methods on the EndoVis17-VQLA and
EndoVis18-VQLA datasets, significantly improving the performance of the
Surgical-VQLA task.
\\ ( https://arxiv.org/abs/2509.16618 ,  6826kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16623
Date: Sat, 20 Sep 2025 10:48:51 GMT   (329kb)

Title: CGTGait: Collaborative Graph and Transformer for Gait Emotion
  Recognition
Authors: Junjie Zhou, Haijun Xiong, Junhao Lu, Ziyu Lin, Bin Feng
Categories: cs.CV
Comments: Accepted by IJCB2025
\\
  Skeleton-based gait emotion recognition has received significant attention
due to its wide-ranging applications. However, existing methods primarily focus
on extracting spatial and local temporal motion information, failing to capture
long-range temporal representations. In this paper, we propose
\textbf{CGTGait}, a novel framework that collaboratively integrates graph
convolution and transformers to extract discriminative spatiotemporal features
for gait emotion recognition. Specifically, CGTGait consists of multiple CGT
blocks, where each block employs graph convolution to capture frame-level
spatial topology and the transformer to model global temporal dependencies.
Additionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to
effectively aggregate posture and motion spatiotemporal features, facilitating
the exchange of complementary information between the two streams. We evaluate
our method on two widely used datasets, Emotion-Gait and ELMD, demonstrating
that our CGTGait achieves state-of-the-art or at least competitive performance
while reducing computational complexity by approximately \textbf{82.2\%} (only
requiring 0.34G FLOPs) during testing. Code is available at
\small{https://github.com/githubzjj1/CGTGait.}
\\ ( https://arxiv.org/abs/2509.16623 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16628
Date: Sat, 20 Sep 2025 11:07:36 GMT   (2628kb)

Title: Enhancing Scientific Visual Question Answering via Vision-Caption aware
  Supervised Fine-Tuning
Authors: Janak Kapuriya, Anwar Shaikh, Arnav Goel, Medha Hira, Apoorv Singh,
  Jay Saraf, Sanjana, Vaibhav Nauriyal, Avinash Anand, Zhengkui Wang, Rajiv
  Ratn Shah
Categories: cs.CV
\\
  In this study, we introduce Vision-Caption aware Supervised FineTuning
(VCASFT), a novel learning paradigm designed to enhance the performance of
smaller Vision Language Models(VLMs) on scientific visual question
answering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts
alongside question-answer pairs and instruction-tunes models to yield
significant performance improvements. To comprehensively evaluate VCASFT, we
benchmark it on ScienceQA, which consists of questions across diverse
languages, subjects, and fields, demonstrating its adaptability and
effectiveness in a variety of educational contexts. Additionally, to further
demonstrate the effectiveness of this technique on lowresource languages, we
developed HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated
Hindi multimodal Q&A pairs. This dataset addresses the critical need for
low-resource language Q&A datasets and serves as a foundation for testing
VCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to
evaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness
surpassing traditional n-gram matching accuracy metrics. We are committed to
advancing the field by open-sourcing all code files and the HiSciVQA dataset
for the research community.
\\ ( https://arxiv.org/abs/2509.16628 ,  2628kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16630
Date: Sat, 20 Sep 2025 11:09:01 GMT   (25834kb)

Title: Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and
  Expressive Freestyle Portrait Animation
Authors: Yue Ma, Zexuan Yan, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He,
  Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Zhifeng Li, Wei
  Liu, Linfeng Zhang, Qifeng Chen
Categories: cs.CV
Comments: accepted by IJCV2025. project
  page:https://follow-your-emoji.github.io
\\
  We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework
for freestyle portrait animation driven by facial landmarks. The main
challenges in this task are preserving the identity of the reference portrait,
accurately transferring target expressions, and maintaining long-term temporal
consistency while ensuring generation efficiency. To address identity
preservation and accurate expression retargeting, we enhance Stable Diffusion
with two key components: a expression-aware landmarks as explicit motion
signals, which improve motion alignment, support exaggerated expressions, and
reduce identity leakage; and a fine-grained facial loss that leverages both
expression and facial masks to better capture subtle expressions and faithfully
preserve the reference appearance. With these components, our model supports
controllable and expressive animation across diverse portrait types, including
real faces, cartoons, sculptures, and animals. However, diffusion-based
frameworks typically struggle to efficiently generate long-term stable
animation results, which remains a core challenge in this task. To address
this, we propose a progressive generation strategy for stable long-term
animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless
acceleration. These two strategies ensure that our method produces high-quality
results efficiently, making it user-friendly and accessible. Finally, we
introduce EmojiBench++, a more comprehensive benchmark comprising diverse
portraits, driving videos, and landmark sequences. Extensive evaluations on
EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior
performance in both animation quality and controllability. The code, training
dataset and benchmark will be found in https://follow-your-emoji.github.io/.
\\ ( https://arxiv.org/abs/2509.16630 ,  25834kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16632
Date: Sat, 20 Sep 2025 11:12:15 GMT   (6635kb)

Title: DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration
Authors: Weiran Chen, Guiqian Zhu, Ying Li, Yi Ji, Chunping Liu
Categories: cs.CV
Comments: Accepted by ACM MM 2025
\\
  Few-shot font generation aims to create new fonts with a limited number of
glyph references. It can be used to significantly reduce the labor cost of
manual font design. However, due to the variety and complexity of font styles,
the results generated by existing methods often suffer from visible defects,
such as stroke errors, artifacts and blurriness. To address these issues, we
propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid
Module (DAHM). Specifically, we introduce two synergistic attention blocks: the
component attention block that leverages component information from content
images to guide the style transfer process, and the relation attention block
that further refines spatial relationships through interacting the content
feature with both original and stylized component-wise representations. These
two blocks collaborate to preserve accurate character shapes and stylistic
textures. Moreover, we also design a corner consistency loss and an elastic
mesh feature loss to better improve geometric alignment. Extensive experiments
show that our DA-Font outperforms the state-of-the-art methods across diverse
font styles and characters, demonstrating its effectiveness in enhancing
structural integrity and local fidelity. The source code can be found at
\href{https://github.com/wrchen2001/DA-Font}{\textit{https://github.com/wrchen2001/DA-Font}}.
\\ ( https://arxiv.org/abs/2509.16632 ,  6635kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16633
Date: Sat, 20 Sep 2025 11:12:23 GMT   (2847kb)

Title: When Big Models Train Small Ones: Label-Free Model Parity Alignment for
  Efficient Visual Question Answering using Small VLMs
Authors: Abhirama Subramanyam Penamakuri, Navlika Singh, Piyush Arora, Anand
  Mishra
Categories: cs.CV cs.AI cs.CL
Comments: Accepted to EMNLP (Main) 2025
\\
  Large Vision-Language Models (L-VLMs) have demonstrated remarkable
performance in various vision and language tasks, including visual question
answering (VQA). However, their high computational cost makes them impractical
for resource-constrained settings and inference-heavy applications. In
contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer
from a significant performance gap compared to their larger counterparts. In
this work, we introduce the Model Parity Aligner (MPA), a novel framework
designed to systematically improve S-VLMs by leveraging unlabeled images and
effective knowledge transfer from L-VLMs. Instead of traditional knowledge
distillation methods that rely on labeled training data, MPA employs a
strategic parity-based approach that precisely identifies the knowledge
disparities between S-VLMs and L-VLMs, and optimizes training by targeting only
these disparities. We conduct extensive experiments on four diverse VQA
benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires
specialized reasoning capabilities such as text recognition, chart
interpretation, and commonsense and factual understanding. Our results
demonstrate that MPA consistently enhances the performance of S-VLMs on all
benchmarks, reducing the performance gap while maintaining computational
efficiency. We make our code publicly available.
\\ ( https://arxiv.org/abs/2509.16633 ,  2847kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16635
Date: Sat, 20 Sep 2025 11:20:22 GMT   (1835kb)

Title: Towards Anytime Retrieval: A Benchmark for Anytime Person
  Re-Identification
Authors: Xulin Li, Yan Lu, Bin Liu, Jiaze Li, Qinhong Yang, Tao Gong, Qi Chu,
  Mang Ye, Nenghai Yu
Categories: cs.CV
Comments: Accepted by IJCAI 2025
\\
  In real applications, person re-identification (ReID) is expected to retrieve
the target person at any time, including both daytime and nighttime, ranging
from short-term to long-term. However, existing ReID tasks and datasets can not
meet this requirement, as they are constrained by available time and only
provide training and evaluation for specific scenarios. Therefore, we
investigate a new task called Anytime Person Re-identification (AT-ReID), which
aims to achieve effective retrieval in multiple scenarios based on variations
in time. To address the AT-ReID problem, we collect the first large-scale
dataset, AT-USTC, which contains 403k images of individuals wearing multiple
clothes captured by RGB and IR cameras. Our data collection spans 21 months,
and 270 volunteers were photographed on average 29.1 times across different
dates or scenes, 4-15 times more than current datasets, providing conditions
for follow-up investigations in AT-ReID. Further, to tackle the new challenge
of multi-scenario retrieval, we propose a unified model named Uni-AT, which
comprises a multi-scenario ReID (MS-ReID) framework for scenario-specific
features learning, a Mixture-of-Attribute-Experts (MoAE) module to alleviate
inter-scenario interference, and a Hierarchical Dynamic Weighting (HDW)
strategy to ensure balanced training across all scenarios. Extensive
experiments show that our model leads to satisfactory results and exhibits
excellent generalization to all scenarios.
\\ ( https://arxiv.org/abs/2509.16635 ,  1835kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16639
Date: Sat, 20 Sep 2025 11:33:19 GMT   (803kb)

Title: Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided
  Grouping-Feature Coordination
Authors: Shangzhuo Xie, Qianqian Yang
Categories: cs.CV
Comments: This work has been submitted to the IEEE for possible publication
\\
  Point cloud analysis has evolved with diverse network architectures, while
existing works predominantly focus on introducing novel structural designs.
However, conventional point-based architectures - processing raw points through
sequential sampling, grouping, and feature extraction layers - demonstrate
underutilized potential. We notice that substantial performance gains can be
unlocked through strategic module integration rather than structural
modifications. In this paper, we propose the Grouping-Feature Coordination
Module (GF-Core), a lightweight separable component that simultaneously
regulates both grouping layer and feature extraction layer to enable more
nuanced feature aggregation. Besides, we introduce a self-supervised
pretraining strategy specifically tailored for point-based inputs to enhance
model robustness in complex point cloud analysis scenarios. On ModelNet40
dataset, our method elevates baseline networks to 94.0% accuracy, matching
advanced frameworks' performance while preserving architectural simplicity. On
three variants of the ScanObjectNN dataset, we obtain improvements of 2.96%,
6.34%, and 6.32% respectively.
\\ ( https://arxiv.org/abs/2509.16639 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16645
Date: Sat, 20 Sep 2025 11:48:11 GMT   (7653kb)

Title: ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents
Authors: Yichen Wang, Hangtao Zhang, Hewen Pan, Ziqi Zhou, Xianlong Wang,
  Peijin Guo, Lulu Xue, Shengshan Hu, Minghui Li, Leo Yu Zhang
Categories: cs.CV
\\
  Vision-Language Models (VLMs), with their strong reasoning and planning
capabilities, are widely used in embodied decision-making (EDM) tasks in
embodied agents, such as autonomous driving and robotic manipulation. Recent
research has increasingly explored adversarial attacks on VLMs to reveal their
vulnerabilities. However, these attacks either rely on overly strong
assumptions, requiring full knowledge of the victim VLM, which is impractical
for attacking VLM-based agents, or exhibit limited effectiveness. The latter
stems from disrupting most semantic information in the image, which leads to a
misalignment between the perception and the task context defined by system
prompts. This inconsistency interrupts the VLM's reasoning process, resulting
in invalid outputs that fail to affect interactions in the physical world. To
this end, we propose a fine-grained adversarial attack framework, ADVEDM, which
modifies the VLM's perception of only a few key objects while preserving the
semantics of the remaining regions. This attack effectively reduces conflicts
with the task context, making VLMs output valid but incorrect decisions and
affecting the actions of agents, thus posing a more substantial safety threat
in the physical world. We design two variants of based on this framework,
ADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific
object from the image and add the semantics of a new object into the image. The
experimental results in both general scenarios and EDM tasks demonstrate
fine-grained control and excellent attack performance.
\\ ( https://arxiv.org/abs/2509.16645 ,  7653kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16654
Date: Sat, 20 Sep 2025 12:02:39 GMT   (1945kb)

Title: Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?
Authors: Xin Chen (1), Jia He (1), Maozheng Li (1), Dongliang Xu (1), Tianyu
  Wang (2), Yixiao Chen (3), Zhixin Lin (1), Yue Yao (1) ((1) Shandong
  University, (2) MBZUAI, (3) Sems)
Categories: cs.CV
Comments: 5 pages, 5 figures
\\
  Vision-Language Models (VLMs) have recently shown remarkable progress in
multimodal reasoning, yet their applications in autonomous driving remain
limited. In particular, the ability to understand road topology, a key
requirement for safe navigation, has received relatively little attention.
While some recent works have begun to explore VLMs in driving contexts, their
performance on topology reasoning is far from satisfactory. In this work, we
systematically evaluate VLMs' capabilities in road topology understanding.
Specifically, multi-view images are projected into unified ground-plane
coordinate system and fused into bird's-eye-view (BEV) lanes. Based on these
BEV lanes, we formulate four topology-related diagnostic VQA tasks, which
together capture essential components of spatial topology reasoning. Through
extensive evaluation, we find that while frontier closed-source models (e.g.,
GPT-4o) achieve relatively high accuracy in some tasks, they still fail in some
temporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in
vector, a two-class classification problem). Furthermore, we find open-source
VLMs, even at 30B scale, struggle significantly. These results indicate that
spatial reasoning remains a fundamental bottleneck for current VLMs. We also
find that the model's capability is positively correlated with model size,
length of reasoning tokens and shots provided as examples, showing direction
for future research.
\\ ( https://arxiv.org/abs/2509.16654 ,  1945kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16673
Date: Sat, 20 Sep 2025 12:51:14 GMT   (1241kb)

Title: MedCutMix: A Data-Centric Approach to Improve Radiology Vision-Language
  Pre-training with Disease Awareness
Authors: Sinuo Wang, Yutong Xie, Yuyuan Liu, Qi Wu
Categories: cs.CV
\\
  Vision-Language Pre-training (VLP) is drawing increasing interest for its
ability to minimize manual annotation requirements while enhancing semantic
understanding in downstream tasks. However, its reliance on image-text datasets
poses challenges due to privacy concerns and the high cost of obtaining paired
annotations. Data augmentation emerges as a viable strategy to address this
issue, yet existing methods often fall short of capturing the subtle and
complex variations in medical data due to limited diversity. To this end, we
propose MedCutMix, a novel multi-modal disease-centric data augmentation
method. MedCutMix performs diagnostic sentence CutMix within medical reports
and establishes the cross-attention between the diagnostic sentence and medical
image to guide attentive manifold mix within the imaging modality. Our approach
surpasses previous methods across four downstream radiology diagnosis datasets,
highlighting its effectiveness in enhancing performance and generalizability in
radiology VLP.
\\ ( https://arxiv.org/abs/2509.16673 ,  1241kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16674
Date: Sat, 20 Sep 2025 12:55:18 GMT   (4892kb)

Title: FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian
  Retrieval in Open World
Authors: Zengli Luo, Canlong Zhang, Xiaochun Lu and Zhixin Li
Categories: cs.CV
Comments: 15pages,6 figures
\\
  Text-based Pedestrian Retrieval (TPR) aims to retrieve specific target
pedestrians in visual scenes according to natural language descriptions.
Although existing methods have achieved progress under constrained settings,
interactive retrieval in the open-world scenario still suffers from limited
model generalization and insufficient semantic understanding. To address these
challenges, we propose FitPro, an open-world interactive zero-shot TPR
framework with enhanced semantic comprehension and cross-scene adaptability.
FitPro has three innovative components: Feature Contrastive Decoding (FCD),
Incremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval
(QHR). The FCD integrates prompt-guided contrastive decoding to generate
high-quality structured pedestrian descriptions from denoised images,
effectively alleviating semantic drift in zero-shot scenarios. The ISM
constructs holistic pedestrian representations from multi-view observations to
achieve global semantic modeling in multi-turn interactions,thereby improving
robustness against viewpoint shifts and fine-grained variations in
descriptions. The QHR dynamically optimizes the retrieval pipeline according to
query types, enabling efficient adaptation to multi-modal and multi-view
inputs. Extensive experiments on five public datasets and two evaluation
protocols demonstrate that FitPro significantly overcomes the generalization
limitations and semantic modeling constraints of existing methods in
interactive retrieval, paving the way for practical deployment. The code and
data will be released at https://github.com/
lilo4096/FitPro-Interactive-Person-Retrieval.
\\ ( https://arxiv.org/abs/2509.16674 ,  4892kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16677
Date: Sat, 20 Sep 2025 13:03:43 GMT   (915kb)

Title: Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation
  Towards Embodied Intelligence
Authors: Wenxin Li, Kunyu Peng, Di Wen, Ruiping Liu, Mengfei Duan, Kai Luo,
  Kailun Yang
Categories: cs.CV cs.LG cs.RO eess.IV
Comments: The established benchmark and source code will be made publicly
  available at https://github.com/mylwx/ActiSeg-NL
\\
  Embodied intelligence relies on accurately segmenting objects actively
involved in interactions. Action-based video object segmentation addresses this
by linking segmentation with action semantics, but it depends on large-scale
annotations and prompts that are costly, inconsistent, and prone to multimodal
noise such as imprecise masks and referential ambiguity. To date, this
challenge remains unexplored. In this work, we take the first step by studying
action-based video object segmentation under label noise, focusing on two
sources: textual prompt noise (category flips and within-category noun
substitutions) and mask annotation noise (perturbed object boundaries to mimic
imprecise supervision). Our contributions are threefold. First, we introduce
two types of label noises for the action-based video object segmentation task.
Second, we build up the first action-based video object segmentation under a
label noise benchmark ActiSeg-NL and adapt six label-noise learning strategies
to this setting, and establish protocols for evaluating them under textual,
boundary, and mixed noise. Third, we provide a comprehensive analysis linking
noise types to failure modes and robustness gains, and we introduce a Parallel
Mask Head Mechanism (PMHM) to address mask annotation noise. Qualitative
evaluations further reveal characteristic failure modes, including boundary
leakage and mislocalization under boundary perturbations, as well as occasional
identity substitutions under textual flips. Our comparative analysis reveals
that different learning strategies exhibit distinct robustness profiles,
governed by a foreground-background trade-off where some achieve balanced
performance while others prioritize foreground accuracy at the cost of
background precision. The established benchmark and source code will be made
publicly available at https://github.com/mylwx/ActiSeg-NL.
\\ ( https://arxiv.org/abs/2509.16677 ,  915kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16678
Date: Sat, 20 Sep 2025 13:06:45 GMT   (2381kb)

Title: IPF-RDA: An Information-Preserving Framework for Robust Data
  Augmentation
Authors: Suorong Yang, Hongchao Yang, Suhan Guo, Furao Shen, Jian Zhao
Categories: cs.CV
Comments: IEEE Transactions on Pattern Analysis and Machine Intelligence
\\
  Data augmentation is widely utilized as an effective technique to enhance the
generalization performance of deep models. However, data augmentation may
inevitably introduce distribution shifts and noises, which significantly
constrain the potential and deteriorate the performance of deep networks. To
this end, we propose a novel information-preserving framework, namely IPF-RDA,
to enhance the robustness of data augmentations in this paper. IPF-RDA combines
the proposal of (i) a new class-discriminative information estimation algorithm
that identifies the points most vulnerable to data augmentation operations and
corresponding importance scores; And (ii) a new information-preserving scheme
that preserves the critical information in the augmented samples and ensures
the diversity of augmented data adaptively. We divide data augmentation methods
into three categories according to the operation types and integrate these
approaches into our framework accordingly. After being integrated into our
framework, the robustness of data augmentation methods can be enhanced and
their full potential can be unleashed. Extensive experiments demonstrate that
although being simple, IPF-RDA consistently improves the performance of
numerous commonly used state-of-the-art data augmentation methods with popular
deep models on a variety of datasets, including CIFAR-10, CIFAR-100,
Tiny-ImageNet, CUHK03, Market1501, Oxford Flower, and MNIST, where its
performance and scalability are stressed. The implementation is available at
https://github.com/Jackbrocp/IPF-RDA.
\\ ( https://arxiv.org/abs/2509.16678 ,  2381kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16680
Date: Sat, 20 Sep 2025 13:12:08 GMT   (1389kb)

Title: ProtoVQA: An Adaptable Prototypical Framework for Explainable
  Fine-Grained Visual Question Answering
Authors: Xingjian Diao, Weiyi Wu, Keyi Kong, Peijun Qing, Xinwen Xu, Ming
  Cheng, Soroush Vosoughi and Jiang Gui
Categories: cs.CV cs.AI cs.LG
Comments: Accepted to EMNLP 2025 Main Conference
\\
  Visual Question Answering (VQA) is increasingly used in diverse applications
ranging from general visual reasoning to safety-critical domains such as
medical imaging and autonomous systems, where models must provide not only
accurate answers but also explanations that humans can easily understand and
verify. Prototype-based modeling has shown promise for interpretability by
grounding predictions in semantically meaningful regions for purely visual
reasoning tasks, yet remains underexplored in the context of VQA. We present
ProtoVQA, a unified prototypical framework that (i) learns question-aware
prototypes that serve as reasoning anchors, connecting answers to
discriminative image regions, (ii) applies spatially constrained matching to
ensure that the selected evidence is coherent and semantically relevant, and
(iii) supports both answering and grounding tasks through a shared prototype
backbone. To assess explanation quality, we propose the Visual-Linguistic
Alignment Score (VLAS), which measures how well the model's attended regions
align with ground-truth evidence. Experiments on Visual7W show that ProtoVQA
yields faithful, fine-grained explanations while maintaining competitive
accuracy, advancing the development of transparent and trustworthy VQA systems.
\\ ( https://arxiv.org/abs/2509.16680 ,  1389kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16684
Date: Sat, 20 Sep 2025 13:23:46 GMT   (7332kb)

Title: Active View Selection for Scene-level Multi-view Crowd Counting and
  Localization with Limited Labels
Authors: Qi Zhang and Bin Li and Antoni B. Chan and Hui Huang
Categories: cs.CV
Comments: 8 pages, 5 figures
\\
  Multi-view crowd counting and localization fuse the input multi-views for
estimating the crowd number or locations on the ground. Existing methods mainly
focus on accurately predicting on the crowd shown in the input views, which
neglects the problem of choosing the `best' camera views to perceive all crowds
well in the scene. Besides, existing view selection methods require massive
labeled views and images, and lack the ability for cross-scene settings,
reducing their application scenarios. Thus, in this paper, we study the view
selection issue for better scene-level multi-view crowd counting and
localization results with cross-scene ability and limited label demand, instead
of input-view-level results. We first propose an independent view selection
method (IVS) that considers view and scene geometries in the view selection
strategy and conducts the view selection, labeling, and downstream tasks
independently. Based on IVS, we also put forward an active view selection
method (AVS) that jointly optimizes the view selection, labeling, and
downstream tasks. In AVS, we actively select the labeled views and consider
both the view/scene geometries and the predictions of the downstream task
models in the view selection process. Experiments on multi-view counting and
localization tasks demonstrate the cross-scene and the limited label demand
advantages of the proposed active view selection method (AVS), outperforming
existing methods and with wider application scenarios.
\\ ( https://arxiv.org/abs/2509.16684 ,  7332kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16685
Date: Sat, 20 Sep 2025 13:26:31 GMT   (6811kb)

Title: Towards a Transparent and Interpretable AI Model for Medical Image
  Classifications
Authors: Binbin Wen, Yihang Wu, Tareef Daqqaq, Ahmad Chaddad
Categories: cs.CV cs.LG
Comments: Published in Cognitive Neurodynamics
DOI: 10.1007/s11571-025-10343-w
\\
  The integration of artificial intelligence (AI) into medicine is remarkable,
offering advanced diagnostic and therapeutic possibilities. However, the
inherent opacity of complex AI models presents significant challenges to their
clinical practicality. This paper focuses primarily on investigating the
application of explainable artificial intelligence (XAI) methods, with the aim
of making AI decisions transparent and interpretable. Our research focuses on
implementing simulations using various medical datasets to elucidate the
internal workings of the XAI model. These dataset-driven simulations
demonstrate how XAI effectively interprets AI predictions, thus improving the
decision-making process for healthcare professionals. In addition to a survey
of the main XAI methods and simulations, ongoing challenges in the XAI field
are discussed. The study highlights the need for the continuous development and
exploration of XAI, particularly from the perspective of diverse medical
datasets, to promote its adoption and effectiveness in the healthcare domain.
\\ ( https://arxiv.org/abs/2509.16685 ,  6811kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16690
Date: Sat, 20 Sep 2025 13:37:14 GMT   (12729kb)

Title: Spectral Compressive Imaging via Chromaticity-Intensity Decomposition
Authors: Xiaodong Wang, Zijun He, Ping Wang, Lishun Wang, Yanan Hu, Xin Yuan
Categories: cs.CV
\\
  In coded aperture snapshot spectral imaging (CASSI), the captured measurement
entangles spatial and spectral information, posing a severely ill-posed inverse
problem for hyperspectral images (HSIs) reconstruction. Moreover, the captured
radiance inherently depends on scene illumination, making it difficult to
recover the intrinsic spectral reflectance that remains invariant to lighting
conditions. To address these challenges, we propose a chromaticity-intensity
decomposition framework, which disentangles an HSI into a spatially smooth
intensity map and a spectrally variant chromaticity cube. The chromaticity
encodes lighting-invariant reflectance, enriched with high-frequency spatial
details and local spectral sparsity. Building on this decomposition, we develop
CIDNet, a Chromaticity-Intensity Decomposition unfolding network within a
dual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral
Transformer tailored to reconstruct fine-grained and sparse spectral
chromaticity and a degradation-aware, spatially-adaptive noise estimation
module that captures anisotropic noise across iterative stages. Extensive
experiments on both synthetic and real-world CASSI datasets demonstrate that
our method achieves superior performance in both spectral and chromaticity
fidelity. Code and models will be publicly available.
\\ ( https://arxiv.org/abs/2509.16690 ,  12729kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16691
Date: Sat, 20 Sep 2025 13:37:37 GMT   (18442kb)

Title: InstanceAssemble: Layout-Aware Image Generation via Instance Assembling
  Attention
Authors: Qiang Xiang, Shuang Sun, Binglei Li, Dejia Song, Huaxia Li, Nemo Chen,
  Xu Tang, Yao Hu, Junping Zhang
Categories: cs.CV
Comments: Accepted in NeurIPS 2025
\\
  Diffusion models have demonstrated remarkable capabilities in generating
high-quality images. Recent advancements in Layout-to-Image (L2I) generation
have leveraged positional conditions and textual descriptions to facilitate
precise and controllable image synthesis. Despite overall progress, current L2I
methods still exhibit suboptimal performance. Therefore, we propose
InstanceAssemble, a novel architecture that incorporates layout conditions via
instance-assembling attention, enabling position control with bounding boxes
(bbox) and multimodal content control including texts and additional visual
content. Our method achieves flexible adaption to existing DiT-based T2I models
through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image
benchmark, Denselayout, a comprehensive benchmark for layout-to-image
generation, containing 5k images with 90k instances in total. We further
introduce Layout Grounding Score (LGS), an interpretable evaluation metric to
more precisely assess the accuracy of L2I generation. Experiments demonstrate
that our InstanceAssemble method achieves state-of-the-art performance under
complex layout conditions, while exhibiting strong compatibility with diverse
style LoRA modules.
\\ ( https://arxiv.org/abs/2509.16691 ,  18442kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16702
Date: Sat, 20 Sep 2025 14:09:48 GMT   (12994kb)

Title: Animalbooth: multimodal feature enhancement for animal subject
  personalization
Authors: Chen Liu, Haitao Wu, Kafeng Wang, Xiaowang Zhang
Categories: cs.CV
\\
  Personalized animal image generation is challenging due to rich appearance
cues and large morphological variability. Existing approaches often exhibit
feature misalignment across domains, which leads to identity drift. We present
AnimalBooth, a framework that strengthens identity preservation with an Animal
Net and an adaptive attention module, mitigating cross domain alignment errors.
We further introduce a frequency controlled feature integration module that
applies Discrete Cosine Transform filtering in the latent space to guide the
diffusion process, enabling a coarse to fine progression from global structure
to detailed texture. To advance research in this area, we curate AnimalBench, a
high resolution dataset for animal personalization. Extensive experiments show
that AnimalBooth consistently outperforms strong baselines on multiple
benchmarks and improves both identity fidelity and perceptual quality.
\\ ( https://arxiv.org/abs/2509.16702 ,  12994kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16704
Date: Sat, 20 Sep 2025 14:23:09 GMT   (7329kb)

Title: When Confidence Fails: Revisiting Pseudo-Label Selection in
  Semi-supervised Semantic Segmentation
Authors: Pan Liu, Jinshi Liu
Categories: cs.CV
\\
  While significant advances exist in pseudo-label generation for
semi-supervised semantic segmentation, pseudo-label selection remains
understudied. Existing methods typically use fixed confidence thresholds to
retain high-confidence predictions as pseudo-labels. However, these methods
cannot cope with network overconfidence tendency, where correct and incorrect
predictions overlap significantly in high-confidence regions, making separation
challenging and amplifying model cognitive bias. Meanwhile, the direct
discarding of low-confidence predictions disrupts spatial-semantic continuity,
causing critical context loss. We propose Confidence Separable Learning (CSL)
to address these limitations. CSL formulates pseudo-label selection as a convex
optimization problem within the confidence distribution feature space,
establishing sample-specific decision boundaries to distinguish reliable from
unreliable predictions. Additionally, CSL introduces random masking of reliable
pixels to guide the network in learning contextual relationships from
low-reliability regions, thereby mitigating the adverse effects of discarding
uncertain predictions. Extensive experimental results on the Pascal,
Cityscapes, and COCO benchmarks show that CSL performs favorably against
state-of-the-art methods. Code and model weights are available at
https://github.com/PanLiuCSU/CSL.
\\ ( https://arxiv.org/abs/2509.16704 ,  7329kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16721
Date: Sat, 20 Sep 2025 15:10:45 GMT   (28729kb)

Title: Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene
  Understanding
Authors: Haoyuan Li, Rui Liu, Hehe Fan, Yi Yang
Categories: cs.CV cs.AI cs.RO
Comments: 19 pages, 12 figures, 6 tables
\\
  Enabling agents to understand and interact with complex 3D scenes is a
fundamental challenge for embodied artificial intelligence systems. While
Multimodal Large Language Models (MLLMs) have achieved significant progress in
2D image understanding, extending such capabilities to 3D scenes remains
difficult: 1) 3D environment involves richer concepts such as spatial
relationships, affordances, physics, layout, and so on, 2) the absence of
large-scale 3D vision-language datasets has posed a significant obstacle. In
this paper, we introduce Text-Scene, a framework that automatically parses 3D
scenes into textual descriptions for scene understanding. Given a 3D scene, our
model identifies object attributes and spatial relationships, and then
generates a coherent summary of the whole scene, bridging the gap between 3D
observation and language without requiring human-in-the-loop intervention. By
leveraging both geometric analysis and MLLMs, Text-Scene produces descriptions
that are accurate, detailed, and human-interpretable, capturing object-level
details and global-level context. Experimental results on benchmarks
demonstrate that our textual parses can faithfully represent 3D scenes and
benefit downstream tasks. To evaluate the reasoning capability of MLLMs, we
present InPlan3D, a comprehensive benchmark for 3D task planning, consisting of
3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity
and accessibility in our approach, aiming to make 3D scene content
understandable through language. Code and datasets will be released.
\\ ( https://arxiv.org/abs/2509.16721 ,  28729kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16727
Date: Sat, 20 Sep 2025 15:41:23 GMT   (1097kb)

Title: Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain
  Assessment
Authors: Xin Lei Lin, Soroush Mehraban, Abhishek Moturu, Babak Taati
Categories: cs.CV cs.LG
\\
  Automated pain assessment from facial expressions is crucial for
non-communicative patients, such as those with dementia. Progress has been
limited by two challenges: (i) existing datasets exhibit severe demographic and
label imbalance due to ethical constraints, and (ii) current generative models
cannot precisely control facial action units (AUs), facial structure, or
clinically validated pain levels.
  We present 3DPain, a large-scale synthetic dataset specifically designed for
automated pain assessment, featuring unprecedented annotation richness and
demographic diversity. Our three-stage framework generates diverse 3D meshes,
textures them with diffusion models, and applies AU-driven face rigging to
synthesize multi-view faces with paired neutral and pain images, AU
configurations, PSPI scores, and the first dataset-level annotations of
pain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain
expression heatmaps and 2,500 synthetic identities balanced by age, gender, and
ethnicity.
  We further introduce ViTPain, a Vision Transformer based cross-modal
distillation framework in which a heatmap-trained teacher guides a student
trained on RGB images, enhancing accuracy, interpretability, and clinical
reliability. Together, 3DPain and ViTPain establish a controllable, diverse,
and clinically grounded foundation for generalizable automated pain assessment.
\\ ( https://arxiv.org/abs/2509.16727 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16738
Date: Sat, 20 Sep 2025 16:07:20 GMT   (2879kb)

Title: Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental
  Learning
Authors: Kai Jiang, Zhengyan Shi, Dell Zhang, Hongyuan Zhang and Xuelong Li
Categories: cs.CV cs.LG
Comments: Accepted by NeurIPS 2025. Source Code will be released in the next
  version
\\
  Class Incremental Learning (CIL) aims to continuously learn new categories
while retaining the knowledge of old ones. Pre-trained models (PTMs) show
promising capabilities in CIL. However, existing approaches that apply
lightweight fine-tuning to backbones still induce parameter drift, thereby
compromising the generalization capability of pre-trained models. Parameter
drift can be conceptualized as a form of noise that obscures critical patterns
learned for previous tasks. However, recent researches have shown that noise is
not always harmful. For example, the large number of visual patterns learned
from pre-training can be easily abused by a single task, and introducing
appropriate noise can suppress some low-correlation features, thus leaving a
margin for future tasks. To this end, we propose learning beneficial noise for
CIL guided by information theory and propose Mixture of Noise (Min), aiming to
mitigate the degradation of backbone generalization from adapting new tasks.
Specifically, task-specific noise is learned from high-dimension features of
new tasks. Then, a set of weights is adjusted dynamically for optimal mixture
of different task noise. Finally, Min embeds the beneficial noise into the
intermediate features to mask the response of inefficient patterns. Extensive
experiments on six benchmark datasets demonstrate that Min achieves
state-of-the-art performance in most incremental settings, with particularly
outstanding results in 50-steps incremental settings. This shows the
significant potential for beneficial noise in continual learning.
\\ ( https://arxiv.org/abs/2509.16738 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16745
Date: Sat, 20 Sep 2025 17:13:38 GMT   (1448kb)

Title: CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with
  QR Understanding
Authors: Ritabrata Chakraborty, Avijit Dasgupta, Sandeep Chaurasia
Categories: cs.CV cs.AI
Comments: 9 pages, 5 figures, 6 tables
\\
  Visual explanations are often plausible but not structurally faithful. We
introduce CAMBench-QR, a structure-aware benchmark that leverages the canonical
geometry of QR codes (finder patterns, timing lines, module grid) to test
whether CAM methods place saliency on requisite substructures while avoiding
background. CAMBench-QR synthesizes QR/non-QR data with exact masks and
controlled distortions, and reports structure-aware metrics (Finder/Timing Mass
Ratios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside
causal occlusion, insertion/deletion faithfulness, robustness, and latency. We
benchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM)
under two practical regimes of zero-shot and last-block fine-tuning. The
benchmark, metrics, and training recipes provide a simple, reproducible
yardstick for structure-aware evaluation of visual explanations. Hence we
propose that CAMBENCH-QR can be used as a litmus test of whether visual
explanations are truly structure-aware.
\\ ( https://arxiv.org/abs/2509.16745 ,  1448kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16748
Date: Sat, 20 Sep 2025 17:17:01 GMT   (12189kb)

Title: HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head
  Image Synthesis
Authors: Heyuan Li, Kenkun Liu, Lingteng Qiu, Qi Zuo, Keru Zheng, Zilong Dong,
  Xiaoguang Han
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\
  Tri-plane-like representations have been widely adopted in 3D-aware GANs for
head image synthesis and other 3D object/scene modeling tasks due to their
efficiency. However, querying features via Cartesian coordinate projection
often leads to feature entanglement, which results in mirroring artifacts. A
recent work, SphereHead, attempted to address this issue by introducing
spherical tri-planes based on a spherical coordinate system. While it
successfully mitigates feature entanglement, SphereHead suffers from uneven
mapping between the square feature maps and the spherical planes, leading to
inefficient feature map utilization during rendering and difficulties in
generating fine image details. Moreover, both tri-plane and spherical tri-plane
representations share a subtle yet persistent issue: feature penetration across
convolutional channels can cause interference between planes, particularly when
one plane dominates the others. These challenges collectively prevent
tri-plane-based methods from reaching their full potential. In this paper, we
systematically analyze these problems for the first time and propose innovative
solutions to address them. Specifically, we introduce a novel hybrid-plane
(hy-plane for short) representation that combines the strengths of both planar
and spherical planes while avoiding their respective drawbacks. We further
enhance the spherical plane by replacing the conventional theta-phi warping
with a novel near-equal-area warping strategy, which maximizes the effective
utilization of the square feature map. In addition, our generator synthesizes a
single-channel unified feature map instead of multiple feature maps in separate
channels, thereby effectively eliminating feature penetration. With a series of
technical improvements, our hy-plane representation enables our method,
HyPlaneHead, to achieve state-of-the-art performance in full-head image
synthesis.
\\ ( https://arxiv.org/abs/2509.16748 ,  12189kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16767
Date: Sat, 20 Sep 2025 18:20:51 GMT   (37770kb)

Title: DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation
  Conditioned on Natural Images
Authors: Ozgur Kara, Harris Nisar, James M. Rehg
Categories: cs.CV
Comments: Accepted to NeurIPS 2025
\\
  Numerous models have been developed for scanpath and saliency prediction,
which are typically trained on scanpaths, which model eye movement as a
sequence of discrete fixation points connected by saccades, while the rich
information contained in the raw trajectories is often discarded. Moreover,
most existing approaches fail to capture the variability observed among human
subjects viewing the same image. They generally predict a single scanpath of
fixed, pre-defined length, which conflicts with the inherent diversity and
stochastic nature of real-world visual attention. To address these challenges,
we propose DiffEye, a diffusion-based training framework designed to model
continuous and diverse eye movement trajectories during free viewing of natural
images. Our method builds on a diffusion model conditioned on visual stimuli
and introduces a novel component, namely Corresponding Positional Embedding
(CPE), which aligns spatial gaze information with the patch-based semantic
features of the visual input. By leveraging raw eye-tracking trajectories
rather than relying on scanpaths, DiffEye captures the inherent variability in
human gaze behavior and generates high-quality, realistic eye movement
patterns, despite being trained on a comparatively small dataset. The generated
trajectories can also be converted into scanpaths and saliency maps, resulting
in outputs that more accurately reflect the distribution of human visual
attention. DiffEye is the first method to tackle this task on natural images
using a diffusion model while fully leveraging the richness of raw eye-tracking
data. Our extensive evaluation shows that DiffEye not only achieves
state-of-the-art performance in scanpath generation but also enables, for the
first time, the generation of continuous eye movement trajectories. Project
webpage: https://diff-eye.github.io/
\\ ( https://arxiv.org/abs/2509.16767 ,  37770kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16768
Date: Sat, 20 Sep 2025 18:25:14 GMT   (228kb)

Title: MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D
  Generation
Authors: Omid Bonakdar, Nasser Mozayani
Categories: cs.CV
\\
  Generative 3D modeling has advanced rapidly, driven by applications in VR/AR,
metaverse, and robotics. However, most methods represent the target object as a
closed mesh devoid of any structural information, limiting editing, animation,
and semantic understanding. Part-aware 3D generation addresses this problem by
decomposing objects into meaningful components, but existing pipelines face
challenges: in existing methods, the user has no control over which objects are
separated and how model imagine the occluded parts in isolation phase. In this
paper, we introduce MMPart, an innovative framework for generating part-aware
3D models from a single image. We first use a VLM to generate a set of prompts
based on the input image and user descriptions. In the next step, a generative
model generates isolated images of each object based on the initial image and
the previous step's prompts as supervisor (which control the pose and guide
model how imagine previously occluded areas). Each of those images then enters
the multi-view generation stage, where a number of consistent images from
different views are generated. Finally, a reconstruction model converts each of
these multi-view images into a 3D model.
\\ ( https://arxiv.org/abs/2509.16768 ,  228kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16771
Date: Sat, 20 Sep 2025 18:38:30 GMT   (10798kb)

Title: Artificial Satellite Trails Detection Using U-Net Deep Neural Network
  and Line Segment Detector Algorithm
Authors: Xiaohan Chen, Hongrui Gu, Cunshi Wang, Haiyang Mu, Jie Zheng, Junju
  Du, Jing Ren, Zhou Fan, Jing Li
Categories: cs.CV astro-ph.IM
Comments: 15 pages, 7 figures, 2 tables, PASP accepted
\\
  With the rapid increase in the number of artificial satellites, astronomical
imaging is experiencing growing interference. When these satellites reflect
sunlight, they produce streak-like artifacts in photometry images. Such
satellite trails can introduce false sources and cause significant photometric
errors. As a result, accurately identifying the positions of satellite trails
in observational data has become essential. In this work, we propose a
satellite trail detection model that combines the U-Net deep neural network for
image segmentation with the Line Segment Detector (LSD) algorithm. The model is
trained on 375 simulated images of satellite trails, generated using data from
the Mini-SiTian Array. Experimental results show that for trails with a
signal-to-noise ratio (SNR) greater than 3, the detection rate exceeds 99.
Additionally, when applied to real observational data from the Mini-SiTian
Array, the model achieves a recall of 79.57 and a precision of 74.56.
\\ ( https://arxiv.org/abs/2509.16771 ,  10798kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16805
Date: Sat, 20 Sep 2025 20:45:47 GMT   (677kb)

Title: Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language
  Models
Authors: Md. Atabuzzaman, Ali Asgarov, Chris Thomas
Categories: cs.CV
Comments: Accepted to EMNLP 2025 (Main Conference)
\\
  Large Vision-Language Models (LVLMs) have achieved strong performance on
vision-language tasks, particularly Visual Question Answering (VQA). While
prior work has explored unimodal biases in VQA, the problem of selection bias
in Multiple-Choice Question Answering (MCQA), where models may favor specific
option tokens (e.g., "A") or positions, remains underexplored. In this paper,
we investigate both the presence and nature of selection bias in LVLMs through
fine-grained MCQA benchmarks spanning easy, medium, and hard difficulty levels,
defined by the semantic similarity of the options. We further propose an
inference-time logit-level debiasing method that estimates an ensemble bias
vector from general and contextual prompts and applies confidence-adaptive
corrections to the model's output. Our method mitigates bias without retraining
and is compatible with frozen LVLMs. Extensive experiments across several
state-of-the-art models reveal consistent selection biases that intensify with
task difficulty, and show that our mitigation approach significantly reduces
bias while improving accuracy in challenging settings. This work offers new
insights into the limitations of LVLMs in MCQA and presents a practical
approach to improve their robustness in fine-grained visual reasoning. Datasets
and code are available at:
https://github.com/Atabuzzaman/Selection-Bias-of-LVLMs
\\ ( https://arxiv.org/abs/2509.16805 ,  677kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16806
Date: Sat, 20 Sep 2025 20:52:26 GMT   (6153kb)

Title: MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging
Authors: Kacper Marzol, Ignacy Kolton, Weronika Smolak-Dy\.zewska, Joanna
  Kaleta, Marcin Mazur, Przemys{\l}aw Spurek
Categories: cs.CV
\\
  Multi-modal three-dimensional (3D) medical imaging data, derived from
ultrasound, magnetic resonance imaging (MRI), and potentially computed
tomography (CT), provide a widely adopted approach for non-invasive anatomical
visualization. Accurate modeling, registration, and visualization in this
setting depend on surface reconstruction and frame-to-frame interpolation.
Traditional methods often face limitations due to image noise and incomplete
information between frames. To address these challenges, we present MedGS, a
semi-supervised neural implicit surface reconstruction framework that employs a
Gaussian Splatting (GS)-based interpolation mechanism. In this framework,
medical imaging data are represented as consecutive two-dimensional (2D) frames
embedded in 3D space and modeled using Gaussian-based distributions. This
representation enables robust frame interpolation and high-fidelity surface
reconstruction across imaging modalities. As a result, MedGS offers more
efficient training than traditional neural implicit methods. Its explicit
GS-based representation enhances noise robustness, allows flexible editing, and
supports precise modeling of complex anatomical structures with fewer
artifacts. These features make MedGS highly suitable for scalable and practical
applications in medical imaging.
\\ ( https://arxiv.org/abs/2509.16806 ,  6153kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16822
Date: Sat, 20 Sep 2025 22:21:20 GMT   (8196kb)

Title: Looking in the mirror: A faithful counterfactual explanation method for
  interpreting deep image classification models
Authors: Townim Faisal Chowdhury, Vu Minh Hieu Phan, Kewen Liao, Nanyu Dong,
  Minh-Son To, Anton Hengel, Johan Verjans, Zhibin Liao
Categories: cs.CV
Comments: Accepted at IEEE/CVF International Conference on Computer Vision
  (ICCV), 2025
\\
  Counterfactual explanations (CFE) for deep image classifiers aim to reveal
how minimal input changes lead to different model decisions, providing critical
insights for model interpretation and improvement. However, existing CFE
methods often rely on additional image encoders and generative models to create
plausible images, neglecting the classifier's own feature space and decision
boundaries. As such, they do not explain the intrinsic feature space and
decision boundaries learned by the classifier. To address this limitation, we
propose Mirror-CFE, a novel method that generates faithful counterfactual
explanations by operating directly in the classifier's feature space, treating
decision boundaries as mirrors that ``reflect'' feature representations in the
mirror. Mirror-CFE learns a mapping function from feature space to image space
while preserving distance relationships, enabling smooth transitions between
source images and their counterfactuals. Through extensive experiments on four
image datasets, we demonstrate that Mirror-CFE achieves superior performance in
validity while maintaining input resemblance compared to state-of-the-art
explanation methods. Finally, mirror-CFE provides interpretable visualization
of the classifier's decision process by generating step-wise transitions that
reveal how features evolve as classification confidence changes.
\\ ( https://arxiv.org/abs/2509.16822 ,  8196kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16832
Date: Sat, 20 Sep 2025 23:13:27 GMT   (28201kb)

Title: L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR
  Point Clouds and Semantic 3D City Models
Authors: Ziyang Xu, Benedikt Schwab, Yihui Yang, Thomas H. Kolbe, Christoph
  Holst
Categories: cs.CV cs.RO eess.IV
Comments: submit to ISPRS Journal of Photogrammetry and Remote Sensing
\\
  Accurate registration between LiDAR (Light Detection and Ranging) point
clouds and semantic 3D city models is a fundamental topic in urban digital
twinning and a prerequisite for downstream tasks, such as digital construction,
change detection and model refinement. However, achieving accurate
LiDAR-to-Model registration at individual building level remains challenging,
particularly due to the generalization uncertainty in semantic 3D city models
at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing
L2M-Reg, a plane-based fine registration method that explicitly accounts for
model uncertainty. L2M-Reg consists of three key steps: establishing reliable
plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,
and adaptively estimating vertical translation. Experiments on three real-world
datasets demonstrate that L2M-Reg is both more accurate and computationally
efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg
provides a novel building-level solution regarding LiDAR-to-Model registration
when model uncertainty is present.
\\ ( https://arxiv.org/abs/2509.16832 ,  28201kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16853
Date: Sun, 21 Sep 2025 00:44:15 GMT   (8518kb)

Title: ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image
  Compression
Authors: Jinhao Wang, Cihan Ruan, Nam Ling, Wei Wang, Wei Jiang
Categories: cs.CV
\\
  Prior studies in learned image compression (LIC) consistently show that only
a small subset of latent channels is critical for reconstruction, while many
others carry limited information. Exploiting this imbalance could improve both
coding and computational efficiency, yet existing approaches often rely on
costly, dataset-specific ablation tests and typically analyze channels in
isolation, ignoring their interdependencies.
  We propose a generalizable, dataset-agnostic method to identify and organize
important channels in pretrained VAE-based LIC models. Instead of brute-force
empirical evaluations, our approach leverages intrinsic parameter
statistics-weight variances, bias magnitudes, and pairwise correlations-to
estimate channel importance. This analysis reveals a consistent organizational
structure, termed the Invariant Salient Channel Space (ISCS), where
Salient-Core channels capture dominant structures and Salient-Auxiliary
channels provide complementary details. Building on ISCS, we introduce a
deterministic channel ordering and grouping strategy that enables
slice-parallel decoding, reduces redundancy, and improves bitrate efficiency.
  Experiments across multiple LIC architectures demonstrate that our method
effectively reduces bitrate and computation while maintaining reconstruction
quality, providing a practical and modular enhancement to existing learned
compression frameworks.
\\ ( https://arxiv.org/abs/2509.16853 ,  8518kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16863
Date: Sun, 21 Sep 2025 01:28:03 GMT   (9001kb)

Title: ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D
  Gaussian Splatting SLAM
Authors: Amanuel T. Dufera, Yuan-Li Cai
Categories: cs.CV
Report-no: ID 119820
MSC-class: 68T20, 68U20
DOI: 10.1109/IEEECONF65522.2025.11137090
\\
  We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM
system for robust, highfidelity RGB-only reconstruction. Addressing geometric
inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable
depth estimation, ConfidentSplat incorporates a core innovation: a
confidence-weighted fusion mechanism. This mechanism adaptively integrates
depth cues from multiview geometry with learned monocular priors (Omnidata
ViT), dynamically weighting their contributions based on explicit reliability
estimates-derived predominantly from multi-view geometric consistency-to
generate high-fidelity proxy depth for map supervision. The resulting proxy
depth guides the optimization of a deformable 3DGS map, which efficiently
adapts online to maintain global consistency following pose updates from a
DROID-SLAM-inspired frontend and backend optimizations (loop closure, global
bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD,
ScanNet) and diverse custom mobile datasets demonstrates significant
improvements in reconstruction accuracy (L1 depth error) and novel view
synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in
challenging conditions. ConfidentSplat underscores the efficacy of principled,
confidence-aware sensor fusion for advancing state-of-the-art dense visual
SLAM.
\\ ( https://arxiv.org/abs/2509.16863 ,  9001kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16873
Date: Sun, 21 Sep 2025 01:50:14 GMT   (26969kb)

Title: $\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized
  Benchmark Dataset for Image Restoration and Content Creation
Authors: Yuanzhi Li, Lebin Zhou, Nam Ling, Zhenghao Chen, Wei Wang, Wei Jiang
Categories: cs.CV
\\
  The gaming and entertainment industry is rapidly evolving, driven by
immersive experiences and the integration of generative AI (GAI) technologies.
Training such models effectively requires large-scale datasets that capture the
diversity and context of gaming environments. However, existing datasets are
often limited to specific domains or rely on artificial degradations, which do
not accurately capture the unique characteristics of gaming content. Moreover,
benchmarks for controllable video generation remain absent.
  To address these limitations, we introduce $\mathtt{M^3VIR}$, a large-scale,
multi-modal, multi-view dataset specifically designed to overcome the
shortcomings of current resources. Unlike existing datasets, $\mathtt{M^3VIR}$
provides diverse, high-fidelity gaming content rendered with Unreal Engine 5,
offering authentic ground-truth LR-HR paired and multi-view frames across 80
scenes in 8 categories. It includes $\mathtt{M^3VIR\_MR}$ for super-resolution
(SR), novel view synthesis (NVS), and combined NVS+SR tasks, and
$\mathtt{M^3VIR\_{MS}}$, the first multi-style, object-level ground-truth set
enabling research on controlled video generation. Additionally, we benchmark
several state-of-the-art SR and NVS methods to establish performance baselines.
While no existing approaches directly handle controlled video generation,
$\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing
the dataset, we aim to facilitate research in AI-powered restoration,
compression, and controllable content generation for next-generation cloud
gaming and entertainment.
\\ ( https://arxiv.org/abs/2509.16873 ,  26969kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16886
Date: Sun, 21 Sep 2025 02:39:48 GMT   (529kb)

Title: SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in
  Medical Segmentation
Authors: Yingzhen Hu, Yiheng Zhong, Ruobing Li, Yingxue Su, Jiabao An, Feilong
  Tang, Jionglong Su, Imran Razzak
Categories: cs.CV
\\
  The Segment Anything Model (SAM) demonstrates impressive zero-shot
segmentation ability on natural images but encounters difficulties in medical
imaging due to domain shifts, anatomical variability, and its reliance on
user-provided prompts. Recent prompt-free adaptations alleviate the need for
expert intervention, yet still suffer from limited robustness and adaptability,
often overlooking the issues of semantic over-smoothing and token uniformity.
We propose SAM-DCE, which balances local discrimination and global semantics
while mitigating token uniformity, enhancing inter-class separability, and
enriching mask decoding with fine-grained, consistent representations.
Extensive experiments on diverse medical benchmarks validate its effectiveness.
\\ ( https://arxiv.org/abs/2509.16886 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16888
Date: Sun, 21 Sep 2025 02:45:07 GMT   (5821kb)

Title: Rethinking Evaluation of Infrared Small Target Detection
Authors: Youwei Pang, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu, Georges El Fakhri,
  Xiaofeng Liu, Shijian Lu
Categories: cs.CV
Comments: NeurIPS 2025; Evaluation Toolkit:
  https://github.com/lartpang/PyIRSTDMetrics
\\
  As an essential vision task, infrared small target detection (IRSTD) has seen
significant advancements through deep learning. However, critical limitations
in current evaluation protocols impede further progress. First, existing
methods rely on fragmented pixel- and target-level specific metrics, which
fails to provide a comprehensive view of model capabilities. Second, an
excessive emphasis on overall performance scores obscures crucial error
analysis, which is vital for identifying failure modes and improving real-world
system performance. Third, the field predominantly adopts dataset-specific
training-testing paradigms, hindering the understanding of model robustness and
generalization across diverse infrared scenarios. This paper addresses these
issues by introducing a hybrid-level metric incorporating pixel- and
target-level performance, proposing a systematic error analysis method, and
emphasizing the importance of cross-dataset evaluation. These aim to offer a
more thorough and rational hierarchical analysis framework, ultimately
fostering the development of more effective and robust IRSTD models. An
open-source toolkit has be released to facilitate standardized benchmarking.
\\ ( https://arxiv.org/abs/2509.16888 ,  5821kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16892
Date: Sun, 21 Sep 2025 03:05:17 GMT   (11275kb)

Title: Learning from Gene Names, Expression Values and Images: Contrastive
  Masked Text-Image Pretraining for Spatial Transcriptomics Representation
  Learning
Authors: Jiahe Qian, Yaoyu Fang, Ziqiao Weng, Xinkun Wang, Lee A. Cooper, Bo
  Zhou
Categories: cs.CV cs.AI
Comments: 9 pages, 3 figures
\\
  Spatial transcriptomics aims to connect high-resolution histology images with
spatially resolved gene expression. To achieve better performance on downstream
tasks such as gene expression prediction, large-scale pre-training is required
to obtain generalisable representations that can bridge histology and
transcriptomics across tissues, protocols, and laboratories. Existing
cross-modal pre-training approaches for spatial transcriptomics rely on either
gene names or expression values in isolation, which strips the gene branch of
essential semantics and breaks the association between each gene and its
quantitative magnitude. In addition, by restricting supervision to image-text
alignment, these methods ignore intrinsic visual cues that are critical for
learning robust image features. We present CoMTIP, the first Contrastive Masked
Text-Image Pretraining framework that jointly learns from images, gene names,
and expression values while capturing fine-grained visual context for spatial
transcriptomics. The vision branch uses Masked Feature Modeling to reconstruct
occluded patches and learn context-aware image embeddings. The text branch
applies a scalable Gene-Text Encoder that processes all gene sentences in
parallel, enriches each gene and its numerical value with dedicated embeddings,
and employs Pair-aware Adversarial Training (PAAT) to preserve correct
gene-value associations. Image and text representations are aligned in a shared
InfoNCE-optimised space. Experiments on public spatial transcriptomics datasets
show that CoMTIP not only surpasses previous methods on diverse downstream
tasks but also achieves zero-shot gene expression prediction, a capability that
existing approaches do not provide.
\\ ( https://arxiv.org/abs/2509.16892 ,  11275kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16897
Date: Sun, 21 Sep 2025 03:16:07 GMT   (4270kb)

Title: PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via
  Generative Diffusion
Authors: Xuewan He, Jielei Wang, Zihan Cheng, Yuchen Su, Shiyue Huang, Guoming
  Lu
Categories: cs.CV
\\
  Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to
a student without access to the real in-distribution (ID) data. While existing
methods perform well on small-scale images, they suffer from mode collapse when
synthesizing large-scale images, resulting in limited knowledge transfer.
Recently, leveraging advanced generative models to synthesize photorealistic
images has emerged as a promising alternative. Nevertheless, directly using
off-the-shelf diffusion to generate datasets faces the precision-recall
challenges: 1) ensuring synthetic data aligns with the real distribution, and
2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a
precision-recall informed synthesis method. Specifically, we introduce
Energy-guided Distribution Alignment to avoid the generation of
out-of-distribution samples, and design the Diversified Prompt Engineering to
enhance coverage of the real ID manifold. Extensive experiments on various
large-scale image datasets demonstrate the superiority of PRISM. Moreover, we
demonstrate that models trained with PRISM exhibit strong domain
generalization.
\\ ( https://arxiv.org/abs/2509.16897 ,  4270kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16900
Date: Sun, 21 Sep 2025 03:23:04 GMT   (2093kb)

Title: ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion
  for Multimodal Survival Analysis
Authors: Chengsheng Zhang, Linhao Qu, Xiaoyu Liu and Zhijian Song
Categories: cs.CV cs.AI
\\
  Survival analysis using whole-slide images (WSIs) is crucial in cancer
research. Despite significant successes, pathology images typically only
provide slide-level labels, which hinders the learning of discriminative
representations from gigapixel WSIs. With the rapid advancement of
high-throughput sequencing technologies, multimodal survival analysis
integrating pathology images and genomics data has emerged as a promising
approach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures
discriminative pathological and genomic features while enabling efficient
integration of both modalities. This approach achieves complementary
information fusion without losing critical information from individual
modalities, thereby facilitating accurate cancer survival analysis.
Specifically, we first introduce a Pathology Expert and a Genomics Expert to
process unimodal data separately. Both experts are designed with Mamba
architectures that incorporate conventional scanning and attention-based
scanning mechanisms, allowing them to extract discriminative features from long
instance sequences containing substantial redundant or irrelevant information.
Second, we design a Synergistic Expert responsible for modality fusion. It
explicitly learns token-level local correspondences between the two modalities
via Optimal Transport, and implicitly enhances distribution consistency through
a global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused
feature representations are then passed to a mamba backbone for further
integration. Through the collaboration of the Pathology Expert, Genomics
Expert, and Synergistic Expert, our method achieves stable and accurate
survival analysis with relatively low computational complexity. Extensive
experimental results on five datasets in The Cancer Genome Atlas (TCGA)
demonstrate our state-of-the-art performance.
\\ ( https://arxiv.org/abs/2509.16900 ,  2093kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16909
Date: Sun, 21 Sep 2025 04:04:47 GMT   (3189kb)

Title: SLAM-Former: Putting SLAM into One Transformer
Authors: Yijun Yuan, Zhuoguang Chen, Kenan Li, Weibang Wang, Hang Zhao
Categories: cs.CV cs.RO
Comments: Project Page:https://tsinghua-mars-lab.github.io/SLAM-Former
\\
  We present SLAM-Former, a novel neural approach that integrates full SLAM
capabilities into a single transformer. Similar to traditional SLAM systems,
SLAM-Former comprises both a frontend and a backend that operate in tandem. The
frontend processes sequential monocular images in real-time for incremental
mapping and tracking, while the backend performs global refinement to ensure a
geometrically consistent result. This alternating execution allows the frontend
and backend to mutually promote one another, enhancing overall system
performance. Comprehensive experimental results demonstrate that SLAM-Former
achieves superior or highly competitive performance compared to
state-of-the-art dense SLAM methods.
\\ ( https://arxiv.org/abs/2509.16909 ,  3189kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16935
Date: Sun, 21 Sep 2025 05:46:54 GMT   (18kb)

Title: Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for
  Atypical Mitotic Figure Classification
Authors: Lavish Ramchandani, Gunjan Deotale, Dev Kumar Das
Categories: cs.CV
Comments: MIDOG'25
MSC-class: 68T07
ACM-class: I.2.10; I.4.9; I.5.4
\\
  Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated
with tumor aggressiveness and poor prognosis. Their detection remains a
significant challenge due to subtle morphological cues, class imbalance, and
inter-observer variability among pathologists. The MIDOG 2025 challenge
introduced a dedicated track for atypical mitosis classification, enabling
systematic evaluation of deep learning methods. In this study, we investigated
the use of large vision foundation models, including Virchow, Virchow2, and
UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We
conducted extensive experiments with different LoRA ranks, as well as random
and group-based data splits, to analyze robustness under varied conditions. Our
best approach, Virchow with LoRA rank 8 and ensemble of three-fold
cross-validation, achieved a balanced accuracy of 88.37% on the preliminary
test set, ranking joint 9th in the challenge leaderboard. These results
highlight the promise of foundation models with efficient adaptation strategies
for the classification of atypical mitosis, while underscoring the need for
improvements in specificity and domain generalization.
\\ ( https://arxiv.org/abs/2509.16935 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16942
Date: Sun, 21 Sep 2025 06:33:59 GMT   (1469kb)

Title: Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation
  in Remote Sensing Semantic Segmentation
Authors: Bin Wang, Fei Deng, Zeyu Chen, Zhicheng Yu, Yiguang Liu
Categories: cs.CV
\\
  Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic
segmentation of Remote Sensing Images (RSIs) using only a well-trained source
model and unlabeled target domain data. However, the lack of ground-truth
labels in the target domain often leads to the generation of noisy
pseudo-labels. Such noise impedes the effective mitigation of domain shift
(DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA
framework. It employs prototype-weighted pseudo-labels to facilitate reliable
self-training (ST) under pseudo-labels noise. We, in addition, introduce a
prototype-contrast strategy that encourages the aggregation of features
belonging to the same class, enabling the model to learn discriminative target
domain representations without relying on ground-truth supervision. Extensive
experiments show that our approach substantially outperforms existing methods.
\\ ( https://arxiv.org/abs/2509.16942 ,  1469kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16944
Date: Sun, 21 Sep 2025 06:54:04 GMT   (1406kb)

Title: Catching the Details: Self-Distilled RoI Predictors for Fine-Grained
  MLLM Perception
Authors: Yuheng Shi, Xiaohuan Pei, Minjing Dong, Chang Xu
Categories: cs.CV
Comments: 19 pages, 5 figures
\\
  Multimodal Large Language Models (MLLMs) require high-resolution visual
information to perform fine-grained perception, yet processing entire
high-resolution images is computationally prohibitive. While recent methods
leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they
typically present a difficult trade-off: training-based approaches depend on
large-scale annotated datasets, while training-free methods that utilize the
model's internal attention are computationally inefficient and less accurate,
requiring either multi-pass prefill stages or reliance on the slow
auto-regressive decoding process. In this paper, we propose an efficient,
annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves
this trade-off. The SD-RPN is built around a pipeline that transforms the noisy
attention maps from the MLLM's middle layers into high-quality pseudo-RoI
labels by explicitly denoising the signal and resolving ambiguity. We use these
labels to train a lightweight Region Proposal Network (RPN) that learns a more
precise localization. This RPN is also highly efficient, predicting the RoI in
a single forward pass using features from the MLLM's middle layers, decoupling
RoI identification from the auto-regressive generation and avoiding costly
multi-pass operations.To validate our approach, we integrate the framework into
the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K)
question-answer pairs, our method demonstrates exceptional data efficiency and
generalization, achieving over a 10% absolute accuracy improvement on unseen
benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a
practical and scalable solution for enhancing the fine-grained perception of
MLLMs without requiring costly supervision or full model fine-tuning. Code is
available at https://github.com/YuHengsss/SD-RPN.
\\ ( https://arxiv.org/abs/2509.16944 ,  1406kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16949
Date: Sun, 21 Sep 2025 07:07:49 GMT   (701kb)

Title: Leveraging RGB Images for Pre-Training of Event-Based Hand Pose
  Estimation
Authors: Ruicong Liu, Takehiko Ohkawa, Tze Ho Elden Tse, Mingfang Zhang, Angela
  Yao, Yoichi Sato
Categories: cs.CV
\\
  This paper presents RPEP, the first pre-training method for event-based 3D
hand pose estimation using labeled RGB images and unpaired, unlabeled event
data. Event data offer significant benefits such as high temporal resolution
and low latency, but their application to hand pose estimation is still limited
by the scarcity of labeled training data. To address this, we repurpose real
RGB datasets to train event-based estimators. This is done by constructing
pseudo-event-RGB pairs, where event data is generated and aligned with the
ground-truth poses of RGB images. Unfortunately, existing pseudo-event
generation techniques assume stationary objects, thus struggling to handle
non-stationary, dynamically moving hands. To overcome this, RPEP introduces a
novel generation strategy that decomposes hand movements into smaller,
step-by-step motions. This decomposition allows our method to capture temporal
changes in articulation, constructing more realistic event data for a moving
hand. Additionally, RPEP imposes a motion reversal constraint, regularizing
event generation using reversed motion. Extensive experiments show that our
pre-trained model significantly outperforms state-of-the-art methods on real
event data, achieving up to 24% improvement on EvRealHands. Moreover, it
delivers strong performance with minimal labeled samples for fine-tuning,
making it well-suited for practical deployment.
\\ ( https://arxiv.org/abs/2509.16949 ,  701kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16956
Date: Sun, 21 Sep 2025 07:34:19 GMT   (4073kb)

Title: VidCLearn: A Continual Learning Approach for Text-to-Video Generation
Authors: Luca Zanchetta, Lorenzo Papa, Luca Maiano, Irene Amerini
Categories: cs.CV
\\
  Text-to-video generation is an emerging field in generative AI, enabling the
creation of realistic, semantically accurate videos from text prompts. While
current models achieve impressive visual quality and alignment with input text,
they typically rely on static knowledge, making it difficult to incorporate new
data without retraining from scratch. To address this limitation, we propose
VidCLearn, a continual learning framework for diffusion-based text-to-video
generation. VidCLearn features a student-teacher architecture where the student
model is incrementally updated with new text-video pairs, and the teacher model
helps preserve previously learned knowledge through generative replay.
Additionally, we introduce a novel temporal consistency loss to enhance motion
smoothness and a video retrieval module to provide structural guidance at
inference. Our architecture is also designed to be more computationally
efficient than existing models while retaining satisfactory generation
performance. Experimental results show VidCLearn's superiority over baseline
methods in terms of visual quality, semantic alignment, and temporal coherence.
\\ ( https://arxiv.org/abs/2509.16956 ,  4073kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16957
Date: Sun, 21 Sep 2025 07:38:54 GMT   (10210kb)

Title: MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote
  Sensing Image
Authors: Leiyu Wang, Biao Jin, Feng Huang, Liqiong Chen, Zhengyong Wang,
  Xiaohai He, Honggang Chen
Categories: cs.CV
\\
  Oriented object detection for multi-spectral imagery faces significant
challenges due to differences both within and between modalities. Although
existing methods have improved detection accuracy through complex network
architectures, their high computational complexity and memory consumption
severely restrict their performance. Motivated by the success of large kernel
convolutions in remote sensing, we propose MO R-CNN, a lightweight framework
for multi-spectral oriented detection featuring heterogeneous feature
extraction network (HFEN), single modality supervision (SMS), and
condition-based multimodal label fusion (CMLF). HFEN leverages inter-modal
differences to adaptively align, merge, and enhance multi-modal features. SMS
constrains multi-scale features and enables the model to learn from multiple
modalities. CMLF fuses multimodal labels based on specific rules, providing the
model with a more robust and consistent supervisory signal. Experiments on the
DroneVehicle, VEDAI and OGSOD datasets prove the superiority of our method. The
source code is available at:https://github.com/Iwill-github/MORCNN.
\\ ( https://arxiv.org/abs/2509.16957 ,  10210kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16968
Date: Sun, 21 Sep 2025 07:58:48 GMT   (27359kb)

Title: Penalizing Boundary Activation for Object Completeness in Diffusion
  Models
Authors: Haoyang Xu, Tianhao Zhao, Sibei Yang and Yutian Li
Categories: cs.CV
\\
  Diffusion models have emerged as a powerful technique for text-to-image (T2I)
generation, creating high-quality, diverse images across various domains.
However, a common limitation in these models is the incomplete display of
objects, where fragments or missing parts undermine the model's performance in
downstream applications. In this study, we conduct an in-depth analysis of the
incompleteness issue and reveal that the primary factor behind incomplete
object generation is the usage of RandomCrop during model training. This widely
used data augmentation method, though enhances model generalization ability,
disrupts object continuity during training. To address this, we propose a
training-free solution that penalizes activation values at image boundaries
during the early denoising steps. Our method is easily applicable to
pre-trained Stable Diffusion models with minimal modifications and negligible
computational overhead. Extensive experiments demonstrate the effectiveness of
our method, showing substantial improvements in object integrity and image
quality.
\\ ( https://arxiv.org/abs/2509.16968 ,  27359kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16970
Date: Sun, 21 Sep 2025 08:05:43 GMT   (4649kb)

Title: LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing
  Object Detection
Authors: Wei Liao, Chunyan Xu, Chenxu Wang, Zhen Cui
Categories: cs.CV
\\
  Sparse annotation in remote sensing object detection poses significant
challenges due to dense object distributions and category imbalances. Although
existing Dense Pseudo-Label methods have demonstrated substantial potential in
pseudo-labeling tasks, they remain constrained by selection ambiguities and
inconsistencies in confidence estimation.In this paper, we introduce an
LLM-assisted semantic guidance framework tailored for sparsely annotated remote
sensing object detection, exploiting the advanced semantic reasoning
capabilities of large language models (LLMs) to distill high-confidence
pseudo-labels.By integrating LLM-generated semantic priors, we propose a
Class-Aware Dense Pseudo-Label Assignment mechanism that adaptively assigns
pseudo-labels for both unlabeled and sparsely labeled data, ensuring robust
supervision across varying data distributions. Additionally, we develop an
Adaptive Hard-Negative Reweighting Module to stabilize the supervised learning
branch by mitigating the influence of confounding background information.
Extensive experiments on DOTA and HRSC2016 demonstrate that the proposed method
outperforms existing single-stage detector-based frameworks, significantly
improving detection performance under sparse annotations.
\\ ( https://arxiv.org/abs/2509.16970 ,  4649kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16972
Date: Sun, 21 Sep 2025 08:08:17 GMT   (3140kb)

Title: The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA
Authors: Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo
  Yuan, Lu Qi, Xiangtai Li, Shunping Ji
Categories: cs.CV cs.AI
Comments: 1st place report of 7th LSVOS RVOS track in ICCV 2025. The code is
  released in Sa2VA repository: https://github.com/magic-research/Sa2VA
\\
  Referring video object segmentation (RVOS) requires segmenting and tracking
objects in videos conditioned on natural-language expressions, demanding
fine-grained understanding of both appearance and motion. Building on Sa2VA,
which couples a Multi-modal Large Language Model (MLLM) with the video
segmentation model SAM2, we identify two key bottlenecks that limit
segmentation performance: sparse frame sampling and reliance on a single [SEG]
token for an entire video. We propose Segmentation Augmented and Selective
Averaged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge
(RVOS track), SaSaSa2VA achieves a $J\&F$ of 67.45, ranking first and
surpassing the runner-up by 2.80 points. This result and ablation studies
demonstrate that efficient segmentation augmentation and test-time ensembling
substantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA
repository: https://github.com/magic-research/Sa2VA.
\\ ( https://arxiv.org/abs/2509.16972 ,  3140kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16977
Date: Sun, 21 Sep 2025 08:25:22 GMT   (226kb)

Title: Optimal Transport for Handwritten Text Recognition in a Low-Resource
  Regime
Authors: Petros Georgoulas Wraight, Giorgos Sfikas, Ioannis Kordonis, Petros
  Maragos, George Retsinas
Categories: cs.CV cs.LG
\\
  Handwritten Text Recognition (HTR) is a task of central importance in the
field of document image understanding. State-of-the-art methods for HTR require
the use of extensive annotated sets for training, making them impractical for
low-resource domains like historical archives or limited-size modern
collections. This paper introduces a novel framework that, unlike the standard
HTR model paradigm, can leverage mild prior knowledge of lexical
characteristics; this is ideal for scenarios where labeled data are scarce. We
propose an iterative bootstrapping approach that aligns visual features
extracted from unlabeled images with semantic word representations using
Optimal Transport (OT). Starting with a minimal set of labeled examples, the
framework iteratively matches word images to text labels, generates
pseudo-labels for high-confidence alignments, and retrains the recognizer on
the growing dataset. Numerical experiments demonstrate that our iterative
visual-semantic alignment scheme significantly improves recognition accuracy on
low-resource HTR benchmarks.
\\ ( https://arxiv.org/abs/2509.16977 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16986
Date: Sun, 21 Sep 2025 09:00:27 GMT   (11645kb)

Title: VCE: Safe Autoregressive Image Generation via Visual Contrast
  Exploitation
Authors: Feng Han, Chao Gong, Zhipeng Wei, Jingjing Chen, Yu-Gang Jiang
Categories: cs.CV
\\
  Recently, autoregressive image generation models have wowed audiences with
their remarkable capability in creating surprisingly realistic images. Models
such as GPT-4o and LlamaGen can not only produce images that faithfully mimic
renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also
potentially generate Not-Safe-For-Work (NSFW) content, raising significant
concerns regarding copyright infringement and ethical use. Despite these
concerns, methods to safeguard autoregressive text-to-image models remain
underexplored. Previous concept erasure methods, primarily designed for
diffusion models that operate in denoising latent space, are not directly
applicable to autoregressive models that generate images token by token. To
address this critical gap, we propose Visual Contrast Exploitation (VCE), a
novel framework comprising: (1) an innovative contrastive image pair
construction paradigm that precisely decouples unsafe concepts from their
associated content semantics, and (2) a sophisticated DPO-based training
approach that enhances the model's ability to identify and leverage visual
contrastive features from image pairs, enabling precise concept erasure. Our
comprehensive experiments across three challenging tasks-artist style erasure,
explicit content erasure, and object removal-demonstrate that our method
effectively secures the model, achieving state-of-the-art results while erasing
unsafe concepts and maintaining the integrity of unrelated safe concepts. The
code and models are available at https://github.com/Maplebb/VCE.
\\ ( https://arxiv.org/abs/2509.16986 ,  11645kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16988
Date: Sun, 21 Sep 2025 09:04:28 GMT   (23112kb)

Title: A Cross-Hierarchical Multi-Feature Fusion Network Based on Multiscale
  Encoder-Decoder for Hyperspectral Change Detection
Authors: Mingshuai Sheng, Bhatti Uzair Aslam, Junfeng Zhang, Siling Feng, Yonis
  Gulzar
Categories: cs.CV
Comments: This work has been submitted to the IEEE for possible publication
\\
  Hyperspectral change detection (HCD) aims to accurately identify land-cover
changes in hyperspectral images of the same area acquired at different times,
with key applications in environmental monitoring and disaster assessment. To
address limitations of existing methods, such as insufficient use of multiscale
features and low efficiency in differential feature fusion, this paper proposes
a cross-hierarchical multi-feature fusion network (CHMFFN) based on a
multiscale encoder-decoder architecture. The front-end adopts a multiscale
feature extraction subnetwork, built on an encoder-decoder backbone with
residual connections and a dual-core channel-spatial attention (DCCSA) module
to extract spectral-spatial-temporal features (SSTF). The encoder captures
multiscale features from shallow details to deep semantics via residual blocks
and convolutional kernels with varying receptive fields. The decoder restores
spatial resolution and suppresses noise information through skip connections
integrating encoder features. Additionally, a spectral-temporal change feature
learning (STCFL) module learns cross-temporal change features at different
levels, strengthening inter-temporal difference capture. An adaptive fusion of
advanced features (AFAF) module dynamically balances hierarchical differential
features via adaptive weights, enhancing representation of complex changes.
Experiments on four public hyperspectral datasets show CHMFFN outperforms
state-of-the-art methods, verifying its effectiveness.
\\ ( https://arxiv.org/abs/2509.16988 ,  23112kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17012
Date: Sun, 21 Sep 2025 10:01:43 GMT   (736kb)

Title: DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image
  Quality Assessment
Authors: Zhichao Ma, Fan Huang, Lu Zhao, Fengjun Guo, Guangtao Zhai, Xiongkuo
  Min
Categories: cs.CV cs.LG eess.IV
\\
  Document image quality assessment (DIQA) is an important component for
various applications, including optical character recognition (OCR), document
restoration, and the evaluation of document image processing systems. In this
paper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset
comprises 5,000 document images, generated by applying multiple document
enhancement techniques to 500 real-world images with diverse distortions. Each
enhanced image was rated by 15 subjects across three rating dimensions: overall
quality, sharpness, and color fidelity. Furthermore, we propose a specialized
no-reference DIQA model that exploits document layout features to maintain
quality perception at reduced resolutions to lower computational cost.
Recognizing that image quality is influenced by both low-level and high-level
visual features, we designed a feature fusion module to extract and integrate
multi-level features from document images. To generate multi-dimensional
scores, our model employs independent quality heads for each dimension to
predict score distributions, allowing it to learn distinct aspects of document
image quality. Experimental results demonstrate that our method outperforms
current state-of-the-art general-purpose IQA models on both DIQA-5000 and an
additional document image dataset focused on OCR accuracy.
\\ ( https://arxiv.org/abs/2509.17012 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17024
Date: Sun, 21 Sep 2025 10:39:06 GMT   (21532kb)

Title: When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image
  Restoration
Authors: Wenxuan Fang, Jili Fan, Chao Wang, Xiantao Hu, Jiangwei Weng, Ying
  Tai, Jian Yang and Jun Li
Categories: cs.CV cs.AI
\\
  Adverse Weather Image Restoration (AWIR) is a highly challenging task due to
the unpredictable and dynamic nature of weather-related degradations.
Traditional task-specific methods often fail to generalize to unseen or complex
degradation types, while recent prompt-learning approaches depend heavily on
the degradation estimation capabilities of vision-language models, resulting in
inconsistent restorations. In this paper, we propose \textbf{LCDiff}, a novel
framework comprising two key components: \textit{Lumina-Chroma Decomposition
Network} (LCDN) and \textit{Lumina-Guided Diffusion Model} (LGDM). LCDN
processes degraded images in the YCbCr color space, separately handling
degradation-related luminance and degradation-invariant chrominance components.
This decomposition effectively mitigates weather-induced degradation while
preserving color fidelity. To further enhance restoration quality, LGDM
leverages degradation-related luminance information as a guiding condition,
eliminating the need for explicit degradation prompts. Additionally, LGDM
incorporates a \textit{Dynamic Time Step Loss} to optimize the denoising
network, ensuring a balanced recovery of both low- and high-frequency features
in the image. Finally, we present DriveWeather, a comprehensive all-weather
driving dataset designed to enable robust evaluation. Extensive experiments
demonstrate that our approach surpasses state-of-the-art methods, setting a new
benchmark in AWIR. The dataset and code are available at:
https://github.com/fiwy0527/LCDiff.
\\ ( https://arxiv.org/abs/2509.17024 ,  21532kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17027
Date: Sun, 21 Sep 2025 10:40:36 GMT   (23110kb)

Title: Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic
  Views
Authors: Zhenya Yang
Categories: cs.CV
Comments: Workshop Paper of AECAI@MICCAI 2025
\\
  Surgical simulation is essential for medical training, enabling practitioners
to develop crucial skills in a risk-free environment while improving patient
safety and surgical outcomes. However, conventional methods for building
simulation environments are cumbersome, time-consuming, and difficult to scale,
often resulting in poor details and unrealistic simulations. In this paper, we
propose a Gaussian Splatting-based framework to directly reconstruct
interactive surgical scenes from endoscopic data while ensuring efficiency,
rendering quality, and realism. A key challenge in this data-driven simulation
paradigm is the restricted movement of endoscopic cameras, which limits
viewpoint diversity. As a result, the Gaussian Splatting representation
overfits specific perspectives, leading to reduced geometric accuracy. To
address this issue, we introduce a novel virtual camera-based regularization
method that adaptively samples virtual viewpoints around the scene and
incorporates them into the optimization process to mitigate overfitting. An
effective depth-based regularization is applied to both real and virtual views
to further refine the scene geometry. To enable fast deformation simulation, we
propose a sparse control node-based Material Point Method, which integrates
physical properties into the reconstructed scene while significantly reducing
computational costs. Experimental results on representative surgical data
demonstrate that our method can efficiently reconstruct and simulate surgical
scenes from sparse endoscopic views. Notably, our method takes only a few
minutes to reconstruct the surgical scene and is able to produce physically
plausible deformations in real-time with user-defined interactions.
\\ ( https://arxiv.org/abs/2509.17027 ,  23110kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17040
Date: Sun, 21 Sep 2025 11:19:02 GMT   (4796kb)

Title: From Easy to Hard: The MIR Benchmark for Progressive Interleaved
  Multi-Image Reasoning
Authors: Hang Du, Jiayang Zhang, Guoshun Nan, Wendi Deng, Zhenyan Chen,
  Chenyang Zhang, Wang Xiao, Shan Huang, Yuqi Pan, Tao Qi, and Sicong Leng
Categories: cs.CV cs.AI
\\
  Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language
Models (MLLMs) ability to jointly comprehend and reason across multiple images
and their associated textual contexts, introducing unique challenges beyond
single-image or non-interleaved multi-image tasks. While current multi-image
benchmarks overlook interleaved textual contexts and neglect distinct
relationships between individual images and their associated texts, enabling
models to reason over multi-image interleaved data may significantly enhance
their comprehension of complex scenes and better capture cross-modal
correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring
joint reasoning over multiple images accompanied by interleaved textual
contexts to accurately associate image regions with corresponding texts and
logically connect information across images. To enhance MLLMs ability to
comprehend multi-image interleaved data, we introduce reasoning steps for each
instance within the benchmark and propose a stage-wise curriculum learning
strategy. This strategy follows an "easy to hard" approach, progressively
guiding models from simple to complex scenarios, thereby enhancing their
ability to handle challenging tasks. Extensive experiments benchmarking
multiple MLLMs demonstrate that our method significantly enhances models
reasoning performance on MIR and other established benchmarks. We believe that
MIR will encourage further research into multi-image interleaved reasoning,
facilitating advancements in MLLMs capability to handle complex inter-modal
tasks.Our code and dataset are available at
https://github.com/Shelly-coder239/MIRBench.
\\ ( https://arxiv.org/abs/2509.17040 ,  4796kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17041
Date: Sun, 21 Sep 2025 11:40:49 GMT   (13282kb)

Title: Towards Generalized Synapse Detection Across Invertebrate Species
Authors: Samia Mohinta, Daniel Franco-Barranco, Shi Yan Lee, Albert Cardona
Categories: cs.CV
\\
  Behavioural differences across organisms, whether healthy or pathological,
are closely tied to the structure of their neural circuits. Yet, the fine-scale
synaptic changes that give rise to these variations remain poorly understood,
in part due to persistent challenges in detecting synapses reliably and at
scale. Volume electron microscopy (EM) offers the resolution required to
capture synaptic architecture, but automated detection remains difficult due to
sparse annotations, morphological variability, and cross-dataset domain shifts.
To address this, we make three key contributions. First, we curate a diverse EM
benchmark spanning four datasets across two invertebrate species: adult and
larval Drosophila melanogaster, and Megaphragma viggianii (micro-WASP). Second,
we propose SimpSyn, a single-stage Residual U-Net trained to predict
dual-channel spherical masks around pre- and post-synaptic sites, designed to
prioritize training and inference speeds and annotation efficiency over
architectural complexity. Third, we benchmark SimpSyn against Buhmann et al.'s
Synful [1], a state-of-the-art multi-task model that jointly infers synaptic
pairs. Despite its simplicity, SimpSyn consistently outperforms Synful in
F1-score across all volumes for synaptic site detection. While generalization
across datasets remains limited, SimpSyn achieves competitive performance when
trained on the combined cohort. Finally, ablations reveal that simple
post-processing strategies - such as local peak detection and distance-based
filtering - yield strong performance without complex test-time heuristics.
Taken together, our results suggest that lightweight models, when aligned with
task structure, offer a practical and scalable solution for synapse detection
in large-scale connectomic pipelines.
\\ ( https://arxiv.org/abs/2509.17041 ,  13282kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17044
Date: Sun, 21 Sep 2025 11:51:57 GMT   (1854kb)

Title: AgriDoctor: A Multimodal Intelligent Assistant for Agriculture
Authors: Mingqing Zhang, Zhuoning Xu, Peijie Wang, Rongji Li, Liang Wang, Qiang
  Liu, Jian Xu, Xuyao Zhang, Shu Wu, Liang Wang
Categories: cs.CV
\\
  Accurate crop disease diagnosis is essential for sustainable agriculture and
global food security. Existing methods, which primarily rely on unimodal models
such as image-based classifiers and object detectors, are limited in their
ability to incorporate domain-specific agricultural knowledge and lack support
for interactive, language-based understanding. Recent advances in large
language models (LLMs) and large vision-language models (LVLMs) have opened new
avenues for multimodal reasoning. However, their performance in agricultural
contexts remains limited due to the absence of specialized datasets and
insufficient domain adaptation. In this work, we propose AgriDoctor, a modular
and extensible multimodal framework designed for intelligent crop disease
diagnosis and agricultural knowledge interaction. As a pioneering effort to
introduce agent-based multimodal reasoning into the agricultural domain,
AgriDoctor offers a novel paradigm for building interactive and domain-adaptive
crop health solutions. It integrates five core components: a router,
classifier, detector, knowledge retriever and LLMs. To facilitate effective
training and evaluation, we construct AgriMM, a comprehensive benchmark
comprising 400000 annotated disease images, 831 expert-curated knowledge
entries, and 300000 bilingual prompts for intent-driven tool selection.
Extensive experiments demonstrate that AgriDoctor, trained on AgriMM,
significantly outperforms state-of-the-art LVLMs on fine-grained agricultural
tasks, establishing a new paradigm for intelligent and sustainable farming
applications.
\\ ( https://arxiv.org/abs/2509.17044 ,  1854kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17049
Date: Sun, 21 Sep 2025 12:14:37 GMT   (2768kb)

Title: Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via
  Query Optimization
Authors: Peng Wang, Yong Li, Lin Zhao, Xiu-Shen Wei
Categories: cs.CV
\\
  Fine-grained hashing has become a powerful solution for rapid and efficient
image retrieval, particularly in scenarios requiring high discrimination
between visually similar categories. To enable each hash bit to correspond to
specific visual attributes, we propoe a novel method that harnesses learnable
queries for attribute-aware hash codes learning. This method deploys a tailored
set of queries to capture and represent nuanced attribute-level information
within the hashing process, thereby enhancing both the interpretability and
relevance of each hash bit. Building on this query-based optimization
framework, we incorporate an auxiliary branch to help alleviate the challenges
of complex landscape optimization often encountered with low-bit hash codes.
This auxiliary branch models high-order attribute interactions, reinforcing the
robustness and specificity of the generated hash codes. Experimental results on
benchmark datasets demonstrate that our method generates attribute-aware hash
codes and consistently outperforms state-of-the-art techniques in retrieval
accuracy and robustness, especially for low-bit hash codes, underscoring its
potential in fine-grained image hashing tasks.
\\ ( https://arxiv.org/abs/2509.17049 ,  2768kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17050
Date: Sun, 21 Sep 2025 12:15:04 GMT   (4339kb)

Title: Geodesic Prototype Matching via Diffusion Maps for Interpretable
  Fine-Grained Recognition
Authors: Junhao Jia, Yunyou Liu, Yifei Sun, Huangwei Chen, Feiwei Qin,
  Changmiao Wang, Yong Peng
Categories: cs.CV
\\
  Nonlinear manifolds are widespread in deep visual features, where Euclidean
distances often fail to capture true similarity. This limitation becomes
particularly severe in prototype-based interpretable fine-grained recognition,
where subtle semantic distinctions are essential. To address this challenge, we
propose a novel paradigm for prototype-based recognition that anchors
similarity within the intrinsic geometry of deep features. Specifically, we
distill the latent manifold structure of each class into a diffusion space and
introduce a differentiable Nystr\"om interpolation, making the geometry
accessible to both unseen samples and learnable prototypes. To ensure
efficiency, we employ compact per-class landmark sets with periodic updates.
This design keeps the embedding aligned with the evolving backbone, enabling
fast and scalable inference. Extensive experiments on the CUB-200-2011 and
Stanford Cars datasets show that our GeoProto framework produces prototypes
focusing on semantically aligned parts, significantly outperforming Euclidean
prototype networks.
\\ ( https://arxiv.org/abs/2509.17050 ,  4339kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17065
Date: Sun, 21 Sep 2025 12:52:08 GMT   (912kb)

Title: CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a
  Few-shot Manner
Authors: Yao Du, Jiarong Guo, Xiaomeng Li
Categories: cs.CV
Comments: Accepted by MICCAI 2025
\\
  Echocardiography is a vital non-invasive modality for cardiac assessment,
with left ventricular ejection fraction (LVEF) serving as a key indicator of
heart function. Existing LVEF estimation methods depend on large-scale
annotated video datasets, which are costly and limit adaptability across
various clinical settings. Recent vision-language models for echocardiography,
such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial
temporal dynamics and localized cardiac structures essential for accurate
diagnosis. To address these challenges, we propose CardiacCLIP, a video-based
framework that enhances LVEF prediction through attention-based frame
aggregation and multi-resolution input scaling. Specifically, we introduce MFL
(Multi Frame Learning), a novel attention-based mechanism for selectively
fusing informative frames, and EchoZoom, a multi-scale feature extraction
strategy that refines spatial representations of cardiac structures. As a novel
adaptation of CLIP models for few-shot echocardiogram video analysis, our
approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on
the EchoNet-Dynamic dataset under 1-shot setting. The code is available at
https://github.com/xmed-lab/CardiacCLIP.
\\ ( https://arxiv.org/abs/2509.17065 ,  912kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17074
Date: Sun, 21 Sep 2025 13:21:16 GMT   (3291kb)

Title: Informative Text-Image Alignment for Visual Affordance Learning with
  Foundation Models
Authors: Qian Zhang, Lin Zhang, Xing Fang, Mingxin Zhang, Zhiyuan Wei, Ran
  Song, Wei Zhang
Categories: cs.CV cs.AI
Comments: Submitted to the IEEE International Conference on Robotics and
  Automation (ICRA) 2026
\\
  Visual affordance learning is crucial for robots to understand and interact
effectively with the physical world. Recent advances in this field attempt to
leverage pre-trained knowledge of vision-language foundation models to learn
affordance properties with limited training data, providing a novel paradigm
for visual affordance learning. However, these methods overlook the
significance of maintaining feature alignment between visual images and
language descriptions for identifying affordance areas with textual guidance,
and thus may lead to suboptimal results. In this paper, we present an
informative framework for text-guided affordance learning, which involves
information-based constraints to achieve text-image alignment at feature level.
Specifically, we design an affordance mutual information constraint that helps
learn appropriate textual prompts and task-oriented visual features
simultaneously by maximizing the mutual information between the features of the
affordance areas in the input images and the corresponding textual prompts. In
addition, we propose an object-level information constraint that maximizes the
mutual information between the visual features of a given object and the text
features of the category it belongs to. This enables the model to capture
high-quality representations for the object, providing more reliable semantic
priors for identifying affordance regions. Experimental results on the AGD20K
dataset show that the proposed method outperforms existing approaches and
achieves the new state-of-the-art in one-shot affordance learning.
\\ ( https://arxiv.org/abs/2509.17074 ,  3291kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17078
Date: Sun, 21 Sep 2025 13:49:51 GMT   (1855kb)

Title: Enhanced Detection of Tiny Objects in Aerial Images
Authors: Kihyun Kim, Michalis Lazarou, Tania Stathaki
Categories: cs.CV
\\
  While one-stage detectors like YOLOv8 offer fast training speed, they often
under-perform on detecting small objects as a trade-off. This becomes even more
critical when detecting tiny objects in aerial imagery due to low-resolution
targets and cluttered backgrounds. To address this, we introduce three
enhancement strategies -- input image resolution adjustment, data augmentation,
and attention mechanisms -- that can be easily implemented on YOLOv8. We
demonstrate that image size enlargement and the proper use of augmentation can
lead to enhancement. Additionally, we designed a Mixture of Orthogonal
Neural-modules Network (MoonNet) pipeline which consists of attention-augmented
CNNs. Two well-known attention modules, the Squeeze-and-Excitation Block (SE
Block) and the Convolutional Block Attention Module (CBAM), were integrated
into the backbone of YOLOv8 with an increased number of channels, and the
MoonNet backbone obtained improved detection accuracy compared to the original
YOLOv8. MoonNet further proved its adaptability and potential by achieving
state-of-the-art performance on a tiny-object benchmark when integrated with
the YOLC model. Our codes are available at: https://github.com/Kihyun11/MoonNet
\\ ( https://arxiv.org/abs/2509.17078 ,  1855kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17079
Date: Sun, 21 Sep 2025 13:52:29 GMT   (1016kb)

Title: A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially
  Modulated Attention and Adaptive Fusion
Authors: Yuhong Feng, Hongtao Chen, Qi Zhang, Jie Chen, Zhaoxi He, Mingzhe Liu,
  Jianghai Liao
Categories: cs.CV
Comments: Submitted to ICASSP 2026
\\
  Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in
challenging conditions. While recent Transformer-based methods excel at
capturing global context, their inherent lack of spatial inductive bias causes
attention to spread to irrelevant background regions, compromising crowd
localization precision. Furthermore, effectively bridging the gap between these
distinct modalities remains a major hurdle. To tackle this, we propose the Dual
Modulation Framework, comprising two modules: Spatially Modulated Attention
(SMA), which improves crowd localization by using a learnable Spatial Decay
Mask to penalize attention between distant tokens and prevent focus from
spreading to the background; and Adaptive Fusion Modulation (AFM), which
implements a dynamic gating mechanism to prioritize the most reliable modality
for adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting
datasets demonstrate the superior performance of our method compared to
previous works. Code available at
https://github.com/Cht2924/RGBT-Crowd-Counting.
\\ ( https://arxiv.org/abs/2509.17079 ,  1016kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17083
Date: Sun, 21 Sep 2025 13:59:26 GMT   (6618kb)

Title: HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel
  View Synthesis
Authors: Zipeng Wang, Dan Xu
Categories: cs.CV
\\
  Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative
to NeRF-based approaches, enabling real-time, high-quality novel view synthesis
through explicit, optimizable 3D Gaussians. However, 3DGS suffers from
significant memory overhead due to its reliance on per-Gaussian parameters to
model view-dependent effects and anisotropic shapes. While recent works propose
compressing 3DGS with neural fields, these methods struggle to capture
high-frequency spatial variations in Gaussian properties, leading to degraded
reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a
novel scene representation that combines the strengths of explicit Gaussians
and neural fields. HyRF decomposes the scene into (1) a compact set of explicit
Gaussians storing only critical high-frequency parameters and (2) grid-based
neural fields that predict remaining properties. To enhance representational
capacity, we introduce a decoupled neural field architecture, separately
modeling geometry (scale, opacity, rotation) and view-dependent color.
Additionally, we propose a hybrid rendering scheme that composites Gaussian
splatting with a neural field-predicted background, addressing limitations in
distant scene representation. Experiments demonstrate that HyRF achieves
state-of-the-art rendering quality while reducing model size by over 20 times
compared to 3DGS and maintaining real-time performance. Our project page is
available at https://wzpscott.github.io/hyrf/.
\\ ( https://arxiv.org/abs/2509.17083 ,  6618kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17084
Date: Sun, 21 Sep 2025 14:02:38 GMT   (3949kb)

Title: MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion
  Vectors
Authors: Binhua Huang, Nan Wang, Arjun Parakash and Soumyabrata Dev
Categories: cs.CV
Comments: 6 pages, 3 figures
\\
  Video action recognition is a fundamental task in computer vision, but
state-of-the-art models are often computationally expensive and rely on
extensive video pre-training. In parallel, large-scale vision-language models
like Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot
capabilities on static images, while motion vectors (MV) provide highly
efficient temporal information directly from compressed video streams. To
synergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple
yet powerful two-stream late fusion framework for efficient video recognition.
Our approach combines features from a frozen CLIP image encoder with features
from a lightweight, supervised network trained on raw MV. During fusion, both
backbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is
trained, ensuring extreme efficiency. Through comprehensive experiments on the
UCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,
significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)
baselines. Our work provides a new, highly efficient baseline for video
understanding that effectively bridges the gap between large static models and
dynamic, low-cost motion cues. Our code and models are available at
https://github.com/microa/MoCLIP-Lite.
\\ ( https://arxiv.org/abs/2509.17084 ,  3949kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17086
Date: Sun, 21 Sep 2025 14:03:48 GMT   (7214kb)

Title: SFN-YOLO: Towards Free-Range Poultry Detection via Scale-aware Fusion
  Networks
Authors: Jie Chen, Yuhong Feng, Tao Dai, Mingzhe Liu, Hongtao Chen, Zhaoxi He,
  Jiancong Bai
Categories: cs.CV
Comments: Submitted to ICASSP 2026
\\
  Detecting and localizing poultry is essential for advancing smart poultry
farming. Despite the progress of detection-centric methods, challenges persist
in free-range settings due to multiscale targets, obstructions, and complex or
dynamic backgrounds. To tackle these challenges, we introduce an innovative
poultry detection approach named SFN-YOLO that utilizes scale-aware fusion.
This approach combines detailed local features with broader global context to
improve detection in intricate environments. Furthermore, we have developed a
new expansive dataset (M-SCOPE) tailored for varied free-range conditions.
Comprehensive experiments demonstrate our model achieves an mAP of 80.7% with
just 7.2M parameters, which is 35.1% fewer than the benchmark, while retaining
strong generalization capability across different domains. The efficient and
real-time detection capabilities of SFN-YOLO support automated smart poultry
farming. The code and dataset can be accessed at
https://github.com/chenjessiee/SFN-YOLO.
\\ ( https://arxiv.org/abs/2509.17086 ,  7214kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17088
Date: Sun, 21 Sep 2025 14:07:25 GMT   (21048kb)

Title: AlignedGen: Aligning Style Across Generated Images
Authors: Jiexuan Zhang, Yiheng Du, Qian Wang, Weiqi Li, Yu Gu, Jian Zhang
Categories: cs.CV
\\
  Despite their generative power, diffusion models struggle to maintain style
consistency across images conditioned on the same style prompt, hindering their
practical deployment in creative workflows. While several training-free methods
attempt to solve this, they are constrained to the U-Net architecture, which
not only leads to low-quality results and artifacts like object repetition but
also renders them incompatible with superior Diffusion Transformer (DiT). To
address these issues, we introduce AlignedGen, a novel training-free framework
that enhances style consistency across images generated by DiT models. Our work
first reveals a critical insight: naive attention sharing fails in DiT due to
conflicting positional signals from improper position embeddings. We introduce
Shifted Position Embedding (ShiftPE), an effective solution that resolves this
conflict by allocating a non-overlapping set of positional indices to each
image. Building on this foundation, we develop Advanced Attention Sharing
(AAS), a suite of three techniques meticulously designed to fully unleash the
potential of attention sharing within the DiT. Furthermore, to broaden the
applicability of our method, we present an efficient query, key, and value
feature extraction algorithm, enabling our method to seamlessly incorporate
external images as style references. Extensive experimental results validate
that our method effectively enhances style consistency across generated images
while maintaining precise text-to-image alignment.
\\ ( https://arxiv.org/abs/2509.17088 ,  21048kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17098
Date: Sun, 21 Sep 2025 14:31:47 GMT   (6500kb)

Title: Uncertainty-Supervised Interpretable and Robust Evidential Segmentation
Authors: Yuzhu Li, An Sui, Fuping Wu, and Xiahai Zhuang
Categories: cs.CV cs.LG
\\
  Uncertainty estimation has been widely studied in medical image segmentation
as a tool to provide reliability, particularly in deep learning approaches.
However, previous methods generally lack effective supervision in uncertainty
estimation, leading to low interpretability and robustness of the predictions.
In this work, we propose a self-supervised approach to guide the learning of
uncertainty. Specifically, we introduce three principles about the
relationships between the uncertainty and the image gradients around boundaries
and noise. Based on these principles, two uncertainty supervision losses are
designed. These losses enhance the alignment between model predictions and
human interpretation. Accordingly, we introduce novel quantitative metrics for
evaluating the interpretability and robustness of uncertainty. Experimental
results demonstrate that compared to state-of-the-art approaches, the proposed
method can achieve competitive segmentation performance and superior results in
out-of-distribution (OOD) scenarios while significantly improving the
interpretability and robustness of uncertainty estimation. Code is available
via https://github.com/suiannaius/SURE.
\\ ( https://arxiv.org/abs/2509.17098 ,  6500kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17100
Date: Sun, 21 Sep 2025 14:41:26 GMT   (7006kb)

Title: The SAGES Critical View of Safety Challenge: A Global Benchmark for
  AI-Assisted Surgical Quality Assessment
Authors: Deepak Alapatt, Jennifer Eckhoff, Zhiliang Lyu, Yutong Ban, Jean-Paul
  Mazellier, Sarah Choksi, Kunyi Yang, 2024 CVS Challenge Consortium, Quanzheng
  Li, Filippo Filicori, Xiang Li, Pietro Mascagni, Daniel A. Hashimoto, Guy
  Rosman, Ozanan Meireles, Nicolas Padoy
Categories: cs.CV
Comments: 18 pages, 10 figures
MSC-class: 68T07
ACM-class: I.2.10; J.3
\\
  Advances in artificial intelligence (AI) for surgical quality assessment
promise to democratize access to expertise, with applications in training,
guidance, and accreditation. This study presents the SAGES Critical View of
Safety (CVS) Challenge, the first AI competition organized by a surgical
society, using the CVS in laparoscopic cholecystectomy, a universally
recommended yet inconsistently performed safety step, as an exemplar of
surgical quality assessment. A global collaboration across 54 institutions in
24 countries engaged hundreds of clinicians and engineers to curate 1,000
videos annotated by 20 surgical experts according to a consensus-validated
protocol. The challenge addressed key barriers to real-world deployment in
surgery, including achieving high performance, capturing uncertainty in
subjective assessment, and ensuring robustness to clinical variability. To
enable this scale of effort, we developed EndoGlacier, a framework for managing
large, heterogeneous surgical video and multi-annotator workflows. Thirteen
international teams participated, achieving up to a 17\% relative gain in
assessment performance, over 80\% reduction in calibration error, and a 17\%
relative improvement in robustness over the state-of-the-art. Analysis of
results highlighted methodological trends linked to model performance,
providing guidance for future research toward robust, clinically deployable AI
for surgical quality assessment.
\\ ( https://arxiv.org/abs/2509.17100 ,  7006kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17107
Date: Sun, 21 Sep 2025 14:56:05 GMT   (1152kb)

Title: CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic
  Mixture-of-Experts for Collaborative Perception
Authors: Lingzhao Kong, Jiacheng Lin, Siyu Li, Kai Luo, Zhiyong Li, Kailun Yang
Categories: cs.CV cs.RO eess.IV
Comments: The source code will be made publicly available at
  https://github.com/godk0509/CoBEVMoE
\\
  Collaborative perception aims to extend sensing coverage and improve
perception accuracy by sharing information among multiple agents. However, due
to differences in viewpoints and spatial positions, agents often acquire
heterogeneous observations. Existing intermediate fusion methods primarily
focus on aligning similar features, often overlooking the perceptual diversity
among agents. To address this limitation, we propose CoBEVMoE, a novel
collaborative perception framework that operates in the Bird's Eye View (BEV)
space and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In
DMoE, each expert is dynamically generated based on the input features of a
specific agent, enabling it to extract distinctive and reliable cues while
attending to shared semantics. This design allows the fusion process to
explicitly model both feature similarity and heterogeneity across agents.
Furthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance
inter-expert diversity and improve the discriminability of the fused
representation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets
demonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,
it improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the
AP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the
effectiveness of expert-based heterogeneous feature modeling in multi-agent
collaborative perception. The source code will be made publicly available at
https://github.com/godk0509/CoBEVMoE.
\\ ( https://arxiv.org/abs/2509.17107 ,  1152kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17120
Date: Sun, 21 Sep 2025 15:19:08 GMT   (4658kb)

Title: Stencil: Subject-Driven Generation with Context Guidance
Authors: Gordon Chen, Ziqi Huang, Cheston Tan, Ziwei Liu
Categories: cs.CV
Comments: Accepted as Spotlight at ICIP 2025
Journal-ref: Proc. IEEE Int. Conf. Image Process. (ICIP), Anchorage, AK, USA,
  Sept. 14-17, 2025, pp. 719-724
DOI: 10.1109/ICIP55913.2025.11084422
\\
  Recent text-to-image diffusion models can generate striking visuals from text
prompts, but they often fail to maintain subject consistency across generations
and contexts. One major limitation of current fine-tuning approaches is the
inherent trade-off between quality and efficiency. Fine-tuning large models
improves fidelity but is computationally expensive, while fine-tuning
lightweight models improves efficiency but compromises image fidelity.
Moreover, fine-tuning pre-trained models on a small set of images of the
subject can damage the existing priors, resulting in suboptimal results. To
this end, we present Stencil, a novel framework that jointly employs two
diffusion models during inference. Stencil efficiently fine-tunes a lightweight
model on images of the subject, while a large frozen pre-trained model provides
contextual guidance during inference, injecting rich priors to enhance
generation with minimal overhead. Stencil excels at generating high-fidelity,
novel renditions of the subject in less than a minute, delivering
state-of-the-art performance and setting a new benchmark in subject-driven
generation.
\\ ( https://arxiv.org/abs/2509.17120 ,  4658kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17136
Date: Sun, 21 Sep 2025 15:58:31 GMT   (1616kb)

Title: SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision
  Inspection with Multimodal LLM
Authors: Yuhao Tian, Zheming Yang
Categories: cs.CV cs.AI
Comments: 5 pages, 5 figures
\\
  Industrial vision inspection requires high accuracy under stringent resource
constraints, yet existing approaches face a fundamental trade-off. Multimodal
LLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive
computational costs, while lightweight edge models often fail on complex cases.
In this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative
industrial vision inspection framework with MLLM. The framework is composed of
three synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect
Inspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)
Adaptive Edge-Cloud Scheduler. Together, these modules enable robust defect
detection by tailoring multimodal reasoning to scene complexity and dynamically
balancing computation between edge and cloud resources. Experimental results on
MVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%
accuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It
also reduces runtime by up to 22.4% and cuts energy per correct decision by
40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.
\\ ( https://arxiv.org/abs/2509.17136 ,  1616kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17172
Date: Sun, 21 Sep 2025 17:36:42 GMT   (22kb)

Title: SynergyNet: Fusing Generative Priors and State-Space Models for Facial
  Beauty Prediction
Authors: Djamel Eddine Boukhari
Categories: cs.CV
\\
  The automated prediction of facial beauty is a benchmark task in affective
computing that requires a sophisticated understanding of both local aesthetic
details (e.g., skin texture) and global facial harmony (e.g., symmetry,
proportions). Existing models, based on either Convolutional Neural Networks
(CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases
that limit their performance; CNNs excel at local feature extraction but
struggle with long-range dependencies, while ViTs model global relationships at
a significant computational cost. This paper introduces the
\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture
that resolves this trade-off by delegating specialized roles to
state-of-the-art models. The first stream leverages a frozen U-Net encoder from
a pre-trained latent diffusion model, providing a powerful generative prior for
fine-grained aesthetic qualities. The second stream employs a Vision Mamba
(Vim), a modern state-space model, to efficiently capture global facial
structure with linear-time complexity. By synergistically integrating these
complementary representations through a cross-attention mechanism, MD-Net
creates a holistic and nuanced feature space for prediction. Evaluated on the
SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson
Correlation of \textbf{0.9235} and demonstrating the significant potential of
hybrid architectures that fuse generative and sequential modeling paradigms for
complex visual assessment tasks.
\\ ( https://arxiv.org/abs/2509.17172 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17187
Date: Sun, 21 Sep 2025 18:16:06 GMT   (1504kb)

Title: Ambiguous Medical Image Segmentation Using Diffusion Schr\"{o}dinger
  Bridge
Authors: Lalith Bharadwaj Baru, Kamalaker Dadi, Tapabrata Chakraborti, Raju S.
  Bapi
Categories: cs.CV cs.AI
Comments: MICCAI 2025 (11 pages, 2 figures, 1 table, and 26 references)
\\
  Accurate segmentation of medical images is challenging due to unclear lesion
boundaries and mask variability. We introduce \emph{Segmentation Sch\"{o}dinger
Bridge (SSB)}, the first application of Sch\"{o}dinger Bridge for ambiguous
medical image segmentation, modelling joint image-mask dynamics to enhance
performance. SSB preserves structural integrity, delineates unclear boundaries
without additional guidance, and maintains diversity using a novel loss
function. We further propose the \emph{Diversity Divergence Index} ($D_{DDI}$)
to quantify inter-rater variability, capturing both diversity and consensus.
SSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER
(in-house) datasets.
\\ ( https://arxiv.org/abs/2509.17187 ,  1504kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17190
Date: Sun, 21 Sep 2025 18:31:28 GMT   (1342kb)

Title: Echo-Path: Pathology-Conditioned Echo Video Generation
Authors: Kabir Hamzah Muhammad, Marawan Elbatel, Yi Qin, Xiaomeng Li
Categories: cs.CV cs.AI
Comments: 10 pages, 3 figures, MICCAI-AMAI2025 Workshop
\\
  Cardiovascular diseases (CVDs) remain the leading cause of mortality
globally, and echocardiography is critical for diagnosis of both common and
congenital cardiac conditions. However, echocardiographic data for certain
pathologies are scarce, hindering the development of robust automated diagnosis
models. In this work, we propose Echo-Path, a novel generative framework to
produce echocardiogram videos conditioned on specific cardiac pathologies.
Echo-Path can synthesize realistic ultrasound video sequences that exhibit
targeted abnormalities, focusing here on atrial septal defect (ASD) and
pulmonary arterial hypertension (PAH). Our approach introduces a
pathology-conditioning mechanism into a state-of-the-art echo video generator,
allowing the model to learn and control disease-specific structural and motion
patterns in the heart. Quantitative evaluation demonstrates that the synthetic
videos achieve low distribution distances, indicating high visual fidelity.
Clinically, the generated echoes exhibit plausible pathology markers.
Furthermore, classifiers trained on our synthetic data generalize well to real
data and, when used to augment real training sets, it improves downstream
diagnosis of ASD and PAH by 7\% and 8\% respectively. Code, weights and dataset
are available here https://github.com/Marshall-mk/EchoPathv1
\\ ( https://arxiv.org/abs/2509.17190 ,  1342kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17191
Date: Sun, 21 Sep 2025 18:36:54 GMT   (1478kb)

Title: VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery
Authors: Jinchao Ge, Tengfei Cheng, Biao Wu, Zeyu Zhang, Shiya Huang, Judith
  Bishop, Gillian Shepherd, Meng Fang, Ling Chen, Yang Zhao
Categories: cs.CV cs.CL
\\
  Analyzing cultural-heritage artifacts remains challenging for MLLMs: general
models lack domain expertise, and SFT often overfits superficial patterns,
yielding brittle reasoning for authentication and historical attribution. This
raises the question of how to equip MLLMs with robust, expert-level reasoning
for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns
evaluation into supervision: we construct a taxonomy of question types, probe
the SFT model to localize type-specific performance gaps, and optimize with
type-conditioned, compositionality-oriented rewards targeting those gaps. We
also release VaseVQA, a comprehensive benchmark of 31,773 images designed to
probe deep understanding. Experiments show state-of-the-art results on style
classification and historical attribution with marked gains in compositional
robustness over SFT-only baselines, validating diagnosis-guided,
taxonomy-conditioned reward engineering and providing a reusable resource for
future research. Code and dataset will be available at
https://github.com/AIGeeksGroup/VaseVQA.
\\ ( https://arxiv.org/abs/2509.17191 ,  1478kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17206
Date: Sun, 21 Sep 2025 19:19:36 GMT   (1609kb)

Title: Guided and Unguided Conditional Diffusion Mechanisms for Structured and
  Semantically-Aware 3D Point Cloud Generation
Authors: Gunner Stone, Sushmita Sarker, Alireza Tavakkoli
Categories: cs.CV cs.AI cs.LG
\\
  Generating realistic 3D point clouds is a fundamental problem in computer
vision with applications in remote sensing, robotics, and digital object
modeling. Existing generative approaches primarily capture geometry, and when
semantics are considered, they are typically imposed post hoc through external
segmentation or clustering rather than integrated into the generative process
itself. We propose a diffusion-based framework that embeds per-point semantic
conditioning directly within generation. Each point is associated with a
conditional variable corresponding to its semantic label, which guides the
diffusion dynamics and enables the joint synthesis of geometry and semantics.
This design produces point clouds that are both structurally coherent and
segmentation-aware, with object parts explicitly represented during synthesis.
Through a comparative analysis of guided and unguided diffusion processes, we
demonstrate the significant impact of conditional variables on diffusion
dynamics and generation quality. Extensive experiments validate the efficacy of
our approach, producing detailed and accurate 3D point clouds tailored to
specific parts and features.
\\ ( https://arxiv.org/abs/2509.17206 ,  1609kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17207
Date: Sun, 21 Sep 2025 19:25:25 GMT   (1209kb)

Title: Point-RTD: Replaced Token Denoising for Pretraining Transformer Models
  on Point Clouds
Authors: Gunner Stone, Youngsook Choi, Alireza Tavakkoli, Ankita Shukla
Categories: cs.CV cs.AI cs.LG
\\
  Pre-training strategies play a critical role in advancing the performance of
transformer-based models for 3D point cloud tasks. In this paper, we introduce
Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to
improve token robustness through a corruption-reconstruction framework. Unlike
traditional mask-based reconstruction tasks that hide data segments for later
prediction, Point-RTD corrupts point cloud tokens and leverages a
discriminator-generator architecture for denoising. This shift enables more
effective learning of structural priors and significantly enhances model
performance and efficiency. On the ShapeNet dataset, Point-RTD reduces
reconstruction error by over 93% compared to PointMAE, and achieves more than
14x lower Chamfer Distance on the test set. Our method also converges faster
and yields higher classification accuracy on ShapeNet, ModelNet10, and
ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework
in every case.
\\ ( https://arxiv.org/abs/2509.17207 ,  1209kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17220
Date: Sun, 21 Sep 2025 20:00:33 GMT   (690kb)

Title: MirrorSAM2: Segment Mirror in Videos with Depth Perception
Authors: Mingchen Xu, Yukun Lai, Ze Ji, Jing Wu
Categories: cs.CV
Comments: 8 pages
\\
  This paper presents MirrorSAM2, the first framework that adapts Segment
Anything Model 2 (SAM2) to the task of RGB-D video mirror segmentation.
MirrorSAM2 addresses key challenges in mirror detection, such as reflection
ambiguity and texture confusion, by introducing four tailored modules: a Depth
Warping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point
Prompt Generator for automatic prompt generation, a Frequency Detail Attention
Fusion Module to enhance structural boundaries, and a Mirror Mask Decoder with
a learnable mirror token for refined segmentation. By fully leveraging the
complementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities
to the prompt-free setting. To our knowledge, this is the first work to enable
SAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD
benchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under
challenging conditions such as small mirrors, weak boundaries, and strong
reflections.
\\ ( https://arxiv.org/abs/2509.17220 ,  690kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17232
Date: Sun, 21 Sep 2025 20:45:22 GMT   (730kb)

Title: DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for
  Neural Radiance Fields in 3D Reconstruction
Authors: Bo Liu, Runlong Li, Li Zhou, Yan Zhou
Categories: cs.CV
Comments: 15 pages
\\
  This paper proposes a Diffusion Model-Optimized Neural Radiance Field
(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency
in 3D scene reconstruction. By combining diffusion models with Transformers,
DT-NeRF effectively restores details under sparse viewpoints and maintains high
accuracy in complex geometric scenes. Experimental results demonstrate that
DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art
methods on the Matterport3D and ShapeNet datasets, particularly in metrics such
as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further
confirm the critical role of the diffusion and Transformer modules in the
model's performance, with the removal of either module leading to a decline in
performance. The design of DT-NeRF showcases the synergistic effect between
modules, providing an efficient and accurate solution for 3D scene
reconstruction. Future research may focus on further optimizing the model,
exploring more advanced generative models and network architectures to enhance
its performance in large-scale dynamic scenes.
\\ ( https://arxiv.org/abs/2509.17232 ,  730kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17246
Date: Sun, 21 Sep 2025 21:37:56 GMT   (2807kb)

Title: SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting
  from Sparse Views
Authors: Ranran Huang, Krystian Mikolajczyk
Categories: cs.CV
\\
  We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian
splatting from sparse multi-view images, requiring no ground-truth poses during
training and inference. It employs a shared feature extraction backbone,
enabling simultaneous prediction of 3D Gaussian primitives and camera poses in
a canonical space from unposed inputs. A masked attention mechanism is
introduced to efficiently estimate target poses during training, while a
reprojection loss enforces pixel-aligned Gaussian primitives, providing
stronger geometric constraints. We further demonstrate the compatibility of our
training framework with different reconstruction architectures, resulting in
two model variants. Remarkably, despite the absence of pose supervision, our
method achieves state-of-the-art performance in both in-domain and
out-of-domain novel view synthesis, even under extreme viewpoint changes and
limited image overlap, and surpasses recent methods that rely on geometric
supervision for relative pose estimation. By eliminating dependence on
ground-truth poses, our method offers the scalability to leverage larger and
more diverse datasets. Code and pretrained models will be available on our
project page: https://ranrhuang.github.io/spfsplatv2/.
\\ ( https://arxiv.org/abs/2509.17246 ,  2807kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17262
Date: Sun, 21 Sep 2025 22:35:19 GMT   (2642kb)

Title: Optimized Learned Image Compression for Facial Expression Recognition
Authors: Xiumei Li, Marc Windsheimer, Misha Sadeghi, Bj\"orn Eskofier, Andr\'e
  Kaup
Categories: cs.CV cs.MM
Comments: Accepted at ICIP 2025
\\
  Efficient data compression is crucial for the storage and transmission of
visual data. However, in facial expression recognition (FER) tasks, lossy
compression often leads to feature degradation and reduced accuracy. To address
these challenges, this study proposes an end-to-end model designed to preserve
critical features and enhance both compression and recognition performance. A
custom loss function is introduced to optimize the model, tailored to balance
compression and recognition performance effectively. This study also examines
the influence of varying loss term weights on this balance. Experimental
results indicate that fine-tuning the compression model alone improves
classification accuracy by 0.71% and compression efficiency by 49.32%, while
joint optimization achieves significant gains of 4.04% in accuracy and 89.12%
in efficiency. Moreover, the findings demonstrate that the jointly optimized
classification model maintains high accuracy on both compressed and
uncompressed data, while the compression model reliably preserves image
details, even at high compression rates.
\\ ( https://arxiv.org/abs/2509.17262 ,  2642kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17282
Date: Sun, 21 Sep 2025 23:40:11 GMT   (1688kb)

Title: Task-Oriented Communications for 3D Scene Representation: Balancing
  Timeliness and Fidelity
Authors: Xiangmin Xu, Zhen Meng, Kan Chen, Jiaming Yang, Emma Li, Philip G.
  Zhao, David Flynn
Categories: cs.CV cs.NI
Comments: Submitted to IEEE Transactions on Mobile Computing
\\
  Real-time Three-dimensional (3D) scene representation is a foundational
element that supports a broad spectrum of cutting-edge applications, including
digital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and
the emerging metaverse. Despite advancements in real-time communication and
computing, achieving a balance between timeliness and fidelity in 3D scene
representation remains a challenge. This work investigates a wireless network
where multiple homogeneous mobile robots, equipped with cameras, capture an
environment and transmit images to an edge server over channels for 3D
representation. We propose a contextual-bandit Proximal Policy Optimization
(PPO) framework incorporating both Age of Information (AoI) and semantic
information to optimize image selection for representation, balancing data
freshness and representation quality. Two policies -- the $\omega$-threshold
and $\omega$-wait policies -- together with two benchmark methods are
evaluated, timeliness embedding and weighted sum, on standard datasets and
baseline 3D scene representation models. Experimental results demonstrate
improved representation fidelity while maintaining low latency, offering
insight into the model's decision-making process. This work advances real-time
3D scene representation by optimizing the trade-off between timeliness and
fidelity in dynamic environments.
\\ ( https://arxiv.org/abs/2509.17282 ,  1688kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17283
Date: Sun, 21 Sep 2025 23:41:44 GMT   (379kb)

Title: Automated Facility Enumeration for Building Compliance Checking using
  Door Detection and Large Language Models
Authors: Licheng Zhan, Bach Le, Naveed Akhtar and Tuan Ngo
Categories: cs.CV cs.AI cs.ET
\\
  Building compliance checking (BCC) is a critical process for ensuring that
constructed facilities meet regulatory standards. A core component of BCC is
the accurate enumeration of facility types and their spatial distribution.
Despite its importance, this problem has been largely overlooked in the
literature, posing a significant challenge for BCC and leaving a critical gap
in existing workflows. Performing this task manually is time-consuming and
labor-intensive. Recent advances in large language models (LLMs) offer new
opportunities to enhance automation by combining visual recognition with
reasoning capabilities. In this paper, we introduce a new task for BCC:
automated facility enumeration, which involves validating the quantity of each
facility type against statutory requirements. To address it, we propose a novel
method that integrates door detection with LLM-based reasoning. We are the
first to apply LLMs to this task and further enhance their performance through
a Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse
datasets and facility types. Experiments on both real-world and synthetic floor
plan data demonstrate the effectiveness and robustness of our method.
\\ ( https://arxiv.org/abs/2509.17283 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17323
Date: Mon, 22 Sep 2025 02:58:04 GMT   (2581kb)

Title: DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory
  Refinement for Multi-Object Tracking
Authors: Buyin Deng, Lingxin Huang, Kai Luo, Fei Teng, Kailun Yang
Categories: cs.CV cs.RO eess.IV
Comments: The source code will be made publicly available at
  https://github.com/warriordby/DepTR-MOT
\\
  Visual Multi-Object Tracking (MOT) is a crucial component of robotic
perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D
cues, such as bounding boxes and motion modeling, which struggle under
occlusions and close-proximity interactions. Trackers relying on these 2D cues
are particularly unreliable in robotic environments, where dense targets and
frequent occlusions are common. While depth information has the potential to
alleviate these issues, most existing MOT datasets lack depth annotations,
leading to its underexploited role in the domain. To unveil the potential of
depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based
detector enhanced with instance-level depth information. Specifically, we
propose two key innovations: (i) foundation model-based instance-level soft
depth label supervision, which refines depth prediction, and (ii) the
distillation of dense depth maps to maintain global depth consistency. These
strategies enable DepTR-MOT to output instance-level depth during inference,
without requiring foundation models and without additional computational cost.
By incorporating depth cues, our method enhances the robustness of the TBD
paradigm, effectively resolving occlusion and close-proximity challenges.
Experiments on both the QuadTrack and DanceTrack datasets demonstrate the
effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,
respectively. In particular, results on QuadTrack, a robotic platform MOT
dataset, highlight the advantages of our method in handling occlusion and
close-proximity challenges in robotic tracking. The source code will be made
publicly available at https://github.com/warriordby/DepTR-MOT.
\\ ( https://arxiv.org/abs/2509.17323 ,  2581kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17328
Date: Mon, 22 Sep 2025 03:04:53 GMT   (4111kb)

Title: UIPro: Unleashing Superior Interaction Capability For GUI Agents
Authors: Hongxin Li, Jingran Su, Jingfan Chen, Zheng Ju, Yuntao Chen, Qing Li,
  Zhaoxiang Zhang
Categories: cs.CV cs.HC
Comments: Accepted to ICCV 2025
\\
  Building autonomous agents that perceive and operate graphical user
interfaces (GUIs) like humans has long been a vision in the field of artificial
intelligence. Central to these agents is the capability for GUI interaction,
which involves GUI understanding and planning capabilities. Existing methods
have tried developing GUI agents based on the multi-modal comprehension ability
of vision-language models (VLMs). However, the limited scenario, insufficient
size, and heterogeneous action spaces hinder the progress of building
generalist GUI agents. To resolve these issues, this paper proposes
\textbf{UIPro}, a novel generalist GUI agent trained with extensive
multi-platform and multi-task GUI interaction data, coupled with a unified
action space. We first curate a comprehensive dataset encompassing 20.6 million
GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding
capability, which is key to downstream GUI agent tasks. Subsequently, we
establish a unified action space to harmonize heterogeneous GUI agent task
datasets and produce a merged dataset to foster the action prediction ability
of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's
superior performance across multiple GUI task benchmarks on various platforms,
highlighting the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2509.17328 ,  4111kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17329
Date: Mon, 22 Sep 2025 03:05:22 GMT   (20490kb)

Title: SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene
  Reconstruction
Authors: Neham Jain, Andrew Jong, Sebastian Scherer, Ioannis Gkioulekas
Categories: cs.CV
Comments: Project website: https://imaging.cs.cmu.edu/smokeseer
\\
  Smoke in real-world scenes can severely degrade the quality of images and
hamper visibility. Recent methods for image restoration either rely on
data-driven priors that are susceptible to hallucinations, or are limited to
static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D
scene reconstruction and smoke removal from a video capturing multiple views of
a scene. Our method uses thermal and RGB images, leveraging the fact that the
reduced scattering in thermal images enables us to see through the smoke. We
build upon 3D Gaussian splatting to fuse information from the two image
modalities, and decompose the scene explicitly into smoke and non-smoke
components. Unlike prior approaches, SmokeSeer handles a broad range of smoke
densities and can adapt to temporally varying smoke. We validate our approach
on synthetic data and introduce a real-world multi-view smoke dataset with RGB
and thermal images. We provide open-source code and data at the project
website.
\\ ( https://arxiv.org/abs/2509.17329 ,  20490kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17365
Date: Mon, 22 Sep 2025 05:32:52 GMT   (820kb)

Title: Pre-Trained CNN Architecture for Transformer-Based Image Caption
  Generation Model
Authors: Amanuel Tafese Dufera
Categories: cs.CV cs.AI
\\
  Automatic image captioning, a multifaceted task bridging computer vision and
natural lan- guage processing, aims to generate descriptive textual content
from visual input. While Convolutional Neural Networks (CNNs) and Long
Short-Term Memory (LSTM) networks have achieved significant advancements, they
present limitations. The inherent sequential nature of RNNs leads to sluggish
training and inference times. LSTMs further struggle with retaining information
from earlier sequence elements when dealing with very long se- quences. This
project presents a comprehensive guide to constructing and comprehending
transformer models for image captioning. Transformers employ self-attention
mechanisms, capturing both short- and long-range dependencies within the data.
This facilitates efficient parallelization during both training and inference
phases. We leverage the well-established Transformer architecture, recognized
for its effectiveness in managing sequential data, and present a meticulous
methodology. Utilizing the Flickr30k dataset, we conduct data pre- processing,
construct a model architecture that integrates an EfficientNetB0 CNN for fea-
ture extraction, and train the model with attention mechanisms incorporated.
Our approach exemplifies the utilization of parallelization for efficient
training and inference. You can find the project on GitHub.
\\ ( https://arxiv.org/abs/2509.17365 ,  820kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17374
Date: Mon, 22 Sep 2025 06:24:42 GMT   (40759kb)

Title: Revisiting Vision Language Foundations for No-Reference Image Quality
  Assessment
Authors: Ankit Yadav, Ta Duc Huy, Lingqiao Liu
Categories: cs.CV
Comments: 23 pages, 16 figures
\\
  Large-scale vision language pre-training has recently shown promise for
no-reference image-quality assessment (NR-IQA), yet the relative merits of
modern Vision Transformer foundations remain poorly understood. In this work,
we present the first systematic evaluation of six prominent pretrained
backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task
of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an
identical lightweight MLP head. Our study uncovers two previously overlooked
factors: (1) SigLIP2 consistently achieves strong performance; and (2) the
choice of activation function plays a surprisingly crucial role, particularly
for enhancing the generalization ability of image quality assessment models.
Notably, we find that simple sigmoid activations outperform commonly used ReLU
and GELU on several benchmarks. Motivated by this finding, we introduce a
learnable activation selection mechanism that adaptively determines the
nonlinearity for each channel, eliminating the need for manual activation
design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and
AGIQA3K. Extensive ablations confirm the benefits across architectures and
regimes, establishing strong, resource-efficient NR-IQA baselines.
\\ ( https://arxiv.org/abs/2509.17374 ,  40759kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17397
Date: Mon, 22 Sep 2025 06:57:06 GMT   (9460kb)

Title: Diff-GNSS: Diffusion-based Pseudorange Error Estimation
Authors: Jiaqi Zhu, Shouyi Lu, Ziyao Li, Guirong Zhuo, and Lu Xiong
Categories: cs.CV cs.ET
\\
  Global Navigation Satellite Systems (GNSS) are vital for reliable urban
positioning. However, multipath and non-line-of-sight reception often introduce
large measurement errors that degrade accuracy. Learning-based methods for
predicting and compensating pseudorange errors have gained traction, but their
performance is limited by complex error distributions. To address this
challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement
(pseudorange) error estimation framework that leverages a conditional diffusion
model to capture such complex distributions. Firstly, a Mamba-based module
performs coarse estimation to provide an initial prediction with appropriate
scale and trend. Then, a conditional denoising diffusion layer refines the
estimate, enabling fine-grained modeling of pseudorange errors. To suppress
uncontrolled generative diversity and achieve controllable synthesis, three key
features related to GNSS measurement quality are used as conditions to
precisely guide the reverse denoising process. We further incorporate
per-satellite uncertainty modeling within the diffusion stage to assess the
reliability of the predicted errors. We have collected and publicly released a
real-world dataset covering various scenes. Experiments on public and
self-collected datasets show that DiffGNSS consistently outperforms
state-of-the-art baselines across multiple metrics. To the best of our
knowledge, this is the first application of diffusion models to pseudorange
error estimation. The proposed diffusion-based refinement module is
plug-and-play and can be readily integrated into existing networks to markedly
improve estimation accuracy.
\\ ( https://arxiv.org/abs/2509.17397 ,  9460kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17401
Date: Mon, 22 Sep 2025 07:00:57 GMT   (40851kb)

Title: Interpreting vision transformers via residual replacement model
Authors: Jinyeong Kim, Junhyeok Kim, Yumin Shim, Joohyeok Kim, Sunyoung Jung,
  Seong Jae Hwang
Categories: cs.CV cs.AI
\\
  How do vision transformers (ViTs) represent and process the world? This paper
addresses this long-standing question through the first systematic analysis of
6.6K features across all layers, extracted via sparse autoencoders, and by
introducing the residual replacement model, which replaces ViT computations
with interpretable features in the residual stream. Our analysis reveals not
only a feature evolution from low-level patterns to high-level semantics, but
also how ViTs encode curves and spatial positions through specialized feature
types. The residual replacement model scalably produces a faithful yet
parsimonious circuit for human-scale interpretability by significantly
simplifying the original computations. As a result, this framework enables
intuitive understanding of ViT mechanisms. Finally, we demonstrate the utility
of our framework in debiasing spurious correlations.
\\ ( https://arxiv.org/abs/2509.17401 ,  40851kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17406
Date: Mon, 22 Sep 2025 07:02:48 GMT   (407kb)

Title: Real-Time Fish Detection in Indonesian Marine Ecosystems Using
  Lightweight YOLOv10-nano Architecture
Authors: Jonathan Wuntu, Muhamad Dwisnanto Putro, Rendy Syahputra
Categories: cs.CV cs.AI
\\
  Indonesia's marine ecosystems, part of the globally recognized Coral
Triangle, are among the richest in biodiversity, requiring efficient monitoring
tools to support conservation. Traditional fish detection methods are
time-consuming and demand expert knowledge, prompting the need for automated
solutions. This study explores the implementation of YOLOv10-nano, a
state-of-the-art deep learning model, for real-time marine fish detection in
Indonesian waters, using test data from Bunaken National Marine Park. YOLOv10's
architecture, featuring improvements like the CSPNet backbone, PAN for feature
fusion, and Pyramid Spatial Attention Block, enables efficient and accurate
object detection even in complex environments. The model was evaluated on the
DeepFish and OpenImages V7-Fish datasets. Results show that YOLOv10-nano
achieves a high detection accuracy with mAP50 of 0.966 and mAP50:95 of 0.606
while maintaining low computational demand (2.7M parameters, 8.4 GFLOPs). It
also delivered an average inference speed of 29.29 FPS on the CPU, making it
suitable for real-time deployment. Although OpenImages V7-Fish alone provided
lower accuracy, it complemented DeepFish in enhancing model robustness.
Overall, this study demonstrates YOLOv10-nano's potential for efficient,
scalable marine fish monitoring and conservation applications in data-limited
environments.
\\ ( https://arxiv.org/abs/2509.17406 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17427
Date: Mon, 22 Sep 2025 07:20:30 GMT   (31133kb)

Title: Single-Image Depth from Defocus with Coded Aperture and Diffusion
  Posterior Sampling
Authors: Hodaka Kawachi, Jose Reinaldo Cunha Santos A. V. Silva Neto, Yasushi
  Yagi, Hajime Nagahara and Tomoya Nakamura
Categories: cs.CV
\\
  We propose a single-snapshot depth-from-defocus (DFD) reconstruction method
for coded-aperture imaging that replaces hand-crafted priors with a learned
diffusion prior used purely as regularization. Our optimization framework
enforces measurement consistency via a differentiable forward model while
guiding solutions with the diffusion prior in the denoised image domain,
yielding higher accuracy and stability than clas- sical optimization. Unlike
U-Net-style regressors, our approach requires no paired defocus-RGBD training
data and does not tie training to a specific camera configuration. Experiments
on comprehensive simulations and a prototype camera demonstrate consistently
strong RGBD reconstructions across noise levels, outperforming both U-Net
baselines and a classical coded- aperture DFD method.
\\ ( https://arxiv.org/abs/2509.17427 ,  31133kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17429
Date: Mon, 22 Sep 2025 07:22:27 GMT   (8409kb)

Title: Multi-scale Temporal Prediction via Incremental Generation and
  Multi-agent Collaboration
Authors: Zhitao Zeng and Guojian Yuan and Junyuan Mao and Yuxuan Wang and
  Xiaoshuang Jia and Yueming Jin
Categories: cs.CV
Comments: 20 pages, 6 figures
MSC-class: 68T45
ACM-class: I.2.10
Journal-ref: NeurIPS 2025
\\
  Accurate temporal prediction is the bridge between comprehensive scene
understanding and embodied artificial intelligence. However, predicting
multiple fine-grained states of a scene at multiple temporal scales is
difficult for vision-language models. We formalize the Multi-Scale Temporal
Prediction (MSTP) task in general and surgical scenes by decomposing
multi-scale into two orthogonal dimensions: the temporal scale, forecasting
states of humans and surgery at varying look-ahead intervals, and the state
scale, modeling a hierarchy of states in general and surgical scenes. For
example, in general scenes, states of contact relationships are finer-grained
than states of spatial relationships. In surgical scenes, medium-level steps
are finer-grained than high-level phases yet remain constrained by their
encompassing phase. To support this unified task, we introduce the first MSTP
Benchmark, featuring synchronized annotations across multiple state scales and
temporal scales. We further propose a method, Incremental Generation and
Multi-agent Collaboration (IG-MC), which integrates two key innovations. First,
we present a plug-and-play incremental generation module that continuously
synthesizes up-to-date visual previews at expanding temporal scales to inform
multiple decision-making agents, keeping decisions and generated visuals
synchronized and preventing performance degradation as look-ahead intervals
lengthen. Second, we present a decision-driven multi-agent collaboration
framework for multi-state prediction, comprising generation, initiation, and
multi-state assessment agents that dynamically trigger and evaluate prediction
cycles to balance global coherence and local fidelity.
\\ ( https://arxiv.org/abs/2509.17429 ,  8409kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17430
Date: Mon, 22 Sep 2025 07:22:31 GMT   (13338kb)

Title: EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian
  Splats from a Mobile Device
Authors: Gunjan Chhablani, Xiaomeng Ye, Muhammad Zubair Irshad, Zsolt Kira
Categories: cs.CV cs.RO
Comments: 16 pages, 18 figures, paper accepted at ICCV, 2025
\\
  The field of Embodied AI predominantly relies on simulation for training and
evaluation, often using either fully synthetic environments that lack
photorealism or high-fidelity real-world reconstructions captured with
expensive hardware. As a result, sim-to-real transfer remains a major
challenge. In this paper, we introduce EmbodiedSplat, a novel approach that
personalizes policy training by efficiently capturing the deployment
environment and fine-tuning policies within the reconstructed scenes. Our
method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to
bridge the gap between realistic scene capture and effective training
environments. Using iPhone-captured deployment scenes, we reconstruct meshes
via GS, enabling training in settings that closely approximate real-world
conditions. We conduct a comprehensive analysis of training strategies,
pre-training datasets, and mesh reconstruction techniques, evaluating their
impact on sim-to-real predictivity in real-world scenarios. Experimental
results demonstrate that agents fine-tuned with EmbodiedSplat outperform both
zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and
synthetically generated datasets (HSSD), achieving absolute success rate
improvements of 20\% and 40\% on real-world Image Navigation task. Moreover,
our approach yields a high sim-vs-real correlation (0.87--0.97) for the
reconstructed meshes, underscoring its effectiveness in adapting policies to
diverse environments with minimal effort. Project page:
https://gchhablani.github.io/embodied-splat
\\ ( https://arxiv.org/abs/2509.17430 ,  13338kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17431
Date: Mon, 22 Sep 2025 07:23:07 GMT   (26628kb)

Title: Emergent 3D Correspondence from Neural Shape Representation
Authors: Keyu Du, Jingyu Hu, Haipeng Li, Hao Xu, Haibing Huang, Chi-Wing Fu,
  Shuaicheng Liu
Categories: cs.CV
Comments: This paper is accepted by Siggraph Asia 2025 conference track
\\
  This paper presents a new approach to estimate accurate and robust 3D
semantic correspondence with the hierarchical neural semantic representation.
Our work has three key contributions. First, we design the hierarchical neural
semantic representation (HNSR), which consists of a global semantic feature to
capture high-level structure and multi-resolution local geometric features to
preserve fine details, by carefully harnessing 3D priors from pre-trained 3D
generative models. Second, we design a progressive global-to-local matching
strategy, which establishes coarse semantic correspondence using the global
semantic feature, then iteratively refines it with local geometric features,
yielding accurate and semantically-consistent mappings. Third, our framework is
training-free and broadly compatible with various pre-trained 3D generative
backbones, demonstrating strong generalization across diverse shape categories.
Our method also supports various applications, such as shape co-segmentation,
keypoint matching, and texture transfer, and generalizes well to structurally
diverse shapes, with promising results even in cross-category scenarios. Both
qualitative and quantitative evaluations show that our method outperforms
previous state-of-the-art techniques.
\\ ( https://arxiv.org/abs/2509.17431 ,  26628kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17452
Date: Mon, 22 Sep 2025 07:46:10 GMT   (2524kb)

Title: Training-Free Label Space Alignment for Universal Domain Adaptation
Authors: Dujin Lee, Sojung An, Jungmyung Wi, Kuniaki Saito and Donghyun Kim
Categories: cs.CV cs.AI
Comments: 22 pages, 12 figures
\\
  Universal domain adaptation (UniDA) transfers knowledge from a labeled source
domain to an unlabeled target domain, where label spaces may differ and the
target domain may contain private classes. Previous UniDA methods primarily
focused on visual space alignment but often struggled with visual ambiguities
due to content differences, which limited their robustness and
generalizability. To overcome this, we introduce a novel approach that
leverages the strong \textit{zero-shot capabilities} of recent vision-language
foundation models (VLMs) like CLIP, concentrating solely on label space
alignment to enhance adaptation stability. CLIP can generate task-specific
classifiers based only on label names. However, adapting CLIP to UniDA is
challenging because the label space is not fully known in advance. In this
study, we first utilize generative vision-language models to identify unknown
categories in the target domain. Noise and semantic ambiguities in the
discovered labels -- such as those similar to source labels (e.g., synonyms,
hypernyms, hyponyms) -- complicate label alignment. To address this, we propose
a training-free label-space alignment method for UniDA (\ours). Our method
aligns label spaces instead of visual spaces by filtering and refining noisy
labels between the domains. We then construct a \textit{universal classifier}
that integrates both shared knowledge and target-private class information,
thereby improving generalizability under domain shifts. The results reveal that
the proposed method considerably outperforms existing UniDA techniques across
key DomainBed benchmarks, delivering an average improvement of
\textcolor{blue}{+7.9\%}in H-score and \textcolor{blue}{+6.1\%} in H$^3$-score.
Furthermore, incorporating self-training further enhances performance and
achieves an additional (\textcolor{blue}{+1.6\%}) increment in both H- and
H$^3$-scores.
\\ ( https://arxiv.org/abs/2509.17452 ,  2524kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17457
Date: Mon, 22 Sep 2025 07:51:11 GMT   (5026kb)

Title: Explainable AI for Analyzing Person-Specific Patterns in Facial
  Recognition Tasks
Authors: Pawe{\l} Jakub Borsukiewicz, Jordan Samhi, Jacques Klein, Tegawend\'e
  F. Bissyand\'e
Categories: cs.CV cs.AI
Comments: 22 pages; 24 tables; 11 figures
MSC-class: 68T10
ACM-class: I.2.10; I.4.m
\\
  The proliferation of facial recognition systems presents major privacy risks,
driving the need for effective countermeasures. Current adversarial techniques
apply generalized methods rather than adapting to individual facial
characteristics, limiting their effectiveness and inconspicuousness. In this
work, we introduce Layer Embedding Activation Mapping (LEAM), a novel technique
that identifies which facial areas contribute most to recognition at an
individual level. Unlike adversarial attack methods that aim to fool
recognition systems, LEAM is an explainability technique designed to understand
how these systems work, providing insights that could inform future privacy
protection research. We integrate LEAM with a face parser to analyze data from
1000 individuals across 9 pre-trained facial recognition models.
  Our analysis reveals that while different layers within facial recognition
models vary significantly in their focus areas, these models generally
prioritize similar facial regions across architectures when considering their
overall activation patterns, which show significantly higher similarity between
images of the same individual (Bhattacharyya Coefficient: 0.32-0.57) vs.
different individuals (0.04-0.13), validating the existence of person-specific
recognition patterns. Our results show that facial recognition models
prioritize the central region of face images (with nose areas accounting for
18.9-29.7% of critical recognition regions), while still distributing attention
across multiple facial fragments. Proper selection of relevant facial areas was
confirmed using validation occlusions, based on just 1% of the most relevant,
LEAM-identified, image pixels, which proved to be transferable across different
models. Our findings establish the foundation for future individually tailored
privacy protection systems centered around LEAM's choice of areas to be
perturbed.
\\ ( https://arxiv.org/abs/2509.17457 ,  5026kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17458
Date: Mon, 22 Sep 2025 07:51:28 GMT   (20823kb)

Title: CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial
  Noise Optimization and Exploration
Authors: Seyed Amir Kasaei, Ali Aghayari, Arash Marioriyad, Niki Sepasian,
  Shayan Baghayi Nejad, MohammadAmin Fazli, Mahdieh Soleymani Baghshah, and
  Mohammad Hossein Rohban
Categories: cs.CV
\\
  Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.
\\ ( https://arxiv.org/abs/2509.17458 ,  20823kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17461
Date: Mon, 22 Sep 2025 07:55:03 GMT   (208kb)

Title: CSDformer: A Conversion Method for Fully Spike-Driven Transformer
Authors: Yuhao Zhang, Chengjun Zhang, Di Wu, Jie Yang, Mohamad Sawan
Categories: cs.CV
\\
  Spike-based transformer is a novel architecture aiming to enhance the
performance of spiking neural networks while mitigating the energy overhead
inherent to transformers. However, methods for generating these models suffer
from critical limitations: excessive training costs introduced by direct
training methods, or unavoidably hardware-unfriendly operations in existing
conversion methods. In this paper, we propose CSDformer, a novel conversion
method for fully spike-driven transformers. We tailor a conversion-oriented
transformer-based architecture and propose a new function NReLU to replace
softmax in self-attention. Subsequently, this model is quantized and trained,
and converted into a fully spike-driven model with temporal decomposition
technique. Also, we propose delayed Integrate-andFire neurons to reduce
conversion errors and improve the performance of spiking models. We evaluate
CSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1
accuracy under 7 time-steps on ImageNet, demonstrating superiority over
state-of-the-art models. Furthermore, CSDformer eliminates the need for
training SNNs, thereby reducing training costs (reducing computational resource
by 75% and accelerating training speed by 2-3$\times$). To the best of our
knowledge, this is the first fully spike-driven transformer-based model
developed via conversion method, achieving high performance under ultra-low
latency, while dramatically reducing both computational complexity and training
overhead.
\\ ( https://arxiv.org/abs/2509.17461 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17462
Date: Mon, 22 Sep 2025 07:55:43 GMT   (853kb)

Title: MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and
  Suppression for Multi-task 3D Perception
Authors: Changwon Kang, Jisong Kim, Hongjae Shin, Junseo Park, Jun Won Choi
Categories: cs.CV
Comments: Accepted to ICCV 2025
\\
  The goal of multi-task learning is to learn to conduct multiple tasks
simultaneously based on a shared data representation. While this approach can
improve learning efficiency, it may also cause performance degradation due to
task conflicts that arise when optimizing the model for different objectives.
To address this challenge, we introduce MAESTRO, a structured framework
designed to generate task-specific features and mitigate feature interference
in multi-task 3D perception, including 3D object detection, bird's-eye view
(BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises three
components: the Class-wise Prototype Generator (CPG), the Task-Specific Feature
Generator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups class
categories into foreground and background groups and generates group-wise
prototypes. The foreground and background prototypes are assigned to the 3D
object detection task and the map segmentation task, respectively, while both
are assigned to the 3D occupancy prediction task. TSFG leverages these
prototype groups to retain task-relevant features while suppressing irrelevant
features, thereby enhancing the performance for each task. SPA enhances the
prototype groups assigned for 3D occupancy prediction by utilizing the
information produced by the 3D object detection head and the map segmentation
head. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate
that MAESTRO consistently outperforms existing methods across 3D object
detection, BEV map segmentation, and 3D occupancy prediction tasks.
\\ ( https://arxiv.org/abs/2509.17462 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17476
Date: Mon, 22 Sep 2025 08:11:08 GMT   (23223kb)

Title: Stable Video-Driven Portraits
Authors: Mallikarjun B. R. and Fei Yin and Vikram Voleti and Nikita Drobyshev
  and Maksim Lapin and Aaryaman Vasishta and Varun Jampani
Categories: cs.CV
Comments: https://stable-video-driven-portraits.github.io/
\\
  Portrait animation aims to generate photo-realistic videos from a single
source image by reenacting the expression and pose from a driving video. While
early methods relied on 3D morphable models or feature warping techniques, they
often suffered from limited expressivity, temporal inconsistency, and poor
generalization to unseen identities or large pose variations. Recent advances
using diffusion models have demonstrated improved quality but remain
constrained by weak control signals and architectural limitations. In this
work, we propose a novel diffusion based framework that leverages masked facial
regions specifically the eyes, nose, and mouth from the driving video as strong
motion control cues. To enable robust training without appearance leakage, we
adopt cross identity supervision. To leverage the strong prior from the
pretrained diffusion model, our novel architecture introduces minimal new
parameters that converge faster and help in better generalization. We introduce
spatial temporal attention mechanisms that allow inter frame and intra frame
interactions, effectively capturing subtle motions and reducing temporal
artifacts. Our model uses history frames to ensure continuity across segments.
At inference, we propose a novel signal fusion strategy that balances motion
fidelity with identity preservation. Our approach achieves superior temporal
consistency and accurate expression control, enabling high-quality,
controllable portrait animation suitable for real-world applications.
\\ ( https://arxiv.org/abs/2509.17476 ,  23223kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17481
Date: Mon, 22 Sep 2025 08:15:55 GMT   (456kb)

Title: ChartHal: A Fine-grained Framework Evaluating Hallucination of Large
  Vision Language Models in Chart Understanding
Authors: Xingqi Wang, Yiming Cui, Xin Yao, Shijin Wang, Guoping Hu, Xiaoyu Qin
Categories: cs.CV cs.AI cs.CL
\\
  Large Vision-Language Models (LVLMs) have recently demonstrated remarkable
progress, yet hallucination remains a critical barrier, particularly in chart
understanding, which requires sophisticated perceptual and cognitive abilities
as well as rigorous factual accuracy. While prior work has investigated
hallucinations and chart comprehension independently, their intersection
remains largely unexplored. To address this gap, we present ChartHal, a
benchmark that features a fine-grained taxonomy of hallucination scenarios in
chart understanding, along with a human-validated dataset of 1,062 samples. Our
evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations
on ChartHal, including proprietary models such as GPT-5 and o4-mini, which
achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals
that questions involving information absent from or contradictory to charts are
especially likely to trigger hallucinations, underscoring the urgent need for
more robust mitigation strategies. Code and data are available at
https://github.com/ymcui/ChartHal .
\\ ( https://arxiv.org/abs/2509.17481 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17492
Date: Mon, 22 Sep 2025 08:21:19 GMT   (1180kb)

Title: Multimodal Medical Image Classification via Synergistic Learning
  Pre-training
Authors: Qinghua Lin, Guang-Hai Liu, Zuoyong Li, Yang Li, Yuting Jiang and
  Xiang Wu
Categories: cs.CV cs.AI
\\
  Multimodal pathological images are usually in clinical diagnosis, but
computer vision-based multimodal image-assisted diagnosis faces challenges with
modality fusion, especially in the absence of expert-annotated data. To achieve
the modality fusion in multimodal images with label scarcity, we propose a
novel ``pretraining + fine-tuning" framework for multimodal semi-supervised
medical image classification. Specifically, we propose a synergistic learning
pretraining framework of consistency, reconstructive, and aligned learning. By
treating one modality as an augmented sample of another modality, we implement
a self-supervised learning pre-train, enhancing the baseline model's feature
representation capability. Then, we design a fine-tuning method for multimodal
fusion. During the fine-tuning stage, we set different encoders to extract
features from the original modalities and provide a multimodal fusion encoder
for fusion modality. In addition, we propose a distribution shift method for
multimodal fusion features, which alleviates the prediction uncertainty and
overfitting risks caused by the lack of labeled samples. We conduct extensive
experiments on the publicly available gastroscopy image datasets Kvasir and
Kvasirv2. Quantitative and qualitative results demonstrate that the proposed
method outperforms the current state-of-the-art classification methods. The
code will be released at: https://github.com/LQH89757/MICS.
\\ ( https://arxiv.org/abs/2509.17492 ,  1180kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17498
Date: Mon, 22 Sep 2025 08:30:02 GMT   (1511kb)

Title: Vision-Based Driver Drowsiness Monitoring: Comparative Analysis of
  YOLOv5-v11 Models
Authors: Dilshara Herath, Chinthaka Abeyrathne, Prabhani Jayaweera
Categories: cs.CV eess.IV
Comments: Drowsiness Detection using state of the art YOLO algorithms
\\
  Driver drowsiness remains a critical factor in road accidents, accounting for
thousands of fatalities and injuries each year. This paper presents a
comprehensive evaluation of real-time, non-intrusive drowsiness detection
methods, focusing on computer vision based YOLO (You Look Only Once)
algorithms. A publicly available dataset namely, UTA-RLDD was used, containing
both awake and drowsy conditions, ensuring variability in gender, eyewear,
illumination, and skin tone. Seven YOLO variants (v5s, v9c, v9t, v10n, v10l,
v11n, v11l) are fine-tuned, with performance measured in terms of Precision,
Recall, mAP0.5, and mAP 0.5-0.95. Among these, YOLOv9c achieved the highest
accuracy (0.986 mAP 0.5, 0.978 Recall) while YOLOv11n strikes the optimal
balance between precision (0.954) and inference efficiency, making it highly
suitable for embedded deployment. Additionally, we implement an Eye Aspect
Ratio (EAR) approach using Dlib's facial landmarks, which despite its low
computational footprint exhibits reduced robustness under pose variation and
occlusions. Our findings illustrate clear trade offs between accuracy, latency,
and resource requirements, and offer practical guidelines for selecting or
combining detection methods in autonomous driving and industrial safety
applications.
\\ ( https://arxiv.org/abs/2509.17498 ,  1511kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17500
Date: Mon, 22 Sep 2025 08:30:34 GMT   (3116kb)

Title: SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge
Authors: Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan
Categories: cs.CV
\\
  Large-scale Video Object Segmentation (LSVOS) addresses the challenge of
accurately tracking and segmenting objects in long video sequences, where
difficulties stem from object reappearance, small-scale targets, heavy
occlusions, and crowded scenes. Existing approaches predominantly adopt
SAM2-based frameworks with various memory mechanisms for complex video mask
generation. In this report, we proposed Segment Anything with Memory
Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE
track of ICCV 2025, which integrates the strengths of stateof-the-art VOS
models into an effective paradigm. To handle visually similar instances and
long-term object disappearance in MOSE, we incorporate a long-term memorymodule
for reliable object re-identification. Additionly, we adopt SAM2Long as a
post-processing strategy to reduce error accumulation and enhance segmentation
stability in long video sequences. Our method achieved a final performance of
0.8427 in terms of J &F in the test-set leaderboard.
\\ ( https://arxiv.org/abs/2509.17500 ,  3116kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17506
Date: Mon, 22 Sep 2025 08:35:46 GMT   (11168kb)

Title: 4D-MoDe: Towards Editable and Scalable Volumetric Streaming via
  Motion-Decoupled 4D Gaussian Compression
Authors: Houqiang Zhong, Zihan Zheng, Qiang Hu, Yuan Tian, Ning Cao, Lan Xu,
  Xiaoyun Zhang, Zhengxue Cheng, Li Song, Wenjun Zhang
Categories: cs.CV
\\
  Volumetric video has emerged as a key medium for immersive telepresence and
augmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation
and realistic spatial interactions. However, delivering high-quality dynamic
volumetric content at scale remains challenging due to massive data volume,
complex motion, and limited editability of existing representations. In this
paper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework
designed for scalable and editable volumetric video streaming. Our method
introduces a layered representation that explicitly separates static
backgrounds from dynamic foregrounds using a lookahead-based motion
decomposition strategy, significantly reducing temporal redundancy and enabling
selective background/foreground streaming. To capture continuous motion
trajectories, we employ a multi-resolution motion estimation grid and a
lightweight shared MLP, complemented by a dynamic Gaussian compensation
mechanism to model emergent content. An adaptive grouping scheme dynamically
inserts background keyframes to balance temporal consistency and compression
efficiency. Furthermore, an entropy-aware training pipeline jointly optimizes
the motion fields and Gaussian parameters under a rate-distortion (RD)
objective, while employing range-based and KD-tree compression to minimize
storage overhead. Extensive experiments on multiple datasets demonstrate that
4D-MoDe consistently achieves competitive reconstruction quality with an order
of magnitude lower storage cost (e.g., as low as \textbf{11.4} KB/frame)
compared to state-of-the-art methods, while supporting practical applications
such as background replacement and foreground-only streaming.
\\ ( https://arxiv.org/abs/2509.17506 ,  11168kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17513
Date: Mon, 22 Sep 2025 08:38:17 GMT   (31147kb)

Title: 4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive
  Volumetric Video Streaming
Authors: Zihan Zheng, Zhenlong Wu, Houqiang Zhong, Yuan Tian, Ning Cao, Lan Xu,
  Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Wenjun Zhang
Categories: cs.CV
Comments: NeurIPS 2025
\\
  Achieving seamless viewing of high-fidelity volumetric video, comparable to
2D video experiences, remains an open challenge. Existing volumetric video
compression methods either lack the flexibility to adjust quality and bitrate
within a single model for efficient streaming across diverse networks and
devices, or struggle with real-time decoding and rendering on lightweight
mobile platforms. To address these challenges, we introduce 4DGCPro, a novel
hierarchical 4D Gaussian compression framework that facilitates real-time
mobile decoding and high-quality rendering via progressive volumetric video
streaming in a single bitstream. Specifically, we propose a
perceptually-weighted and compression-friendly hierarchical 4D Gaussian
representation with motion-aware adaptive grouping to reduce temporal
redundancy, preserve coherence, and enable scalable multi-level detail
streaming. Furthermore, we present an end-to-end entropy-optimized training
scheme, which incorporates layer-wise rate-distortion (RD) supervision and
attribute-specific entropy modeling for efficient bitstream generation.
Extensive experiments show that 4DGCPro enables flexible quality and multiple
bitrate within a single model, achieving real-time decoding and rendering on
mobile devices while outperforming existing methods in RD performance across
multiple datasets. Project Page: https://mediax-sjtu.github.io/4DGCPro
\\ ( https://arxiv.org/abs/2509.17513 ,  31147kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17520
Date: Mon, 22 Sep 2025 08:45:39 GMT   (2694kb)

Title: Unified Multimodal Coherent Field: Synchronous Semantic-Spatial-Vision
  Fusion for Brain Tumor Segmentation
Authors: Mingda Zhang, Yuyang Zheng, Ruixiang Tang, Jingru Qiu and Haiyan Ding
Categories: cs.CV
Comments: 8 pages, 3 figures
\\
  Brain tumor segmentation requires accurate identification of hierarchical
regions including whole tumor (WT), tumor core (TC), and enhancing tumor (ET)
from multi-sequence magnetic resonance imaging (MRI) images. Due to tumor
tissue heterogeneity, ambiguous boundaries, and contrast variations across MRI
sequences, methods relying solely on visual information or post-hoc loss
constraints show unstable performance in boundary delineation and hierarchy
preservation. To address this challenge, we propose the Unified Multimodal
Coherent Field (UMCF) method. This method achieves synchronous interactive
fusion of visual, semantic, and spatial information within a unified 3D latent
space, adaptively adjusting modal contributions through parameter-free
uncertainty gating, with medical prior knowledge directly participating in
attention computation, avoiding the traditional "process-then-concatenate"
separated architecture. On Brain Tumor Segmentation (BraTS) 2020 and 2021
datasets, UMCF+nnU-Net achieves average Dice coefficients of 0.8579 and 0.8977
respectively, with an average 4.18% improvement across mainstream
architectures. By deeply integrating clinical knowledge with imaging features,
UMCF provides a new technical pathway for multimodal information fusion in
precision medicine.
\\ ( https://arxiv.org/abs/2509.17520 ,  2694kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17522
Date: Mon, 22 Sep 2025 08:48:04 GMT   (8849kb)

Title: Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen
  Large Language Models
Authors: Hangzhou He, Lei Zhu, Kaiwen Li, Xinliang Zhang, Jiakui Hu, Ourui Fu,
  Zhengjian Yao, Yanye Lu
Categories: cs.CV
\\
  Concept Bottleneck Models (CBMs) provide inherent interpretability by first
predicting a set of human-understandable concepts and then mapping them to
labels through a simple classifier. While users can intervene in the concept
space to improve predictions, traditional CBMs typically employ a fixed linear
classifier over concept scores, which restricts interventions to manual value
adjustments and prevents the incorporation of new concepts or domain knowledge
at test time. These limitations are particularly severe in unsupervised CBMs,
where concept activations are often noisy and densely activated, making user
interventions ineffective. We introduce Chat-CBM, which replaces score-based
classifiers with a language-based classifier that reasons directly over concept
semantics. By grounding prediction in the semantic space of concepts, Chat-CBM
preserves the interpretability of CBMs while enabling richer and more intuitive
interventions, such as concept correction, addition or removal of concepts,
incorporation of external knowledge, and high-level reasoning guidance.
Leveraging the language understanding and few-shot capabilities of frozen large
language models, Chat-CBM extends the intervention interface of CBMs beyond
numerical editing and remains effective even in unsupervised settings.
Experiments on nine datasets demonstrate that Chat-CBM achieves higher
predictive performance and substantially improves user interactivity while
maintaining the concept-based interpretability of CBMs.
\\ ( https://arxiv.org/abs/2509.17522 ,  8849kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17537
Date: Mon, 22 Sep 2025 08:55:04 GMT   (514kb)

Title: SimToken: A Simple Baseline for Referring Audio-Visual Segmentation
Authors: Dian Jin, Yanghao Zhou, Jinxing Zhou, Jiaqi Ma, Ruohao Guo, Dan Guo
Categories: cs.CV
\\
  Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific
objects in videos based on natural language expressions involving audio,
vision, and text information. This task poses significant challenges in
cross-modal reasoning and fine-grained object localization. In this paper, we
propose a simple framework, SimToken, that integrates a multimodal large
language model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided
to generate a special semantic token representing the referred object. This
compact token, enriched with contextual information from all modalities, acts
as a prompt to guide SAM to segment objectsacross video frames. To further
improve semantic learning, we introduce a novel target-consistent semantic
alignment loss that aligns token embeddings from different expressions but
referring to the same object. Experiments on the Ref-AVS benchmark demonstrate
that our approach achieves superior performance compared to existing
methods.Code will be available at https://github.com/DianJin-HFUT/SimToken
\\ ( https://arxiv.org/abs/2509.17537 ,  514kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17561
Date: Mon, 22 Sep 2025 10:55:21 GMT   (9364kb)

Title: An Empirical Study on the Robustness of YOLO Models for Underwater
  Object Detection
Authors: Edwine Nabahirwa, Wei Song, Minghua Zhang, Shufan Chen
Categories: cs.CV cs.AI
Comments: 28 Pages, 12 Figures
\\
  Underwater object detection (UOD) remains a critical challenge in computer
vision due to underwater distortions which degrade low-level features and
compromise the reliability of even state-of-the-art detectors. While YOLO
models have become the backbone of real-time object detection, little work has
systematically examined their robustness under these uniquely challenging
conditions. This raises a critical question: Are YOLO models genuinely robust
when operating under the chaotic and unpredictable conditions of underwater
environments? In this study, we present one of the first comprehensive
evaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated
underwater environments. Using a unified dataset of 10,000 annotated images
from DUO and Roboflow100, we not only benchmark model robustness but also
analyze how distortions affect key low-level features such as texture, edges,
and color. Our findings show that (1) YOLOv12 delivers the strongest overall
performance but is highly vulnerable to noise, and (2) noise disrupts edge and
texture features, explaining the poor detection performance in noisy images.
Class imbalance is a persistent challenge in UOD. Experiments revealed that (3)
image counts and instance frequency primarily drive detection performance,
while object appearance exerts only a secondary influence. Finally, we
evaluated lightweight training-aware strategies: noise-aware sample injection,
which improves robustness in both noisy and real-world conditions, and
fine-tuning with advanced enhancement, which boosts accuracy in enhanced
domains but slightly lowers performance in original data, demonstrating strong
potential for domain adaptation, respectively. Together, these insights provide
practical guidance for building resilient and cost-efficient UOD systems.
\\ ( https://arxiv.org/abs/2509.17561 ,  9364kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17562
Date: Mon, 22 Sep 2025 10:57:42 GMT   (418kb)

Title: Visual Instruction Pretraining for Domain-Specific Foundation Models
Authors: Yuxuan Li, Yicheng Zhang, Wenhao Tang, Yimian Dai, Ming-Ming Cheng,
  Xiang Li, Jian Yang
Categories: cs.CV
\\
  Modern computer vision is converging on a closed loop in which perception,
reasoning and generation mutually reinforce each other. However, this loop
remains incomplete: the top-down influence of high-level reasoning on the
foundational learning of low-level perceptual features is not yet
underexplored. This paper addresses this gap by proposing a new paradigm for
pretraining foundation models in downstream domains. We introduce Visual
insTruction Pretraining (ViTP), a novel approach that directly leverages
reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)
backbone within a Vision-Language Model and pretrains it end-to-end using a
rich corpus of visual instruction data curated from target downstream domains.
ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels
the ViT to learn robust and domain-relevant features from a sparse set of
visual tokens. Extensive experiments on 16 challenging remote sensing and
medical imaging benchmarks demonstrate that ViTP establishes new
state-of-the-art performance across a diverse range of downstream tasks. The
code is available at github.com/zcablii/ViTP.
\\ ( https://arxiv.org/abs/2509.17562 ,  418kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17566
Date: Mon, 22 Sep 2025 10:59:27 GMT   (553kb)

Title: MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's
  Disease with Limited 3D MR Data
Authors: Ding Shaodong, Liu Ziyang, Zhou Yijun, Liu Tao
Categories: cs.CV cs.AI
Comments: First-place solution of the classification track for MICCAI'2025
  PDCADxFoundation Challenge
\\
  The automatic diagnosis of Parkinson's disease is in high clinical demand due
to its prevalence and the importance of targeted treatment. Current clinical
practice often relies on diagnostic biomarkers in QSM and NM-MRI images.
However, the lack of large, high-quality datasets makes training diagnostic
models from scratch prone to overfitting. Adapting pre-trained 3D medical
models is also challenging, as the diversity of medical imaging leads to
mismatches in voxel spacing and modality between pre-training and fine-tuning
data. In this paper, we address these challenges by leveraging 2D vision
foundation models (VFMs). Specifically, we crop multiple key ROIs from NM and
QSM images, process each ROI through separate branches to compress the ROI into
a token, and then combine these tokens into a unified patient representation
for classification. Within each branch, we use 2D VFMs to encode axial slices
of the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary
segmentation head that steers the feature extraction toward specific brain
nuclei. Additionally, we introduce multi-ROI supervised contrastive learning,
which improves diagnostic performance by pulling together representations of
patients from the same class while pushing away those from different classes.
Our approach achieved first place in the MICCAI 2025 PDCADxFoundation
challenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled
QSM and NM-MRI scans, outperforming the second-place method by 5.5%.These
results highlight the potential of 2D VFMs for clinical analysis of 3D MR
images.
\\ ( https://arxiv.org/abs/2509.17566 ,  553kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17581
Date: Mon, 22 Sep 2025 11:07:15 GMT   (5007kb)

Title: PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera
  Identification
Authors: Florinel Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu
Categories: cs.CV cs.CR cs.LG
\\
  We propose a novel benchmark for camera identification via Photo Response
Non-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with
120+ cameras, where the training and test photos are taken in different
scenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel
PRNU-based camera identification model that employs a hybrid architecture,
comprising a denoising autoencoder to estimate the PRNU signal and a
convolutional network that can perform 1:N verification of camera devices.
Instead of using a conventional approach based on contrastive learning, our
method takes the Hadamard product between reference and query PRNU signals as
input. This novel design leads to significantly better results compared with
state-of-the-art models based on denoising autoencoders and contrastive
learning. We release our dataset and code at:
https://github.com/CroitoruAlin/PRNU-Bench.
\\ ( https://arxiv.org/abs/2509.17581 ,  5007kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17588
Date: Mon, 22 Sep 2025 11:12:12 GMT   (22097kb)

Title: Interpreting Attention Heads for Image-to-Text Information Flow in Large
  Vision-Language Models
Authors: Jinyeong Kim, Seil Kang, Jiwoo Park, Junhyeok Kim, Seong Jae Hwang
Categories: cs.CV cs.AI cs.LG
\\
  Large Vision-Language Models (LVLMs) answer visual questions by transferring
information from images to text through a series of attention heads. While this
image-to-text information flow is central to visual question answering, its
underlying mechanism remains difficult to interpret due to the simultaneous
operation of numerous attention heads. To address this challenge, we propose
head attribution, a technique inspired by component attribution methods, to
identify consistent patterns among attention heads that play a key role in
information transfer. Using head attribution, we investigate how LVLMs rely on
specific attention heads to identify and answer questions about the main object
in an image. Our analysis reveals that a distinct subset of attention heads
facilitates the image-to-text information flow. Remarkably, we find that the
selection of these heads is governed by the semantic content of the input image
rather than its visual appearance. We further examine the flow of information
at the token level and discover that (1) text information first propagates to
role-related tokens and the final token before receiving image information, and
(2) image information is embedded in both object-related and background tokens.
Our work provides evidence that image-to-text information flow follows a
structured process, and that analysis at the attention-head level offers a
promising direction toward understanding the mechanisms of LVLMs.
\\ ( https://arxiv.org/abs/2509.17588 ,  22097kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17593
Date: Mon, 22 Sep 2025 11:17:14 GMT   (1221kb)

Title: Domain Adaptive Object Detection for Space Applications with Real-Time
  Constraints
Authors: Samet Hicsonmez, Abd El Rahman Shabayek, Arunkumar Rathinam, Djamila
  Aouada
Categories: cs.CV
Comments: Advanced Space Technologies in Robotics and Automation (ASTRA) 2025
\\
  Object detection is essential in space applications targeting Space Domain
Awareness and also applications involving relative navigation scenarios.
Current deep learning models for Object Detection in space applications are
often trained on synthetic data from simulators, however, the model performance
drops significantly on real-world data due to the domain gap. However, domain
adaptive object detection is an overlooked problem in the community. In this
work, we first show the importance of domain adaptation and then explore
Supervised Domain Adaptation (SDA) to reduce this gap using minimal labeled
real data. We build on a recent semi-supervised adaptation method and tailor it
for object detection. Our approach combines domain-invariant feature learning
with a CNN-based domain discriminator and invariant risk minimization using a
domain-independent regression head. To meet real-time deployment needs, we test
our method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet
backbone and on the more advanced Fully Convolutional One-Stage object detector
(FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and
SPARK. The results show up to 20-point improvements in average precision (AP)
with just 250 labeled real images.
\\ ( https://arxiv.org/abs/2509.17593 ,  1221kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17598
Date: Mon, 22 Sep 2025 11:19:17 GMT   (2209kb)

Title: COLA: Context-aware Language-driven Test-time Adaptation
Authors: Aiming Zhang and Tianyuan Yu, Liang Bai, Jun Tang, Yanming Guo, Yirun
  Ruan, Yun Zhou, Zhihe Lu
Categories: cs.CV
Journal-ref: IEEE Trans. Image Process. (2025)
DOI: 10.1109/TIP.2025.3607634
\\
  Test-time adaptation (TTA) has gained increasing popularity due to its
efficacy in addressing ``distribution shift'' issue while simultaneously
protecting data privacy.
  However, most prior methods assume that a paired source domain model and
target domain sharing the same label space coexist, heavily limiting their
applicability.
  In this paper, we investigate a more general source model capable of
adaptation to multiple target domains without needing shared labels.
  This is achieved by using a pre-trained vision-language model (VLM), \egno,
CLIP, that can recognize images through matching with class descriptions.
  While the zero-shot performance of VLMs is impressive, they struggle to
effectively capture the distinctive attributes of a target domain.
  To that end, we propose a novel method -- Context-aware Language-driven TTA
(COLA).
  The proposed method incorporates a lightweight context-aware module that
consists of three key components: a task-aware adapter, a context-aware unit,
and a residual connection unit for exploring task-specific knowledge,
domain-specific knowledge from the VLM and prior knowledge of the VLM,
respectively.
  It is worth noting that the context-aware module can be seamlessly integrated
into a frozen VLM, ensuring both minimal effort and parameter efficiency.
  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy
to mitigate the adverse effects caused by class imbalance.
  We demonstrate the effectiveness of our method not only in TTA scenarios but
also in class generalisation tasks.
  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.
\\ ( https://arxiv.org/abs/2509.17598 ,  2209kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17602
Date: Mon, 22 Sep 2025 11:21:53 GMT   (23313kb)

Title: Overview of PlantCLEF 2025: Multi-Species Plant Identification in
  Vegetation Quadrat Images
Authors: Giulio Martellucci, Herve Goeau, Pierre Bonnet, Fabrice Vinatier,
  Alexis Joly
Categories: cs.CV
Comments: 13 pages, 4 figures, CLEF 2025 Conference and Labs of the Evaluation
  Forum, September 09 to 12, 2024, Madrid, Spain
\\
  Quadrat images are essential for ecological studies, as they enable
standardized sampling, the assessment of plant biodiversity, long-term
monitoring, and large-scale field campaigns. These images typically cover an
area of fifty centimetres or one square meter, and botanists carefully identify
all the species present. Integrating AI could help specialists accelerate their
inventories and expand the spatial coverage of ecological studies. To assess
progress in this area, the PlantCLEF 2025 challenge relies on a new test set of
2,105 high-resolution multi-label images annotated by experts and covering
around 400 species. It also provides a large training set of 1.4 million
individual plant images, along with vision transformer models pre-trained on
this data. The task is formulated as a (weakly labelled) multi-label
classification problem, where the goal is to predict all species present in a
quadrat image using single-label training data. This paper provides a detailed
description of the data, the evaluation methodology, the methods and models
used by participants, and the results achieved.
\\ ( https://arxiv.org/abs/2509.17602 ,  23313kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17615
Date: Mon, 22 Sep 2025 11:27:49 GMT   (240kb)

Title: From Benchmarks to Reality: Advancing Visual Anomaly Detection by the
  VAND 3.0 Challenge
Authors: Lars Heckler-Kram, Ashwin Vaidya, Jan-Hendrik Neudeck, Ulla Scheler,
  Dick Ameln, Samet Akcay, Paula Ramos
Categories: cs.CV
\\
  Visual anomaly detection is a strongly application-driven field of research.
Consequently, the connection between academia and industry is of paramount
importance. In this regard, we present the VAND 3.0 Challenge to showcase
current progress in anomaly detection across different practical settings
whilst addressing critical issues in the field. The challenge hosted two
tracks, fostering the development of anomaly detection methods robust against
real-world distribution shifts (Category 1) and exploring the capabilities of
Vision Language Models within the few-shot regime (Category 2), respectively.
The participants' solutions reached significant improvements over previous
baselines by combining or adapting existing approaches and fusing them with
novel pipelines. While for both tracks the progress in large pre-trained vision
(language) backbones played a pivotal role for the performance increase,
scaling up anomaly detection methods more efficiently needs to be addressed by
future research to meet real-time and computational constraints on-site.
\\ ( https://arxiv.org/abs/2509.17615 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17620
Date: Mon, 22 Sep 2025 11:31:57 GMT   (625kb)

Title: Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method
Authors: Gregory Schroeder, Mohamed Sabry and Cristina Olaverri-Monreal
Categories: cs.CV
\\
  Estimating camera intrinsic parameters without prior scene knowledge is a
fundamental challenge in computer vision. This capability is particularly
important for applications such as autonomous driving and vehicle platooning,
where precalibrated setups are impractical and real-time adaptability is
necessary. To advance the state-of-the-art, we present a set of equations based
on the calibrated trifocal tensor, enabling projective camera self-calibration
from minimal image data. Our method, termed TrifocalCalib, significantly
improves accuracy and robustness compared to both recent learning-based and
classical approaches. Unlike many existing techniques, our approach requires no
calibration target, imposes no constraints on camera motion, and simultaneously
estimates both focal length and principal point. Evaluations in both
procedurally generated synthetic environments and structured dataset-based
scenarios demonstrate the effectiveness of our approach. To support
reproducibility, we make the code publicly available.
\\ ( https://arxiv.org/abs/2509.17620 ,  625kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17622
Date: Mon, 22 Sep 2025 11:34:10 GMT   (152kb)

Title: Overview of PlantCLEF 2023: Image-based Plant Identification at Global
  Scale
Authors: Herve Goeau, Pierre Bonnet, Alexis Joly
Categories: cs.CV
Comments: 10 pages, 1 figure, CLEF 2023 Conference and Labs of the Evaluation
  Forum, September 18 to 21, 2023, Thessaloniki, Greece
\\
  The world is estimated to be home to over 300,000 species of vascular plants.
In the face of the ongoing biodiversity crisis, expanding our understanding of
these species is crucial for the advancement of human civilization,
encompassing areas such as agriculture, construction, and pharmacopoeia.
However, the labor-intensive process of plant identification undertaken by
human experts poses a significant obstacle to the accumulation of new data and
knowledge. Fortunately, recent advancements in automatic identification,
particularly through the application of deep learning techniques, have shown
promising progress. Despite challenges posed by data-related issues such as a
vast number of classes, imbalanced class distribution, erroneous
identifications, duplications, variable visual quality, and diverse visual
contents (such as photos or herbarium sheets), deep learning approaches have
reached a level of maturity which gives us hope that in the near future we will
have an identification system capable of accurately identifying all plant
species worldwide. The PlantCLEF2023 challenge aims to contribute to this
pursuit by addressing a multi-image (and metadata) classification problem
involving an extensive set of classes (80,000 plant species). This paper
provides an overview of the challenge's resources and evaluations, summarizes
the methods and systems employed by participating research groups, and presents
an analysis of key findings.
\\ ( https://arxiv.org/abs/2509.17622 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17627
Date: Mon, 22 Sep 2025 11:35:55 GMT   (36888kb)

Title: OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion
  Transformer Models
Authors: Jinshu Chen, Xinghui Li, Xu Bai, Tianxiang Ma, Pengze Zhang, Zhuowei
  Chen, Gen Li, Lijie Liu, Songtao Zhao, Bingchuan Li, Qian He
Categories: cs.CV
Comments: Github Page: https://phantom-video.github.io/OmniInsert/
\\
  Recent advances in video insertion based on diffusion models are impressive.
However, existing methods rely on complex control signals but struggle with
subject consistency, limiting their practical applicability. In this paper, we
focus on the task of Mask-free Video Insertion and aim to resolve three key
challenges: data scarcity, subject-scene equilibrium, and insertion
harmonization. To address the data scarcity, we propose a new data pipeline
InsertPipe, constructing diverse cross-pair data automatically. Building upon
our data pipeline, we develop OmniInsert, a novel unified framework for
mask-free video insertion from both single and multiple subject references.
Specifically, to maintain subject-scene equilibrium, we introduce a simple yet
effective Condition-Specific Feature Injection mechanism to distinctly inject
multi-source conditions and propose a novel Progressive Training strategy that
enables the model to balance feature injection from subjects and source video.
Meanwhile, we design the Subject-Focused Loss to improve the detailed
appearance of the subjects. To further enhance insertion harmonization, we
propose an Insertive Preference Optimization methodology to optimize the model
by simulating human preferences, and incorporate a Context-Aware Rephraser
module during reference to seamlessly integrate the subject into the original
scenes. To address the lack of a benchmark for the field, we introduce
InsertBench, a comprehensive benchmark comprising diverse scenes with
meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert
outperforms state-of-the-art closed-source commercial solutions. The code will
be released.
\\ ( https://arxiv.org/abs/2509.17627 ,  36888kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17632
Date: Mon, 22 Sep 2025 11:40:21 GMT   (532kb)

Title: Overview of PlantCLEF 2022: Image-based plant identification at global
  scale
Authors: Herve Goeau, Pierre Bonnet, Alexis Joly
Categories: cs.CV
Comments: 13 pages, 2 figures, CLEF 2022 Conference and Labs of the Evaluation
  Forum, September 05 to 08, 2022, Bologna, Italy
\\
  It is estimated that there are more than 300,000 species of vascular plants
in the world. Increasing our knowledge of these species is of paramount
importance for the development of human civilization (agriculture,
construction, pharmacopoeia, etc.), especially in the context of the
biodiversity crisis. However, the burden of systematic plant identification by
human experts strongly penalizes the aggregation of new data and knowledge.
Since then, automatic identification has made considerable progress in recent
years as highlighted during all previous editions of PlantCLEF. Deep learning
techniques now seem mature enough to address the ultimate but realistic problem
of global identification of plant biodiversity in spite of many problems that
the data may present (a huge number of classes, very strongly unbalanced
classes, partially erroneous identifications, duplications, variable visual
quality, diversity of visual contents such as photos or herbarium sheets, etc).
The PlantCLEF2022 challenge edition proposes to take a step in this direction
by tackling a multi-image (and metadata) classification problem with a very
large number of classes (80k plant species). This paper presents the resources
and evaluations of the challenge, summarizes the approaches and systems
employed by the participating research groups, and provides an analysis of key
findings.
\\ ( https://arxiv.org/abs/2509.17632 ,  532kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17638
Date: Mon, 22 Sep 2025 11:44:14 GMT   (30629kb)

Title: A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot
  Action Recognition
Authors: Zilin Gao, Qilong Wang, Bingbing Zhang, Qinghua Hu, Peihua Li
Categories: cs.CV cs.AI
Comments: 27 pages, 13 figures, 7 tables
Journal-ref: Published in IJCV, 2025
DOI: 10.1007/s11263-025-02432-4
\\
  Thanks to capability to alleviate the cost of large-scale annotation,
few-shot action recognition (FSAR) has attracted increased attention of
researchers in recent years. Existing FSAR approaches typically neglect the
role of individual motion pattern in comparison, and under-explore the feature
statistics for video dynamics. Thereby, they struggle to handle the challenging
temporal misalignment in video dynamics, particularly by using 2D backbones. To
overcome these limitations, this work proposes an adaptively aligned
multi-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the
latent video dynamics with a collection of powerful representation candidates
and adaptively align them in an instance-guided manner. To this end, our
A$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$
module) for matching, and multi-scale second-order moment (M$^2$ block) for
strong representation. Specifically, M$^2$ block develops a collection of
semantic second-order descriptors at multiple spatio-temporal scales.
Furthermore, A$^2$ module aims to adaptively select informative candidate
descriptors while considering the individual motion pattern. By such means, our
A$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem
by establishing an adaptive alignment protocol for strong representation.
Notably, our proposed method generalizes well to various few-shot settings and
diverse metrics. The experiments are conducted on five widely used FSAR
benchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive
performance compared to state-of-the-arts, demonstrating its effectiveness and
generalization.
\\ ( https://arxiv.org/abs/2509.17638 ,  30629kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17647
Date: Mon, 22 Sep 2025 11:52:02 GMT   (2207kb)

Title: VideoArtGS: Building Digital Twins of Articulated Objects from Monocular
  Video
Authors: Yu Liu, Baoxiong Jia, Ruijie Lu, Chuyue Gan, Huayu Chen, Junfeng Ni,
  Song-Chun Zhu, Siyuan Huang
Categories: cs.CV cs.AI cs.RO
\\
  Building digital twins of articulated objects from monocular video presents
an essential challenge in computer vision, which requires simultaneous
reconstruction of object geometry, part segmentation, and articulation
parameters from limited viewpoint inputs. Monocular video offers an attractive
input format due to its simplicity and scalability; however, it's challenging
to disentangle the object geometry and part dynamics with visual supervision
alone, as the joint movement of the camera and parts leads to ill-posed
estimation. While motion priors from pre-trained tracking models can alleviate
the issue, how to effectively integrate them for articulation learning remains
largely unexplored. To address this problem, we introduce VideoArtGS, a novel
approach that reconstructs high-fidelity digital twins of articulated objects
from monocular video. We propose a motion prior guidance pipeline that analyzes
3D tracks, filters noise, and provides reliable initialization of articulation
parameters. We also design a hybrid center-grid part assignment module for
articulation-based deformation fields that captures accurate part motion.
VideoArtGS demonstrates state-of-the-art performance in articulation and mesh
reconstruction, reducing the reconstruction error by about two orders of
magnitude compared to existing methods. VideoArtGS enables practical digital
twin creation from monocular video, establishing a new benchmark for
video-based articulated object reconstruction. Our work is made publicly
available at: https://videoartgs.github.io.
\\ ( https://arxiv.org/abs/2509.17647 ,  2207kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17650
Date: Mon, 22 Sep 2025 11:54:58 GMT   (9041kb)

Title: Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming
  Visual Geometry Transformers
Authors: Soroush Mahdi, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi
  Javanmardi
Categories: cs.CV
\\
  Streaming visual transformers like StreamVGGT achieve strong 3D perception
but suffer from unbounded growth of key value (KV) memory, which limits
scalability. We propose a training-free, inference-time token eviction policy
that bounds memory by discarding redundant tokens while keeping the most
informative ones. Our method uses significantly less memory with little to no
drop in accuracy: on 7-Scenes with long sequences it reduces peak memory from
18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under
strict memory budgets, eviction enables denser frame sampling, which improves
reconstruction accuracy compared to the baseline. Experiments across video
depth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and
camera pose estimation (Sintel, TUM-dynamics) show that our approach closely
matches StreamVGGT at a fraction of the memory and makes long-horizon streaming
inference more practical.
\\ ( https://arxiv.org/abs/2509.17650 ,  9041kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17651
Date: Mon, 22 Sep 2025 11:55:04 GMT   (4059kb)

Title: SISMA: Semantic Face Image Synthesis with Mamba
Authors: Filippo Botti, Alex Ergasti, Tomaso Fontanini, Claudio Ferrari,
  Massimo Bertozzi and Andrea Prati
Categories: cs.CV
\\
  Diffusion Models have become very popular for Semantic Image Synthesis (SIS)
of human faces. Nevertheless, their training and inference is computationally
expensive and their computational requirements are high due to the quadratic
complexity of attention layers. In this paper, we propose a novel architecture
called SISMA, based on the recently proposed Mamba. SISMA generates high
quality samples by controlling their shape using a semantic mask at a reduced
computational demand. We validated our approach through comprehensive
experiments with CelebAMask-HQ, revealing that our architecture not only
achieves a better FID score yet also operates at three times the speed of
state-of-the-art architectures. This indicates that the proposed design is a
viable, lightweight substitute to transformer-based models.
\\ ( https://arxiv.org/abs/2509.17651 ,  4059kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17654
Date: Mon, 22 Sep 2025 11:58:20 GMT   (3017kb)

Title: Clothing agnostic Pre-inpainting Virtual Try-ON
Authors: Sehyun Kim, Hye Jun Lee, Jiwoo Lee, Taemin Lee
Categories: cs.CV
\\
  With the development of deep learning technology, virtual try-on technology
has become an important application value in the fields of e-commerce, fashion,
and entertainment. The recently proposed Leffa has improved the texture
distortion problem of diffu-sion-based models, but there are limitations in
that the bottom detection inaccuracy and the existing clothing silhouette
remain in the synthesis results. To solve this problem, this study proposes
CaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has
improved the naturalness and consistency of whole-body clothing syn-thesis by
integrating multi-category masking based on Dress Code and skin inpainting
based on Stable Diffusion. In particular, a generate skin module was introduced
to solve the skin restoration problem that occurs when long-sleeved images are
converted into short-sleeved or sleeveless ones, and high-quality restoration
was implemented consider-ing the human body posture and color. As a result,
CaP-VTON recorded 92.5\%, which is 15.4\% better than Leffa in short-sleeved
synthesis accuracy, and showed the performance of consistently reproducing the
style and shape of reference clothing in visual evaluation. These structures
maintain model-agnostic properties and are applicable to various
diffu-sion-based virtual inspection systems, and can contribute to applications
that require high-precision virtual wearing, such as e-commerce, custom
styling, and avatar creation.
\\ ( https://arxiv.org/abs/2509.17654 ,  3017kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17660
Date: Mon, 22 Sep 2025 12:03:40 GMT   (14178kb)

Title: Development and validation of an AI foundation model for endoscopic
  diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep
  learning study
Authors: Yikun Ma, Bo Li, Ying Chen, Zijie Yue, Shuchang Xu, Jingyao Li, Lei
  Ma, Liang Zhong, Duowu Zou, Leiming Xu, Yunshi Zhong, Xiaobo Li, Weiqun Ding,
  Minmin Zhang, Dongli He, Zhenghong Li, Ye Chen, Ye Zhao, Jialong Zhuo,
  Xiaofen Wu, Lisha Yi, Miaojing Shi, Huihui Sun
Categories: cs.CV
\\
  The early detection of esophagogastric junction adenocarcinoma (EGJA) is
crucial for improving patient prognosis, yet its current diagnosis is highly
operator-dependent. This paper aims to make the first attempt to develop an
artificial intelligence (AI) foundation model-based method for both screening
and staging diagnosis of EGJA using endoscopic images. In this cohort and
learning study, we conducted a multicentre study across seven Chinese hospitals
between December 28, 2016 and December 30, 2024. It comprises 12,302 images
from 1,546 patients; 8,249 of them were employed for model training, while the
remaining were divided into the held-out (112 patients, 914 images), external
(230 patients, 1,539 images), and prospective (198 patients, 1,600 images) test
sets for evaluation. The proposed model employs DINOv2 (a vision foundation
model) and ResNet50 (a convolutional neural network) to extract features of
global appearance and local details of endoscopic images for EGJA staging
diagnosis. Our model demonstrates satisfactory performance for EGJA staging
diagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and
0.8956, respectively. In contrast, among representative AI models, the best one
(ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three test
sets, respectively; the expert endoscopists achieve an accuracy of 0.8147 on
the held-out test set. Moreover, with the assistance of our model, the overall
accuracy for the trainee, competent, and expert endoscopists improves from
0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To our
knowledge, our model is the first application of foundation models for EGJA
staging diagnosis and demonstrates great potential in both diagnostic accuracy
and efficiency.
\\ ( https://arxiv.org/abs/2509.17660 ,  14178kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17664
Date: Mon, 22 Sep 2025 12:08:12 GMT   (6683kb)

Title: SD-VLM: Spatial Measuring and Understanding with Depth-Encoded
  Vision-Language Models
Authors: Pingyi Chen, Yujing Lou, Shen Cao, Jinhui Guo, Lubin Fan, Yue Wu, Lin
  Yang, Lizhuang Ma, Jieping Ye
Categories: cs.CV cs.AI
Comments: Accepted by NeurIPS 2025
\\
  While vision language models (VLMs) excel in 2D semantic visual
understanding, their ability to quantitatively reason about 3D spatial
relationships remains under-explored, due to the deficiency of 2D images'
spatial representation ability. In this paper, we analyze the problem hindering
VLMs' spatial understanding abilities and propose SD-VLM, a novel framework
that significantly enhances fundamental spatial perception abilities of VLMs
through two key contributions: (1) propose Massive Spatial Measuring and
Understanding (MSMU) dataset with precise spatial annotations, and (2)
introduce a simple depth positional encoding method strengthening VLMs' spatial
awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA
pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented
samples. We have trained SD-VLM, a strong generalist VLM which shows superior
quantitative spatial measuring and understanding capability. SD-VLM not only
achieves state-of-the-art performance on our proposed MSMU-Bench, but also
shows spatial generalization abilities on other spatial understanding
benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments
demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and
25.56% respectively on MSMU-Bench. Code and models are released at
https://github.com/cpystan/SD-VLM.
\\ ( https://arxiv.org/abs/2509.17664 ,  6683kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17670
Date: Mon, 22 Sep 2025 12:13:58 GMT   (22147kb)

Title: Tailored Transformation Invariance for Industrial Anomaly Detection
Authors: Mariette Sch\"onfeld, Wannes Meert, Hendrik Blockeel
Categories: cs.CV cs.LG
\\
  Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision
Anomaly Detection that has been receiving increasing amounts of attention due
to its applicability to real-life scenarios. Recent research has focused on how
to extract the most informative features, contrasting older kNN-based methods
that use only pretrained features. These recent methods are much more expensive
to train however and could complicate real-life application. Careful study of
related work with regards to transformation invariance leads to the idea that
popular benchmarks require robustness to only minor translations. With this
idea we then formulate LWinNN, a local window based approach that creates a
middle ground between kNN based methods that have either complete or no
translation invariance. Our experiments demonstrate that this small change
increases accuracy considerably, while simultaneously decreasing both train and
test time. This teaches us two things: first, the gap between kNN-based
approaches and more complex state-of-the-art methodology can still be narrowed
by effective usage of the limited data available. Second, our assumption of
requiring only limited translation invariance highlights potential areas of
interest for future work and the need for more spatially diverse benchmarks,
for which our method can hopefully serve as a new baseline. Our code can be
found at https://github.com/marietteschonfeld/LWinNN .
\\ ( https://arxiv.org/abs/2509.17670 ,  22147kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17684
Date: Mon, 22 Sep 2025 12:27:26 GMT   (632kb)

Title: DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for
  Visuomotor Diffusion Policy Learning
Authors: ThankGod Egbe, Peng Wang, Zhihao Guo, Zidong Chen
Categories: cs.CV cs.RO
\\
  This paper evaluates DINOv3, a recent large-scale self-supervised vision
backbone, for visuomotor diffusion policy learning in robotic manipulation. We
investigate whether a purely self-supervised encoder can match or surpass
conventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under
three regimes: training from scratch, frozen, and finetuned. Across four
benchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned
diffusion policy, we find that (i) finetuned DINOv3 matches or exceeds
ResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating
strong transferable priors, and (iii) self-supervised features improve sample
efficiency and robustness. These results support self-supervised large visual
models as effective, generalizable perceptual front-ends for action diffusion
policies, motivating further exploration of scalable label-free pretraining in
robotic manipulation. Compared to using ResNet18 as a backbone, our approach
with DINOv3 achieves up to a 10% absolute increase in test-time success rates
on challenging tasks such as Can, and on-the-par performance in tasks like
Lift, PushT, and Square.
\\ ( https://arxiv.org/abs/2509.17684 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17686
Date: Mon, 22 Sep 2025 12:28:29 GMT   (12247kb)

Title: Predicting Depth Maps from Single RGB Images and Addressing Missing
  Information in Depth Estimation
Authors: Mohamad Mofeed Chaar, Jamal Raiyn, and Galia Weidl
Categories: cs.CV cs.AI
Comments: 8 pages, 10 figures, VEHITS conference 2025
DOI: 10.5220/0013365900003941
\\
  Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it
plays a key role in detecting and measuring objects in the vehicle's
surroundings. However, a significant challenge in this domain arises from
missing information in Depth images, where certain points are not measurable
due to gaps or inconsistencies in pixel data. Our research addresses two key
tasks to overcome this challenge. First, we developed an algorithm using a
multi-layered training approach to generate Depth images from a single RGB
image. Second, we addressed the issue of missing information in Depth images by
applying our algorithm to rectify these gaps, resulting in Depth images with
complete and accurate data. We further tested our algorithm on the Cityscapes
dataset and successfully resolved the missing information in its Depth images,
demonstrating the effectiveness of our approach in real-world urban
environments.
\\ ( https://arxiv.org/abs/2509.17686 ,  12247kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17689
Date: Mon, 22 Sep 2025 12:29:44 GMT   (567kb)

Title: FROQ: Observing Face Recognition Models for Efficient Quality Assessment
Authors: \v{Z}iga Babnik, Deepak Kumar Jain, Peter Peer, Vitomir \v{S}truc
Categories: cs.CV
Comments: Presented at the International Joint Conference on Biometrics (IJCB
  2025)
\\
  Face Recognition (FR) plays a crucial role in many critical (high-stakes)
applications, where errors in the recognition process can lead to serious
consequences. Face Image Quality Assessment (FIQA) techniques enhance FR
systems by providing quality estimates of face samples, enabling the systems to
discard samples that are unsuitable for reliable recognition or lead to
low-confidence recognition decisions. Most state-of-the-art FIQA techniques
rely on extensive supervised training to achieve accurate quality estimation.
In contrast, unsupervised techniques eliminate the need for additional training
but tend to be slower and typically exhibit lower performance. In this paper,
we introduce FROQ (Face Recognition Observer of Quality), a semi-supervised,
training-free approach that leverages specific intermediate representations
within a given FR model to estimate face-image quality, and combines the
efficiency of supervised FIQA models with the training-free approach of
unsupervised methods. A simple calibration step based on pseudo-quality labels
allows FROQ to uncover specific representations, useful for quality assessment,
in any modern FR model. To generate these pseudo-labels, we propose a novel
unsupervised FIQA technique based on sample perturbations. Comprehensive
experiments with four state-of-the-art FR models and eight benchmark datasets
show that FROQ leads to highly competitive results compared to the
state-of-the-art, achieving both strong performance and efficient runtime,
without requiring explicit training.
\\ ( https://arxiv.org/abs/2509.17689 ,  567kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17702
Date: Mon, 22 Sep 2025 12:42:10 GMT   (2548kb)

Title: Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised
  Semantic Segmentation
Authors: Patrick Schmidt, Vasileios Belagiannis, Lazaros Nalpantidis
Categories: cs.CV
Comments: Submitted to IEEE
\\
  Autonomous robotic systems applied to new domains require an abundance of
expensive, pixel-level dense labels to train robust semantic segmentation
models under full supervision. This study proposes a model-agnostic Depth Edge
Alignment Loss to improve Weakly Supervised Semantic Segmentation models across
different datasets. The methodology generates pixel-level semantic labels from
image-level supervision, avoiding expensive annotation processes. While weak
supervision is widely explored in traditional computer vision, our approach
adds supervision with pixel-level depth information, a modality commonly
available in robotic systems. We demonstrate how our approach improves
segmentation performance across datasets and models, but can also be combined
with other losses for even better performance, with improvements up to +5.439,
+1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC /
MS COCO validation, and the HOPE static onboarding split, respectively. Our
code will be made publicly available.
\\ ( https://arxiv.org/abs/2509.17702 ,  2548kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17704
Date: Mon, 22 Sep 2025 12:43:19 GMT   (3458kb)

Title: Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image
  Fusion
Authors: Bo Li, Yunkuo Lei, Tingting Bao, Yaxian Wang, Lingling Zhang, and Jun
  Liu
Categories: cs.CV
Comments: 10 pages, 8 figures
\\
  Multi-focus image fusion (MFIF) is a crucial technique in image processing,
with a key challenge being the generation of decision maps with precise
boundaries. However, traditional methods based on heuristic rules and deep
learning methods with black-box mechanisms are difficult to generate
high-quality decision maps. To overcome this challenge, we introduce
neurodynamics-driven coupled neural P (CNP) systems, which are third-generation
neural computation models inspired by spiking mechanisms, to enhance the
accuracy of decision maps. Specifically, we first conduct an in-depth analysis
of the model's neurodynamics to identify the constraints between the network
parameters and the input signals. This solid analysis avoids abnormal
continuous firing of neurons and ensures the model accurately distinguishes
between focused and unfocused regions, generating high-quality decision maps
for MFIF. Based on this analysis, we propose a
\textbf{N}eurodynamics-\textbf{D}riven \textbf{CNP} \textbf{F}usion model
(\textbf{ND-CNPFuse}) tailored for the challenging MFIF task. Unlike current
ideas of decision map generation, ND-CNPFuse distinguishes between focused and
unfocused regions by mapping the source image into interpretable spike
matrices. By comparing the number of spikes, an accurate decision map can be
generated directly without any post-processing. Extensive experimental results
show that ND-CNPFuse achieves new state-of-the-art performance on four
classical MFIF datasets, including Lytro, MFFW, MFI-WHU, and Real-MFF. The code
is available at https://github.com/MorvanLi/ND-CNPFuse.
\\ ( https://arxiv.org/abs/2509.17704 ,  3458kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17707
Date: Mon, 22 Sep 2025 12:45:35 GMT   (3805kb)

Title: Automatic Intermodal Loading Unit Identification using Computer Vision:
  A Scoping Review
Authors: Emre G\"ulsoylu, Alhassan Abdelhalim, Derya Kara Boztas, Ole Grasse,
  Carlos Jahn, Simone Frintrop, Janick Edinger
Categories: cs.CV
Comments: Submission to Transport Reviews. 36 pages, 2 figures, 4 tables
\\
  The standardisation of Intermodal Loading Units (ILUs), such as containers,
semi-trailers and swap bodies, has revolutionised global trade yet their
efficient and robust identification remains a critical bottleneck in
high-throughput ports and terminals. This paper reviews 63 empirical studies
that propose computer vision (CV) based solutions. It covers the last 35 years
(1990-2025), tracing the field's evolution from early digital image processing
(DIP) and traditional machine learning (ML) to the current dominance of deep
learning (DL) techniques. While CV offers cost-effective alternatives for other
types of identification techniques, its development is hindered by the lack of
publicly available benchmarking datasets. This results in high variance for the
reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring. To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.
\\ ( https://arxiv.org/abs/2509.17707 ,  3805kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17712
Date: Mon, 22 Sep 2025 12:49:49 GMT   (9190kb)

Title: RCTDistill: Cross-Modal Knowledge Distillation Framework for
  Radar-Camera 3D Object Detection with Temporal Fusion
Authors: Geonho Bang, Minjae Seong, Jisong Kim, Geunju Baek, Daye Oh, Junhyung
  Kim, Junho Koh, Jun Won Choi
Categories: cs.CV
Comments: Accepted at ICCV 2025
\\
  Radar-camera fusion methods have emerged as a cost-effective approach for 3D
object detection but still lag behind LiDAR-based methods in performance.
Recent works have focused on employing temporal fusion and Knowledge
Distillation (KD) strategies to overcome these limitations. However, existing
approaches have not sufficiently accounted for uncertainties arising from
object motion or sensor-specific errors inherent in radar and camera
modalities. In this work, we propose RCTDistill, a novel cross-modal KD method
based on temporal fusion, comprising three key modules: Range-Azimuth Knowledge
Distillation (RAKD), Temporal Knowledge Distillation (TKD), and
Region-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider
the inherent errors in the range and azimuth directions, enabling effective
knowledge transfer from LiDAR features to refine inaccurate BEV
representations. TKD mitigates temporal misalignment caused by dynamic objects
by aligning historical radar-camera BEV features with current LiDAR
representations. RDKD enhances feature discrimination by distilling relational
knowledge from the teacher model, allowing the student to differentiate
foreground and background features. RCTDistill achieves state-of-the-art
radar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)
datasets, with the fastest inference speed of 26.2 FPS.
\\ ( https://arxiv.org/abs/2509.17712 ,  9190kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17726
Date: Mon, 22 Sep 2025 12:57:21 GMT   (8314kb)

Title: Automated Labeling of Intracranial Arteries with Uncertainty
  Quantification Using Deep Learning
Authors: Javier Bisbal, Patrick Winter, Sebastian Jofre, Aaron Ponce, Sameer A.
  Ansari, Ramez Abdalla, Michael Markl, Oliver Welin Odeback, Sergio Uribe,
  Cristian Tejos, Julio Sotelo, Susanne Schnell, David Marlevi
Categories: cs.CV cs.LG
Comments: 16 pages, 6 figures
MSC-class: I.4.0
\\
  Accurate anatomical labeling of intracranial arteries is essential for
cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming
and subject to interoperator variability. We present a deep learning-based
framework for automated artery labeling from 3D Time-of-Flight Magnetic
Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating
uncertainty quantification to enhance interpretability and reliability. We
evaluated three convolutional neural network architectures: (1) a UNet with
residual encoder blocks, reflecting commonly used baselines in vascular
labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and
spatial attention mechanisms for enhanced curvilinear structure recognition;
and (3) nnUNet, a self-configuring framework that automates preprocessing,
training, and architectural adaptation based on dataset characteristics. Among
these, nnUNet achieved the highest labeling performance (average Dice score:
0.922; average surface distance: 0.387 mm), with improved robustness in
anatomically complex vessels. To assess predictive confidence, we implemented
test-time augmentation (TTA) and introduced a novel coordinate-guided strategy
to reduce interpolation errors during augmented inference. The resulting
uncertainty maps reliably indicated regions of anatomical ambiguity,
pathological variation, or manual labeling inconsistency. We further validated
clinical utility by comparing flow velocities derived from automated and manual
labels in co-registered 4D Flow MRI datasets, observing close agreement with no
statistically significant differences. Our framework offers a scalable,
accurate, and uncertainty-aware solution for automated cerebrovascular
labeling, supporting downstream hemodynamic analysis and facilitating clinical
integration.
\\ ( https://arxiv.org/abs/2509.17726 ,  8314kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17740
Date: Mon, 22 Sep 2025 13:05:29 GMT   (7740kb)

Title: WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal
  LLMs in Image Classification
Authors: Yiwen Jiang, Deval Mehta, Siyuan Yan, Yaling Shen, Zimu Wang, Zongyuan
  Ge
Categories: cs.CV cs.CL
Comments: Accepted at EMNLP 2025 (Main)
\\
  Multimodal Large Language Models (MLLMs) have shown promise in visual-textual
reasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly
enhancing interpretability. However, existing MCoT methods rely on
rationale-rich datasets and largely focus on inter-object reasoning,
overlooking the intra-object understanding crucial for image classification. To
address this gap, we propose WISE, a Weak-supervision-guided Step-by-step
Explanation method that augments any image classification dataset with MCoTs by
reformulating the concept-based representations from Concept Bottleneck Models
(CBMs) into concise, interpretable reasoning chains under weak supervision.
Experiments across ten datasets show that our generated MCoTs not only improve
interpretability by 37% but also lead to gains in classification accuracy when
used to fine-tune MLLMs. Our work bridges concept-based interpretability and
generative MCoT reasoning, providing a generalizable framework for enhancing
MLLMs in fine-grained visual understanding.
\\ ( https://arxiv.org/abs/2509.17740 ,  7740kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17743
Date: Mon, 22 Sep 2025 13:06:17 GMT   (3098kb)

Title: Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA
Authors: Chenglin Li, Feng Han, FengTao, Ruilin Li, Qianglong Chen, Jingqi
  Tong, Yin Zhang, Jiaqi Wang
Categories: cs.CV
\\
  Large language models (LLMs) have shown promise in generating program
workflows for visual tasks. However, previous approaches often rely on
closed-source models, lack systematic reasoning, and struggle with long-form
video question answering (videoQA). To address these challenges, we introduce
the FS-VisPR framework, an adaptive visual program reasoning approach that
balances fast reasoning for simple queries with slow reasoning for difficult
ones. First, we design efficient visual modules (e.g., key clip retrieval and
subtitle retrieval) to support long-form video tasks. Then, we construct a
diverse and high-quality fast-slow reasoning dataset with a strong LLM to align
open-source language models' ability to generate visual program workflows as
FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple
queries are directly solved by VideoLLMs, while difficult ones invoke visual
program reasoning, motivated by human-like reasoning processes. During this
process, low-confidence fast-thinking answers will trigger a second-stage
slow-reasoning process, and a fallback mechanism to fast reasoning is activated
if the program execution fails. Moreover, we improve visual programs through
parameter search during both training and inference. By adjusting the
parameters of the visual modules within the program, multiple variants are
generated: during training, programs that yield correct answers are selected,
while during inference, the program with the highest confidence result is
applied. Experiments show that FS-VisPR improves both efficiency and
reliability in visual program workflows. It achieves 50.4% accuracy on LVBench,
surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.
\\ ( https://arxiv.org/abs/2509.17743 ,  3098kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17747
Date: Mon, 22 Sep 2025 13:11:12 GMT   (1708kb)

Title: Dual-View Alignment Learning with Hierarchical-Prompt for
  Class-Imbalance Multi-Label Classification
Authors: Sheng Huang and Jiexuan Yan and Beiyan Liu and Bo Liu and Richang Hong
Categories: cs.CV cs.AI
Comments: accepted by IEEE Transactions on Image Processing
\\
  Real-world datasets often exhibit class imbalance across multiple categories,
manifesting as long-tailed distributions and few-shot scenarios. This is
especially challenging in Class-Imbalanced Multi-Label Image Classification
(CI-MLIC) tasks, where data imbalance and multi-object recognition present
significant obstacles. To address these challenges, we propose a novel method
termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which
leverages multi-modal knowledge from vision-language pretrained (VLP) models to
mitigate the class-imbalance problem in multi-label settings. Specifically,
HP-DVAL employs dual-view alignment learning to transfer the powerful feature
representation capabilities from VLP models by extracting complementary
features for accurate image-text alignment. To better adapt VLP models for
CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes
global and local prompts to learn task-specific and context-related prior
knowledge. Additionally, we design a semantic consistency loss during prompt
tuning to prevent learned prompts from deviating from general knowledge
embedded in VLP models. The effectiveness of our approach is validated on two
CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results
demonstrate the superiority of our method over SOTA approaches, achieving mAP
improvements of 10.0\% and 5.2\% on the long-tailed multi-label image
classification task, and 6.8\% and 2.9\% on the multi-label few-shot image
classification task.
\\ ( https://arxiv.org/abs/2509.17747 ,  1708kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17757
Date: Mon, 22 Sep 2025 13:20:06 GMT   (4809kb)

Title: Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained
  Semantic Guidance
Authors: Hongxing Fan, Lipeng Wang, Haohua Chen, Zehuan Huang, Jiangtao Wu, Lu
  Sheng
Categories: cs.CV cs.MA
DOI: 10.1145/3746027.3755225
\\
  Amodal completion, generating invisible parts of occluded objects, is vital
for applications like image editing and AR. Prior methods face challenges with
data needs, generalization, or error accumulation in progressive pipelines. We
propose a Collaborative Multi-Agent Reasoning Framework based on upfront
collaborative reasoning to overcome these issues. Our framework uses multiple
agents to collaboratively analyze occlusion relationships and determine
necessary boundary expansion, yielding a precise mask for inpainting.
Concurrently, an agent generates fine-grained textual descriptions, enabling
Fine-Grained Semantic Guidance. This ensures accurate object synthesis and
prevents the regeneration of occluders or other unwanted elements, especially
within large inpainting areas. Furthermore, our method directly produces
layered RGBA outputs guided by visible masks and attention maps from a
Diffusion Transformer, eliminating extra segmentation. Extensive evaluations
demonstrate our framework achieves state-of-the-art visual quality.
\\ ( https://arxiv.org/abs/2509.17757 ,  4809kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17762
Date: Mon, 22 Sep 2025 13:24:58 GMT   (19191kb)

Title: Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene
  Reconstruction
Authors: Sitian Shen, Georgi Pramatarov, Yifu Tao, Daniele De Martini
Categories: cs.CV
\\
  This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal
large-scale scene reconstruction that fuses multiple sensing modalities in a
per-gaussian compact, learnable embedding. While recent works focusing on
large-scale scene reconstruction have incorporated LiDAR data to provide more
accurate geometric constraints, we argue that LiDAR's rich physical properties
remain underexplored. Similarly, semantic information has been used for object
retrieval, but could provide valuable high-level context for scene
reconstruction. Traditional approaches append these properties to Gaussians as
separate parameters, increasing memory usage and limiting information exchange
across modalities. Instead, our approach fuses all modalities -- image, LiDAR,
and semantics -- into a compact, learnable embedding that implicitly encodes
optical, physical, and semantic features in each Gaussian. We then train
lightweight neural decoders to map these embeddings to Gaussian parameters,
enabling the reconstruction of each sensing modality with lower memory overhead
and improved scalability. We evaluate Neural-MMGS on the Oxford Spires and
KITTI-360 datasets. On Oxford Spires, we achieve higher-quality
reconstructions, while on KITTI-360, our method reaches competitive results
with less storage consumption compared with current approaches in LiDAR-based
novel-view synthesis.
\\ ( https://arxiv.org/abs/2509.17762 ,  19191kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17769
Date: Mon, 22 Sep 2025 13:33:31 GMT   (988kb)

Title: Incorporating the Refractory Period into Spiking Neural Networks through
  Spike-Triggered Threshold Dynamics
Authors: Yang Li, Xinyi Zeng, Zhe Xue, Pinxian Zeng, Zikai Zhang, Yan Wang
Categories: cs.CV
\\
  As the third generation of neural networks, spiking neural networks (SNNs)
have recently gained widespread attention for their biological plausibility,
energy efficiency, and effectiveness in processing neuromorphic datasets. To
better emulate biological neurons, various models such as Integrate-and-Fire
(IF) and Leaky Integrate-and-Fire (LIF) have been widely adopted in SNNs.
However, these neuron models overlook the refractory period, a fundamental
characteristic of biological neurons. Research on excitable neurons reveal that
after firing, neurons enter a refractory period during which they are
temporarily unresponsive to subsequent stimuli. This mechanism is critical for
preventing over-excitation and mitigating interference from aberrant signals.
Therefore, we propose a simple yet effective method to incorporate the
refractory period into spiking LIF neurons through spike-triggered threshold
dynamics, termed RPLIF. Our method ensures that each spike accurately encodes
neural information, effectively preventing neuron over-excitation under
continuous inputs and interference from anomalous inputs. Incorporating the
refractory period into LIF neurons is seamless and computationally efficient,
enhancing robustness and efficiency while yielding better performance with
negligible overhead. To the best of our knowledge, RPLIF achieves
state-of-the-art performance on Cifar10-DVS(82.40%) and N-Caltech101(83.35%)
with fewer timesteps and demonstrates superior performance on DVS128
Gesture(97.22%) at low latency.
\\ ( https://arxiv.org/abs/2509.17769 ,  988kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17773
Date: Mon, 22 Sep 2025 13:37:37 GMT   (1187kb)

Title: I2VWM: Robust Watermarking for Image to Video Generation
Authors: Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang
Categories: cs.CV
Comments: 10 pages
\\
  The rapid progress of image-guided video generation (I2V) has raised concerns
about its potential misuse in misinformation and fraud, underscoring the urgent
need for effective digital watermarking. While existing watermarking methods
demonstrate robustness within a single modality, they fail to trace source
images in I2V settings. To address this gap, we introduce the concept of Robust
Diffusion Distance, which measures the temporal persistence of watermark
signals in generated videos. Building on this, we propose I2VWM, a cross-modal
watermarking framework designed to enhance watermark robustness across time.
I2VWM leverages a video-simulation noise layer during training and employs an
optical-flow-based alignment module during inference. Experiments on both
open-source and commercial I2V models demonstrate that I2VWM significantly
improves robustness while maintaining imperceptibility, establishing a new
paradigm for cross-modal watermarking in the era of generative video.
\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code
Released.}
\\ ( https://arxiv.org/abs/2509.17773 ,  1187kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17786
Date: Mon, 22 Sep 2025 13:48:15 GMT   (919kb)

Title: Accurate and Efficient Low-Rank Model Merging in Core Space
Authors: Aniello Panariello, Daniel Marczak, Simone Magistri, Angelo Porrello,
  Bart{\l}omiej Twardowski, Andrew D. Bagdanov, Simone Calderara, Joost van de
  Weijer
Categories: cs.CV cs.AI
Comments: Accepted at 39th Conference on Neural Information Processing Systems
  (NeurIPS 2025), San Diego, USA
\\
  In this paper, we address the challenges associated with merging low-rank
adaptations of large neural networks. With the rise of parameter-efficient
adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning
has become more accessible. While fine-tuning models with LoRA is highly
efficient, existing merging methods often sacrifice this efficiency by merging
fully-sized weight matrices. We propose the Core Space merging framework, which
enables the merging of LoRA-adapted models within a common alignment basis,
thereby preserving the efficiency of low-rank adaptation while substantially
improving accuracy across tasks. We further provide a formal proof that
projection into Core Space ensures no loss of information and provide a
complexity analysis showing the efficiency gains. Extensive empirical results
demonstrate that Core Space significantly improves existing merging techniques
and achieves state-of-the-art results on both vision and language tasks while
utilizing a fraction of the computational resources. Codebase is available at
https://github.com/apanariello4/core-space-merging.
\\ ( https://arxiv.org/abs/2509.17786 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17789
Date: Mon, 22 Sep 2025 13:50:20 GMT   (5074kb)

Title: From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for
  Underwater Scenes
Authors: Guoxi Huang, Haoran Wang, Zipeng Qi, Wenjun Lu, David Bull, Nantheera
  Anantrasirichai
Categories: cs.CV
\\
  Underwater image degradation poses significant challenges for 3D
reconstruction, where simplified physical models often fail in complex scenes.
We propose \textbf{R-Splatting}, a unified framework that bridges underwater
image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both
rendering quality and geometric fidelity. Our method integrates multiple
enhanced views produced by diverse UIR models into a single reconstruction
pipeline. During inference, a lightweight illumination generator samples latent
codes to support diverse yet coherent renderings, while a contrastive loss
ensures disentangled and stable illumination representations. Furthermore, we
propose \textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models
opacity as a stochastic function to regularize training. This suppresses abrupt
gradient responses triggered by illumination variation and mitigates
overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF
and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong
baselines in both rendering quality and geometric accuracy.
\\ ( https://arxiv.org/abs/2509.17789 ,  5074kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17792
Date: Mon, 22 Sep 2025 13:51:09 GMT   (14336kb)

Title: Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding
Authors: S M A Sharif, Abdur Rehman, Fayaz Ali Dharejo, Radu Timofte, Rizwan
  Ali Naqvi
Categories: cs.CV
\\
  Real-world images often suffer from spatially diverse degradations such as
haze, rain, snow, and low-light, significantly impacting visual quality and
downstream vision tasks. Existing all-in-one restoration (AIR) approaches
either depend on external text prompts or embed hand-crafted architectural
priors (e.g., frequency heuristics); both impose discrete, brittle assumptions
that weaken generalization to unseen or mixed degradations. To address this
limitation, we propose to reframe AIR as learned latent prior inference, where
degradation-aware representations are automatically inferred from the input
without explicit task cues. Based on latent priors, we formulate AIR as a
structured reasoning paradigm: (1) which features to route (adaptive feature
selection), (2) where to restore (spatial localization), and (3) what to
restore (degradation semantics). We design a lightweight decoding module that
efficiently leverages these latent encoded cues for spatially-adaptive
restoration. Extensive experiments across six common degradation tasks, five
compound settings, and previously unseen degradations demonstrate that our
method outperforms state-of-the-art (SOTA) approaches, achieving an average
PSNR improvement of 1.68 dB while being three times more efficient.
\\ ( https://arxiv.org/abs/2509.17792 ,  14336kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17802
Date: Mon, 22 Sep 2025 13:57:58 GMT   (905kb)

Title: TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided
  Medical Time Series Classification
Authors: Qi'ao Xu, Pengfei Wang, Bo Zhong, Tianwen Qian, Xiaoling Wang, Ye
  Wang, Hong Yu
Categories: cs.CV cs.AI
Comments: 12 pages, 4 figures
\\
  Medical time series (MedTS) classification is pivotal for intelligent
healthcare, yet its efficacy is severely limited by poor cross-subject
generation due to the profound cross-individual heterogeneity. Despite advances
in architectural innovations and transfer learning techniques, current methods
remain constrained by modality-specific inductive biases that limit their
ability to learn universally invariant representations. To overcome this, we
propose TS-P$^2$CL, a novel plug-and-play framework that leverages the
universal pattern recognition capabilities of pre-trained vision models. We
introduce a vision-guided paradigm that transforms 1D physiological signals
into 2D pseudo-images, establishing a bridge to the visual domain. This
transformation enables implicit access to rich semantic priors learned from
natural images. Within this unified space, we employ a dual-contrastive
learning strategy: intra-modal consistency enforces temporal coherence, while
cross-modal alignment aligns time-series dynamics with visual semantics,
thereby mitigating individual-specific biases and learning robust,
domain-invariant features. Extensive experiments on six MedTS datasets
demonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both
subject-dependent and subject-independent settings.
\\ ( https://arxiv.org/abs/2509.17802 ,  905kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17805
Date: Mon, 22 Sep 2025 14:00:20 GMT   (1071kb)

Title: Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric
  Assessment of 2D Projections
Authors: Dong Chen, Huili Peng, Yong Hu, and Kenneth MC. Cheung
Categories: cs.CV
\\
  Objective: To systematically quantify the effect of the camera view (frontal
vs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D
motion capture ground truth. Methods: Gait data from 18 subjects were recorded
simultaneously using frontal, lateral and 3D motion capture systems. Pose
estimation used YOLOv8. Four metrics were assessed to evaluate agreement:
Dynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation
(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution
differences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank
tests (significance: $p < 0.05$) and Cliff's delta ($\delta$) were used to
measure statistical differences and effect sizes. Results: Lateral views
significantly outperformed frontal views for sagittal plane kinematics: step
length (DTW: $53.08 \pm 24.50$ vs. $69.87 \pm 25.36$, $p = 0.005$) and knee
rotation (DTW: $106.46 \pm 38.57$ vs. $155.41 \pm 41.77$, $p = 0.004$). Frontal
views were superior for symmetry parameters: trunk rotation (KLD: $0.09 \pm
0.06$ vs. $0.30 \pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:
$105.77 \pm 29.72$ vs. $75.20 \pm 20.38$, $p = 0.003$). Effect sizes were
medium-to-large ($\delta: 0.34$--$0.76$). Conclusion: Camera view critically
impacts gait parameter accuracy. Lateral views are optimal for sagittal
kinematics; frontal views excel for trunk symmetry. Significance: This first
systematic evidence enables data-driven camera deployment in 2D gait analysis,
enhancing clinical utility. Future implementations should leverage both views
via disease-oriented setups.
\\ ( https://arxiv.org/abs/2509.17805 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17816
Date: Mon, 22 Sep 2025 14:11:02 GMT   (5385kb)

Title: Enhancing Semantic Segmentation with Continual Self-Supervised
  Pre-training
Authors: Brown Ebouky, Ajad Chhatkuli, Cristiano Malossi, Christoph Studer, Roy
  Assaf, Andrea Bartezzaghi
Categories: cs.CV
Comments: 24 pages, 5 figures
\\
  Self-supervised learning (SSL) has emerged as a central paradigm for training
foundation models by leveraging large-scale unlabeled datasets, often producing
representations with strong generalization capabilities. These models are
typically pre-trained on general-purpose datasets such as ImageNet and
subsequently adapted to various downstream tasks through finetuning. While
recent advances have explored parameter-efficient strategies for adapting
pre-trained models, extending SSL pre-training itself to new domains -
particularly under limited data regimes and for dense prediction tasks -
remains underexplored. In this work, we address the problem of adapting vision
foundation models to new domains in an unsupervised and data-efficient manner,
specifically targeting downstream semantic segmentation. We propose GLARE
(Global Local and Regional Enforcement), a novel continual self-supervised
pre-training task designed to enhance downstream segmentation performance.
GLARE introduces patch-level augmentations to encourage local consistency and
incorporates a regional consistency constraint that leverages spatial semantics
in the data. For efficient continual pre-training, we initialize Vision
Transformers (ViTs) with weights from existing SSL models and update only
lightweight adapter modules - specifically UniAdapter - while keeping the rest
of the backbone frozen. Experiments across multiple semantic segmentation
benchmarks on different domains demonstrate that GLARE consistently improves
downstream performance with minimal computational and parameter overhead.
\\ ( https://arxiv.org/abs/2509.17816 ,  5385kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17818
Date: Mon, 22 Sep 2025 14:13:31 GMT   (26920kb)

Title: ContextFlow: Training-Free Video Object Editing via Adaptive Context
  Enrichment
Authors: Yiyang Chen, Xuanhua He, Xiujun Ma, Yue Ma
Categories: cs.CV
Comments: The project page is at https://yychen233.github.io/ContextFlow-page
\\
  Training-free video object editing aims to achieve precise object-level
manipulation, including object insertion, swapping, and deletion. However, it
faces significant challenges in maintaining fidelity and temporal consistency.
Existing methods, often designed for U-Net architectures, suffer from two
primary limitations: inaccurate inversion due to first-order solvers, and
contextual conflicts caused by crude "hard" feature replacement. These issues
are more challenging in Diffusion Transformers (DiTs), where the unsuitability
of prior layer-selection heuristics makes effective guidance challenging. To
address these limitations, we introduce ContextFlow, a novel training-free
framework for DiT-based video object editing. In detail, we first employ a
high-order Rectified Flow solver to establish a robust editing foundation. The
core of our framework is Adaptive Context Enrichment (for specifying what to
edit), a mechanism that addresses contextual conflicts. Instead of replacing
features, it enriches the self-attention context by concatenating Key-Value
pairs from parallel reconstruction and editing paths, empowering the model to
dynamically fuse information. Additionally, to determine where to apply this
enrichment (for specifying where to edit), we propose a systematic, data-driven
analysis to identify task-specific vital layers. Based on a novel Guidance
Responsiveness Metric, our method pinpoints the most influential DiT blocks for
different tasks (e.g., insertion, swapping), enabling targeted and highly
effective guidance. Extensive experiments show that ContextFlow significantly
outperforms existing training-free methods and even surpasses several
state-of-the-art training-based approaches, delivering temporally coherent,
high-fidelity results.
\\ ( https://arxiv.org/abs/2509.17818 ,  26920kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17847
Date: Mon, 22 Sep 2025 14:41:43 GMT   (25670kb)

Title: Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous
  Tissue Synthesis in Histopathology
Authors: Saghir Alfasly, Wataru Uegami, MD Enamul Hoq, Ghazal Alabtah, H.R.
  Tizhoosh
Categories: cs.CV
Comments: NeurIPS 2025
\\
  Synthetic data generation in histopathology faces unique challenges:
preserving tissue heterogeneity, capturing subtle morphological features, and
scaling to unannotated datasets. We present a latent diffusion model that
generates realistic heterogeneous histopathology images through a novel
dual-conditioning approach combining semantic segmentation maps with
tissue-specific visual crops. Unlike existing methods that rely on text prompts
or abstract visual embeddings, our approach preserves critical morphological
details by directly incorporating raw tissue crops from corresponding semantic
regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches
ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we
introduce a self-supervised extension that clusters whole-slide images into 100
tissue types using foundation model embeddings, automatically generating
pseudo-semantic maps for training. Our method synthesizes high-fidelity images
with precise region-wise annotations, achieving superior performance on
downstream segmentation tasks. When evaluated on annotated datasets, models
trained on our synthetic data show competitive performance to those trained on
real data, demonstrating the utility of controlled heterogeneous tissue
generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet
Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower
FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on
synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within
1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA
whole-slide images without manual annotations, our framework offers a practical
solution for an urgent need for generating diverse, annotated histopathology
data, addressing a critical bottleneck in computational pathology.
\\ ( https://arxiv.org/abs/2509.17847 ,  25670kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17864
Date: Mon, 22 Sep 2025 14:58:11 GMT   (18159kb)

Title: ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting
  from Monocular Videos
Authors: Shi Chen, Erik Sandstr\"om, Sandro Lombardi, Siyuan Li, Martin R.
  Oswald
Categories: cs.CV
\\
  Achieving truly practical dynamic 3D reconstruction requires online
operation, global pose and map consistency, detailed appearance modeling, and
the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM
methods typically merely remove the dynamic parts or require RGB-D input, while
offline methods are not scalable to long video sequences, and current
transformer-based feedforward methods lack global consistency and appearance
details. To this end, we achieve online dynamic scene reconstruction by
disentangling the static and dynamic parts within a SLAM system. The poses are
tracked robustly with a novel motion masking strategy, and dynamic parts are
reconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.
Our method yields novel view renderings competitive to offline methods and
achieves on-par tracking with state-of-the-art dynamic SLAM methods.
\\ ( https://arxiv.org/abs/2509.17864 ,  18159kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17888
Date: Mon, 22 Sep 2025 15:19:45 GMT   (866kb)

Title: Trainee Action Recognition through Interaction Analysis in CCATT
  Mixed-Reality Training
Authors: Divya Mereddy, Marcos Quinones-Grueiro, Ashwin T S, Eduardo Davalos,
  Gautam Biswas, Kent Etherton, Tyler Davis, Katelyn Kay, Jill Lear, Benjamin
  Goldberg
Categories: cs.CV cs.AI
\\
  This study examines how Critical Care Air Transport Team (CCATT) members are
trained using mixed-reality simulations that replicate the high-pressure
conditions of aeromedical evacuation. Each team - a physician, nurse, and
respiratory therapist - must stabilize severely injured soldiers by managing
ventilators, IV pumps, and suction devices during flight. Proficient
performance requires clinical expertise and cognitive skills, such as
situational awareness, rapid decision-making, effective communication, and
coordinated task management, all of which must be maintained under stress.
Recent advances in simulation and multimodal data analytics enable more
objective and comprehensive performance evaluation. In contrast, traditional
instructor-led assessments are subjective and may overlook critical events,
thereby limiting generalizability and consistency. However, AI-based automated
and more objective evaluation metrics still demand human input to train the AI
algorithms to assess complex team dynamics in the presence of environmental
noise and the need for accurate re-identification in multi-person tracking. To
address these challenges, we introduce a systematic, data-driven assessment
framework that combines Cognitive Task Analysis (CTA) with Multimodal Learning
Analytics (MMLA). We have developed a domain-specific CTA model for CCATT
training and a vision-based action recognition pipeline using a fine-tuned
Human-Object Interaction model, the Cascade Disentangling Network (CDN), to
detect and track trainee-equipment interactions over time. These interactions
automatically yield performance indicators (e.g., reaction time, task
duration), which are mapped onto a hierarchical CTA model tailored to CCATT
operations, enabling interpretable, domain-relevant performance evaluations.
\\ ( https://arxiv.org/abs/2509.17888 ,  866kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17901
Date: Mon, 22 Sep 2025 15:28:54 GMT   (417kb)

Title: Does Audio Matter for Modern Video-LLMs and Their Benchmarks?
Authors: Geewook Kim and Minjoon Seo
Categories: cs.CV cs.MM cs.SD
Comments: 5 pages, 2 figures, under review. Project page:
  https://github.com/naver-ai/LLaVA-AV-SSM
\\
  Modern multimodal large language models often claim "video understanding,"
yet most evaluations use muted videos or simply discard audio. We ask a direct
question: how much does audio actually matter for contemporary Video-LLMs and
the benchmarks that certify them? We audit widely used suites and observe that
many items are even solvable from a single frame, rendering audio largely
redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio
encoder (e.g., Whisper) and analyze when audio helps, while addressing audio
token explosion with a lightweight Mamba-based state-space token compressor. We
find that audio yields minimal gains on recent video benchmarks but is decisive
on curated, audio-sensitive subsets. To enable faithful evaluation, we release
AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a
growing gap between current academic practice and real-world expectations, and
provide practical tools for scalable audio-visual Video-LLMs. We will fully
open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.
\\ ( https://arxiv.org/abs/2509.17901 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17925
Date: Mon, 22 Sep 2025 15:50:59 GMT   (3541kb)

Title: SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain
  Brain Tumor Segmentation in MRI
Authors: Yuanhan Wang, Yifei Chen, Shuo Jiang, Wenjing Yu, Mingxuan Liu,
  Beining Wu, Jinying Zong, Feiwei Qin, Changmiao Wang, Qiyuan Tian
Categories: cs.CV
Comments: 11 pages, 6 figures
\\
  Reliable brain tumor segmentation in MRI is indispensable for treatment
planning and outcome monitoring, yet models trained on curated benchmarks often
fail under domain shifts arising from scanner and protocol variability as well
as population heterogeneity. Such gaps are especially severe in low-resource
and pediatric cohorts, where conventional test-time or source-free adaptation
strategies often suffer from instability and structural inconsistency. We
propose SmaRT, a style-modulated robust test-time adaptation framework that
enables source-free cross-domain generalization. SmaRT integrates style-aware
augmentation to mitigate appearance discrepancies, a dual-branch momentum
strategy for stable pseudo-label refinement, and structural priors enforcing
consistency, integrity, and connectivity. This synergy ensures both adaptation
stability and anatomical fidelity under extreme domain shifts. Extensive
evaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT
consistently outperforms state-of-the-art methods, with notable gains in Dice
accuracy and boundary precision. Overall, SmaRT bridges the gap between
algorithmic advances and equitable clinical applicability, supporting robust
deployment of MRI-based neuro-oncology tools in diverse clinical environments.
Our source code is available at https://github.com/baiyou1234/SmaRT.
\\ ( https://arxiv.org/abs/2509.17925 ,  3541kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17931
Date: Mon, 22 Sep 2025 15:53:37 GMT   (1358kb)

Title: Multi-needle Localization for Pelvic Seed Implant Brachytherapy based on
  Tip-handle Detection and Matching
Authors: Zhuo Xiao, Fugen Zhou, Jingjing Wang, Chongyu He, Bo Liu, Haitao Sun,
  Zhe Ji, Yuliang Jiang, Junjie Wang, Qiuwen Wu
Categories: cs.CV physics.med-ph
\\
  Accurate multi-needle localization in intraoperative CT images is crucial for
optimizing seed placement in pelvic seed implant brachytherapy. However, this
task is challenging due to poor image contrast and needle adhesion. This paper
presents a novel approach that reframes needle localization as a tip-handle
detection and matching problem to overcome these difficulties. An anchor-free
network, based on HRNet, is proposed to extract multi-scale features and
accurately detect needle tips and handles by predicting their centers and
orientations using decoupled branches for heatmap regression and polar angle
prediction. To associate detected tips and handles into individual needles, a
greedy matching and merging (GMM) method designed to solve the unbalanced
assignment problem with constraints (UAP-C) is presented. The GMM method
iteratively selects the most probable tip-handle pairs and merges them based on
a distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100
patients, the proposed method demonstrates superior performance, achieving
higher precision and F1 score compared to a segmentation-based method utilizing
the nnUNet model,thereby offering a more robust and accurate solution for
needle localization in complex clinical scenarios.
\\ ( https://arxiv.org/abs/2509.17931 ,  1358kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17943
Date: Mon, 22 Sep 2025 16:06:10 GMT   (429kb)

Title: Can multimodal representation learning by alignment preserve
  modality-specific information?
Authors: Romain Thoreau, Jessie Levillain, Dawa Derksen
Categories: cs.CV cs.LG
Comments: Accepted as a workshop paper at MACLEAN - ECML/PKDD 2025
\\
  Combining multimodal data is a key issue in a wide range of machine learning
tasks, including many remote sensing problems. In Earth observation, early
multimodal data fusion methods were based on specific neural network
architectures and supervised learning. Ever since, the scarcity of labeled data
has motivated self-supervised learning techniques. State-of-the-art multimodal
representation learning techniques leverage the spatial alignment between
satellite data from different modalities acquired over the same geographic area
in order to foster a semantic alignment in the latent space. In this paper, we
investigate how this methods can preserve task-relevant information that is not
shared across modalities. First, we show, under simplifying assumptions, when
alignment strategies fundamentally lead to an information loss. Then, we
support our theoretical insight through numerical experiments in more realistic
settings. With those theoretical and empirical evidences, we hope to support
new developments in contrastive learning for the combination of multimodal
satellite data. Our code and data is publicly available at
https://github.com/Romain3Ch216/alg_maclean_25.
\\ ( https://arxiv.org/abs/2509.17943 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17951
Date: Mon, 22 Sep 2025 16:10:13 GMT   (6739kb)

Title: DragOSM: Extract Building Roofs and Footprints from Aerial Images by
  Aligning Historical Labels
Authors: Kai Li, Xingxing Weng, Yupeng Deng, Yu Meng, Chao Pang, Gui-Song Xia,
  Xiangyu Zhao
Categories: cs.CV
Comments: 17 Pages
ACM-class: I.5.4
\\
  Extracting polygonal roofs and footprints from remote sensing images is
critical for large-scale urban analysis. Most existing methods rely on
segmentation-based models that assume clear semantic boundaries of roofs, but
these approaches struggle in off- nadir images, where the roof and footprint
are significantly displaced, and facade pixels are fused with the roof
boundary. With the increasing availability of open vector map annotations,
e.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation
has become viable because remote sensing images are georeferenced once
captured. However, these historical labels commonly suffer from significant
positional discrepancies with new images and only have one annotation (roof or
footprint), which fails to describe the correct structures of a building. To
address these discrepancies, we first introduce a concept of an alignment
token, which encodes the correction vector to guide the label correction. Based
on this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel
model designed to align dislocated historical labels with roofs and footprints.
Specifically, DragOSM formulates the label alignment as an interactive
denoising process, modeling the positional discrepancy as a Gaussian
distribution. During training, it learns to correct these errors by simulating
misalignment with random Gaussian perturbations; during inference, it
iteratively refines the positions of input labels. To validate our method, we
further present a new dataset, Repairing Buildings in OSM (ReBO), comprising
179,265 buildings with both OpenStreetMap and manually corrected annotations
across 5,473 images from 41 cities. Experimental results on ReBO demonstrate
the effectiveness of DragOSM. Code, dataset, and trained models are publicly
available at https://github.com/likaiucas/DragOSM.git.
\\ ( https://arxiv.org/abs/2509.17951 ,  6739kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17955
Date: Mon, 22 Sep 2025 16:10:58 GMT   (33801kb)

Title: Breaking the Discretization Barrier of Continuous Physics Simulation
  Learning
Authors: Fan Xu, Hao Wu, Nan Wang, Lilan Peng, Kun Wang, Wei Gong, Xibin Zhao
Categories: cs.CV
\\
  The modeling of complicated time-evolving physical dynamics from partial
observations is a long-standing challenge. Particularly, observations can be
sparsely distributed in a seemingly random or unstructured manner, making it
difficult to capture highly nonlinear features in a variety of scientific and
engineering problems. However, existing data-driven approaches are often
constrained by fixed spatial and temporal discretization. While some
researchers attempt to achieve spatio-temporal continuity by designing novel
strategies, they either overly rely on traditional numerical methods or fail to
truly overcome the limitations imposed by discretization. To address these, we
propose CoPS, a purely data-driven methods, to effectively model continuous
physics simulation from partial observations. Specifically, we employ
multiplicative filter network to fuse and encode spatial information with the
corresponding observations. Then we customize geometric grids and use
message-passing mechanism to map features from original spatial domain to the
customized grids. Subsequently, CoPS models continuous-time dynamics by
designing multi-scale graph ODEs, while introducing a Markov-based neural
auto-correction module to assist and constrain the continuous extrapolations.
Comprehensive experiments demonstrate that CoPS advances the state-of-the-art
methods in space-time continuous modeling across various scenarios.
\\ ( https://arxiv.org/abs/2509.17955 ,  33801kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17968
Date: Mon, 22 Sep 2025 16:19:00 GMT   (12656kb)

Title: Visual Detector Compression via Location-Aware Discriminant Analysis
Authors: Qizhen Lan, Jung Im Choi, Qing Tian
Categories: cs.CV
\\
  Deep neural networks are powerful, yet their high complexity greatly limits
their potential to be deployed on billions of resource-constrained edge
devices. Pruning is a crucial network compression technique, yet most existing
methods focus on classification models, with limited attention to detection.
Even among those addressing detection, there is a lack of utilization of
essential localization information. Also, many pruning methods passively rely
on pre-trained models, in which useful and useless components are intertwined,
making it difficult to remove the latter without harming the former at the
neuron/filter level. To address the above issues, in this paper, we propose a
proactive detection-discriminants-based network compression approach for deep
visual detectors, which alternates between two steps: (1) maximizing and
compressing detection-related discriminants and aligning them with a subset of
neurons/filters immediately before the detection head, and (2) tracing the
detection-related discriminating power across the layers and discarding
features of lower importance. Object location information is exploited in both
steps. Extensive experiments, employing four advanced detection models and four
state-of-the-art competing methods on the KITTI and COCO datasets, highlight
the superiority of our approach. Remarkably, our compressed models can even
beat the original base models with a substantial reduction in complexity.
\\ ( https://arxiv.org/abs/2509.17968 ,  12656kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17993
Date: Mon, 22 Sep 2025 16:35:19 GMT   (65331kb)

Title: StableGuard: Towards Unified Copyright Protection and Tamper
  Localization in Latent Diffusion Models
Authors: Haoxin Yang, Bangzhen Liu, Xuemiao Xu, Cheng Xu, Yuyang Yu, Zikai
  Huang, Yi Wang, Shengfeng He
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\
  The advancement of diffusion models has enhanced the realism of AI-generated
content but also raised concerns about misuse, necessitating robust copyright
protection and tampering localization. Although recent methods have made
progress toward unified solutions, their reliance on post hoc processing
introduces considerable application inconvenience and compromises forensic
reliability. We propose StableGuard, a novel framework that seamlessly
integrates a binary watermark into the diffusion generation process, ensuring
copyright protection and tampering localization in Latent Diffusion Models
through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE)
by equipping a pretrained Variational Autoencoder (VAE) with a lightweight
latent residual-based adapter, enabling the generation of paired watermarked
and watermark-free images. These pairs, fused via random masks, create a
diverse dataset for training a tampering-agnostic forensic network. To further
enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic
Network (MoE-GFN) that dynamically integrates holistic watermark patterns,
local tampering traces, and frequency-domain cues for precise watermark
verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly
optimized in a self-supervised, end-to-end manner, fostering a reciprocal
training between watermark embedding and forensic accuracy. Extensive
experiments demonstrate that StableGuard consistently outperforms
state-of-the-art methods in image fidelity, watermark verification, and
tampering localization.
\\ ( https://arxiv.org/abs/2509.17993 ,  65331kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18015
Date: Mon, 22 Sep 2025 16:54:23 GMT   (21617kb)

Title: Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization
  in Chest Radiographs
Authors: Advait Gosai, Arun Kavishwar, Stephanie L. McNamara, Soujanya
  Samineni, Renato Umeton, Alexander Chowdhury, William Lotter
Categories: cs.CV cs.AI
\\
  Recent work has shown promising performance of frontier large language models
(LLMs) and their multimodal counterparts in medical quizzes and diagnostic
tasks, highlighting their potential for broad clinical utility given their
accessible, general-purpose nature. However, beyond diagnosis, a fundamental
aspect of medical image interpretation is the ability to localize pathological
findings. Evaluating localization not only has clinical and educational
relevance but also provides insight into a model's spatial understanding of
anatomy and disease. Here, we systematically assess two general-purpose MLLMs
(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to
localize pathologies on chest radiographs, using a prompting pipeline that
overlays a spatial grid and elicits coordinate-based predictions. Averaged
across nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a
localization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),
all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark
(80.1%). Despite modest performance, error analysis revealed that GPT-5's
predictions were largely in anatomically plausible regions, just not always
precisely localized. GPT-4 performed well on pathologies with fixed anatomical
locations, but struggled with spatially variable findings and exhibited
anatomically implausible predictions more frequently. MedGemma demonstrated the
lowest performance on all pathologies, showing limited capacity to generalize
to this novel task. Our findings highlight both the promise and limitations of
current MLLMs in medical imaging and underscore the importance of integrating
them with task-specific tools for reliable use.
\\ ( https://arxiv.org/abs/2509.18015 ,  21617kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18041
Date: Mon, 22 Sep 2025 17:15:13 GMT   (1151kb)

Title: NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and
  Neuro-Symbolic Reasoning
Authors: Sahil Shah, S P Sharan, Harsh Goel, Minkyu Choi, Mustafa Munir, Manvik
  Pasula, Radu Marculescu, Sandeep Chinchali
Categories: cs.CV
\\
  Long-Form Video Question Answering (LVQA) poses challenges beyond traditional
visual question answering (VQA), which is often limited to static images or
short video clips. While current vision-language models (VLMs) perform well in
those settings, they struggle with complex queries in LVQA over long videos
involving multi-step temporal reasoning and causality. Vanilla approaches,
which sample frames uniformly and feed them to a VLM with the question, incur
significant token overhead, forcing severe downsampling. As a result, the model
often misses fine-grained visual structure, subtle event transitions, or key
temporal cues, ultimately leading to incorrect answers. To address these
limitations, recent works have explored query-adaptive frame sampling,
hierarchical keyframe selection, and agent-based iterative querying. However,
these methods remain fundamentally heuristic: they lack explicit temporal
representations and cannot enforce or verify logical event relationships. As a
result, there are no formal guarantees that the sampled context actually
encodes the compositional or causal logic demanded by the question. To address
these foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play
neuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language
question into a formal temporal logic expression, constructs a video automaton
from frame-level semantic propositions, and applies model checking to
rigorously identify video segments satisfying the question's logical
requirements. Only these logic-verified segments are submitted to the VLM, thus
improving interpretability, reducing hallucinations, and enabling compositional
reasoning without modifying or fine-tuning the model. Experiments on
LongVideoBench and CinePile show NeuS-QA improves performance by over 10%,
especially on questions involving event ordering, causality, and multi-step
compositional reasoning.
\\ ( https://arxiv.org/abs/2509.18041 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18056
Date: Mon, 22 Sep 2025 17:30:15 GMT   (6948kb)

Title: TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning
  for Video LLMs
Authors: Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao,
  Qibin Hou, Ming-Ming Cheng
Categories: cs.CV
Comments: Accepted at NeurIPS 2025
\\
  This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks. We reveal that existing
reinforcement learning methods, such as Group Relative Policy Optimization
(GRPO), rely on on-policy sampling for policy updates. However, in tasks with
large temporal search spaces, this strategy becomes both inefficient and
limited in performance, as it often fails to identify temporally accurate
solutions. To address this limitation, TempSamp-R1 leverages ground-truth
annotations as off-policy supervision to provide temporally precise guidance,
effectively compensating for the sparsity and misalignment in on-policy
solutions. To further stabilize training and reduce variance in reward-based
updates, TempSamp-R1 provides a non-linear soft advantage computation method
that dynamically reshapes the reward feedback via an asymmetric transformation.
By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1
optimizes a single unified model to support both CoT and non-CoT inference
modes, enabling efficient handling of queries with varying reasoning
complexity. Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,
TempSamp-R1 shows robust few-shot generalization capabilities under limited
data. Code: https://github.com/HVision-NKU/TempSamp-R1
\\ ( https://arxiv.org/abs/2509.18056 ,  6948kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18081
Date: Mon, 22 Sep 2025 17:56:17 GMT   (5194kb)

Title: GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition
  System utilizing Grapheme-based Tokenizer and Decoder-only Transformer
Authors: Md. Mahmudul Hasan, Ahmed Nesar Tahsin Choudhury, Mahmudul Hasan, Md.
  Mosaddek Khan
Categories: cs.CV
Comments: 7 pages. Accepted at the 2025 Conference on Empirical Methods in
  Natural Language Processing (EMNLP) System Demonstrations. Equal
  Contribution: Md. Mahmudul Hasan and Ahmed Nesar Tahsin Choudhury
\\
  Despite Bengali being the sixth most spoken language in the world,
handwritten text recognition (HTR) systems for Bengali remain severely
underdeveloped. The complexity of Bengali script--featuring conjuncts,
diacritics, and highly variable handwriting styles--combined with a scarcity of
annotated datasets makes this task particularly challenging. We present
GraDeT-HTR, a resource-efficient Bengali handwritten text recognition system
based on a Grapheme-aware Decoder-only Transformer architecture. To address the
unique challenges of Bengali script, we augment the performance of a
decoder-only transformer by integrating a grapheme-based tokenizer and
demonstrate that it significantly improves recognition accuracy compared to
conventional subword tokenizers. Our model is pretrained on large-scale
synthetic data and fine-tuned on real human-annotated samples, achieving
state-of-the-art performance on multiple benchmark datasets.
\\ ( https://arxiv.org/abs/2509.18081 ,  5194kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18090
Date: Mon, 22 Sep 2025 17:58:48 GMT   (26668kb)

Title: GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface
  Reconstruction
Authors: Jiahe Li, Jiawei Zhang, Youmin Zhang, Xiao Bai, Jin Zheng, Xiaohan Yu,
  Lin Gu
Categories: cs.CV
Comments: Accepted at NeurIPS 2025 (Spotlight). Project page:
  https://fictionarry.github.io/GeoSVR-project/
\\
  Reconstructing accurate surfaces with radiance fields has achieved remarkable
progress in recent years. However, prevailing approaches, primarily based on
Gaussian Splatting, are increasingly constrained by representational
bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.
As strengths, sparse voxels support preserving the coverage completeness and
geometric clarity, while corresponding challenges also arise from absent scene
constraints and locality in surface refinement. To ensure correct scene
convergence, we first propose a Voxel-Uncertainty Depth Constraint that
maximizes the effect of monocular depth cues while presenting a voxel-oriented
uncertainty to avoid quality degradation, enabling effective and robust scene
constraints yet preserving highly accurate geometries. Subsequently, Sparse
Voxel Surface Regularization is designed to enhance geometric consistency for
tiny voxels and facilitate the voxel-based formation of sharp and accurate
surfaces. Extensive experiments demonstrate our superior performance compared
to existing methods across diverse challenging scenarios, excelling in
geometric accuracy, detail preservation, and reconstruction completeness while
maintaining high efficiency. Code is available at
https://github.com/Fictionarry/GeoSVR.
\\ ( https://arxiv.org/abs/2509.18090 ,  26668kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18092
Date: Mon, 22 Sep 2025 17:59:30 GMT   (45299kb)

Title: ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image
  Generation
Authors: Guocheng Gordon Qian, Daniil Ostashev, Egor Nemchinov, Avihay
  Assouline, Sergey Tulyakov, Kuan-Chieh Jackson Wang, Kfir Aberman
Categories: cs.CV
Comments: Accepted to SIGGRAPH Asia 2025, webpage:
  https://snap-research.github.io/composeme/
\\
  Generating high-fidelity images of humans with fine-grained control over
attributes such as hairstyle and clothing remains a core challenge in
personalized text-to-image synthesis. While prior methods emphasize identity
preservation from a reference image, they lack modularity and fail to provide
disentangled control over specific visual attributes. We introduce a new
paradigm for attribute-specific image prompting, in which distinct sets of
reference images are used to guide the generation of individual aspects of
human appearance, such as hair, clothing, and identity. Our method encodes
these inputs into attribute-specific tokens, which are injected into a
pre-trained text-to-image diffusion model. This enables compositional and
disentangled control over multiple visual factors, even across multiple people
within a single image. To promote natural composition and robust
disentanglement, we curate a cross-reference training dataset featuring
subjects in diverse poses and expressions, and propose a multi-attribute
cross-reference training strategy that encourages the model to generate
faithful outputs from misaligned attribute inputs while adhering to both
identity and textual conditioning. Extensive experiments show that our method
achieves state-of-the-art performance in accurately following both visual and
textual prompts. Our framework paves the way for more configurable human image
synthesis by combining visual prompting with text-driven generation. Webpage is
available at: https://snap-research.github.io/composeme/.
\\ ( https://arxiv.org/abs/2509.18092 ,  45299kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18094
Date: Mon, 22 Sep 2025 17:59:40 GMT   (11062kb)

Title: UniPixel: Unified Object Referring and Segmentation for Pixel-Level
  Visual Reasoning
Authors: Ye Liu, Zongyang Ma, Junfu Pu, Zhongang Qi, Yang Wu, Ying Shan, Chang
  Wen Chen
Categories: cs.CV cs.AI
Comments: NeurIPS 2025 Camera Ready. Project Page:
  https://polyu-chenlab.github.io/unipixel/
\\
  Recent advances in Large Multi-modal Models (LMMs) have demonstrated their
remarkable success as general-purpose multi-modal assistants, with particular
focuses on holistic image- and video-language understanding. Conversely, less
attention has been given to scaling fine-grained pixel-level understanding
capabilities, where the models are expected to realize pixel-level alignment
between visual signals and language semantics. Some previous studies have
applied LMMs to related tasks such as region-level captioning and referring
expression segmentation. However, these models are limited to performing either
referring or segmentation tasks independently and fail to integrate these
fine-grained perception capabilities into visual reasoning. To bridge this gap,
we propose UniPixel, a large multi-modal model capable of flexibly
comprehending visual prompt inputs and generating mask-grounded responses. Our
model distinguishes itself by seamlessly integrating pixel-level perception
with general visual understanding capabilities. Specifically, UniPixel
processes visual prompts and generates relevant masks on demand, and performs
subsequent reasoning conditioning on these intermediate pointers during
inference, thereby enabling fine-grained pixel-level reasoning. The
effectiveness of our approach has been verified on 10 benchmarks across a
diverse set of tasks, including pixel-level referring/segmentation and
object-centric understanding in images/videos. A novel PixelQA task that
jointly requires referring, segmentation, and question answering is also
designed to verify the flexibility of our method.
\\ ( https://arxiv.org/abs/2509.18094 ,  11062kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18096
Date: Mon, 22 Sep 2025 17:59:54 GMT   (12828kb)

Title: Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image
  Diffusion Transformers
Authors: Chaehyun Kim, Heeseong Shin, Eunbeen Hong, Heeji Yoon, Anurag Arnab,
  Paul Hongsuck Seo, Sunghwan Hong, Seungryong Kim
Categories: cs.CV
Comments: NeurIPS 2025. Project page: https://cvlab-kaist.github.io/Seg4Diff/
\\
  Text-to-image diffusion models excel at translating language prompts into
photorealistic images by implicitly grounding textual concepts through their
cross-modal attention mechanisms. Recent multi-modal diffusion transformers
extend this by introducing joint self-attention over concatenated image and
text tokens, enabling richer and more scalable cross-modal alignment. However,
a detailed understanding of how and where these attention maps contribute to
image generation remains limited. In this paper, we introduce Seg4Diff
(Segmentation for Diffusion), a systematic framework for analyzing the
attention structures of MM-DiT, with a focus on how specific layers propagate
semantic information from text to image. Through comprehensive analysis, we
identify a semantic grounding expert layer, a specific MM-DiT block that
consistently aligns text tokens with spatially coherent image regions,
naturally producing high-quality semantic segmentation masks. We further
demonstrate that applying a lightweight fine-tuning scheme with mask-annotated
image data enhances the semantic grouping capabilities of these layers and
thereby improves both segmentation performance and generated image fidelity.
Our findings demonstrate that semantic grouping is an emergent property of
diffusion transformers and can be selectively amplified to advance both
segmentation and generation performance, paving the way for unified models that
bridge visual perception and generation.
\\ ( https://arxiv.org/abs/2509.18096 ,  12828kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18097
Date: Mon, 22 Sep 2025 17:59:55 GMT   (8964kb)

Title: Preconditioned Deformation Grids
Authors: Julian Kaltheuner, Alexander Oebel, Hannah Droege, Patrick Stotko,
  Reinhard Klein
Categories: cs.CV cs.GR
Comments: GitHub: https://github.com/vc-bonn/preconditioned-deformation-grids
Journal-ref: Computer Graphics Forum, Volume 44, 2025
\\
  Dynamic surface reconstruction of objects from point cloud sequences is a
challenging field in computer graphics. Existing approaches either require
multiple regularization terms or extensive training data which, however, lead
to compromises in reconstruction accuracy as well as over-smoothing or poor
generalization to unseen objects and motions. To address these lim- itations,
we introduce Preconditioned Deformation Grids, a novel technique for estimating
coherent deformation fields directly from unstructured point cloud sequences
without requiring or forming explicit correspondences. Key to our approach is
the use of multi-resolution voxel grids that capture the overall motion at
varying spatial scales, enabling a more flexible deformation representation. In
conjunction with incorporating grid-based Sobolev preconditioning into
gradient-based optimization, we show that applying a Chamfer loss between the
input point clouds as well as to an evolving template mesh is sufficient to
obtain accurate deformations. To ensure temporal consistency along the object
surface, we include a weak isometry loss on mesh edges which complements the
main objective without constraining deformation fidelity. Extensive evaluations
demonstrate that our method achieves superior results, particularly for long
sequences, compared to state-of-the-art techniques.
\\ ( https://arxiv.org/abs/2509.18097 ,  8964kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16407
Date: Fri, 19 Sep 2025 20:31:38 GMT   (2509kb)

Title: WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables
Authors: Hunter McCoy and Prashant Pandey
Categories: cs.DC cs.DS
\\
  GPU hash tables are increasingly used to accelerate data processing, but
their limited functionality restricts adoption in large-scale data processing
applications. Current limitations include incomplete concurrency support and
missing compound operations such as upserts.
  This paper presents WarpSpeed, a library of high-performance concurrent GPU
hash tables with a unified benchmarking framework for performance analysis.
WarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and
provides a rich API designed for modern GPU applications. Our evaluation uses
diverse benchmarks to assess both correctness and scalability, and we
demonstrate real-world impact by integrating these hash tables into three
downstream applications.
  We propose several optimization techniques to reduce concurrency overhead,
including fingerprint-based metadata to minimize cache line probes and
specialized Nvidia GPU instructions for lock-free queries. Our findings provide
new insights into concurrent GPU hash table design and offer practical guidance
for developing efficient, scalable data structures on modern GPUs.
\\ ( https://arxiv.org/abs/2509.16407 ,  2509kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16495
Date: Sat, 20 Sep 2025 01:56:25 GMT   (1941kb)

Title: Shift Parallelism: Low-Latency, High-Throughput LLM Inference for
  Dynamic Workloads
Authors: Mert Hidayetoglu, Aurick Qiao, Michael Wyatt, Jeff Rasley, Yuxiong He,
  Samyam Rajbhandari
Categories: cs.DC
\\
  Efficient parallelism is necessary for achieving low-latency, high-throughput
inference with large language models (LLMs). Tensor parallelism (TP) is the
state-of-the-art method for reducing LLM response latency, however GPU
communications reduces combined token throughput. On the other hand, data
parallelism (DP) obtains a higher throughput yet is slow in response latency.
Best of both worlds does not exist, and it is not possible to combine TP and DP
because of the KV cache variance across the parallelisms.
  We notice Sequence Parallelism (SP - Ulysses in training) has similar
properties as DP but with KV cache invariance. We adapt SP to inference, and
combine it with TP to get the best of both worlds. Our solution: Shift
Parallelism.
  Shift Parallelism dynamically switches across TP and SP, and minimizes
latency in low traffic without losing throughput in high traffic. The efficient
GPU communications of Shift Parallelism yields up to i) 1.51x faster response
in interactive workloads and ii) 50% higher throughput in batch workloads,
compared to a TP-only solution.
  We evaluate Shift Parallelism with real-world production traces with dynamic
traffic patterns as well as synthetic benchmarking patterns across models,
context sizes, and arrival rates. All results affirm the same: Shift
Parallelism has a better the latency vs. throughput tradeoff than TP or DP, and
hence obtains low latency without degrading throughput in dynamic workloads.
\\ ( https://arxiv.org/abs/2509.16495 ,  1941kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16504
Date: Sat, 20 Sep 2025 02:45:41 GMT   (6154kb)

Title: sat-QFL: Secure Quantum Federated Learning for Low Orbit Satellites
Authors: Dev Gurung, Shiva Raj Pokhrel
Categories: cs.DC
\\
  Low Earth orbit (LEO) constellations violate core assumptions of standard
(quantum) federated learning (FL): client-server connectivity is intermittent,
participation is time varying, and latency budgets are strict. We present
sat-QFL, a hierarchical, access aware quantum federated learning (QFL)
framework that partitions satellites into primary (ground connected) and
secondary as inter-satellite links (ISL-only) roles, and schedules sequential,
simultaneous, or asynchronous edge training aligned with visibility windows.
For quantum-resilient confidentiality and integrity, sat-QFL integrates quantum
key distribution (QKD) based key establishment with authenticated encryption
for model exchange; we also assess teleportation as a feasibility primitive for
quantum state transfer. Using derived constellation traces and QFL workloads
(Qiskit), we show that sat-QFL sustains robust aggregation under varying
participation and reduces communication bottlenecks with modest security
overhead. Our implementation and results are available at
https://github.com/s222416822/satQFL.
\\ ( https://arxiv.org/abs/2509.16504 ,  6154kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16505
Date: Sat, 20 Sep 2025 02:50:14 GMT   (2250kb)

Title: orb-QFL: Orbital Quantum Federated Learning
Authors: Dev Gurung, Shiva Raj Pokhrel
Categories: cs.DC cs.LG
\\
  Recent breakthroughs in quantum computing present transformative
opportunities for advancing Federated Learning (FL), particularly in
non-terrestrial environments characterized by stringent communication and
coordination constraints. In this study, we propose orbital QFL, termed
orb-QFL, a novel quantum-assisted Federated Learning framework tailored for Low
Earth Orbit (LEO) satellite constellations. Distinct from conventional FL
paradigms, termed orb-QFL operates without centralized servers or global
aggregation mechanisms (e.g., FedAvg), instead leveraging quantum entanglement
and local quantum processing to facilitate decentralized, inter-satellite
collaboration. This design inherently addresses the challenges of orbital
dynamics, such as intermittent connectivity, high propagation delays, and
coverage variability. The framework enables continuous model refinement through
direct quantum-based synchronization between neighboring satellites, thereby
enhancing resilience and preserving data locality. To validate our approach, we
integrate the Qiskit quantum machine learning toolkit with Poliastro-based
orbital simulations and conduct experiments using Statlog dataset.
\\ ( https://arxiv.org/abs/2509.16505 ,  2250kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16513
Date: Sat, 20 Sep 2025 03:20:37 GMT   (724kb)

Title: Trace Replay Simulation of MIT SuperCloud for Studying Optimal
  Sustainability Policies
Authors: Wesley Brewer, Matthias Maiterth, Damien Fay
Categories: cs.DC
Comments: 2 pages, 2 figures
\\
  The rapid growth of AI supercomputing is creating unprecedented power
demands, with next-generation GPU datacenters requiring hundreds of megawatts
and producing fast, large swings in consumption. To address the resulting
challenges for utilities and system operators, we extend ExaDigiT, an
open-source digital twin framework for modeling power, cooling, and scheduling
of supercomputers. Originally developed for replaying traces from
leadership-class HPC systems, ExaDigiT now incorporates heterogeneity,
multi-tenancy, and cloud-scale workloads. In this work, we focus on trace
replay and rescheduling of jobs on the MIT SuperCloud TX-GAIA system to enable
reinforcement learning (RL)-based experimentation with sustainability policies.
The RAPS module provides a simulation environment with detailed power and
performance statistics, supporting the study of scheduling strategies,
incentive structures, and hardware/software prototyping. Preliminary RL
experiments using Proximal Policy Optimization demonstrate the feasibility of
learning energy-aware scheduling decisions, highlighting ExaDigiT's potential
as a platform for exploring optimal policies to improve throughput, efficiency,
and sustainability.
\\ ( https://arxiv.org/abs/2509.16513 ,  724kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16857
Date: Sun, 21 Sep 2025 00:59:45 GMT   (931kb)

Title: ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix
  Caching
Authors: Xingyu Xiang, Raj Joshi, Yuhan Liu, Jiayi Yao, Chenxingyu Zhao,
  Junchen Jiang, Yang Zhou, Eddie Kohler, Minlan Yu
Categories: cs.DC cs.AI cs.LG
\\
  Distributed prefix caching accelerates long-context LLM serving by reusing KV
cache entries for common context prefixes. However, KV cache fetches can become
a bottleneck when network bandwidth is limited. Compression mitigates the
bandwidth issue, but can degrade overall performance when decompression
interferes with model computation.
  We present ShadowServe, the first SmartNIC-accelerated, interference-free
prefix caching system for LLM serving. ShadowServe separates a control plane on
the host and a data plane fully offloaded to the SmartNIC, which eliminates
interference to both host GPU and CPU. To overcome the SmartNIC's limited
compute and memory resources, we design a chunked pipeline that parallelizes
data plane operations across the SmartNIC's compute resources, and a
minimal-copy memory management scheme that reduces memory pressure on the
SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to
2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token
(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to
up to 1.35x higher throughput.
\\ ( https://arxiv.org/abs/2509.16857 ,  931kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16995
Date: Sun, 21 Sep 2025 09:29:28 GMT   (270kb)

Title: MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with
  Edge-Cloud Collaboration for Efficient Multimodal LLM Inference
Authors: Zheming Yang, Qi Guo, Yunqing Hu, Chang Zhao, Chang Zhang, Jian Zhao,
  and Wen Ji
Categories: cs.DC
Comments: 5 pages, 4 figures
\\
  Multimodal large language models (MLLMs) enable powerful cross-modal
inference but impose significant computational and latency burdens, posing
severe challenges for deployment in resource-constrained environments. In this
paper, we propose MoA-Off, an adaptive heterogeneous modality-aware offloading
framework with edge-cloud collaboration for efficient MLLM inference. MoA-Off
introduces a lightweight heterogeneous modality-aware module that estimates the
complexity of heterogeneous inputs through multi-dimensional feature analysis.
Then, an adaptive edge-cloud collaborative offloading strategy is proposed that
dynamically schedules workloads between edge and cloud based on modality-aware
complexity scores and real-time system states. The experimental results
demonstrate that MoA-Off can achieve over 30% reduction in latency and 30%-65%
decrease in resource overhead while maintaining competitive accuracy compared
to traditional approaches.
\\ ( https://arxiv.org/abs/2509.16995 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17351
Date: Mon, 22 Sep 2025 04:28:29 GMT   (18kb)

Title: Institutional Research Computing Capabilities in Australia: 2024
Authors: Slava Kitaeff, Luc Betbeder-Matibet, Jake Carroll, Stephen Giugni,
  David Abramson, John Zaitseff, Sarah Walters, David Powell, Chris Bording,
  Trung Nguyen, Angus Macoustra, Fabien Voisin, Bowen Chen, Jarrod Hurley
Categories: cs.DC
Comments: 9 pages in IEEE Proceedings format, International Conference on
  eScience 2025, Accepted
\\
  Institutional research computing infrastructure plays a vital role in
Australia's research ecosystem, complementing and extending national
facilities. This paper analyses research computing capabilities across
Australian universities and organisations, showing how institutional systems
support research excellence through local compute resources, specialised
hardware, and cluster solutions. Our study finds that nearly 112,258 CPU cores
and 2,241 GPUs serve over 6,000 researchers as essential bridges between
desktops and national facilities, enabling workflows from development to
large-scale computations. The estimated replacement value of this
infrastructure is $144M AUD. Drawing on detailed data from multiple
institutions, we identify key patterns in deployment, utilisation, and
strategic alignment with research priorities. Institutional resources provide
critical support for data-intensive projects, facilitate training and
higher-degree student research, enable prototyping and development, and ensure
data sovereignty compliance when required. The analysis shows how these
facilities leverage national investments while addressing institution-specific
needs that national systems cannot meet. We present evidence that strategic
investment in institutional capabilities yields significant returns through
greater research productivity, enhanced graduate training, and improved
outcomes. The study offers insights for organisations planning computing
strategies and highlights the importance of maintaining robust institutional
resources alongside national facilities.
\\ ( https://arxiv.org/abs/2509.17351 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17357
Date: Mon, 22 Sep 2025 05:22:50 GMT   (401kb)

Title: Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via
  Partially Disaggregated Prefill
Authors: Yunzhao Liu, Qiang Xu, Y. Charlie Hu
Categories: cs.DC
\\
  Efficient LLM inference is critical for real-world applications, especially
within heterogeneous GPU clusters commonly found in organizations and
on-premise datacenters as GPU architecture rapidly evolves. Current
disaggregated prefill strategies, which separate the prefill and decode stages
of LLM inference across different GPUs, often suffer from suboptimal
performance due to imbalances between GPU capabilities and workload demands. On
the other hand, extending conventional data parallelism and pipeline
parallelism to heterogeneous setups incurs high inference latencies. To address
these challenges, we introduce Cronus, a novel LLM inference system designed to
dynamically balance workloads across heterogeneous GPUs using partially
disaggregated prefill. Cronus partitions each prefill stage and executes its
initial portion on the low-end GPU, while overlapping the remaining prefill and
decode stages of earlier requests on the high-end GPU. Extensive evaluations
across various high-end and low-end GPU combinations demonstrate that Cronus
significantly improves the throughput over disaggregated prefill. It also
reduces TTFT P99 and TBT P99 significantly over DP and PP while maintaining
similar or better throughput.
\\ ( https://arxiv.org/abs/2509.17357 ,  401kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17360
Date: Mon, 22 Sep 2025 05:24:22 GMT   (465kb)

Title: Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access
Authors: Chaoyi Ruan, Chao Bi, Kaiwen Zheng, Ziji Shi, Xinyi Wan, Jialin Li
Categories: cs.DC
\\
  Large Language Model (LLM) agents tackle data-intensive tasks such as deep
research and code generation. However, their effectiveness depends on frequent
interactions with knowledge sources across remote clouds or regions. Such
interactions can create non-trivial latency and cost bottlenecks. Existing
caching solutions focus on exact-match queries, limiting their effectiveness
for semantic knowledge reuse.
  To address this challenge, we introduce Asteria, a novel cross-region
knowledge caching architecture for LLM agents. At its core are two
abstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A
semantic element captures the semantic embedding representation of an LLM query
together with performance-aware metadata such as latency, cost, and staticity.
Sine then provides two-stage retrieval: a vector similar index with semantic
embedding for fast candidate selection and a lightweight LLM-powered semantic
judger for precise validation. Atop these primitives, Asteria builds a new
cache interface that includes a new semantic-aware cache hit definition, a
cost-efficient eviction policy, and proactive prefetching. To reduce overhead,
Asteria co-locates the small LLM judger with the main LLM using adaptive
scheduling and resource sharing. Our evaluation demonstrates that Asteria
delivers substantial performance improvements without compromising correctness.
On representative search workloads, Asteria achieves up to a 3.6$\times$
increase in throughput by maintaining cache hit rates of over 85%, while
preserving accuracy virtually identical to non-cached baselines. Asteria also
improves throughput for complex coding tasks by 20%, showcasing its versatility
across diverse agentic workloads.
\\ ( https://arxiv.org/abs/2509.17360 ,  465kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17388
Date: Mon, 22 Sep 2025 06:52:35 GMT   (254kb)

Title: Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory
Authors: Manel Lurbe, Miguel Avargues, Salvador Petit, Maria E. Gomez, Rui
  Yang, Guanhao Wang and Julio Sahuquillo
Categories: cs.DC
\\
  Emerging applications, such as big data analytics and machine learning,
require increasingly large amounts of main memory, often exceeding the capacity
of current commodity processors built on DRAM technology. To address this,
recent research has focused on off-chip memory controllers that facilitate
access to diverse memory media, each with unique density and latency
characteristics. While these solutions improve memory system performance, they
also exacerbate the already significant memory latency. As a result,
multi-level prefetching techniques are essential to mitigate these extended
latencies.
  This paper investigates the advantages of prefetching across both sides of
the memory system: the off-chip memory and the on-chip cache hierarchy. Our
primary objective is to assess the impact of a multi-level prefetching engine
on overall system performance. Additionally, we analyze the individual
contribution of each prefetching level to system efficiency. To achieve this,
the study evaluates two key prefetching approaches: HMC (Hybrid Memory
Controller) and HMC+L1, both of which employ prefetching mechanisms commonly
used by processor vendors. The HMC approach integrates a prefetcher within the
off-chip hybrid memory controller, while the HMC+L1 approach combines this with
additional L1 on-chip prefetchers.
  Experimental results on an out-of-order execution processor show that on-chip
cache prefetchers are crucial for maximizing the benefits of off-chip
prefetching, which in turn further enhances performance. Specifically, the
off-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and
up to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher
coverage to as much as 92%. Consequently, overall performance increases from 9%
with the HMC approach to 12% when L1 prefetching is also employed.
\\ ( https://arxiv.org/abs/2509.17388 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17496
Date: Mon, 22 Sep 2025 08:27:14 GMT   (455kb)

Title: pBeeGees: A Prudent Approach to Certificate-Decoupled BFT Consensus
Authors: Kaiji Yang, Jingjing Zhang, Junyao Zheng, Qiwen Liu, Weigang Wu, and
  Jieying Zhou
Categories: cs.DC
Comments: Accepted by the 25th International Conference on Algorithms and
  Architectures for Parallel Processing (ICA3PP 2025)
\\
  Pipelined Byzantine Fault Tolerant (BFT) consensus is fundamental to
permissioned blockchains. However, many existing protocols are limited by the
requirement for view-consecutive quorum certificates (QCs). This constraint
impairs performance and creates liveness vulnerabilities under adverse network
conditions. Achieving "certificate decoupling"-committing blocks without this
requirement-is therefore a key research goal. While the recent BeeGees
algorithm achieves this, our work reveals that it suffers from security and
liveness issues. To address this problem, this paper makes two primary
contributions. First, we formally define these flaws as the Invalid Block
Problem and the Hollow Chain Problem. Second, we propose pBeeGees, a new
algorithm that addresses these issues while preserving certificate decoupling
with no additional computational overhead. To achieve this, pBeeGees integrates
traceback and pre-commit validation to solve the Invalid Block Problem.Further,
to mitigate the Hollow Chain Problem, we introduce a prudent validation
mechanism, which prevents unverified branches from growing excessively. To
summarize, pBeeGees is the first protocol to simultaneously achieve safety,
liveness, and certificate decoupling in a pipelined BFT framework. Experiments
confirm that our design significantly reduces block commit latency compared to
classic algorithms, particularly under frequent stopping faults.
\\ ( https://arxiv.org/abs/2509.17496 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17532
Date: Mon, 22 Sep 2025 08:52:53 GMT   (2151kb)

Title: TACTFL: Temporal Contrastive Training for Multi-modal Federated Learning
  with Similarity-guided Model Aggregation
Authors: Guanxiong Sun and Majid Mirmehdi and Zahraa Abdallah and Raul
  Santos-Rodriguez and Ian Craddock and Telmo de Menezes e Silva Filho
Categories: cs.DC
\\
  Real-world federated learning faces two key challenges: limited access to
labelled data and the presence of heterogeneous multi-modal inputs. This paper
proposes TACTFL, a unified framework for semi-supervised multi-modal federated
learning. TACTFL introduces a modality-agnostic temporal contrastive training
scheme that conducts representation learning from unlabelled client data by
leveraging temporal alignment across modalities. However, as clients perform
self-supervised training on heterogeneous data, local models may diverge
semantically. To mitigate this, TACTFL incorporates a similarity-guided model
aggregation strategy that dynamically weights client models based on their
representational consistency, promoting global alignment. Extensive experiments
across diverse benchmarks and modalities, including video, audio, and wearable
sensors, demonstrate that TACTFL achieves state-of-the-art performance. For
instance, on the UCF101 dataset with only 10% labelled data, TACTFL attains
68.48% top-1 accuracy, significantly outperforming the FedOpt baseline of
35.35%. Code will be released upon publication.
\\ ( https://arxiv.org/abs/2509.17532 ,  2151kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17542
Date: Mon, 22 Sep 2025 09:00:32 GMT   (377kb)

Title: Disaggregated Prefill and Decoding Inference System for Large Language
  Model Serving on Multi-Vendor GPUs
Authors: Xing Chen, Rong Shi, Lu Zhao, Lingbin Wang, Xiao Jin, Yueqiang Chen,
  Hongfeng Sun
Categories: cs.DC
\\
  LLM-based applications have been widely used in various industries, but with
the increasing of models size, an efficient large language model (LLM)
inference system is an urgent problem to be solved for service providers. Since
the inference system is divided into two stage with different characteristics:
Prefill and Decode, the two stage will interfere with each other during the
inference process. Toward this end, a P-D disaggregated inference framework is
proposed by some researchers. Current research is done on homogeneous GPUs, and
lacks deployment solutions based on business scenarios. Compared with
homogeneous GPUs, using heterogeneous GPUs to construct inference systems can
better improve resource utilization and reduce costs. Even if GPUs from
different vendors are used to build inference systems, on the basis of reducing
costs, the resource utilization rate can be improved and the dependence on a
single vendor can be reduced. Therefore, a P-D disaggreagetd inference system
based on heterogeneous GPUs is designed, and the heterogeneous compatible
transmission module in the system is designed to address heterogeneous GPU data
compatibility issues. Then, a joint optimization algorithm of parallel strategy
and instance number allocation is proposed to obtain the deployment solutions.
Finally, the experimental results show that the P-D disaggregated inference
system can well solve the hybrid inference problem of heterogeneous GPUs from
different vendors, and the joint optimization algorithm can obtain the optimal
deployment solution.
\\ ( https://arxiv.org/abs/2509.17542 ,  377kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17771
Date: Mon, 22 Sep 2025 13:34:35 GMT   (194kb)

Title: A Lightweight Approach for State Machine Replication
Authors: Christian Cachin, Jinfeng Dou, Christian Scheideler, and Philipp
  Schneider
Categories: cs.DC
\\
  We present a lightweight solution for state machine replication with
commitment certificates. Specifically, we adapt a simple median rule from the
stabilizing consensus problem [Doerr11] to operate in a client-server setting
where arbitrary servers may be blocked adaptively based on past system
information. We further extend our protocol by compressing information about
committed commands, thus keeping the protocol lightweight, while still enabling
clients to easily prove that their commands have indeed been committed on the
shared state. Our approach guarantees liveness as long as at most a constant
fraction of servers are blocked, ensures safety under any number of blocked
servers, and supports fast recovery from massive blocking attacks. In addition
to offering near-optimal performance in several respects, our method is fully
decentralized, unlike other near-optimal solutions that rely on leaders. In
particular, our solution is robust against adversaries that target key servers
(which captures insider-based denial-of-service attacks), whereas leader-based
approaches fail under such a blocking model.
\\ ( https://arxiv.org/abs/2509.17771 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17863
Date: Mon, 22 Sep 2025 14:56:46 GMT   (1291kb)

Title: Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale
  MoE Serving
Authors: Ziming Liu, Boyu Tian, Guoteng Wang, Zhen Jiang, Peng Sun, Zhenhua
  Han, Tian Tang, Xiaohe Hu, Yanmin Jia, Yan Zhang, He Liu, Mingjun Zhang, Yiqi
  Zhang, Qiaoling Chen, Shenggan Cheng, Mingyu Gao, Yang You, Siyuan Feng
Categories: cs.DC
\\
  Mixture-of-Experts (MoE) models challenge serving infrastructures with
dynamic, sparse expert utilization, causing instability on conventional systems
designed for dense architectures. We propose EaaS, a novel serving system to
enable efficient, scalable, and robust MoE deployment. Our system disaggregates
MoE modules into independent, stateless services. This design enables
fine-grained resource scaling and provides inherent fault tolerance by
decoupling compute units. The architecture is powered by a high-performance,
CPU-free peer-to-peer communication library that ensures minimal overhead and
high throughput. Experiments confirm EaaS's scalability and efficiency,
achieving performance comparable to monolithic systems while providing robust
fault tolerance and strong scalability. EaaS incurs less than a 2% throughput
reduction under simulated hardware failures that would otherwise halt
monolithic architectures. It further saves up to 37.5% of computing resources
through dynamic fine-grained adaptation to serving traffic, demonstrating
strong resilience for large-scale MoE deployment in production.
\\ ( https://arxiv.org/abs/2509.17863 ,  1291kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17914
Date: Mon, 22 Sep 2025 15:39:33 GMT   (692kb)

Title: XaaS Containers: Performance-Portable Representation With Source and IR
  Containers
Authors: Marcin Copik, Eiman Alnuaimi, Alok Kamatar, Valerie Hayot-Sasson,
  Alberto Madonna, Todd Gamblin, Kyle Chard, Ian Foster, Torsten Hoefler
Categories: cs.DC
Comments: Accepted at the International Conference for High Performance
  Computing, Networking, Storage and Analysis (SC'25)
DOI: 10.1145/3712285.3759868
\\
  High-performance computing (HPC) systems and cloud data centers are
converging, and containers are becoming the default method of portable software
deployment. Yet, while containers simplify software management, they face
significant performance challenges in HPC environments as they must sacrifice
hardware-specific optimizations to achieve portability. Although HPC containers
can use runtime hooks to access optimized MPI libraries and GPU devices, they
are limited by application binary interface (ABI) compatibility and cannot
overcome the effects of early-stage compilation decisions. Acceleration as a
Service (XaaS) proposes a vision of performance-portable containers, where a
containerized application should achieve peak performance across all HPC
systems. We present a practical realization of this vision through Source and
Intermediate Representation (IR) containers, where we delay
performance-critical decisions until the target system specification is known.
We analyze specialization mechanisms in HPC software and propose a new
LLM-assisted method for automatic discovery of specializations. By examining
the compilation pipeline, we develop a methodology to build containers
optimized for target architectures at deployment time. Our prototype
demonstrates that new XaaS containers combine the convenience of
containerization with the performance benefits of system-specialized builds.
\\ ( https://arxiv.org/abs/2509.17914 ,  692kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16606
Date: Sat, 20 Sep 2025 10:09:37 GMT   (11975kb)

Title: Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement
  Learning
Authors: Wei Duan, Jie Lu, Junyu Xuan
Categories: cs.MA cs.LG
Comments: Accepted at NeurIPS 2025
\\
  In networked multi-agent reinforcement learning (Networked-MARL),
decentralized agents must act under local observability and constrained
communication over fixed physical graphs. Existing methods often assume static
neighborhoods, limiting adaptability to dynamic or heterogeneous environments.
While centralized frameworks can learn dynamic graphs, their reliance on global
state access and centralized infrastructure is impractical in real-world
decentralized systems. We propose a stochastic graph-based policy for
Networked-MARL, where each agent conditions its decision on a sampled subgraph
over its local physical neighborhood. Building on this formulation, we
introduce BayesG, a decentralized actor-framework that learns sparse,
context-aware interaction structures via Bayesian variational inference. Each
agent operates over an ego-graph and samples a latent communication mask to
guide message passing and policy computation. The variational distribution is
trained end-to-end alongside the policy using an evidence lower bound (ELBO)
objective, enabling agents to jointly learn both interaction topology and
decision-making strategies. BayesG outperforms strong MARL baselines on
large-scale traffic control tasks with up to 167 agents, demonstrating superior
scalability, efficiency, and performance.
\\ ( https://arxiv.org/abs/2509.16606 ,  11975kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16736
Date: Sat, 20 Sep 2025 16:00:24 GMT   (3889kb)

Title: Towards Transparent and Incentive-Compatible Collaboration in
  Decentralized LLM Multi-Agent Systems: A Blockchain-Driven Approach
Authors: Minfeng Qi, Tianqing Zhu, Lefeng Zhang, Ningran Li, Wanlei Zhou
Categories: cs.MA cs.CR
Comments: 17 pages, 7 figures
\\
  Large Language Models (LLMs) have enabled the emergence of autonomous agents
capable of complex reasoning, planning, and interaction. However, coordinating
such agents at scale remains a fundamental challenge, particularly in
decentralized environments where communication lacks transparency and agent
behavior cannot be shaped through centralized incentives. We propose a
blockchain-based framework that enables transparent agent registration,
verifiable task allocation, and dynamic reputation tracking through smart
contracts. The core of our design lies in two mechanisms: a matching
score-based task allocation protocol that evaluates agents by reputation,
capability match, and workload; and a behavior-shaping incentive mechanism that
adjusts agent behavior via feedback on performance and reward. Our
implementation integrates GPT-4 agents with Solidity contracts and
demonstrates, through 50-round simulations, strong task success rates, stable
utility distribution, and emergent agent specialization. The results underscore
the potential for trustworthy, incentive-compatible multi-agent coordination in
open environments.
\\ ( https://arxiv.org/abs/2509.16736 ,  3889kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17703
Date: Mon, 22 Sep 2025 12:43:09 GMT   (4727kb)

Title: An LLM-based Agent Simulation Approach to Study Moral Evolution
Authors: Zhou Ziheng, Huacong Tang, Mingjie Bi, Yipeng Kang, Wanying He, Fang
  Sun, Yizhou Sun, Ying Nian Wu, Demetri Terzopoulos, Fangwei Zhong
Categories: cs.MA
\\
  The evolution of morality presents a puzzle: natural selection should favor
self-interest, yet humans developed moral systems promoting altruism. We
address this question by introducing a novel Large Language Model (LLM)-based
agent simulation framework modeling prehistoric hunter-gatherer societies. This
platform is designed to probe diverse questions in social evolution, from
survival advantages to inter-group dynamics. To investigate moral evolution, we
designed agents with varying moral dispositions based on the Expanding Circle
Theory \citep{singer1981expanding}. We evaluated their evolutionary success
across a series of simulations and analyzed their decision-making in specially
designed moral dilemmas. These experiments reveal how an agent's moral
framework, in combination with its cognitive constraints, directly shapes its
behavior and determines its evolutionary outcome. Crucially, the emergent
patterns echo seminal theories from related domains of social science,
providing external validation for the simulations. This work establishes
LLM-based simulation as a powerful new paradigm to complement traditional
research in evolutionary biology and anthropology, opening new avenues for
investigating the complexities of moral and social evolution.
\\ ( https://arxiv.org/abs/2509.17703 ,  4727kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18088
Date: Mon, 22 Sep 2025 17:58:45 GMT   (1222kb)

Title: Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical
  Reinforcement and Collective Learning Approach
Authors: Chuhao Qin and Evangelos Pournaras
Categories: cs.MA cs.LG
Comments: This work has been submitted to the IEEE for possible publication
\\
  Decentralized combinatorial optimization in evolving multi-agent systems
poses significant challenges, requiring agents to balance long-term
decision-making, short-term optimized collective outcomes, while preserving
autonomy of interactive agents under unanticipated changes. Reinforcement
learning offers a way to model sequential decision-making through dynamic
programming to anticipate future environmental changes. However, applying
multi-agent reinforcement learning (MARL) to decentralized combinatorial
optimization problems remains an open challenge due to the exponential growth
of the joint state-action space, high communication overhead, and privacy
concerns in centralized training. To address these limitations, this paper
proposes Hierarchical Reinforcement and Collective Learning (HRCL), a novel
approach that leverages both MARL and decentralized collective learning based
on a hierarchical framework. Agents take high-level strategies using MARL to
group possible plans for action space reduction and constrain the agent
behavior for Pareto optimality. Meanwhile, the low-level collective learning
layer ensures efficient and decentralized coordinated decisions among agents
with minimal communication. Extensive experiments in a synthetic scenario and
real-world smart city application models, including energy self-management and
drone swarm sensing, demonstrate that HRCL significantly improves performance,
scalability, and adaptability compared to the standalone MARL and collective
learning approaches, achieving a win-win synthesis solution.
\\ ( https://arxiv.org/abs/2509.18088 ,  1222kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.09548 (*cross-listing*)
Date: Thu, 14 Mar 2024 16:35:43 GMT   (4146kb,D)
Date (revised v2): Wed, 4 Dec 2024 00:26:21 GMT   (4154kb,D)
Date (revised v3): Wed, 21 May 2025 15:48:31 GMT   (4897kb)

Title: Breast Cancer Classification Using Gradient Boosting Algorithms Focusing
  on Reducing the False Negative and SHAP for Explainability
Authors: Jo\~ao Manoel Herrera Pinheiro, Marcelo Becker
Categories: cs.LG cs.AI cs.CY q-bio.QM
Comments: 9 pages, 16 figures
Journal-ref: Inteligencia Artificial, 28(75) (2025), 63-80
DOI: 10.4114/intartif.vol28iss75pp63-80
\\
  Cancer is one of the diseases that kill the most women in the world, with
breast cancer being responsible for the highest number of cancer cases and
consequently deaths. However, it can be prevented by early detection and,
consequently, early treatment. Any development for detection or perdition this
kind of cancer is important for a better healthy life. Many studies focus on a
model with high accuracy in cancer prediction, but sometimes accuracy alone may
not always be a reliable metric. This study implies an investigative approach
to studying the performance of different machine learning algorithms based on
boosting to predict breast cancer focusing on the recall metric. Boosting
machine learning algorithms has been proven to be an effective tool for
detecting medical diseases. The dataset of the University of California, Irvine
(UCI) repository has been utilized to train and test the model classifier that
contains their attributes. The main objective of this study is to use
state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and
LightGBM to predict and diagnose breast cancer and to find the most effective
metric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study
is the first to use these four boosting algorithms with Optuna, a library for
hyperparameter optimization, and the SHAP method to improve the
interpretability of our model, which can be used as a support to identify and
predict breast cancer. We were able to improve AUC or recall for all the models
and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more
than 99.41\% for all models.
\\ ( https://arxiv.org/abs/2403.09548 ,  4897kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16212 (*cross-listing*)
Date: Fri, 29 Aug 2025 15:55:07 GMT   (5932kb)

Title: EPIC: Generative AI Platform for Accelerating HPC Operational Data
  Analytics
Authors: Ahmad Maroof Karimi, Woong Shin, Jesse Hines, Tirthankar Ghosal, Naw
  Safrin Sattar, Feiyi Wang
Categories: cs.DB cs.AI
\\
  We present EPIC, an AI-driven platform designed to augment operational data
analytics. EPIC employs a hierarchical multi-agent architecture where a
top-level large language model provides query processing, reasoning and
synthesis capabilities. These capabilities orchestrate three specialized
low-level agents for information retrieval, descriptive analytics, and
predictive analytics. This architecture enables EPIC to perform HPC operational
analytics on multi-modal data, including text, images, and tabular formats,
dynamically and iteratively. EPIC addresses the limitations of existing HPC
operational analytics approaches, which rely on static methods that struggle to
adapt to evolving analytics tasks and stakeholder demands.
  Through extensive evaluations on the Frontier HPC system, we demonstrate that
EPIC effectively handles complex queries. Using descriptive analytics as a use
case, fine-tuned smaller models outperform large state-of-the-art foundation
models, achieving up to 26% higher accuracy. Additionally, we achieved 19x
savings in LLM operational costs compared to proprietary solutions by employing
a hybrid approach that combines large foundational models with fine-tuned local
open-weight models.
\\ ( https://arxiv.org/abs/2509.16212 ,  5932kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16213 (*cross-listing*)
Date: Sat, 30 Aug 2025 00:22:09 GMT   (8775kb)

Title: DarwinWafer: A Wafer-Scale Neuromorphic Chip
Authors: Xiaolei Zhu, Xiaofei Jin, Ziyang Kang, Chonghui Sun, Junjie Feng,
  Dingwen Hu, Zengyi Wang, Hanyue Zhuang, Qian Zheng, Huajin Tang, Shi Gu, Xin
  Du, De Ma, Gang Pan
Categories: cs.ET cs.AI cs.AR
\\
  Neuromorphic computing promises brain-like efficiency, yet today's multi-chip
systems scale over PCBs and incur orders-of-magnitude penalties in bandwidth,
latency, and energy, undermining biological algorithms and system efficiency.
We present DarwinWafer, a hyperscale system-on-wafer that replaces off-chip
interconnects with wafer-scale, high-density integration of 64 Darwin3 chiplets
on a 300 mm silicon interposer. A GALS NoC within each chiplet and an AER-based
asynchronous wafer fabric with hierarchical time-step synchronization provide
low-latency, coherent operation across the wafer. Each chiplet implements 2.35
M neurons and 0.1 B synapses, yielding 0.15 B neurons and 6.4 B synapses per
wafer.At 333 MHz and 0.8 V, DarwinWafer consumes ~100 W and achieves 4.9
pJ/SOP, with 64 TSOPS peak throughput (0.64 TSOPS/W). Realization is enabled by
a holistic chiplet-interposer co-design flow (including an in-house
interposer-bump planner with early SI/PI and electro-thermal closure) and a
warpage-tolerant assembly that fans out I/O via PCBlets and compliant pogo-pin
connections, enabling robust, demountable wafer-to-board integration.
Measurements confirm 10 mV supply droop and a uniform thermal profile (34-36
{\deg}C) under ~100 W. Application studies demonstrate whole-brain simulations:
two zebrafish brains per chiplet with high connectivity fidelity (Spearman r =
0.896) and a mouse brain mapped across 32 chiplets (r = 0.645). To our
knowledge, DarwinWafer represents a pioneering demonstration of wafer-scale
neuromorphic computing, establishing a viable and scalable path toward
large-scale, brain-like computation on silicon by replacing PCB-level
interconnects with high-density, on-wafer integration.
\\ ( https://arxiv.org/abs/2509.16213 ,  8775kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16215 (*cross-listing*)
Date: Fri, 5 Sep 2025 15:32:23 GMT   (1117kb)

Title: Discovering Software Parallelization Points Using Deep Neural Networks
Authors: Izavan dos S. Correia, Henrique C. T. Santos and Tiago A. E. Ferreira
Categories: cs.LG cs.AI cs.DC cs.NE cs.PL cs.SE
Comments: 17 pages, 10 figures
\\
  This study proposes a deep learning-based approach for discovering loops in
programming code according to their potential for parallelization. Two genetic
algorithm-based code generators were developed to produce two distinct types of
code: (i) independent loops, which are parallelizable, and (ii) ambiguous
loops, whose dependencies are unclear, making them impossible to define if the
loop is parallelizable or not. The generated code snippets were tokenized and
preprocessed to ensure a robust dataset. Two deep learning models - a Deep
Neural Network (DNN) and a Convolutional Neural Network (CNN) - were
implemented to perform the classification. Based on 30 independent runs, a
robust statistical analysis was employed to verify the expected performance of
both models, DNN and CNN. The CNN showed a slightly higher mean performance,
but the two models had a similar variability. Experiments with varying dataset
sizes highlighted the importance of data diversity for model performance. These
results demonstrate the feasibility of using deep learning to automate the
identification of parallelizable structures in code, offering a promising tool
for software optimization and performance improvement.
\\ ( https://arxiv.org/abs/2509.16215 ,  1117kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16250 (*cross-listing*)
Date: Wed, 17 Sep 2025 18:11:09 GMT   (2728kb)

Title: A study on Deep Convolutional Neural Networks, transfer learning, and
  Mnet model for Cervical Cancer Detection
Authors: Saifuddin Sagor, Md Taimur Ahad, Faruk Ahmed, Rokonozzaman Ayon,
  Sanzida Parvin
Categories: q-bio.TO cs.AI cs.CV
\\
  Early and accurate detection through Pap smear analysis is critical to
improving patient outcomes and reducing mortality of Cervical cancer.
State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) require
substantial computational resources, extended training time, and large
datasets. In this study, a lightweight CNN model, S-Net (Simple Net), is
developed specifically for cervical cancer detection and classification using
Pap smear images to address these limitations. Alongside S-Net, six SOTA CNNs
were evaluated using transfer learning, including multi-path (DenseNet201,
ResNet152), depth-based (Serasnet152), width-based multi-connection (Xception),
depth-wise separable convolutions (MobileNetV2), and spatial exploitation-based
(VGG19). All models, including S-Net, achieved comparable accuracy, with S-Net
reaching 99.99%. However, S-Net significantly outperforms the SOTA CNNs in
terms of computational efficiency and inference time, making it a more
practical choice for real-time and resource-constrained applications. A major
limitation in CNN-based medical diagnosis remains the lack of transparency in
the decision-making process. To address this, Explainable AI (XAI) techniques,
such as SHAP, LIME, and Grad-CAM, were employed to visualize and interpret the
key image regions influencing model predictions. The novelty of this study lies
in the development of a highly accurate yet computationally lightweight model
(S-Net) caPable of rapid inference while maintaining interpretability through
XAI integration. Furthermore, this work analyzes the behavior of SOTA CNNs,
investigates the effects of negative transfer learning on Pap smear images, and
examines pixel intensity patterns in correctly and incorrectly classified
samples.
\\ ( https://arxiv.org/abs/2509.16250 ,  2728kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16251 (*cross-listing*)
Date: Wed, 17 Sep 2025 18:29:44 GMT   (2994kb)

Title: R-Net: A Reliable and Resource-Efficient CNN for Colorectal Cancer
  Detection with XAI Integration
Authors: Rokonozzaman Ayon, Md Taimur Ahad, Bo Song, Yan Li
Categories: q-bio.TO cs.AI cs.CV
\\
  State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) are criticized
for their extensive computational power, long training times, and large
datasets. To overcome this limitation, we propose a reasonable network (R-Net),
a lightweight CNN only to detect and classify colorectal cancer (CRC) using the
Enteroscope Biopsy Histopathological Hematoxylin and Eosin Image Dataset
(EBHI). Furthermore, six SOTA CNNs, including Multipath-based CNNs
(DenseNet121, ResNet50), Depth-based CNNs (InceptionV3), width-based
multi-connection CNNs (Xception), depth-wise separable convolutions
(MobileNetV2), spatial exploitation-based CNNs (VGG16), Transfer learning, and
two ensemble models are also tested on the same dataset. The ensemble models
are a multipath-depth-width combination (DenseNet121-InceptionV3-Xception) and
a multipath-depth-spatial combination (ResNet18-InceptionV3-VGG16). However,
the proposed R-Net lightweight achieved 99.37% accuracy, outperforming
MobileNet (95.83%) and ResNet50 (96.94%). Most importantly, to understand the
decision-making of R-Net, Explainable AI such as SHAP, LIME, and Grad-CAM are
integrated to visualize which parts of the EBHI image contribute to the
detection and classification process of R-Net. The main novelty of this
research lies in building a reliable, lightweight CNN R-Net that requires fewer
computing resources yet maintains strong prediction results. SOTA CNNs,
transfer learning, and ensemble models also extend our knowledge on CRC
classification and detection. XAI functionality and the impact of pixel
intensity on correct and incorrect classification images are also some
novelties in CRC detection and classification.
\\ ( https://arxiv.org/abs/2509.16251 ,  2994kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16254 (*cross-listing*)
Date: Wed, 17 Sep 2025 19:18:05 GMT   (42kb)

Title: Imaging Modalities-Based Classification for Lung Cancer Detection
Authors: Sajim Ahmed, Muhammad Zain Chaudhary, Muhammad Zohaib Chaudhary,
  Mahmoud Abbass, Ahmed Sherif, Mohammad Mahbubur Rahman Khan Mamun
Categories: q-bio.TO cs.AI
Comments: Accepted at ICMI 2025
MSC-class: 68T07, 92C55
\\
  Lung cancer continues to be the predominant cause of cancer-related mortality
globally. This review analyzes various approaches, including advanced image
processing methods, focusing on their efficacy in interpreting CT scans, chest
radiographs, and biological markers. Notably, we identify critical gaps in the
previous surveys, including the need for robust models that can generalize
across diverse populations and imaging modalities. This comprehensive synthesis
aims to serve as a foundational resource for researchers and clinicians,
guiding future efforts toward more accurate and efficient lung cancer
detection. Key findings reveal that 3D CNN architectures integrated with CT
scans achieve the most superior performances, yet challenges such as high false
positives, dataset variability, and computational complexity persist across
modalities.
\\ ( https://arxiv.org/abs/2509.16254 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16262 (*cross-listing*)
Date: Thu, 18 Sep 2025 03:08:24 GMT   (677kb)

Title: Socratic Mind: Impact of a Novel GenAI-Powered Assessment Tool on
  Student Learning and Higher-Order Thinking
Authors: Jeonghyun Lee, Jui-Tse Hung, Meryem Yilmaz Soylu, Diana Popescu,
  Christopher Zhang Cui, Gayane Grigoryan, David A Joyner, Stephen W Harmon
Categories: cs.CY cs.AI
\\
  This study examines the impact of Socratic Mind, a Generative Artificial
Intelligence (GenAI) powered formative assessment tool that employs Socratic
questioning to support student learning in a large, fully online
undergraduate-level computing course. Employing a quasi-experimental,
mixed-methods design, we investigated participants' engagement patterns, the
influence of user experience on engagement, and impacts on both perceived and
actual learning outcomes. Data were collected from the system logs, surveys on
user experience and perceived engagement and learning gains, student
reflections, and course performance data. Results indicated that participants
consistently reported high levels of affective, behavioral, and cognitive
engagement, and these were strongly linked to positive user experiences and
perceived learning outcomes. Quantitative analysis further revealed that
students who engaged with the GenAI tool experienced significant gains in their
quiz scores compared to those who did not, particularly benefiting students
with lower baseline achievement. Additionally, thematic analysis of qualitative
feedback revealed substantial perceived improvements in higher-order thinking
skills, including problem solving, critical thinking, and self-reflection. Our
findings highlight the promise of AI-mediated dialogue in fostering deeper
engagement and higher-order cognitive skills. As higher education institutions
expand GenAI integration in curriculum, this dialogic, GenAI powered assessment
tool can offer a scalable strategy to promote students' meaningful learning
outcomes.
\\ ( https://arxiv.org/abs/2509.16262 ,  677kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16268 (*cross-listing*)
Date: Thu, 18 Sep 2025 08:30:26 GMT   (1135kb)

Title: Digging Into the Internal: Causality-Based Analysis of LLM Function
  Calling
Authors: Zhenlan Ji, Daoyuan Wu, Wenxuan Wang, Pingchuan Ma, Shuai Wang, Lei Ma
Categories: cs.SE cs.AI
\\
  Function calling (FC) has emerged as a powerful technique for facilitating
large language models (LLMs) to interact with external systems and perform
structured tasks. However, the mechanisms through which it influences model
behavior remain largely under-explored. Besides, we discover that in addition
to the regular usage of FC, this technique can substantially enhance the
compliance of LLMs with user instructions. These observations motivate us to
leverage causality, a canonical analysis method, to investigate how FC works
within LLMs. In particular, we conduct layer-level and token-level causal
interventions to dissect FC's impact on the model's internal computational
logic when responding to user queries. Our analysis confirms the substantial
influence of FC and reveals several in-depth insights into its mechanisms. To
further validate our findings, we conduct extensive experiments comparing the
effectiveness of FC-based instructions against conventional prompting methods.
We focus on enhancing LLM safety robustness, a critical LLM application
scenario, and evaluate four mainstream LLMs across two benchmark datasets. The
results are striking: FC shows an average performance improvement of around
135% over conventional prompting methods in detecting malicious inputs,
demonstrating its promising potential to enhance LLM reliability and capability
in practical applications.
\\ ( https://arxiv.org/abs/2509.16268 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16273 (*cross-listing*)
Date: Thu, 18 Sep 2025 14:48:12 GMT   (5397kb)

Title: SubDyve: Subgraph-Driven Dynamic Propagation for Virtual Screening
  Enhancement Controlling False Positive
Authors: Jungseob Yi, Seoyoung Choi, Sun Kim, Sangseon Lee
Categories: cs.LG cs.AI
Comments: 33 pages, 12 figures
\\
  Virtual screening (VS) aims to identify bioactive compounds from vast
chemical libraries, but remains difficult in low-label regimes where only a few
actives are known. Existing methods largely rely on general-purpose molecular
fingerprints and overlook class-discriminative substructures critical to
bioactivity. Moreover, they consider molecules independently, limiting
effectiveness in low-label regimes. We introduce SubDyve, a network-based VS
framework that constructs a subgraph-aware similarity network and propagates
activity signals from a small known actives. When few active compounds are
available, SubDyve performs iterative seed refinement, incrementally promoting
new candidates based on local false discovery rate. This strategy expands the
seed set with promising candidates while controlling false positives from
topological bias and overexpansion. We evaluate SubDyve on ten DUD-E targets
under zero-shot conditions and on the CDK7 target with a 10-million-compound
ZINC dataset. SubDyve consistently outperforms existing fingerprint or
embedding-based approaches, achieving margins of up to +34.0 on the BEDROC and
+24.6 on the EF1% metric.
\\ ( https://arxiv.org/abs/2509.16273 ,  5397kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16275 (*cross-listing*)
Date: Thu, 18 Sep 2025 15:45:43 GMT   (317kb)

Title: SecureFixAgent: A Hybrid LLM Agent for Automated Python Static
  Vulnerability Repair
Authors: Jugal Gajjar, Kamalasankari Subramaniakuppusamy, Relsy Puthal, Kaustik
  Ranaware
Categories: cs.CR cs.AI cs.SE
Comments: 6 pages, 3 figures, 4 tables, 1 algorithm, accepted in the Robustness
  and Security of Large Language Models (ROSE-LLM) special session at ICMLA
  2025
\\
  Modern software development pipelines face growing challenges in securing
large codebases with extensive dependencies. Static analysis tools like Bandit
are effective at vulnerability detection but suffer from high false positives
and lack repair capabilities. Large Language Models (LLMs), in contrast, can
suggest fixes but often hallucinate changes and lack self-validation. We
present SecureFixAgent, a hybrid repair framework integrating Bandit with
lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate
loop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning
on a diverse, curated dataset spanning multiple Python project domains,
mitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses
Bandit for detection, the LLM for candidate fixes with explanations, and Bandit
re-validation for verification, all executed locally to preserve privacy and
reduce cloud reliance. Experiments show SecureFixAgent reduces false positives
by 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers
false positives by 5.46% compared to pre-trained LLMs, typically converging
within three iterations. Beyond metrics, developer studies rate explanation
quality 4.5/5, highlighting its value for human trust and adoption. By
combining verifiable security improvements with transparent rationale in a
resource-efficient local framework, SecureFixAgent advances trustworthy,
automated vulnerability remediation for modern pipelines.
\\ ( https://arxiv.org/abs/2509.16275 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16276 (*cross-listing*)
Date: Thu, 18 Sep 2025 16:20:18 GMT   (464kb)

Title: Comparative Analysis of STEM and non-STEM Teachers' Needs for
  Integrating AI into Educational Environments
Authors: Bahare Riahi, Veronica Catete
Categories: cs.CY cs.AI cs.HC
Comments: 16 pages, 3 figures, Published in HCII 2025 Conference Proceedings
ACM-class: I.2.1; I.2.4; K.3.1; H.5.2
Journal-ref: In: Smith, B.K., Borge, M. (eds) Learning and Collaboration
  Technologies. HCII 2025, Lecture Notes in Computer Science, vol 15807 (2025)
DOI: 10.1007/978-3-031-93567-1_9
\\
  There is an increasing imperative to integrate programming platforms within
AI frameworks to enhance educational tasks for both teachers and students.
However, commonly used platforms such as Code.org, Scratch, and Snap fall short
of providing the desired AI features and lack adaptability for
interdisciplinary applications. This study explores how educational platforms
can be improved by incorporating AI and analytics features to create more
effective learning environments across various subjects and domains. We
interviewed 8 K-12 teachers and asked their practices and needs while using any
block-based programming (BBP) platform in their classes. We asked for their
approaches in assessment, course development and expansion of resources, and
student monitoring in their classes. Thematic analysis of the interview
transcripts revealed both commonalities and differences in the AI tools needed
between the STEM and non-STEM groups. Our results indicated advanced AI
features that could promote BBP platforms. Both groups stressed the need for
integrity and plagiarism checks, AI adaptability, customized rubrics, and
detailed feedback in assessments. Non-STEM teachers also emphasized the
importance of creative assignments and qualitative assessments. Regarding
resource development, both AI tools desired for updating curricula, tutoring
libraries, and generative AI features. Non-STEM teachers were particularly
interested in supporting creative endeavors, such as art simulations. For
student monitoring, both groups prioritized desktop control, daily tracking,
behavior monitoring, and distraction prevention tools. Our findings identify
specific AI-enhanced features needed by K-12 teachers across various
disciplines and lay the foundation for creating more efficient, personalized,
and engaging educational experiences.
\\ ( https://arxiv.org/abs/2509.16276 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16277 (*cross-listing*)
Date: Thu, 18 Sep 2025 17:01:27 GMT   (1695kb)

Title: Stabilizing Information Flow Entropy: Regularization for Safe and
  Interpretable Autonomous Driving Perception
Authors: Haobo Yang, Shiyan Zhang, Zhuoyi Yang, Jilong Guo, Jun Yang, Xinyu
  Zhang
Categories: cs.LG cs.AI
\\
  Deep perception networks in autonomous driving traditionally rely on
data-intensive training regimes and post-hoc anomaly detection, often
disregarding fundamental information-theoretic constraints governing stable
information processing. We reconceptualize deep neural encoders as hierarchical
communication chains that incrementally compress raw sensory inputs into
task-relevant latent features. Within this framework, we establish two
theoretically justified design principles for robust perception: (D1) smooth
variation of mutual information between consecutive layers, and (D2) monotonic
decay of latent entropy with network depth. Our analysis shows that, under
realistic architectural assumptions, particularly blocks comprising repeated
layers of similar capacity, enforcing smooth information flow (D1) naturally
encourages entropy decay (D2), thus ensuring stable compression. Guided by
these insights, we propose Eloss, a novel entropy-based regularizer designed as
a lightweight, plug-and-play training objective. Rather than marginal accuracy
improvements, this approach represents a conceptual shift: it unifies
information-theoretic stability with standard perception tasks, enabling
explicit, principled detection of anomalous sensor inputs through entropy
deviations. Experimental validation on large-scale 3D object detection
benchmarks (KITTI and nuScenes) demonstrates that incorporating Eloss
consistently achieves competitive or improved accuracy while dramatically
enhancing sensitivity to anomalies, amplifying distribution-shift signals by up
to two orders of magnitude. This stable information-compression perspective not
only improves interpretability but also establishes a solid theoretical
foundation for safer, more robust autonomous driving perception systems.
\\ ( https://arxiv.org/abs/2509.16277 ,  1695kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16279 (*cross-listing*)
Date: Thu, 18 Sep 2025 19:13:39 GMT   (370kb)

Title: Energy Equity, Infrastructure and Demographic Analysis with XAI Methods
Authors: Sarahana Shrestha, Aparna S. Varde, Pankaj Lal
Categories: cs.CY cs.AI
ACM-class: I.2.6; H.4.2
Journal-ref: AAAI Conference 2025, Bridge Program
\\
  This study deploys methods in explainable artificial intelligence (XAI), e.g.
decision trees and Pearson's correlation coefficient (PCC), to investigate
electricity usage in multiple locales. It addresses the vital issue of energy
burden, i.e. total amount spent on energy divided by median household income.
Socio-demographic data is analyzed with energy features, especially using
decision trees and PCC, providing explainable predictors on factors affecting
energy burden. Based on the results of the analysis, a pilot energy equity web
portal is designed along with a novel energy burden calculator. Leveraging XAI,
this portal (with its calculator) serves as a prototype information system that
can offer tailored actionable advice to multiple energy stakeholders. The
ultimate goal of this study is to promote greater energy equity through the
adaptation of XAI methods for energy-related analysis with suitable
recommendations.
\\ ( https://arxiv.org/abs/2509.16279 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16293 (*cross-listing*)
Date: Fri, 19 Sep 2025 15:08:33 GMT   (542kb)

Title: Robust LLM Training Infrastructure at ByteDance
Authors: Borui Wan, Gaohong Liu, Zuquan Song, Jun Wang, Yun Zhang, Guangming
  Sheng, Shuguang Wang, Houmin Wei, Chenyuan Wang, Weiqiang Lou, Xi Yang, Mofan
  Zhang, Kaihua Jiang, Cheng Ren, Xiaoyun Zhi, Menghan Yu, Zhe Nan, Zhuolin
  Zheng, Baoquan Zhong, Qinlong Wang, Huan Yu, Jinxin Chi, Wang Zhang, Yuhan
  Li, Zixian Du, Sida Zhao, Yongqiang Zhang, Jingzhe Tang, Zherui Liu, Chuan
  Wu, Yanghua Peng, Haibin Lin, Wencong Xiao, Xin Liu, Liang Xiang
Categories: cs.LG cs.AI cs.DC
\\
  The training scale of large language models (LLMs) has reached tens of
thousands of GPUs and is still continuously expanding, enabling faster learning
of larger models. Accompanying the expansion of the resource scale is the
prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses
significant challenges to training stability. Any large-scale LLM training
infrastructure should strive for minimal training interruption, efficient fault
diagnosis, and effective failure tolerance to enable highly efficient
continuous training. This paper presents ByteRobust, a large-scale GPU
infrastructure management system tailored for robust and stable training of
LLMs. It exploits the uniqueness of LLM training process and gives top
priorities to detecting and recovering failures in a routine manner. Leveraging
parallelisms and characteristics of LLM training, ByteRobust enables
high-capacity fault tolerance, prompt fault demarcation, and localization with
an effective data-driven approach, comprehensively ensuring continuous and
efficient training of LLM tasks. ByteRobust is deployed on a production GPU
platform with over 200,000 GPUs and achieves 97% ETTR for a three-month
training job on 9,600 GPUs.
\\ ( https://arxiv.org/abs/2509.16293 ,  542kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16295 (*cross-listing*)
Date: Fri, 19 Sep 2025 15:55:08 GMT   (1496kb)

Title: Patterns in the Transition From Founder-Leadership to Community
  Governance of Open Source
Authors: Mobina Noori, Mahasweta Chakraborti, Amy X Zhang, Seth Frey
Categories: cs.CY cs.AI cs.CL
\\
  Open digital public infrastructure needs community management to ensure
accountability, sustainability, and robustness. Yet open-source projects often
rely on centralized decision-making, and the determinants of successful
community management remain unclear. We analyze 637 GitHub repositories to
trace transitions from founder-led to shared governance. Specifically, we
document trajectories to community governance by extracting institutional
roles, actions, and deontic cues from version-controlled project constitutions
GOVERNANCE.md. With a semantic parsing pipeline, we cluster elements into
broader role and action types. We find roles and actions grow, and regulation
becomes more balanced, reflecting increases in governance scope and
differentiation over time. Rather than shifting tone, communities grow by
layering and refining responsibilities. As transitions to community management
mature, projects increasingly regulate ecosystem-level relationships and add
definition to project oversight roles. Overall, this work offers a scalable
pipeline for tracking the growth and development of community governance
regimes from open-source software's familiar default of founder-ownership.
\\ ( https://arxiv.org/abs/2509.16295 ,  1496kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16297 (*cross-listing*)
Date: Fri, 19 Sep 2025 16:46:27 GMT   (481kb)

Title: How Large Language Models are Designed to Hallucinate
Authors: Richard Ackermann and Simeon Emanuilov
Categories: cs.CY cs.AI cs.CL
Comments: 23 pages, 2 tables, 2 figures
\\
  Large language models (LLMs) achieve remarkable fluency across linguistic and
reasoning tasks but remain systematically prone to hallucination. Prevailing
accounts attribute hallucinations to data gaps, limited context, or
optimization errors. We argue instead that hallucination is a structural
outcome of the transformer architecture. As coherence engines, transformers are
compelled to produce fluent continuations, with self-attention simulating the
relational structure of meaning but lacking the existential grounding of
temporality, mood, and care that stabilizes human understanding. On this basis,
we distinguish ontological hallucination, arising when continuations require
disclosure of beings in world, and residual reasoning hallucination, where
models mimic inference by recycling traces of human reasoning in text. We
illustrate these patterns through case studies aligned with Heideggerian
categories and an experiment across twelve LLMs showing how simulated
"self-preservation" emerges under extended prompts. Our contribution is
threefold: (1) a comparative account showing why existing explanations are
insufficient; (2) a predictive taxonomy of hallucination linked to existential
structures with proposed benchmarks; and (3) design directions toward
"truth-constrained" architectures capable of withholding or deferring when
disclosure is absent. We conclude that hallucination is not an incidental
defect but a defining limit of transformer-based models, an outcome scaffolding
can mask but never resolve.
\\ ( https://arxiv.org/abs/2509.16297 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16339 (*cross-listing*)
Date: Fri, 19 Sep 2025 18:26:58 GMT   (1352kb)

Title: Highly Imbalanced Regression with Tabular Data in SEP and Other
  Applications
Authors: Josias K. Moukpe, Philip K. Chan, Ming Zhang
Categories: cs.LG cs.AI
Comments: ICMLA 2025
\\
  We investigate imbalanced regression with tabular data that have an imbalance
ratio larger than 1,000 ("highly imbalanced"). Accurately estimating the target
values of rare instances is important in applications such as forecasting the
intensity of rare harmful Solar Energetic Particle (SEP) events. For
regression, the MSE loss does not consider the correlation between predicted
and actual values. Typical inverse importance functions allow only convex
functions. Uniform sampling might yield mini-batches that do not have rare
instances. We propose CISIR that incorporates correlation, Monotonically
Decreasing Involution (MDI) importance, and stratified sampling. Based on five
datasets, our experimental results indicate that CISIR can achieve lower error
and higher correlation than some recent methods. Also, adding our correlation
component to other recent methods can improve their performance. Lastly, MDI
importance can outperform other importance functions. Our code can be found in
https://github.com/Machine-Earning/CISIR.
\\ ( https://arxiv.org/abs/2509.16339 ,  1352kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16345 (*cross-listing*)
Date: Fri, 19 Sep 2025 18:38:06 GMT   (214kb)

Title: Estimating Clinical Lab Test Result Trajectories from PPG using
  Physiological Foundation Model and Patient-Aware State Space Model -- a
  UNIPHY+ Approach
Authors: Minxiao Wang, Runze Yan, Carol Li, Saurabh Kataria, Xiao Hu, Matthew
  Clark, Timothy Ruchti, Timothy G. Buchman, Sivasubramanium V Bhavani, Randall
  J. Lee
Categories: cs.LG cs.AI
\\
  Clinical laboratory tests provide essential biochemical measurements for
diagnosis and treatment, but are limited by intermittent and invasive sampling.
In contrast, photoplethysmogram (PPG) is a non-invasive, continuously recorded
signal in intensive care units (ICUs) that reflects cardiovascular dynamics and
can serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, a
framework that combines a large-scale PPG foundation model for local waveform
encoding with a patient-aware Mamba model for long-range temporal modeling. Our
architecture addresses three challenges: (1) capturing extended temporal trends
in laboratory values, (2) accounting for patient-specific baseline variation
via FiLM-modulated initial states, and (3) performing multi-task estimation for
interrelated biomarkers. We evaluate our method on the two ICU datasets for
predicting the five key laboratory tests. The results show substantial
improvements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$
among most of the estimation targets. This work demonstrates the feasibility of
continuous, personalized lab value estimation from routine PPG monitoring,
offering a pathway toward non-invasive biochemical surveillance in critical
care.
\\ ( https://arxiv.org/abs/2509.16345 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16347 (*cross-listing*)
Date: Fri, 19 Sep 2025 18:40:30 GMT   (2278kb)

Title: QUINTA: Reflexive Sensibility For Responsible AI Research and
  Data-Driven Processes
Authors: Alicia E. Boyd
Categories: cs.SI cs.AI
Comments: 14 pages, 5 figures, 1 Table, This paper was accepted as a poster
  presentation at Equity and Access in Algorithms, Mechanisms, and Optimization
  (EAAMO) Conference in 2023
\\
  As the field of artificial intelligence (AI) and machine learning (ML)
continues to prioritize fairness and the concern for historically marginalized
communities, the importance of intersectionality in AI research has gained
significant recognition. However, few studies provide practical guidance on how
researchers can effectively incorporate intersectionality into critical praxis.
In response, this paper presents a comprehensive framework grounded in critical
reflexivity as intersectional praxis. Operationalizing intersectionality within
the AI/DS (Artificial Intelligence/Data Science) pipeline, Quantitative
Intersectional Data (QUINTA) is introduced as a methodological paradigm that
challenges conventional and superficial research habits, particularly in
data-centric processes, to identify and mitigate negative impacts such as the
inadvertent marginalization caused by these practices. The framework centers
researcher reflexivity to call attention to the AI researchers' power in
creating and analyzing AI/DS artifacts through data-centric approaches. To
illustrate the effectiveness of QUINTA, we provide a reflexive AI/DS researcher
demonstration utilizing the \#metoo movement as a case study. Note: This paper
was accepted as a poster presentation at Equity and Access in Algorithms,
Mechanisms, and Optimization (EAAMO) Conference in 2023.
\\ ( https://arxiv.org/abs/2509.16347 ,  2278kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16352 (*cross-listing*)
Date: Fri, 19 Sep 2025 18:49:49 GMT   (565kb)

Title: Secure Confidential Business Information When Sharing Machine Learning
  Models
Authors: Yunfan Yang, Jiarong Xu, Hongzhe Zhang, Xiao Fang
Categories: cs.CR cs.AI
\\
  Model-sharing offers significant business value by enabling firms with
well-established Machine Learning (ML) models to monetize and share their
models with others who lack the resources to develop ML models from scratch.
However, concerns over data confidentiality remain a significant barrier to
model-sharing adoption, as Confidential Property Inference (CPI) attacks can
exploit shared ML models to uncover confidential properties of the model
provider's private model training data. Existing defenses often assume that CPI
attacks are non-adaptive to the specific ML model they are targeting. This
assumption overlooks a key characteristic of real-world adversaries: their
responsiveness, i.e., adversaries' ability to dynamically adjust their attack
models based on the information of the target and its defenses. To overcome
this limitation, we propose a novel defense method that explicitly accounts for
the responsive nature of real-world adversaries via two methodological
innovations: a novel Responsive CPI attack and an attack-defense arms race
framework. The former emulates the responsive behaviors of adversaries in the
real world, and the latter iteratively enhances both the target and attack
models, ultimately producing a secure ML model that is robust against
responsive CPI attacks. Furthermore, we propose and integrate a novel
approximate strategy into our defense, which addresses a critical computational
bottleneck of defense methods and improves defense efficiency. Through
extensive empirical evaluations across various realistic model-sharing
scenarios, we demonstrate that our method outperforms existing defenses by more
effectively defending against CPI attacks, preserving ML model utility, and
reducing computational overhead.
\\ ( https://arxiv.org/abs/2509.16352 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16369 (*cross-listing*)
Date: Fri, 19 Sep 2025 19:24:30 GMT   (174kb)

Title: Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach
  to Knowledge Retrieval and Hallucination Reduction
Authors: Akshay Govind Srinivasan, Ryan Jacob George, Jayden Koshy Joe,
  Hrushikesh Kant, Harshith M R, Sachin Sundar, Sudharshan Suresh, Rahul
  Vimalkanth, Vijayavallabh
Categories: cs.IR cs.AI cs.CE
Comments: 14 Pages, 8 Tables, 2 Figures. Accepted and to be published in the
  proceedings of FinNLP, Empirical Methods in Natural Language Processing 2025
ACM-class: H.4; H.5; H.3.3
\\
  Accurate and reliable knowledge retrieval is vital for financial
question-answering, where continually updated data sources and complex,
high-stakes contexts demand precision. Traditional retrieval systems rely on a
single database and retriever, but financial applications require more
sophisticated approaches to handle intricate regulatory filings, market
analyses, and extensive multi-year reports. We introduce a framework for
financial Retrieval Augmented Generation (RAG) that leverages agentic AI and
the Multi-HyDE system, an approach that generates multiple, nonequivalent
queries to boost the effectiveness and coverage of retrieval from large,
structured financial corpora. Our pipeline is optimized for token efficiency
and multi-step financial reasoning, and we demonstrate that their combination
improves accuracy by 11.2% and reduces hallucinations by 15%. Our method is
evaluated on standard financial QA benchmarks, showing that integrating
domain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets,
including keyword and table-based retrieval, significantly enhances both the
accuracy and reliability of answers. This research not only delivers a modular,
adaptable retrieval framework for finance but also highlights the importance of
structured agent workflows and multi-perspective retrieval for trustworthy
deployment of AI in high-stakes financial applications.
\\ ( https://arxiv.org/abs/2509.16369 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16391 (*cross-listing*)
Date: Fri, 19 Sep 2025 20:12:49 GMT   (645kb)

Title: CoUn: Empowering Machine Unlearning via Contrastive Learning
Authors: Yasser H. Khalil, Mehdi Setayesh, Hongliang Li
Categories: cs.LG cs.AI cs.CV
\\
  Machine unlearning (MU) aims to remove the influence of specific "forget"
data from a trained model while preserving its knowledge of the remaining
"retain" data. Existing MU methods based on label manipulation or model weight
perturbations often achieve limited unlearning effectiveness. To address this,
we introduce CoUn, a novel MU framework inspired by the observation that a
model retrained from scratch using only retain data classifies forget data
based on their semantic similarity to the retain data. CoUn emulates this
behavior by adjusting learned data representations through contrastive learning
(CL) and supervised learning, applied exclusively to retain data. Specifically,
CoUn (1) leverages semantic similarity between data samples to indirectly
adjust forget representations using CL, and (2) maintains retain
representations within their respective clusters through supervised learning.
Extensive experiments across various datasets and model architectures show that
CoUn consistently outperforms state-of-the-art MU baselines in unlearning
effectiveness. Additionally, integrating our CL module into existing baselines
empowers their unlearning effectiveness.
\\ ( https://arxiv.org/abs/2509.16391 ,  645kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16397 (*cross-listing*)
Date: Fri, 19 Sep 2025 20:19:48 GMT   (10104kb)

Title: GRID: Graph-based Reasoning for Intervention and Discovery in Built
  Environments
Authors: Taqiya Ehsan, Shuren Xia, Jorge Ortiz
Categories: cs.LG cs.AI
\\
  Manual HVAC fault diagnosis in commercial buildings takes 8-12 hours per
incident and achieves only 60 percent diagnostic accuracy, reflecting analytics
that stop at correlation instead of causation. To close this gap, we present
GRID (Graph-based Reasoning for Intervention and Discovery), a three-stage
causal discovery pipeline that combines constraint-based search, neural
structural equation modeling, and language model priors to recover directed
acyclic graphs from building sensor data. Across six benchmarks: synthetic
rooms, EnergyPlus simulation, the ASHRAE Great Energy Predictor III dataset,
and a live office testbed, GRID achieves F1 scores ranging from 0.65 to 1.00,
with exact recovery (F1 = 1.00) in three controlled environments (Base, Hidden,
Physical) and strong performance on real-world data (F1 = 0.89 on ASHRAE, 0.86
in noisy conditions). The method outperforms ten baseline approaches across all
evaluation scenarios. Intervention scheduling achieves low operational impact
in most scenarios (cost <= 0.026) while reducing risk metrics compared to
baseline approaches. The framework integrates constraint-based methods, neural
architectures, and domain-specific language model prompts to address the
observational-causal gap in building analytics.
\\ ( https://arxiv.org/abs/2509.16397 ,  10104kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16418 (*cross-listing*)
Date: Fri, 19 Sep 2025 20:59:30 GMT   (2003kb)

Title: LenslessMic: Audio Encryption and Authentication via Lensless
  Computational Imaging
Authors: Petr Grinberg, Eric Bezzam, Paolo Prandoni, Martin Vetterli
Categories: cs.CR cs.AI cs.CV cs.SD eess.AS
Comments: Submitted to ICASSP 2026
\\
  With society's increasing reliance on digital data sharing, the protection of
sensitive information has become critical. Encryption serves as one of the
privacy-preserving methods; however, its realization in the audio domain
predominantly relies on signal processing or software methods embedded into
hardware. In this paper, we introduce LenslessMic, a hybrid optical
hardware-based encryption method that utilizes a lensless camera as a physical
layer of security applicable to multiple types of audio. We show that
LenslessMic enables (1) robust authentication of audio recordings and (2)
encryption strength that can rival the search space of 256-bit digital
standards, while maintaining high-quality signals and minimal loss of content
information. The approach is validated with a low-cost Raspberry Pi prototype
and is open-sourced together with datasets to facilitate research in the area.
\\ ( https://arxiv.org/abs/2509.16418 ,  2003kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16437 (*cross-listing*)
Date: Fri, 19 Sep 2025 21:32:24 GMT   (1034kb)

Title: SENSE-7: Taxonomy and Dataset for Measuring User Perceptions of Empathy
  in Sustained Human-AI Conversations
Authors: Jina Suh, Lindy Le, Erfan Shayegani, Gonzalo Ramos, Judith Amores,
  Desmond C. Ong, Mary Czerwinski, Javier Hernandez
Categories: cs.HC cs.AI
\\
  Empathy is increasingly recognized as a key factor in human-AI communication,
yet conventional approaches to "digital empathy" often focus on simulating
internal, human-like emotional states while overlooking the inherently
subjective, contextual, and relational facets of empathy as perceived by users.
In this work, we propose a human-centered taxonomy that emphasizes observable
empathic behaviors and introduce a new dataset, Sense-7, of real-world
conversations between information workers and Large Language Models (LLMs),
which includes per-turn empathy annotations directly from the users, along with
user characteristics, and contextual details, offering a more user-grounded
representation of empathy. Analysis of 695 conversations from 109 participants
reveals that empathy judgments are highly individualized, context-sensitive,
and vulnerable to disruption when conversational continuity fails or user
expectations go unmet. To promote further research, we provide a subset of 672
anonymized conversation and provide exploratory classification analysis,
showing that an LLM-based classifier can recognize 5 levels of empathy with an
encouraging average Spearman $\rho$=0.369 and Accuracy=0.487 over this set.
Overall, our findings underscore the need for AI designs that dynamically
tailor empathic behaviors to user contexts and goals, offering a roadmap for
future research and practical development of socially attuned, human-centered
artificial agents.
\\ ( https://arxiv.org/abs/2509.16437 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16443 (*cross-listing*)
Date: Fri, 19 Sep 2025 21:45:26 GMT   (3159kb)

Title: LightCode: Compiling LLM Inference for Photonic-Electronic Systems
Authors: Ryan Tomich, Zhizhen Zhong, Dirk Englund
Categories: physics.app-ph cs.AI cs.PL
Comments: 9 pages, 8 figures
\\
  The growing demand for low-latency, energy-efficient inference in large
language models (LLMs) has catalyzed interest in heterogeneous architectures.
While GPUs remain dominant, they are poorly suited for integration with
emerging domain-specific accelerators like the Photonic Tensor Units (PTUs),
which offer low-power, high-throughput linear computation. This motivates
hybrid compilation strategies that combine photonic and electronic resources.
We present LightCode, a compiler framework and simulator for mapping LLM
inference workloads across hybrid photonic-electronic systems. LightCode
introduces the Stacked Graph, an intermediate representation that encodes
multiple hardware-specific realizations of each tensor operation. Hardware
assignment is formulated as a constrained subgraph selection problem optimized
for latency or energy under parametric cost models. We evaluate LightCode on
the prefill stage of GPT-2 and Llama-7B showing that under our workload and
hardware assumptions, (i) Photonic hardware reduced energy by up to 50% in our
simulated workloads at maximum sequence length; (ii) multiplexing and
assignment strategy yielded latency improvements exceeding 10x; and (iii)
Optimizing for latency or energy resulted in distinct hardware mappings in our
simulations. LightCode offers a module, foundational framework and simulator
for compiling LLMs to emerging photonic accelerators.
\\ ( https://arxiv.org/abs/2509.16443 ,  3159kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16454 (*cross-listing*)
Date: Fri, 19 Sep 2025 22:20:24 GMT   (536kb)

Title: A Generative AI System for Biomedical Data Discovery with Grammar-Based
  Visualizations
Authors: Devin Lange, Shanghua Gao, Pengwei Sui, Austen Money, Priya Misner,
  Marinka Zitnik, Nils Gehlenborg
Categories: cs.HC cs.AI
\\
  We explore the potential for combining generative AI with grammar-based
visualizations for biomedical data discovery. In our prototype, we use a
multi-agent system to generate visualization specifications and apply filters.
These visualizations are linked together, resulting in an interactive dashboard
that is progressively constructed. Our system leverages the strengths of
natural language while maintaining the utility of traditional user interfaces.
Furthermore, we utilize generated interactive widgets enabling user adjustment.
Finally, we demonstrate the potential utility of this system for biomedical
data discovery with a case study.
\\ ( https://arxiv.org/abs/2509.16454 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16463 (*cross-listing*)
Date: Fri, 19 Sep 2025 23:10:10 GMT   (206kb)

Title: Entropic Causal Inference: Graph Identifiability
Authors: Spencer Compton, Kristjan Greenewald, Dmitriy Katz, Murat Kocaoglu
Categories: cs.LG cs.AI
Comments: Presented at ICML 2022. This version corrects a bug in semi-synthetic
  experiments
\\
  Entropic causal inference is a recent framework for learning the causal graph
between two variables from observational data by finding the
information-theoretically simplest structural explanation of the data, i.e.,
the model with smallest entropy. In our work, we first extend the causal graph
identifiability result in the two-variable setting under relaxed assumptions.
We then show the first identifiability result using the entropic approach for
learning causal graphs with more than two nodes. Our approach utilizes the
property that ancestrality between a source node and its descendants can be
determined using the bivariate entropic tests. We provide a sound sequential
peeling algorithm for general graphs that relies on this property. We also
propose a heuristic algorithm for small graphs that shows strong empirical
performance. We rigorously evaluate the performance of our algorithms on
synthetic data generated from a variety of models, observing improvement over
prior work. Finally we test our algorithms on real-world datasets.
\\ ( https://arxiv.org/abs/2509.16463 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16496 (*cross-listing*)
Date: Sat, 20 Sep 2025 02:00:07 GMT   (564kb)

Title: Synergies between Federated Foundation Models and Smart Power Grids
Authors: Seyyedali Hosseinalipour, Shimiao Li, Adedoyin Inaolaji, Filippo
  Malandra, Luis Herrera, Nicholas Mastronarde
Categories: eess.SY cs.AI cs.LG cs.SY
\\
  The recent emergence of large language models (LLMs) such as GPT-3 has marked
a significant paradigm shift in machine learning. Trained on massive corpora of
data, these models demonstrate remarkable capabilities in language
understanding, generation, summarization, and reasoning, transforming how
intelligent systems process and interact with human language. Although LLMs may
still seem like a recent breakthrough, the field is already witnessing the rise
of a new and more general category: multi-modal, multi-task foundation models
(M3T FMs). These models go beyond language and can process heterogeneous data
types/modalities, such as time-series measurements, audio, imagery, tabular
records, and unstructured logs, while supporting a broad range of downstream
tasks spanning forecasting, classification, control, and retrieval. When
combined with federated learning (FL), they give rise to M3T Federated
Foundation Models (FedFMs): a highly recent and largely unexplored class of
models that enable scalable, privacy-preserving model training/fine-tuning
across distributed data sources. In this paper, we take one of the first steps
toward introducing these models to the power systems research community by
offering a bidirectional perspective: (i) M3T FedFMs for smart grids and (ii)
smart grids for FedFMs. In the former, we explore how M3T FedFMs can enhance
key grid functions, such as load/demand forecasting and fault detection, by
learning from distributed, heterogeneous data available at the grid edge in a
privacy-preserving manner. In the latter, we investigate how the constraints
and structure of smart grids, spanning energy, communication, and regulatory
dimensions, shape the design, training, and deployment of M3T FedFMs.
\\ ( https://arxiv.org/abs/2509.16496 ,  564kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16525 (*cross-listing*)
Date: Sat, 20 Sep 2025 04:19:37 GMT   (666kb)

Title: Causal Fuzzing for Verifying Machine Unlearning
Authors: Anna Mazhar and Sainyam Galhotra
Categories: cs.SE cs.AI cs.LG
\\
  As machine learning models become increasingly embedded in decision-making
systems, the ability to "unlearn" targeted data or features is crucial for
enhancing model adaptability, fairness, and privacy in models which involves
expensive training. To effectively guide machine unlearning, a thorough testing
is essential. Existing methods for verification of machine unlearning provide
limited insights, often failing in scenarios where the influence is indirect.
In this work, we propose CAF\'E, a new causality based framework that unifies
datapoint- and feature-level unlearning for verification of black-box ML
models. CAF\'E evaluates both direct and indirect effects of unlearning targets
through causal dependencies, providing actionable insights with fine-grained
analysis. Our evaluation across five datasets and three model architectures
demonstrates that CAF\'E successfully detects residual influence missed by
baselines while maintaining computational efficiency.
\\ ( https://arxiv.org/abs/2509.16525 ,  666kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16532 (*cross-listing*)
Date: Sat, 20 Sep 2025 04:43:42 GMT   (3639kb)

Title: No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for
  Robotic Manipulation Learning
Authors: Run Yu, Yangdi Liu, Wen-Da Wei, Chen Li
Categories: cs.RO cs.AI
\\
  Recently,vision-based robotic manipulation has garnered significant attention
and witnessed substantial advancements. 2D image-based and 3D point cloud-based
policy learning represent two predominant paradigms in the field, with recent
studies showing that the latter consistently outperforms the former in terms of
both policy performance and generalization, thereby underscoring the value and
significance of 3D information. However, 3D point cloud-based approaches face
the significant challenge of high data acquisition costs, limiting their
scalability and real-world deployment. To address this issue, we propose a
novel framework NoReal3D: which introduces the 3DStructureFormer, a learnable
3D perception module capable of transforming monocular images into
geometrically meaningful pseudo-point cloud features, effectively fused with
the 2D encoder output features. Specially, the generated pseudo-point clouds
retain geometric and topological structures so we design a pseudo-point cloud
encoder to preserve these properties, making it well-suited for our framework.
We also investigate the effectiveness of different feature fusion
strategies.Our framework enhances the robot's understanding of 3D spatial
structures while completely eliminating the substantial costs associated with
3D point cloud acquisition.Extensive experiments across various tasks validate
that our framework can achieve performance comparable to 3D point cloud-based
methods, without the actual point cloud data.
\\ ( https://arxiv.org/abs/2509.16532 ,  3639kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16546 (*cross-listing*)
Date: Sat, 20 Sep 2025 06:05:23 GMT   (518kb)

Title: Train to Defend: First Defense Against Cryptanalytic Neural Network
  Parameter Extraction Attacks
Authors: Ashley Kurian and Aydin Aysu
Categories: cs.CR cs.AI
Comments: 18 pages, 3 Figures
MSC-class: F.2.2, I.2.7
ACM-class: F.2.2, I.2.7
\\
  Neural networks are valuable intellectual property due to the significant
computational cost, expert labor, and proprietary data involved in their
development. Consequently, protecting their parameters is critical not only for
maintaining a competitive advantage but also for enhancing the model's security
and privacy. Prior works have demonstrated the growing capability of
cryptanalytic attacks to scale to deeper models. In this paper, we present the
first defense mechanism against cryptanalytic parameter extraction attacks. Our
key insight is to eliminate the neuron uniqueness necessary for these attacks
to succeed. We achieve this by a novel, extraction-aware training method.
Specifically, we augment the standard loss function with an additional
regularization term that minimizes the distance between neuron weights within a
layer. Therefore, the proposed defense has zero area-delay overhead during
inference. We evaluate the effectiveness of our approach in mitigating
extraction attacks while analyzing the model accuracy across different
architectures and datasets. When re-trained with the same model architecture,
the results show that our defense incurs a marginal accuracy change of less
than 1% with the modified loss function. Moreover, we present a theoretical
framework to quantify the success probability of the attack. When tested
comprehensively with prior attack settings, our defense demonstrated empirical
success for sustained periods of extraction, whereas unprotected networks are
extracted between 14 minutes to 4 hours.
\\ ( https://arxiv.org/abs/2509.16546 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16550 (*cross-listing*)
Date: Sat, 20 Sep 2025 06:25:59 GMT   (3475kb)

Title: TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic
  Manipulation
Authors: Yinghao Wu, Shuhong Hou, Haowen Zheng, Yichen Li, Weiyi Lu, Xun Zhou,
  Yitian Shao
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: 8 pages, 7 figures
\\
  Robotic manipulation tasks such as inserting a key into a lock or plugging a
USB device into a port can fail when visual perception is insufficient to
detect misalignment. In these situations, touch sensing is crucial for the
robot to monitor the task's states and make precise, timely adjustments.
Current touch sensing solutions are either insensitive to detect subtle changes
or demand excessive sensor data. Here, we introduce TranTac, a data-efficient
and low-cost tactile sensing and control framework that integrates a single
contact-sensitive 6-axis inertial measurement unit within the elastomeric tips
of a robotic gripper for completing fine insertion tasks. Our customized
sensing system can detect dynamic translational and torsional deformations at
the micrometer scale, enabling the tracking of visually imperceptible pose
changes of the grasped object. By leveraging transformer-based encoders and
diffusion policy, TranTac can imitate human insertion behaviors using transient
tactile cues detected at the gripper's tip during insertion processes. These
cues enable the robot to dynamically control and correct the 6-DoF pose of the
grasped object. When combined with vision, TranTac achieves an average success
rate of 79% on object grasping and insertion tasks, outperforming both
vision-only policy and the one augmented with end-effector 6D force/torque
sensing. Contact localization performance is also validated through
tactile-only misaligned insertion tasks, achieving an average success rate of
88%. We assess the generalizability by training TranTac on a single prism-slot
pair and testing it on unseen data, including a USB plug and a metal key, and
find that the insertion tasks can still be completed with an average success
rate of nearly 70%. The proposed framework may inspire new robotic tactile
sensing systems for delicate manipulation tasks.
\\ ( https://arxiv.org/abs/2509.16550 ,  3475kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16622 (*cross-listing*)
Date: Sat, 20 Sep 2025 10:48:06 GMT   (185kb)

Title: Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing
Authors: Mengqi Wang, Zhan Liu, Zengrui Jin, Guangzhi Sun, Chao Zhang, Philip
  C. Woodland
Categories: eess.AS cs.AI cs.SD
\\
  Diffusion-based large language models (DLLMs) have recently attracted growing
interest as an alternative to autoregressive decoders. In this work, we present
an empirical study on using the diffusion-based large language model LLaDA for
automatic speech recognition (ASR). We first investigate its use as an external
deliberation-based processing module for Whisper-LLaMA transcripts. By
leveraging the bidirectional attention and denoising capabilities of LLaDA, we
explore random masking, low-confidence masking, and semi-autoregressive
strategies, showing that Whisper-LLaDA substantially reduces WER compared with
the baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER
on test-clean/test-other, representing a 12.3% relative improvement over the
Whisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA
without acoustic features fails to improve accuracy, highlighting the
importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA
as a standalone decoder for ASR with diffusion-based and semi-autoregressive
decoding. Most experimental configurations achieve faster inference than the
Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These
findings offer an empirical view of diffusion-based LLMs for ASR and point to
promising directions for improvements.
\\ ( https://arxiv.org/abs/2509.16622 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16638 (*cross-listing*)
Date: Sat, 20 Sep 2025 11:31:14 GMT   (4683kb)

Title: KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body
  Control
Authors: Jinrui Han, Weiji Xie, Jiakun Zheng, Jiyuan Shi, Weinan Zhang, Ting
  Xiao, Chenjia Bai
Categories: cs.RO cs.AI
\\
  Learning versatile whole-body skills by tracking various human motions is a
fundamental step toward general-purpose humanoid robots. This task is
particularly challenging because a single policy must master a broad repertoire
of motion skills while ensuring stability over long-horizon sequences. To this
end, we present VMS, a unified whole-body controller that enables humanoid
robots to learn diverse and dynamic behaviors within a single policy. Our
framework integrates a hybrid tracking objective that balances local motion
fidelity with global trajectory consistency, and an Orthogonal
Mixture-of-Experts (OMoE) architecture that encourages skill specialization
while enhancing generalization across motions. A segment-level tracking reward
is further introduced to relax rigid step-wise matching, enhancing robustness
when handling global displacements and transient inaccuracies. We validate VMS
extensively in both simulation and real-world experiments, demonstrating
accurate imitation of dynamic skills, stable performance over minute-long
sequences, and strong generalization to unseen motions. These results highlight
the potential of VMS as a scalable foundation for versatile humanoid whole-body
control. The project page is available at
https://kungfubot2-humanoid.github.io.
\\ ( https://arxiv.org/abs/2509.16638 ,  4683kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16649 (*cross-listing*)
Date: Sat, 20 Sep 2025 11:53:18 GMT   (97kb)

Title: AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval
Authors: Hyun Jun Kim, Hyeong Yong Choi, and Changwon Lim
Categories: cs.SD cs.AI eess.AS
Comments: 5 pages, 1 figure, DCASE2025 Task2 technical report
\\
  This report presents the AISTAT team's submission to the language-based audio
retrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder
architecture, where audio and text modalities are encoded separately, and their
representations are aligned using contrastive learning. Drawing inspiration
from methodologies of the previous year's challenge, we implemented a
distillation approach and leveraged large language models (LLMs) for effective
data augmentation techniques, including back-translation and LLM mix.
Additionally, we incorporated clustering to introduce an auxiliary
classification task for further finetuning. Our best single system achieved a
mAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on
the Clotho development test split.
\\ ( https://arxiv.org/abs/2509.16649 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16662 (*cross-listing*)
Date: Sat, 20 Sep 2025 12:31:30 GMT   (129kb)

Title: On the de-duplication of the Lakh MIDI dataset
Authors: Eunjin Choi, Hyerin Kim, Jiwoo Ryu, Juhan Nam, Dasaem Jeong
Categories: cs.SD cs.AI cs.LG cs.MM eess.AS
Comments: The paper has been accepted for publication at ISMIR 2025
\\
  A large-scale dataset is essential for training a well-generalized
deep-learning model. Most such datasets are collected via scraping from various
internet sources, inevitably introducing duplicated data. In the symbolic music
domain, these duplicates often come from multiple user arrangements and
metadata changes after simple editing. However, despite critical issues such as
unreliable training evaluation from data leakage during random splitting,
dataset duplication has not been extensively addressed in the MIR community.
This study investigates the dataset duplication issues regarding Lakh MIDI
Dataset (LMD), one of the largest publicly available sources in the symbolic
music domain. To find and evaluate the best retrieval method for duplicated
data, we employed the Clean MIDI subset of the LMD as a benchmark test set, in
which different versions of the same songs are grouped together. We first
evaluated rule-based approaches and previous symbolic music retrieval models
for de-duplication and also investigated with a contrastive learning-based BERT
model with various augmentations to find duplicate files. As a result, we
propose three different versions of the filtered list of LMD, which filters out
at least 38,134 samples in the most conservative settings among 178,561 files.
\\ ( https://arxiv.org/abs/2509.16662 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16676 (*cross-listing*)
Date: Sat, 20 Sep 2025 13:03:11 GMT   (120kb)

Title: Governed By Agents: A Survey On The Role Of Agentic AI In Future
  Computing Environments
Authors: Nauman Ali Murad and Safia Baloch
Categories: cs.ET cs.AI
\\
  The emergence of agentic Artificial Intelligence (AI), which can operate
autonomously, demonstrate goal-directed behavior, and adaptively learn,
indicates the onset of a massive change in today's computing infrastructure.
This study investigates how agentic AI models' multiple characteristics may
impact the architecture, governance, and operation under which computing
environments function. Agentic AI has the potential to reduce reliance on
extremely large (public) cloud environments due to resource efficiency,
especially with processing and/or storage. The aforementioned characteristics
provide us with an opportunity to canvas the likelihood of strategic migration
in computing infrastructures away from massive public cloud services, towards
more locally distributed architectures: edge computing and on-premises
computing infrastructures. Many of these likely migrations will be spurred by
factors like on-premises processing needs, diminished data consumption
footprints, and cost savings. This study examines how a solution for
implementing AI's autonomy could result in a re-architecture of the systems and
model a departure from today's governance models to help us manage these
increasingly autonomous agents, and an operational overhaul of processes over a
very diverse computing systems landscape that bring together computing via
cloud, edge, and on-premises computing solutions. To enable us to explore these
intertwined decisions, it will be fundamentally important to understand how to
best position agentic AI, and to navigate the future state of computing
infrastructures.
\\ ( https://arxiv.org/abs/2509.16676 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16682 (*cross-listing*)
Date: Sat, 20 Sep 2025 13:16:07 GMT   (96kb)

Title: Design and Development of an Intelligent LLM-based LDAP Honeypot
Authors: Javier Jim\'enez-Rom\'an, Florina Almenares-Mendoza, Alfonso
  S\'anchez-Maci\'an
Categories: cs.CR cs.AI
\\
  Cybersecurity threats continue to increase, with a growing number of
previously unknown attacks each year targeting both large corporations and
smaller entities. This scenario demands the implementation of advanced security
measures, not only to mitigate damage but also to anticipate emerging attack
trends. In this context, deception tools have become a key strategy, enabling
the detection, deterrence, and deception of potential attackers while
facilitating the collection of information about their tactics and methods.
Among these tools, honeypots have proven their value, although they have
traditionally been limited by rigidity and configuration complexity, hindering
their adaptability to dynamic scenarios. The rise of artificial intelligence,
and particularly general-purpose Large Language Models (LLMs), is driving the
development of new deception solutions capable of offering greater adaptability
and ease of use. This work proposes the design and implementation of an
LLM-based honeypot to simulate an LDAP server, a critical protocol present in
most organizations due to its central role in identity and access management.
The proposed solution aims to provide a flexible and realistic tool capable of
convincingly interacting with attackers, thereby contributing to early
detection and threat analysis while enhancing the defensive capabilities of
infrastructures against intrusions targeting this service.
\\ ( https://arxiv.org/abs/2509.16682 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16724 (*cross-listing*)
Date: Sat, 20 Sep 2025 15:34:50 GMT   (183kb)

Title: Exploring AI Capabilities in Participatory Budgeting within Smart
  Cities: The Case of Sao Paulo
Authors: Italo Alberto Sousa, Mariana Carvalho da Silva, Jorge Machado, Jos\'e
  Carlos Vaz
Categories: cs.CY cs.AI
Comments: 22 pages, Presented at 28th IPSA World Congress of Political Science,
  Seoul 2025
\\
  This research examines how Artificial Intelligence (AI) can improve
participatory budgeting processes within smart cities. In response to
challenges like declining civic participation and resource allocation
conflicts, the study explores how online political participation can be
improved by AI. It investigates the state capacity governments need to
implement AI-enhanced participatory tools, considering technological
dependencies and vulnerabilities. It analyzes technological and administrative
structures, actors, interests, and strategies to understand the dynamics of
online political participation technologies in the case of Sao Paulo, Brazil.
The study contributes to understanding how technological advancements can
reshape participatory budgeting processes. In a broader sense, the research
highlights how AI can transform participatory institutions by offering new
tools for citizens and also for government officials in charge of participatory
processes within smart cities.
\\ ( https://arxiv.org/abs/2509.16724 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16743 (*cross-listing*)
Date: Sat, 20 Sep 2025 17:13:25 GMT   (2626kb)

Title: A Hybrid PCA-PR-Seq2Seq-Adam-LSTM Framework for Time-Series Power Outage
  Prediction
Authors: Subhabrata Das, Bodruzzaman Khan, Xiao-Yang Liu
Categories: cs.LG cs.AI
\\
  Accurately forecasting power outages is a complex task influenced by diverse
factors such as weather conditions [1], vegetation, wildlife, and load
fluctuations. These factors introduce substantial variability and noise into
outage data, making reliable prediction challenging. Long Short-Term Memory
(LSTM) networks, a type of Recurrent Neural Network (RNN), are particularly
effective for modeling nonlinear and dynamic time-series data, with proven
applications in stock price forecasting [2], energy demand prediction, demand
response [3], and traffic flow management [4]. This paper introduces a hybrid
deep learning framework, termed PCA-PR-Seq2Seq-Adam-LSTM, that integrates
Principal Component Analysis (PCA), Poisson Regression (PR), a
Sequence-to-Sequence (Seq2Seq) architecture, and an Adam-optimized LSTM. PCA is
employed to reduce dimensionality and stabilize data variance, while Poisson
Regression effectively models discrete outage events. The Seq2Seq-Adam-LSTM
component enhances temporal feature learning through efficient gradient
optimization and long-term dependency capture. The framework is evaluated using
real-world outage records from Michigan, and results indicate that the proposed
approach significantly improves forecasting accuracy and robustness compared to
existing methods.
\\ ( https://arxiv.org/abs/2509.16743 ,  2626kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16769 (*cross-listing*)
Date: Sat, 20 Sep 2025 18:32:05 GMT   (934kb)

Title: Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture
  of Hyperplanes
Authors: Prasanth K K and Shubham Sharma
Categories: cs.LG cs.AI cs.CL
Comments: 21 pages, 6 figures, 14 tables
MSC-class: 68T05, 62H30, 62M45
ACM-class: I.2.6; I.5.1; I.5.2; G.3
\\
  Many real world categories are multimodal, with single classes occupying
disjoint regions in feature space. Classical linear models (logistic
regression, linear SVM) use a single global hyperplane and perform poorly on
such data, while high-capacity methods (kernel SVMs, deep nets) fit multimodal
structure but at the expense of interpretability, heavier tuning, and higher
computational cost. We propose the Geometric Mixture Classifier (GMC), a
discriminative model that represents each class as a mixture of hyperplanes.
Within each class, GMC combines plane scores via a temperature-controlled
soft-OR (log-sum-exp), smoothly approximating the max; across classes, standard
softmax yields probabilistic posteriors. GMC optionally uses Random Fourier
Features (RFF) for nonlinear mappings while keeping inference linear in the
number of planes and features. Our practical training recipe: geometry-aware
k-means initialization, silhouette-based plane budgeting, alpha annealing,
usage-aware L2 regularization, label smoothing, and early stopping, makes GMC
plug-and-play. Across synthetic multimodal datasets (moons, circles, blobs,
spirals) and tabular/image benchmarks (iris, wine, WDBC, digits), GMC
consistently outperforms linear baselines and k-NN, is competitive with
RBF-SVM, Random Forests, and small MLPs, and provides geometric introspection
via per-plane and class responsibility visualizations. Inference scales
linearly in planes and features, making GMC CPU-friendly, with single-digit
microsecond latency per example, often faster than RBF-SVM and compact MLPs.
Post-hoc temperature scaling reduces ECE from about 0.06 to 0.02. GMC thus
strikes a favorable balance of accuracy, interpretability, and efficiency: it
is more expressive than linear models and lighter, more transparent, and faster
than kernel or deep models.
\\ ( https://arxiv.org/abs/2509.16769 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16780 (*cross-listing*)
Date: Sat, 20 Sep 2025 19:06:49 GMT   (1156kb)

Title: Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering
  on Math Textbook
Authors: Eason Chen, Chuangji Li, Shizhuo Li, Conrad Borchers, Zimo Xiao, Chloe
  Qianhui Zhao, Jionghao Lin and Kenneth R. Koedinger
Categories: cs.IR cs.AI cs.HC
\\
  Technology-enhanced learning environments often help students retrieve
relevant learning content for questions arising during self-paced study. Large
language models (LLMs) have emerged as novel aids for information retrieval
during learning. While LLMs are effective for general-purpose
question-answering, they typically lack alignment with the domain knowledge of
specific course materials such as textbooks and slides. We investigate
Retrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced
RAG approach, for page-level question answering in an undergraduate mathematics
textbook. While RAG has been effective for retrieving discrete, contextually
relevant passages, GraphRAG may excel in modeling interconnected concepts and
hierarchical knowledge structures. We curate a dataset of 477 question-answer
pairs, each tied to a distinct textbook page. We then compare the standard
embedding-based RAG methods to GraphRAG for evaluating both retrieval
accuracy-whether the correct page is retrieved-and generated answer quality via
F1 scores. Our findings show that embedding-based RAG achieves higher retrieval
accuracy and better F1 scores compared to GraphRAG, which tends to retrieve
excessive and sometimes irrelevant content due to its entity-based structure.
We also explored re-ranking the retrieved pages with LLM and observed mixed
results, including performance drop and hallucinations when dealing with larger
context windows. Overall, this study highlights both the promises and
challenges of page-level retrieval systems in educational contexts, emphasizing
the need for more refined retrieval methods to build reliable AI tutoring
solutions in providing reference page numbers.
\\ ( https://arxiv.org/abs/2509.16780 ,  1156kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16812 (*cross-listing*)
Date: Sat, 20 Sep 2025 21:30:55 GMT   (2667kb)

Title: SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree
Authors: Priyanshu Agrawal and Shalabh Gupta and Zongyuan Shen
Categories: cs.RO cs.AI cs.SY eess.SY
\\
  This paper presents SMART-3D, an extension of the SMART algorithm to 3D
environments. SMART-3D is a tree-based adaptive replanning algorithm for
dynamic environments with fast moving obstacles. SMART-3D morphs the underlying
tree to find a new path in real-time whenever the current path is blocked by
obstacles. SMART-3D removed the grid decomposition requirement of the SMART
algorithm by replacing the concept of hot-spots with that of hot-nodes, thus
making it computationally efficient and scalable to 3D environments. The
hot-nodes are nodes which allow for efficient reconnections to morph the
existing tree to find a new safe and reliable path. The performance of SMART-3D
is evaluated by extensive simulations in 2D and 3D environments populated with
randomly moving dynamic obstacles. The results show that SMART-3D achieves high
success rates and low replanning times, thus highlighting its suitability for
real-time onboard applications.
\\ ( https://arxiv.org/abs/2509.16812 ,  2667kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16825 (*cross-listing*)
Date: Sat, 20 Sep 2025 22:32:58 GMT   (1972kb)

Title: KANO: Kolmogorov-Arnold Neural Operator
Authors: Jin Lee, Ziming Liu, Xinling Yu, Yixuan Wang, Haewon Jeong, Murphy
  Yuezhen Niu, Zheng Zhang
Categories: cs.LG cs.AI cs.CE
\\
  We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural
operator jointly parameterized by both spectral and spatial bases with
intrinsic symbolic interpretability. We theoretically demonstrate that KANO
overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO
remains expressive over generic position-dependent dynamics for any physical
input, whereas FNO stays practical only for spectrally sparse operators and
strictly imposes a fast-decaying input Fourier tail. We verify our claims
empirically on position-dependent differential operators, for which KANO
robustly generalizes but FNO fails to. In the quantum Hamiltonian learning
benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic
representations accurate to the fourth decimal place in coefficients and
attains $\approx 6\times10^{-6}$ state infidelity from projective measurement
data, substantially outperforming that of the FNO trained with ideal full wave
function data, $\approx 1.5\times10^{-2}$, by orders of magnitude.
\\ ( https://arxiv.org/abs/2509.16825 ,  1972kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16834 (*cross-listing*)
Date: Sat, 20 Sep 2025 23:18:41 GMT   (51819kb)

Title: Robot Learning with Sparsity and Scarcity
Authors: Jingxi Xu
Categories: cs.RO cs.AI cs.LG
\\
  Unlike in language or vision, one of the fundamental challenges in robot
learning is the lack of access to vast data resources. We can further break
down the problem into (1) data sparsity from the angle of data representation
and (2) data scarcity from the angle of data quantity. In this thesis, I will
discuss selected works on two domains: (1) tactile sensing and (2)
rehabilitation robots, which are exemplars of data sparsity and scarcity,
respectively. Tactile sensing is an essential modality for robotics, but
tactile data are often sparse, and for each interaction with the physical
world, tactile sensors can only obtain information about the local area of
contact. I will discuss my work on learning vision-free tactile-only
exploration and manipulation policies through model-free reinforcement learning
to make efficient use of sparse tactile information. On the other hand,
rehabilitation robots are an example of data scarcity to the extreme due to the
significant challenge of collecting biosignals from disabled-bodied subjects at
scale for training. I will discuss my work in collaboration with the medical
school and clinicians on intent inferral for stroke survivors, where a hand
orthosis developed in our lab collects a set of biosignals from the patient and
uses them to infer the activity that the patient intends to perform, so the
orthosis can provide the right type of physical assistance at the right moment.
My work develops machine learning algorithms that enable intent inferral with
minimal data, including semi-supervised, meta-learning, and generative AI
methods.
\\ ( https://arxiv.org/abs/2509.16834 ,  51819kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16861 (*cross-listing*)
Date: Sun, 21 Sep 2025 01:22:42 GMT   (6052kb)

Title: AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software
Authors: Rui Yang, Michael Fu, Chakkrit Tantithamthavorn, Chetan Arora, Gunel
  Gulmammadova, Joey Chua
Categories: cs.CR cs.AI cs.SE
Comments: Accepted to the ASE 2025 International Conference on Automated
  Software Engineering, Industry Showcase Track
\\
  Guardrails are critical for the safe deployment of Large Language Models
(LLMs)-powered software. Unlike traditional rule-based systems with limited,
predefined input-output spaces that inherently constrain unsafe behavior, LLMs
enable open-ended, intelligent interactions--opening the door to jailbreak
attacks through user inputs. Guardrails serve as a protective layer, filtering
unsafe prompts before they reach the LLM. However, prior research shows that
jailbreak attacks can still succeed over 70% of the time, even against advanced
models like GPT-4o. While guardrails such as LlamaGuard report up to 95%
accuracy, our preliminary analysis shows their performance can drop sharply--to
as low as 12%--when confronted with unseen attacks. This highlights a growing
software engineering challenge: how to build a post-deployment guardrail that
adapts dynamically to emerging threats? To address this, we propose
AdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as
out-of-distribution (OOD) inputs and learns to defend against them through a
continual learning framework. Through empirical evaluation, AdaptiveGuard
achieves 96% OOD detection accuracy, adapts to new attacks in just two update
steps, and retains over 85% F1-score on in-distribution data post-adaptation,
outperforming other baselines. These results demonstrate that AdaptiveGuard is
a guardrail capable of evolving in response to emerging jailbreak strategies
post deployment. We release our AdaptiveGuard and studied datasets at
https://github.com/awsm-research/AdaptiveGuard to support further research.
\\ ( https://arxiv.org/abs/2509.16861 ,  6052kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16869 (*cross-listing*)
Date: Sun, 21 Sep 2025 01:41:40 GMT   (3039kb)

Title: PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR
  Reconstruction
Authors: Hrishav Bakul Barua, Kalin Stefanov, Ganesh Krishnasamy, KokSheik
  Wong, Abhinav Dhall
Categories: cs.GR cs.AI cs.CV cs.LG cs.MM eess.IV
Comments: Submitted to IEEE
MSC-class: Artificial intelligence, Computer vision, Machine learning, Deep
  learning
ACM-class: I.3.3; I.4.5
\\
  Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a
fundamental task in many computational vision problems. Numerous data-driven
methods have been proposed to address this problem; however, they lack explicit
modeling of illumination, lighting, and scene geometry in images. This limits
the quality of the reconstructed HDR images. Since lighting and shadows
interact differently with different materials, (e.g., specular surfaces such as
glass and metal, and lambertian or diffuse surfaces such as wood and stone),
modeling material-specific properties (e.g., specular and diffuse reflectance)
has the potential to improve the quality of HDR image reconstruction. This
paper presents PhysHDR, a simple yet powerful latent diffusion-based generative
model for HDR image reconstruction. The denoising process is conditioned on
lighting and depth information and guided by a novel loss to incorporate
material properties of surfaces in the scene. The experimental results
establish the efficacy of PhysHDR in comparison to a number of recent
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2509.16869 ,  3039kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16882 (*cross-listing*)
Date: Sun, 21 Sep 2025 02:30:04 GMT   (201kb)

Title: Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free
  Multi-Domain MoE Adaptation
Authors: Junzhuo Li, Bo Wang, Xiuze Zhou, Xuming Hu
Categories: cs.LG cs.AI cs.CL
Comments: EMNLP 2025 Main Conference
\\
  Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated
expert subnetworks, yet adapting them to multiple domains without catastrophic
forgetting remains an open challenge. Existing approaches either incur
prohibitive computation, suffer cross-domain interference, or require separate
runs per domain. We propose DES-MoE, a dynamic expert specialization framework
for multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses
catastrophic forgetting through three innovations: (1) an adaptive router
balancing pre-trained knowledge retention and task-specific updates via
distillation, (2) real-time expert-domain correlation mapping to isolate
domain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule
that progressively freezes non-specialized parameters. Evaluated on six domains
(math, code, law, etc.), DES-MoE matches single-domain ESFT performance while
training one unified model, reduces forgetting by 89% compared to full
fine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence
than conventional methods. Our work establishes dynamic expert isolation as a
scalable paradigm for multi-task MoE adaptation.
\\ ( https://arxiv.org/abs/2509.16882 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16902 (*cross-listing*)
Date: Sun, 21 Sep 2025 03:25:46 GMT   (989kb)

Title: FedEL: Federated Elastic Learning for Heterogeneous Devices
Authors: Letian Zhang, Bo Chen, Jieming Bian, Lei Wang, Jie Xu
Categories: cs.LG cs.AI
\\
  Federated learning (FL) enables distributed devices to collaboratively train
machine learning models while maintaining data privacy. However, the
heterogeneous hardware capabilities of devices often result in significant
training delays, as straggler clients with limited resources prolong the
aggregation process. Existing solutions such as client selection, asynchronous
FL, and partial training partially address these challenges but encounter
issues such as reduced accuracy, stale updates, and compromised model
performance due to inconsistent training contributions. To overcome these
limitations, we propose FedEL, a federated elastic learning framework that
enhances training efficiency while maintaining model accuracy. FedEL introduces
a novel window-based training process, sliding the window to locate the
training part of the model and dynamically selecting important tensors for
training within a coordinated runtime budget. This approach ensures progressive
and balanced training across all clients, including stragglers. Additionally,
FedEL employs a tensor importance adjustment module, harmonizing local and
global tensor importance to mitigate biases caused by data heterogeneity. The
experiment results show that FedEL achieves up to 3.87x improvement in
time-to-accuracy compared to baselines while maintaining or exceeding final
test accuracy.
\\ ( https://arxiv.org/abs/2509.16902 ,  989kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16922 (*cross-listing*)
Date: Sun, 21 Sep 2025 05:01:54 GMT   (10622kb)

Title: PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D
  Gaussian Splatting with Pixel-Aware Density Control
Authors: Tianheng Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng
Categories: cs.SD cs.AI eess.IV
Comments: Main paper (15 pages). Accepted for publication by ICONIP(
  International Conference on Neural Information Processing) 2025
\\
  Audio-driven talking head generation is crucial for applications in virtual
reality, digital avatars, and film production. While NeRF-based methods enable
high-fidelity reconstruction, they suffer from low rendering efficiency and
suboptimal audio-visual synchronization. This work presents PGSTalker, a
real-time audio-driven talking head synthesis framework based on 3D Gaussian
Splatting (3DGS). To improve rendering performance, we propose a pixel-aware
density control strategy that adaptively allocates point density, enhancing
detail in dynamic facial regions while reducing redundancy elsewhere.
Additionally, we introduce a lightweight Multimodal Gated Fusion Module to
effectively fuse audio and spatial features, thereby improving the accuracy of
Gaussian deformation prediction. Extensive experiments on public datasets
demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches
in rendering quality, lip-sync precision, and inference speed. Our method
exhibits strong generalization capabilities and practical potential for
real-world deployment.
\\ ( https://arxiv.org/abs/2509.16922 ,  10622kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16926 (*cross-listing*)
Date: Sun, 21 Sep 2025 05:14:06 GMT   (166kb)

Title: Cross-Attention with Confidence Weighting for Multi-Channel Audio
  Alignment
Authors: Ragib Amin Nihal, Benjamin Yen, Takeshi Ashizawa, Kazuhiro Nakadai
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: Accepted on Workshop on Detection and Classification of Acoustic
  Scenes and Events (DCASE 2025)
\\
  Multi-channel audio alignment is a key requirement in bioacoustic monitoring,
spatial audio systems, and acoustic localization. However, existing methods
often struggle to address nonlinear clock drift and lack mechanisms for
quantifying uncertainty. Traditional methods like Cross-correlation and Dynamic
Time Warping assume simple drift patterns and provide no reliability measures.
Meanwhile, recent deep learning models typically treat alignment as a binary
classification task, overlooking inter-channel dependencies and uncertainty
estimation. We introduce a method that combines cross-attention mechanisms with
confidence-weighted scoring to improve multi-channel audio synchronization. We
extend BEATs encoders with cross-attention layers to model temporal
relationships between channels. We also develop a confidence-weighted scoring
function that uses the full prediction distribution instead of binary
thresholding. Our method achieved first place in the BioDCASE 2025 Task 1
challenge with 0.30 MSE average across test datasets, compared to 0.58 for the
deep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU
data (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The
framework supports probabilistic temporal alignment, moving beyond point
estimates. While validated in a bioacoustic context, the approach is applicable
to a broader range of multi-channel audio tasks where alignment confidence is
critical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA
\\ ( https://arxiv.org/abs/2509.16926 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16931 (*cross-listing*)
Date: Sun, 21 Sep 2025 05:33:28 GMT   (240kb)

Title: Equip Pre-ranking with Target Attention by Residual Quantization
Authors: Yutong Li, Yu Zhu, Yichen Qiao, Ziyu Guan, Lv Shao, Tong Liu and Bo
  Zheng
Categories: cs.IR cs.AI cs.LG
Comments: 5 pages, 2 figures, submitted to WSDM 2026 Short Paper Track
ACM-class: I.2.0; I.5.0; I.7.0
\\
  The pre-ranking stage in industrial recommendation systems faces a
fundamental conflict between efficiency and effectiveness. While powerful
models like Target Attention (TA) excel at capturing complex feature
interactions in the ranking stage, their high computational cost makes them
infeasible for pre-ranking, which often relies on simplistic vector-product
models. This disparity creates a significant performance bottleneck for the
entire system. To bridge this gap, we propose TARQ, a novel pre-ranking
framework. Inspired by generative models, TARQ's key innovation is to equip
pre-ranking with an architecture approximate to TA by Residual Quantization.
This allows us to bring the modeling power of TA into the latency-critical
pre-ranking stage for the first time, establishing a new state-of-the-art
trade-off between accuracy and efficiency. Extensive offline experiments and
large-scale online A/B tests at Taobao demonstrate TARQ's significant
improvements in ranking performance. Consequently, our model has been fully
deployed in production, serving tens of millions of daily active users and
yielding substantial business improvements.
\\ ( https://arxiv.org/abs/2509.16931 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16959 (*cross-listing*)
Date: Sun, 21 Sep 2025 07:45:53 GMT   (375kb)

Title: Gradient Interference-Aware Graph Coloring for Multitask Learning
Authors: Santosh Patapati and Trisanth Srinivasan
Categories: cs.LG cs.AI cs.NE stat.ML
\\
  When different objectives conflict with each other in multi-task learning,
gradients begin to interfere and slow convergence, thereby reducing the final
model's performance. To address this, we introduce a scheduler that computes
gradient interference, constructs an interference graph, and then applies
greedy graph-coloring to partition tasks into groups that align well with each
other. At each training step, only one group (color class) of tasks are
activated. The grouping partition is constantly recomputed as task
relationships evolve throughout training. By ensuring that each mini-batch
contains only tasks that pull the model in the same direction, our method
improves the effectiveness of any underlying multi-task learning optimizer
without additional tuning. Since tasks within these groups will update in
compatible directions, model performance will be improved rather than impeded.
Empirical results on six different datasets show that this interference-aware
graph-coloring approach consistently outperforms baselines and state-of-the-art
multi-task optimizers.
\\ ( https://arxiv.org/abs/2509.16959 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16979 (*cross-listing*)
Date: Sun, 21 Sep 2025 08:29:24 GMT   (167kb)

Title: Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility
  Prediction for Hearing-Impaired Listeners
Authors: Boxuan Cao, Linkai Li, Hanlin Yu, Changgeng Mo, Haoshuai Zhou, Shan
  Xiang Wang
Categories: cs.SD cs.AI eess.AS
\\
  Speech intelligibility evaluation for hearing-impaired (HI) listeners is
essential for assessing hearing aid performance, traditionally relying on
listening tests or intrusive methods like HASPI. However, these methods require
clean reference signals, which are often unavailable in real-world conditions,
creating a gap between lab-based and real-world assessments. To address this,
we propose a non-intrusive intelligibility prediction framework that leverages
speech enhancers to provide a parallel enhanced-signal pathway, enabling robust
predictions without reference signals. We evaluate three state-of-the-art
enhancers and demonstrate that prediction performance depends on the choice of
enhancer, with ensembles of strong enhancers yielding the best results. To
improve cross-dataset generalization, we introduce a 2-clips augmentation
strategy that enhances listener-specific variability, boosting robustness on
unseen datasets. Our approach consistently outperforms the non-intrusive
baseline, CPC2 Champion across multiple datasets, highlighting the potential of
enhancer-guided non-intrusive intelligibility prediction for real-world
applications.
\\ ( https://arxiv.org/abs/2509.16979 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16989 (*cross-listing*)
Date: Sun, 21 Sep 2025 09:07:20 GMT   (3049kb)

Title: PTQTP: Post-Training Quantization to Trit-Planes for Large Language
  Models
Authors: He Xiao, Runming Yang, Qingyao Yang, Wendong Xu, Zheng Li, Yupeng Su,
  Zhengwu Liu, Hongxia Yang, Ngai Wong
Categories: cs.LG cs.AI
Comments: under review
\\
  Post-training quantization (PTQ) of large language models (LLMs) to extremely
low bit-widths remains challenging due to the fundamental trade-off between
computational efficiency and model expressiveness. While existing ultra-low-bit
PTQ methods rely on binary approximations or complex compensation mechanisms,
they suffer from either limited representational capacity or computational
overhead that undermines their efficiency gains. We introduce PTQ to
Trit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes
weight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit
representation. PTQTP achieves multiplication-free inference, identical to
1-bit quantization, while maintaining superior expressiveness through its novel
structured decomposition. Our approach provides: (1) a theoretically grounded
progressive approximation algorithm ensuring global weight consistency; (2)
model-agnostic deployment across diverse modern LLMs without architectural
modifications; and (3) uniform ternary operations that eliminate the need for
mixed-precision or compensation schemes. Comprehensive experiments across
LLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP
significantly outperforms existing low-bit PTQ methods, achieving 82.4%
mathematical reasoning retention versus 0% for competing approaches. PTQTP
approaches and sometimes surpasses 1.58-bit quantization-aware training
performance while requiring only single-hour quantization compared to 10-14 GPU
days for training-based methods. These results establish PTQTP as a practical
solution for efficient LLM deployment in resource-constrained environments.
\\ ( https://arxiv.org/abs/2509.16989 ,  3049kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17000 (*cross-listing*)
Date: Sun, 21 Sep 2025 09:40:27 GMT   (379kb)

Title: Adaptive Overclocking: Dynamic Control of Thinking Path Length via
  Real-Time Reasoning Signals
Authors: Shuhao Jiang, Songbo Wang, Yang Qiao, Chun Xu, Chaoyang Zheng, Shengyi
  Zhou, Huanjun Wang, Fangming Li, Cong Zhang, Jiyu Wang
Categories: cs.LG cs.AI
\\
  Large Reasoning Models (LRMs) often suffer from computational inefficiency
due to overthinking, where a fixed reasoning budget fails to match the varying
complexity of tasks. To address this issue, we propose Adaptive Overclocking, a
method that makes the overclocking hyperparameter $\alpha$ dynamic and
context-aware. Our method adjusts reasoning speed in real time through two
complementary signals: (1) token-level model uncertainty for fine-grained
step-wise control, and (2) input complexity estimation for informed
initialization. We implement this approach with three strategies:
Uncertainty-Aware Alpha Scheduling (UA-$\alpha$S), Complexity-Guided Alpha
Initialization (CG-$\alpha$I), and a Hybrid Adaptive Control (HAC) that
combines both. Experiments on GSM8K, MATH, and SVAMP show that HAC achieves
superior accuracy-latency trade-offs, reducing unnecessary computation on
simple problems while allocating more resources to challenging ones. By
mitigating overthinking, Adaptive Overclocking enhances both efficiency and
overall reasoning performance.
\\ ( https://arxiv.org/abs/2509.17000 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17046 (*cross-listing*)
Date: Sun, 21 Sep 2025 11:59:19 GMT   (1253kb)

Title: A Chain-of-thought Reasoning Breast Ultrasound Dataset Covering All
  Histopathology Categories
Authors: Haojun Yu, Youcheng Li, Zihan Niu, Nan Zhang, Xuantong Gong, Huan Li,
  Zhiying Zou, Haifeng Qi, Zhenxiao Cao, Zijie Lan, Xingjian Yuan, Jiating He,
  Haokai Zhang, Shengtao Zhang, Zicheng Wang, Dong Wang, Ziwei Zhao, Congying
  Chen, Yong Wang, Wangyan Qin, and Qingli Zhu
Categories: eess.IV cs.AI cs.CV
\\
  Breast ultrasound (BUS) is an essential tool for diagnosing breast lesions,
with millions of examinations per year. However, publicly available
high-quality BUS benchmarks for AI development are limited in data scale and
annotation richness. In this work, we present BUS-CoT, a BUS dataset for
chain-of-thought (CoT) reasoning analysis, which contains 11,439 images of
10,019 lesions from 4,838 patients and covers all 99 histopathology types. To
facilitate research on incentivizing CoT reasoning, we construct the reasoning
processes based on observation, feature, diagnosis and pathology labels,
annotated and verified by experienced experts. Moreover, by covering lesions of
all histopathology types, we aim to facilitate robust AI systems in rare cases,
which can be error-prone in clinical practice.
\\ ( https://arxiv.org/abs/2509.17046 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17094 (*cross-listing*)
Date: Sun, 21 Sep 2025 14:19:47 GMT   (33193kb)

Title: $\texttt{DiffSyn}$: A Generative Diffusion Approach to Materials
  Synthesis Planning
Authors: Elton Pan, Soonhyoung Kwon, Sulin Liu, Mingrou Xie, Alexander J.
  Hoffman, Yifei Duan, Thorben Prein, Killian Sheriff, Yuriy Roman-Leshkov,
  Manuel Moliner, Rafael Gomez-Bombarelli, Elsa Olivetti
Categories: cond-mat.mtrl-sci cs.AI cs.LG
\\
  The synthesis of crystalline materials, such as zeolites, remains a
significant challenge due to a high-dimensional synthesis space, intricate
structure-synthesis relationships and time-consuming experiments. Considering
the one-to-many relationship between structure and synthesis, we propose
$\texttt{DiffSyn}$, a generative diffusion model trained on over 23,000
synthesis recipes spanning 50 years of literature. $\texttt{DiffSyn}$ generates
probable synthesis routes conditioned on a desired zeolite structure and an
organic template. $\texttt{DiffSyn}$ achieves state-of-the-art performance by
capturing the multi-modal nature of structure-synthesis relationships. We apply
$\texttt{DiffSyn}$ to differentiate among competing phases and generate optimal
synthesis routes. As a proof of concept, we synthesize a UFI material using
$\texttt{DiffSyn}$-generated synthesis routes. These routes, rationalized by
density functional theory binding energies, resulted in the successful
synthesis of a UFI material with a high Si/Al$_{\text{ICP}}$ of 19.0, which is
expected to improve thermal stability and is higher than that of any previously
recorded.
\\ ( https://arxiv.org/abs/2509.17094 ,  33193kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17095 (*cross-listing*)
Date: Sun, 21 Sep 2025 14:22:35 GMT   (13215kb)

Title: Ultra-short-term solar power forecasting by deep learning and data
  reconstruction
Authors: Jinbao Wang, Jun Liu, Shiliang Zhang, Xuehui Ma
Categories: cs.LG cs.AI
\\
  The integration of solar power has been increasing as the green energy
transition rolls out. The penetration of solar power challenges the grid
stability and energy scheduling, due to its intermittent energy generation.
Accurate and near real-time solar power prediction is of critical importance to
tolerant and support the permeation of distributed and volatile solar power
production in the energy system. In this paper, we propose a deep-learning
based ultra-short-term solar power prediction with data reconstruction. We
decompose the data for the prediction to facilitate extensive exploration of
the spatial and temporal dependencies within the data. Particularly, we
reconstruct the data into low- and high-frequency components, using ensemble
empirical model decomposition with adaptive noise (CEEMDAN). We integrate
meteorological data with those two components, and employ deep-learning models
to capture long- and short-term dependencies towards the target prediction
period. In this way, we excessively exploit the features in historical data in
predicting a ultra-short-term solar power production. Furthermore, as
ultra-short-term prediction is vulnerable to local optima, we modify the
optimization in our deep-learning training by penalizing long prediction
intervals. Numerical experiments with diverse settings demonstrate that,
compared to baseline models, the proposed method achieves improved
generalization in data reconstruction and higher prediction accuracy for
ultra-short-term solar power production.
\\ ( https://arxiv.org/abs/2509.17095 ,  13215kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17096 (*cross-listing*)
Date: Sun, 21 Sep 2025 14:24:37 GMT   (1759kb)

Title: Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven
  Software Engineering
Authors: Ziyou Li, Agnia Sergeyuk, Maliheh Izadi
Categories: cs.SE cs.AI cs.HC
Comments: Accepted in the 40th IEEE/ACM International Conference on Automated
  Software Engineering, ASE 2025 (Industry track)
\\
  Large Language Models are transforming software engineering, yet prompt
management in practice remains ad hoc, hindering reliability, reuse, and
integration into industrial workflows. We present Prompt-with-Me, a practical
solution for structured prompt management embedded directly in the development
environment. The system automatically classifies prompts using a
four-dimensional taxonomy encompassing intent, author role, software
development lifecycle stage, and prompt type. To enhance prompt reuse and
quality, Prompt-with-Me suggests language refinements, masks sensitive
information, and extracts reusable templates from a developer's prompt library.
Our taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can
accurately classify software engineering prompts. Furthermore, our user study
with 11 participants shows strong developer acceptance, with high usability
(Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in
prompt quality and efficiency through reduced repetitive effort. Lastly, we
offer actionable insights for building the next generation of prompt management
and maintenance tools for software engineering workflows.
\\ ( https://arxiv.org/abs/2509.17096 ,  1759kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17119 (*cross-listing*)
Date: Sun, 21 Sep 2025 15:18:51 GMT   (3992kb)

Title: ScenGAN: Attention-Intensive Generative Model for Uncertainty-Aware
  Renewable Scenario Forecasting
Authors: Yifei Wu, Bo Wang, Jingshi Cui, Pei-chun Lin, Junzo Watada
Categories: cs.LG cs.AI
\\
  To address the intermittency of renewable energy source (RES) generation,
scenario forecasting offers a series of stochastic realizations for predictive
objects with superior flexibility and direct views. Based on a long time-series
perspective, this paper explores uncertainties in the realms of renewable power
and deep learning. Then, an uncertainty-aware model is meticulously designed
for renewable scenario forecasting, which leverages an attention mechanism and
generative adversarial networks (GANs) to precisely capture complex
spatial-temporal dynamics. To improve the interpretability of uncertain
behavior in RES generation, Bayesian deep learning and adaptive instance
normalization (AdaIN) are incorporated to simulate typical patterns and
variations. Additionally, the integration of meteorological information,
forecasts, and historical trajectories in the processing layer improves the
synergistic forecasting capability for multiscale periodic regularities.
Numerical experiments and case analyses demonstrate that the proposed approach
provides an appropriate interpretation for renewable uncertainty
representation, including both aleatoric and epistemic uncertainties, and shows
superior performance over state-of-the-art methods.
\\ ( https://arxiv.org/abs/2509.17119 ,  3992kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17143 (*cross-listing*)
Date: Sun, 21 Sep 2025 16:14:51 GMT   (133kb)

Title: MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion
  With Increased Controllability via Multiple Guidances
Authors: Junhyeok Lee, Helin Wang, Yaohan Guan, Thomas Thebaud, Laureano
  Moro-Velazquez, Jes\'us Villalba, Najim Dehak
Categories: eess.AS cs.AI
\\
  We introduce MaskVCT, a zero-shot voice conversion (VC) model that offers
multi-factor controllability through multiple classifier-free guidances (CFGs).
While previous VC models rely on a fixed conditioning scheme, MaskVCT
integrates diverse conditions in a single model. To further enhance robustness
and control, the model can leverage continuous or quantized linguistic features
to enhance intellgibility and speaker similarity, and can use or omit pitch
contour to control prosody. These choices allow users to seamlessly balance
speaker identity, linguistic content, and prosodic factors in a zero-shot VC
setting. Extensive experiments demonstrate that MaskVCT achieves the best
target speaker and accent similarities while obtaining competitive word and
character error rates compared to existing baselines. Audio samples are
available at https://maskvct.github.io/.
\\ ( https://arxiv.org/abs/2509.17143 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17153 (*cross-listing*)
Date: Sun, 21 Sep 2025 16:49:09 GMT   (897kb)

Title: Flow-Induced Diagonal Gaussian Processes
Authors: Moule Lin, Andrea Patane, Weipeng Jing, Shuhao Guan, Goetz Botterweck
Categories: cs.LG cs.AI
Comments: 15 pages
\\
  We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression
framework that incorporates a compact inducing weight matrix to project a
neural network's weight uncertainty into a lower-dimensional subspace.
Critically, FiD-GP relies on normalising-flow priors and spectral
regularisations to augment its expressiveness and align the inducing subspace
with feature-gradient geometry through a numerically stable projection
mechanism objective. Furthermore, we demonstrate how the prediction framework
in FiD-GP can help to design a single-pass projection for Out-of-Distribution
(OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation
ability on various tasks compared with SVGP-based baselines, satisfies tight
spectral residual bounds with theoretically guaranteed OoD detection, and
significantly compresses the neural network's storage requirements at the cost
of increased inference computation dependent on the number of inducing weights
employed. Specifically, in a comprehensive empirical study spanning regression,
image classification, semantic segmentation, and out-of-distribution detection
benchmarks, it cuts Bayesian training cost by several orders of magnitude,
compresses parameters by roughly 51%, reduces model size by about 75%, and
matches state-of-the-art accuracy and uncertainty estimation.
\\ ( https://arxiv.org/abs/2509.17153 ,  897kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17165 (*cross-listing*)
Date: Sun, 21 Sep 2025 17:16:32 GMT   (873kb)

Title: Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM
  Embedding Denoising Auto Encoder Transformer
Authors: Sahar Koohfar, Wubeshet Woldemariam
Categories: cs.LG cs.AI
\\
  Time series data is a prevalent form of data found in various fields. It
consists of a series of measurements taken over time. Forecasting is a crucial
application of time series models, where future values are predicted based on
historical data. Accurate forecasting is essential for making well-informed
decisions across industries. When it comes to electric vehicles (EVs), precise
predictions play a key role in planning infrastructure development, load
balancing, and energy management. This study introduces a BI-LSTM embedding
denoising autoencoder model (BDM) designed to address time series problems,
focusing on short-term EV charging load prediction. The performance of the
proposed model is evaluated by comparing it with benchmark models like
Transformer, CNN, RNN, LSTM, and GRU. Based on the results of the study, the
proposed model outperforms the benchmark models in four of the five-time steps,
demonstrating its effectiveness for time series forecasting. This research
makes a significant contribution to enhancing time series forecasting, thereby
improving decision-making processes.
\\ ( https://arxiv.org/abs/2509.17165 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17186 (*cross-listing*)
Date: Sun, 21 Sep 2025 18:15:45 GMT   (2100kb)

Title: Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long
  Sequence Modeling
Authors: Dehao Zhang, Malu Zhang, Shuai Wang, Jingya Wang, Wenjie Wei, Zeyu Ma,
  Guoqing Wang, Yang Yang, HaiZhou Li
Categories: cs.LG cs.AI
\\
  The explosive growth in sequence length has intensified the demand for
effective and efficient long sequence modeling. Benefiting from intrinsic
oscillatory membrane dynamics, Resonate-and-Fire (RF) neurons can efficiently
extract frequency components from input signals and encode them into
spatiotemporal spike trains, making them well-suited for long sequence
modeling. However, RF neurons exhibit limited effective memory capacity and a
trade-off between energy efficiency and training speed on complex temporal
tasks. Inspired by the dendritic structure of biological neurons, we propose a
Dendritic Resonate-and-Fire (D-RF) model, which explicitly incorporates a
multi-dendritic and soma architecture. Each dendritic branch encodes specific
frequency bands by utilizing the intrinsic oscillatory dynamics of RF neurons,
thereby collectively achieving comprehensive frequency representation.
Furthermore, we introduce an adaptive threshold mechanism into the soma
structure that adjusts the threshold based on historical spiking activity,
reducing redundant spikes while maintaining training efficiency in long
sequence tasks. Extensive experiments demonstrate that our method maintains
competitive accuracy while substantially ensuring sparse spikes without
compromising computational efficiency during training. These results underscore
its potential as an effective and efficient solution for long sequence modeling
on edge platforms.
\\ ( https://arxiv.org/abs/2509.17186 ,  2100kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17197 (*cross-listing*)
Date: Sun, 21 Sep 2025 18:54:54 GMT   (3957kb)

Title: SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal
  Processing
Authors: Junlong Ke, Qiying Hu, Shenghai Yuan, Yuecong Xu, Jianfei Yang
Categories: cs.LG cs.AI eess.SP
Comments: 11 pages
\\
  Modern signal processing (SP) pipelines, whether model-based or data-driven,
often constrained by complex and fragmented workflow, rely heavily on expert
knowledge and manual engineering, and struggle with adaptability and
generalization under limited data. In contrast, Large Language Models (LLMs)
offer strong reasoning capabilities, broad general-purpose knowledge,
in-context learning, and cross-modal transfer abilities, positioning them as
powerful tools for automating and generalizing SP workflows. Motivated by these
potentials, we introduce SignalLLM, the first general-purpose LLM-based agent
framework for general SP tasks. Unlike prior LLM-based SP approaches that are
limited to narrow applications or tricky prompting, SignalLLM introduces a
principled, modular architecture. It decomposes high-level SP goals into
structured subtasks via in-context learning and domain-specific retrieval,
followed by hierarchical planning through adaptive retrieval-augmented
generation (RAG) and refinement; these subtasks are then executed through
prompt-based reasoning, cross-modal reasoning, code synthesis, model
invocation, or data-driven LLM-assisted modeling. Its generalizable design
enables the flexible selection of problem solving strategies across different
signal modalities, task types, and data conditions. We demonstrate the
versatility and effectiveness of SignalLLM through five representative tasks in
communication and sensing, such as radar target detection, human activity
recognition, and text compression. Experimental results show superior
performance over traditional and existing LLM-based methods, particularly in
few-shot and zero-shot settings.
\\ ( https://arxiv.org/abs/2509.17197 ,  3957kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17255 (*cross-listing*)
Date: Sun, 21 Sep 2025 22:11:03 GMT   (2596kb)

Title: Agentic AI for Multi-Stage Physics Experiments at a Large-Scale User
  Facility Particle Accelerator
Authors: Thorsten Hellert, Drew Bertwistle, Simon C. Leemann, Antonin Sulc,
  Marco Venturini
Categories: physics.acc-ph cs.AI
\\
  We present the first language-model-driven agentic artificial intelligence
(AI) system to autonomously execute multi-stage physics experiments on a
production synchrotron light source. Implemented at the Advanced Light Source
particle accelerator, the system translates natural language user prompts into
structured execution plans that combine archive data retrieval, control-system
channel resolution, automated script generation, controlled machine
interaction, and analysis. In a representative machine physics task, we show
that preparation time was reduced by two orders of magnitude relative to manual
scripting even for a system expert, while operator-standard safety constraints
were strictly upheld. Core architectural features, plan-first orchestration,
bounded tool access, and dynamic capability selection, enable transparent,
auditable execution with fully reproducible artifacts. These results establish
a blueprint for the safe integration of agentic AI into accelerator experiments
and demanding machine physics studies, as well as routine operations, with
direct portability across accelerators worldwide and, more broadly, to other
large-scale scientific infrastructures.
\\ ( https://arxiv.org/abs/2509.17255 ,  2596kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17280 (*cross-listing*)
Date: Sun, 21 Sep 2025 23:39:04 GMT   (459kb)

Title: From Prediction to Understanding: Will AI Foundation Models Transform
  Brain Science?
Authors: Thomas Serre and Ellie Pavlick
Categories: q-bio.NC cs.AI
\\
  Generative pretraining (the "GPT" in ChatGPT) enables language models to
learn from vast amounts of internet text without human supervision. This
approach has driven breakthroughs across AI by allowing deep neural networks to
learn from massive, unstructured datasets. We use the term foundation models to
refer to large pretrained systems that can be adapted to a wide range of tasks
within and across domains, and these models are increasingly applied beyond
language to the brain sciences. These models achieve strong predictive
accuracy, raising hopes that they might illuminate computational principles.
But predictive success alone does not guarantee scientific understanding. Here,
we outline how foundation models can be productively integrated into the brain
sciences, highlighting both their promise and their limitations. The central
challenge is to move from prediction to explanation: linking model computations
to mechanisms underlying neural activity and cognition.
\\ ( https://arxiv.org/abs/2509.17280 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17281 (*cross-listing*)
Date: Sun, 21 Sep 2025 23:39:32 GMT   (1022kb)

Title: Training the next generation of physicians for artificial
  intelligence-assisted clinical neuroradiology: ASNR MICCAI Brain Tumor
  Segmentation (BraTS) 2025 Lighthouse Challenge education platform
Authors: Raisa Amiruddin, Nikolay Y. Yordanov, Nazanin Maleki, Pascal
  Fehringer, Athanasios Gkampenis, Anastasia Janas, Kiril Krantchev, Ahmed
  Moawad, Fabian Umeh, Salma Abosabie, Sara Abosabie, Albara Alotaibi, Mohamed
  Ghonim, Mohanad Ghonim, Sedra Abou Ali Mhana, Nathan Page, Marko Jakovljevic,
  Yasaman Sharifi, Prisha Bhatia, Amirreza Manteghinejad, Melisa Guelen,
  Michael Veronesi, Virginia Hill, Tiffany So, Mark Krycia, Bojan Petrovic,
  Fatima Memon, Justin Cramer, Elizabeth Schrickel, Vilma Kosovic, Lorenna
  Vidal, Gerard Thompson, Ichiro Ikuta, Basimah Albalooshy, Ali Nabavizadeh,
  Nourel Hoda Tahon, Karuna Shekdar, Aashim Bhatia, Claudia Kirsch, Gennaro
  D'Anna, Philipp Lohmann, Amal Saleh Nour, Andriy Myronenko, Adam
  Goldman-Yassen, Janet R. Reid, Sanjay Aneja, Spyridon Bakas, Mariam Aboian
Categories: cs.LG cs.AI cs.CY
Comments: 23 pages, 9 figures, 1 table, 3 supplementary tables
\\
  High-quality reference standard image data creation by neuroradiology experts
for automated clinical tools can be a powerful tool for neuroradiology &
artificial intelligence education. We developed a multimodal educational
approach for students and trainees during the MICCAI Brain Tumor Segmentation
Lighthouse Challenge 2025, a landmark initiative to develop accurate brain
tumor segmentation algorithms. Fifty-six medical students & radiology trainees
volunteered to annotate brain tumor MR images for the BraTS challenges of 2023
& 2024, guided by faculty-led didactics on neuropathology MRI. Among the 56
annotators, 14 select volunteers were then paired with neuroradiology faculty
for guided one-on-one annotation sessions for BraTS 2025. Lectures on
neuroanatomy, pathology & AI, journal clubs & data scientist-led workshops were
organized online. Annotators & audience members completed surveys on their
perceived knowledge before & after annotations & lectures respectively.
Fourteen coordinators, each paired with a neuroradiologist, completed the data
annotation process, averaging 1322.9+/-760.7 hours per dataset per pair and
1200 segmentations in total. On a scale of 1-10, annotation coordinators
reported significant increase in familiarity with image segmentation software
pre- and post-annotation, moving from initial average of 6+/-2.9 to final
average of 8.9+/-1.1, and significant increase in familiarity with brain tumor
features pre- and post-annotation, moving from initial average of 6.2+/-2.4 to
final average of 8.1+/-1.2. We demonstrate an innovative offering for providing
neuroradiology & AI education through an image segmentation challenge to
enhance understanding of algorithm development, reinforce the concept of data
reference standard, and diversify opportunities for AI-driven image analysis
among future physicians.
\\ ( https://arxiv.org/abs/2509.17281 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17325 (*cross-listing*)
Date: Mon, 22 Sep 2025 03:03:56 GMT   (3562kb)

Title: Generalizable End-to-End Tool-Use RL with Synthetic CodeGym
Authors: Weihua Du, Hailei Gong, Zhan Ling, Kang Liu, Lingfeng Shen, Xuesong
  Yao, Yufei Xu, Dingyuan Shi, Yiming Yang, Jiecao Chen
Categories: cs.LG cs.AI cs.CL
Comments: 22 pages. Project available at https://github.com/StigLidu/CodeGym
\\
  Tool-augmented large language models (LLMs), hereafter LLM agents, leverage
external tools to solve diverse tasks and interface with the real world.
However, current training practices largely rely on supervised fine-tuning
(SFT) over static trajectories or reinforcement learning (RL) on narrow tasks,
and generalize poorly beyond development settings, leading to brittleness with
new tools and unseen workflows. Because code execution reflects many structures
of real-world workflows, coding problems provide a natural basis for building
agent training environments. Motivated by this, we introduce CodeGym, a
scalable framework that synthesizes diverse, verifiable, and controllable
multi-turn tool-use environments for agent RL, enabling LLM agents to explore
and master various workflows actively. CodeGym rewrites static coding problems
into interactive environments by extracting atomic functions or logic into
callable tools, yielding verifiable tasks that span various tool-execution
workflows. Models of varying sizes and chain-of-thought configurations, trained
in CodeGym, exhibit consistent out-of-distribution generalizability; for
example, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points
on the OOD benchmark $\tau$-Bench. These results highlight CodeGym as a step
toward scalable general-purpose RL environments that align with real-world
agent workflows.
\\ ( https://arxiv.org/abs/2509.17325 ,  3562kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17334 (*cross-listing*)
Date: Mon, 22 Sep 2025 03:11:30 GMT   (665kb)

Title: Explainability matters: The effect of liability rules on the healthcare
  sector
Authors: Jiawen Wei, Elena Verona, Andrea Bertolini, Gianmarco Mengaldo
Categories: cs.CY cs.AI cs.CE cs.LG
\\
  Explainability, the capability of an artificial intelligence system (AIS) to
explain its outcomes in a manner that is comprehensible to human beings at an
acceptable level, has been deemed essential for critical sectors, such as
healthcare. Is it really the case? In this perspective, we consider two extreme
cases, ``Oracle'' (without explainability) versus ``AI Colleague'' (with
explainability) for a thorough analysis. We discuss how the level of automation
and explainability of AIS can affect the determination of liability among the
medical practitioner/facility and manufacturer of AIS. We argue that
explainability plays a crucial role in setting a responsibility framework in
healthcare, from a legal standpoint, to shape the behavior of all involved
parties and mitigate the risk of potential defensive medicine practices.
\\ ( https://arxiv.org/abs/2509.17334 ,  665kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17361 (*cross-listing*)
Date: Mon, 22 Sep 2025 05:24:53 GMT   (353kb)

Title: SeqUDA-Rec: Sequential User Behavior Enhanced Recommendation via Global
  Unsupervised Data Augmentation for Personalized Content Marketing
Authors: Ruihan Luo, Xuanjing Chen, Ziyang Ding
Categories: cs.IR cs.AI
\\
  Personalized content marketing has become a crucial strategy for digital
platforms, aiming to deliver tailored advertisements and recommendations that
match user preferences. Traditional recommendation systems often suffer from
two limitations: (1) reliance on limited supervised signals derived from
explicit user feedback, and (2) vulnerability to noisy or unintentional
interactions. To address these challenges, we propose SeqUDA-Rec, a novel deep
learning framework that integrates user behavior sequences with global
unsupervised data augmentation to enhance recommendation accuracy and
robustness. Our approach first constructs a Global User-Item Interaction Graph
(GUIG) from all user behavior sequences, capturing both local and global item
associations. Then, a graph contrastive learning module is applied to generate
robust embeddings, while a sequential Transformer-based encoder models users'
evolving preferences. To further enhance diversity and counteract sparse
supervised labels, we employ a GAN-based augmentation strategy, generating
plausible interaction patterns and supplementing training data. Extensive
experiments on two real-world marketing datasets (Amazon Ads and TikTok Ad
Clicks) demonstrate that SeqUDA-Rec significantly outperforms state-of-the-art
baselines such as SASRec, BERT4Rec, and GCL4SR. Our model achieves a 6.7%
improvement in NDCG@10 and 11.3% improvement in HR@10, proving its
effectiveness in personalized advertising and intelligent content
recommendation.
\\ ( https://arxiv.org/abs/2509.17361 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17404 (*cross-listing*)
Date: Mon, 22 Sep 2025 07:01:41 GMT   (300kb)

Title: SongPrep: A Preprocessing Framework and End-to-end Model for Full-song
  Structure Parsing and Lyrics Transcription
Authors: Wei Tan, Shun Lei, Huaicheng Zhang, Guangzheng Li, Yixuan Zhang,
  Hangting Chen, Jianwei Yu, Rongzhi Gu, Dong Yu
Categories: eess.AS cs.AI cs.SD
\\
  Artificial Intelligence Generated Content (AIGC) is currently a popular
research area. Among its various branches, song generation has attracted
growing interest. Despite the abundance of available songs, effective data
preparation remains a significant challenge. Converting these songs into
training-ready datasets typically requires extensive manual labeling, which is
both time consuming and costly. To address this issue, we propose SongPrep, an
automated preprocessing pipeline designed specifically for song data. This
framework streamlines key processes such as source separation, structure
analysis, and lyric recognition, producing structured data that can be directly
used to train song generation models. Furthermore, we introduce SongPrepE2E, an
end-to-end structured lyrics recognition model based on pretrained language
models. Without the need for additional source separation, SongPrepE2E is able
to analyze the structure and lyrics of entire songs and provide precise
timestamps. By leveraging context from the whole song alongside pretrained
semantic knowledge, SongPrepE2E achieves low Diarization Error Rate (DER) and
Word Error Rate (WER) on the proposed SSLD-200 dataset. Downstream tasks
demonstrate that training song generation models with the data output by
SongPrepE2E enables the generated songs to closely resemble those produced by
humans.
\\ ( https://arxiv.org/abs/2509.17404 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17413 (*cross-listing*)
Date: Mon, 22 Sep 2025 07:04:53 GMT   (204kb)

Title: Distributionally Robust Safety Verification of Neural Networks via
  Worst-Case CVaR
Authors: Masako Kishida
Categories: cs.LG cs.AI cs.SY eess.SY math.OC
\\
  Ensuring the safety of neural networks under input uncertainty is a
fundamental challenge in safety-critical applications. This paper builds on and
expands Fazlyab's quadratic-constraint (QC) and semidefinite-programming (SDP)
framework for neural network verification to a distributionally robust and
tail-risk-aware setting by integrating worst-case Conditional Value-at-Risk
(WC-CVaR) over a moment-based ambiguity set with fixed mean and covariance. The
resulting conditions remain SDP-checkable and explicitly account for tail risk.
This integration broadens input-uncertainty geometry-covering ellipsoids,
polytopes, and hyperplanes-and extends applicability to safety-critical domains
where tail-event severity matters. Applications to closed-loop reachability of
control systems and classification are demonstrated through numerical
experiments, illustrating how the risk level $\varepsilon$ trades conservatism
for tolerance to tail events-while preserving the computational structure of
prior QC/SDP methods for neural network verification and robustness analysis.
\\ ( https://arxiv.org/abs/2509.17413 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17446 (*cross-listing*)
Date: Mon, 22 Sep 2025 07:38:53 GMT   (762kb)

Title: MVCL-DAF++: Enhancing Multimodal Intent Recognition via Prototype-Aware
  Contrastive Alignment and Coarse-to-Fine Dynamic Attention Fusion
Authors: Haofeng Huang, Yifei Han, Long Zhang, Bin Li, Yangfan He
Categories: cs.LG cs.AI
Comments: Submitted to ICASSP 2026
\\
  Multimodal intent recognition (MMIR) suffers from weak semantic grounding and
poor robustness under noisy or rare-class conditions. We propose MVCL-DAF++,
which extends MVCL-DAF with two key modules: (1) Prototype-aware contrastive
alignment, aligning instances to class-level prototypes to enhance semantic
consistency; and (2) Coarse-to-fine attention fusion, integrating global
modality summaries with token-level features for hierarchical cross-modal
interaction. On MIntRec and MIntRec2.0, MVCL-DAF++ achieves new
state-of-the-art results, improving rare-class recognition by +1.05\% and
+4.18\% WF1, respectively. These results demonstrate the effectiveness of
prototype-guided learning and coarse-to-fine fusion for robust multimodal
understanding. The source code is available at
https://github.com/chr1s623/MVCL-DAF-PlusPlus.
\\ ( https://arxiv.org/abs/2509.17446 ,  762kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17466 (*cross-listing*)
Date: Mon, 22 Sep 2025 08:02:09 GMT   (4905kb)

Title: Autiverse: Eliciting Autistic Adolescents' Daily Narratives through
  AI-guided Multimodal Journaling
Authors: Migyeong Yang, Kyungah Lee, Jinyoung Han, SoHyun Park, Young-Ho Kim
Categories: cs.HC cs.AI cs.CL
Comments: 19 pages excluding reference
ACM-class: H.5.2; I.2.7
\\
  Journaling can potentially serve as an effective method for autistic
adolescents to improve narrative skills. However, its text-centric nature and
high executive functioning demands present barriers to practice. We present
Autiverse, an AI-guided multimodal journaling app for tablets that scaffolds
storytelling through conversational prompts and visual supports. Autiverse
elicits key details through a stepwise dialogue with peer-like, customizable AI
and composes them into an editable four-panel comic strip. Through a two-week
deployment study with 10 autistic adolescent-parent dyads, we examine how
Autiverse supports autistic adolescents to organize their daily experience and
emotion. Autiverse helped them construct coherent narratives, while enabling
parents to learn additional details of their child's events and emotions. The
customized AI peer created a comfortable space for sharing, fostering enjoyment
and a strong sense of agency. We discuss the implications of designing
technologies that complement autistic adolescents' strengths while ensuring
their autonomy and safety in sharing experiences.
\\ ( https://arxiv.org/abs/2509.17466 ,  4905kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17470 (*cross-listing*)
Date: Mon, 22 Sep 2025 08:05:44 GMT   (764kb)

Title: Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for
  Entity Resolution
Authors: Mohammadreza Sharifi, Danial Ahmadzadeh
Categories: cs.DB cs.AI cs.LG
Comments: Accepted at ICCKE 2025 Conference. 6 tables, 7 figures
\\
  Entity resolution plays a significant role in enterprise systems where data
integrity must be rigorously maintained. Traditional methods often struggle
with handling noisy data or semantic understanding, while modern methods suffer
from computational costs or the excessive need for parallel computation. In
this study, we introduce a scalable hybrid framework, which is designed to
address several important problems, including scalability, noise robustness,
and reliable results. We utilized a pre-trained language model to encode each
structured data into corresponding semantic embedding vectors. Subsequently,
after retrieving a semantically relevant subset of candidates, we apply a
syntactic verification stage using fuzzy string matching techniques to refine
classification on the unlabeled data. This approach was applied to a real-world
entity resolution task, which exposed a linkage between a central user
management database and numerous shared hosting server records. Compared to
other methods, this approach exhibits an outstanding performance in terms of
both processing time and robustness, making it a reliable solution for a
server-side product. Crucially, this efficiency does not compromise results, as
the system maintains a high retrieval recall of approximately 0.97. The
scalability of the framework makes it deployable on standard CPU-based
infrastructure, offering a practical and effective solution for
enterprise-level data integrity auditing.
\\ ( https://arxiv.org/abs/2509.17470 ,  764kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17477 (*cross-listing*)
Date: Mon, 22 Sep 2025 08:12:10 GMT   (2919kb)

Title: LingoQ: Bridging the Gap between ESL Learning and Work through
  AI-Generated Work-Related Quizzes
Authors: Yeonsun Yang, Sang Won Lee, Jean Y. Song, Sangdoo Yun, Young-Ho Kim
Categories: cs.HC cs.AI cs.CL
Comments: 17 pages except reference
ACM-class: H.5.2; I.2.7
\\
  Non-native English speakers performing English-related tasks at work struggle
to sustain ESL learning, despite their motivation. Often, study materials are
disconnected from their work context. Although workers rely on LLM assistants
to address their immediate needs, these interactions may not directly
contribute to their English skills. We present LingoQ, an AI-mediated system
that allows workers to practice English using quizzes generated from their LLM
queries during work. LingoQ leverages these queries using AI to generate
personalized quizzes that workers can review and practice on their smartphones.
We conducted a three-week deployment study with 28 ESL workers to evaluate
LingoQ. Participants valued the relevance of quizzes that reflect their own
context, constantly engaging with the app during the study. This active
engagement improved self-efficacy and led to learning gains for beginners and,
potentially, for intermediate learners. We discuss opportunities of leveraging
users' reliance on LLMs to situate their learning in the user context for
improved learning.
\\ ( https://arxiv.org/abs/2509.17477 ,  2919kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17488 (*cross-listing*)
Date: Mon, 22 Sep 2025 08:19:06 GMT   (2228kb)

Title: Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation
  for LLM-Powered Agents
Authors: Shouju Wang, Fenglin Yu, Xirui Liu, Xiaoting Qin, Jue Zhang, Qingwei
  Lin, Dongmei Zhang, Saravan Rajmohan
Categories: cs.CR cs.AI
Comments: To appear at EMNLP 2025 (Findings)
\\
  The increasing autonomy of LLM agents in handling sensitive communications,
accelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)
frameworks, creates urgent privacy challenges. While recent work reveals
significant gaps between LLMs' privacy Q&A performance and their agent
behavior, existing benchmarks remain limited to static, simplified scenarios.
We present PrivacyChecker, a model-agnostic, contextual integrity based
mitigation approach that effectively reduces privacy leakage from 36.08% to
7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving
task helpfulness. We also introduce PrivacyLens-Live, transforming static
benchmarks into dynamic MCP and A2A environments that reveal substantially
higher privacy risks in practical. Our modular mitigation approach integrates
seamlessly into agent protocols through three deployment strategies, providing
practical privacy protection for the emerging agentic ecosystem. Our data and
code will be made available at https://aka.ms/privacy_in_action.
\\ ( https://arxiv.org/abs/2509.17488 ,  2228kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17533 (*cross-listing*)
Date: Mon, 22 Sep 2025 08:52:54 GMT   (534kb)

Title: Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning
  Inference on Embedded Microcontrollers
Authors: Anastasios Fanariotis, Theofanis Orphanoudakis and Vasilis Fotopoulos
Categories: cs.ET cs.AI cs.LG
\\
  The deployment of machine learning (ML) models on microcontrollers (MCUs) is
constrained by strict energy, latency, and memory requirements, particularly in
battery-operated and real-time edge devices. While software-level optimizations
such as quantization and pruning reduce model size and computation, hardware
acceleration has emerged as a decisive enabler for efficient embedded
inference. This paper evaluates the impact of Neural Processing Units (NPUs) on
MCU-based ML execution, using the ARM Cortex-M55 core combined with the
Ethos-U55 NPU on the Alif Semiconductor Ensemble E7 development board as a
representative platform. A rigorous measurement methodology was employed,
incorporating per-inference net energy accounting via GPIO-triggered
high-resolution digital multimeter synchronization and idle-state subtraction,
ensuring accurate attribution of energy costs. Experimental results across six
representative ML models -including MiniResNet, MobileNetV2, FD-MobileNet,
MNIST, TinyYolo, and SSD-MobileNet- demonstrate substantial efficiency gains
when inference is offloaded to the NPU. For moderate to large networks, latency
improvements ranged from 7x to over 125x, with per-inference net energy
reductions up to 143x. Notably, the NPU enabled execution of models unsupported
on CPU-only paths, such as SSD-MobileNet, highlighting its functional as well
as efficiency advantages. These findings establish NPUs as a cornerstone of
energy-aware embedded AI, enabling real-time, power-constrained ML inference at
the MCU level.
\\ ( https://arxiv.org/abs/2509.17533 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17608 (*cross-listing*)
Date: Mon, 22 Sep 2025 11:23:10 GMT   (15575kb)

Title: AutiHero: Leveraging Generative AI in Social Narratives to Engage
  Parents in Story-Driven Behavioral Guidance for Autistic Children
Authors: Jungeun Lee, Kyungah Lee, Inseok Hwang, SoHyun Park, Young-Ho Kim
Categories: cs.HC cs.AI cs.CL
Comments: 22 pages except reference
ACM-class: H.5.2; I.2.7
\\
  Social narratives are known to help autistic children understand and navigate
social situations through stories. To ensure effectiveness, however, the
materials need to be customized to reflect each child's unique behavioral
context, requiring considerable time and effort for parents to practice at
home. We present AutiHero, a generative AI-based social narrative system for
behavioral guidance, which supports parents to create personalized stories for
their autistic children and read them together. AutiHero generates text and
visual illustrations that reflect their children's interests, target behaviors,
and everyday contexts. In a two-week deployment study with 16 autistic
child-parent dyads, parents created 218 stories and read an average of 4.25
stories per day, demonstrating a high level of engagement. AutiHero also
provided an effective, low-demanding means to guide children's social
behaviors, encouraging positive change. We discuss the implications of
generative AI-infused tools to empower parents in guiding their children's
behaviors, fostering their social learning.
\\ ( https://arxiv.org/abs/2509.17608 ,  15575kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17621 (*cross-listing*)
Date: Mon, 22 Sep 2025 11:33:17 GMT   (205kb)

Title: SeqBattNet: A Discrete-State Physics-Informed Neural Network with Aging
  Adaptation for Battery Modeling
Authors: Khoa Tran, Hung-Cuong Trinh, Vy-Rin Nguyen, T. Nguyen-Thoi, Vin
  Nguyen-Thai
Categories: cs.LG cs.AI
\\
  Accurate battery modeling is essential for reliable state estimation in
modern applications, such as predicting the remaining discharge time and
remaining discharge energy in battery management systems. Existing approaches
face several limitations: model-based methods require a large number of
parameters; data-driven methods rely heavily on labeled datasets; and current
physics-informed neural networks (PINNs) often lack aging adaptation, or still
depend on many parameters, or continuously regenerate states. In this work, we
propose SeqBattNet, a discrete-state PINN with built-in aging adaptation for
battery modeling, to predict terminal voltage during the discharge process.
SeqBattNet consists of two components: (i) an encoder, implemented as the
proposed HRM-GRU deep learning module, which generates cycle-specific aging
adaptation parameters; and (ii) a decoder, based on the equivalent circuit
model (ECM) combined with deep learning, which uses these parameters together
with the input current to predict voltage. The model requires only three basic
battery parameters and, when trained on data from a single cell, still achieves
robust performance. Extensive evaluations across three benchmark datasets (TRI,
RT-Batt, and NASA) demonstrate that SeqBattNet significantly outperforms
classical sequence models and PINN baselines, achieving consistently lower RMSE
while maintaining computational efficiency.
\\ ( https://arxiv.org/abs/2509.17621 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17665 (*cross-listing*)
Date: Mon, 22 Sep 2025 12:09:21 GMT   (142kb)

Title: Mechanistic Interpretability with SAEs: Probing Religion, Violence, and
  Geography in Large Language Models
Authors: Katharina Simbeck, Mariam Mahran
Categories: cs.LG cs.AI cs.CY
Comments: Accepted at AEQUITAS 2025: Workshop on Fairness and Bias in AI |
  co-located with ECAI, October 26th, 2025, Bologna, Italy. 12 pages, 1 figure
\\
  Despite growing research on bias in large language models (LLMs), most work
has focused on gender and race, with little attention to religious identity.
This paper explores how religion is internally represented in LLMs and how it
intersects with concepts of violence and geography. Using mechanistic
interpretability and Sparse Autoencoders (SAEs) via the Neuronpedia API, we
analyze latent feature activations across five models. We measure overlap
between religion- and violence-related prompts and probe semantic patterns in
activation contexts. While all five religions show comparable internal
cohesion, Islam is more frequently linked to features associated with violent
language. In contrast, geographic associations largely reflect real-world
religious demographics, revealing how models embed both factual distributions
and cultural stereotypes. These findings highlight the value of structural
analysis in auditing not just outputs but also internal representations that
shape model behavior.
\\ ( https://arxiv.org/abs/2509.17665 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17695 (*cross-listing*)
Date: Mon, 22 Sep 2025 12:33:13 GMT   (1094kb)

Title: Cluster Workload Allocation: A Predictive Approach Leveraging Machine
  Learning Efficiency
Authors: Leszek Sliwko
Categories: cs.LG cs.AI cs.DC cs.SE
Comments: This is the accepted version of the paper published in IEEE Access.
  The final version is available at:
  https://doi.org/10.1109/ACCESS.2024.3520422
Journal-ref: IEEE Access (2024)
DOI: 10.1109/ACCESS.2024.3520422
\\
  This research investigates how Machine Learning (ML) algorithms can assist in
workload allocation strategies by detecting tasks with node affinity operators
(referred to as constraint operators), which constrain their execution to a
limited number of nodes. Using real-world Google Cluster Data (GCD) workload
traces and the AGOCS framework, the study extracts node attributes and task
constraints, then analyses them to identify suitable node-task pairings. It
focuses on tasks that can be executed on either a single node or fewer than a
thousand out of 12.5k nodes in the analysed GCD cluster. Task constraint
operators are compacted, pre-processed with one-hot encoding, and used as
features in a training dataset. Various ML classifiers, including Artificial
Neural Networks, K-Nearest Neighbours, Decision Trees, Naive Bayes, Ridge
Regression, Adaptive Boosting, and Bagging, are fine-tuned and assessed for
accuracy and F1-scores. The final ensemble voting classifier model achieved 98%
accuracy and a 1.5-1.8% misclassification rate for tasks with a single suitable
node.
\\ ( https://arxiv.org/abs/2509.17695 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17752 (*cross-listing*)
Date: Mon, 22 Sep 2025 13:16:02 GMT   (24594kb)

Title: GEM-T: Generative Tabular Data via Fitting Moments
Authors: Miao Li, Phuc Nguyen, Christopher Tam, Alexandra Morgan, Kenneth Ge,
  Rahul Bansal, Linzi Yu, Rima Arnaout, Ramy Arnaout
Categories: cs.LG cs.AI stat.ML
Comments: 18 pages, 4 figures
\\
  Tabular data dominates data science but poses challenges for generative
models, especially when the data is limited or sensitive. We present a novel
approach to generating synthetic tabular data based on the principle of maximum
entropy -- MaxEnt -- called GEM-T, for ``generative entropy maximization for
tables.'' GEM-T directly captures nth-order interactions -- pairwise,
third-order, etc. -- among columns of training data. In extensive testing,
GEM-T matches or exceeds deep neural network approaches previously regarded as
state-of-the-art in 23 of 34 publicly available datasets representing diverse
subject domains (68\%). Notably, GEM-T involves orders-of-magnitude fewer
trainable parameters, demonstrating that much of the information in real-world
data resides in low-dimensional, potentially human-interpretable correlations,
provided that the input data is appropriately transformed first. Furthermore,
MaxEnt better handles heterogeneous data types (continuous vs. discrete vs.
categorical), lack of local structure, and other features of tabular data.
GEM-T represents a promising direction for light-weight high-performance
generative models for structured data.
\\ ( https://arxiv.org/abs/2509.17752 ,  24594kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17784 (*cross-listing*)
Date: Mon, 22 Sep 2025 13:45:17 GMT   (18424kb)

Title: Revealing Multimodal Causality with Large Language Models
Authors: Jin Li, Shoujin Wang, Qi Zhang, Feng Liu, Tongliang Liu, Longbing Cao,
  Shui Yu, Fang Chen
Categories: cs.LG cs.AI
Comments: Accepted at NeurIPS 2025
\\
  Uncovering cause-and-effect mechanisms from data is fundamental to scientific
progress. While large language models (LLMs) show promise for enhancing causal
discovery (CD) from unstructured data, their application to the increasingly
prevalent multimodal setting remains a critical challenge. Even with the advent
of multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two
primary limitations: (1) difficulty in exploring intra- and inter-modal
interactions for comprehensive causal variable identification; and (2)
insufficiency to handle structural ambiguities with purely observational data.
To address these challenges, we propose MLLM-CD, a novel framework for
multimodal causal discovery from unstructured data. It consists of three key
components: (1) a novel contrastive factor discovery module to identify genuine
multimodal factors based on the interactions explored from contrastive sample
pairs; (2) a statistical causal structure discovery module to infer causal
relationships among discovered factors; and (3) an iterative multimodal
counterfactual reasoning module to refine the discovery outcomes iteratively by
incorporating the world knowledge and reasoning capabilities of MLLMs.
Extensive experiments on both synthetic and real-world datasets demonstrate the
effectiveness of MLLM-CD in revealing genuine factors and causal relationships
among them from multimodal unstructured data.
\\ ( https://arxiv.org/abs/2509.17784 ,  18424kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17834 (*cross-listing*)
Date: Mon, 22 Sep 2025 14:23:50 GMT   (1433kb)

Title: From Documents to Database: Failure Modes for Industrial Assets
Authors: Duygu Kabakci-Zorlu, Fabio Lorenzi, John Sheehan, Karol Lynch, Bradley
  Eck
Categories: cs.DB cs.AI cs.CL
Comments: 7 pages, 4 figures. Artificial Intelligence for Knowledge Acquisition
  & Management (AI4KAM) Workshop @ IJCAI 2025
\\
  We propose an interactive system using foundation models and user-provided
technical documents to generate Failure Mode and Effects Analyses (FMEA) for
industrial equipment. Our system aggregates unstructured content across
documents to generate an FMEA and stores it in a relational database.
Leveraging this tool, the time required for creation of this
knowledge-intensive content is reduced, outperforming traditional manual
approaches. This demonstration showcases the potential of foundation models to
facilitate the creation of specialized structured content for enterprise asset
management systems.
\\ ( https://arxiv.org/abs/2509.17834 ,  1433kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17866 (*cross-listing*)
Date: Mon, 22 Sep 2025 15:03:36 GMT   (35695kb)

Title: Understanding Post-Training Structural Changes in Large Language Models
Authors: Xinyu He, Xianghui Cao
Categories: cs.LG cs.AI
Comments: 38 pages, 26 figures
\\
  Post-training fundamentally alters the behavior of large language models
(LLMs), yet its impact on the internal parameter space remains poorly
understood. In this work, we conduct a systematic singular value decomposition
(SVD) analysis of principal linear layers in pretrained LLMs, focusing on two
widely adopted post-training methods: instruction tuning and
long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two
consistent and unexpected structural changes:(1) a near-uniform geometric
scaling of singular values across layers, which theoretically modulates
attention scores; and (2) highly consistent orthogonal transformations are
applied to the left and right singular vectors of each matrix. Disrupting this
orthogonal consistency leads to catastrophic performance degradation. Based on
these findings, we propose a simple yet effective framework that interprets
post-training as a reparameterization of fixed subspaces in the pretrained
parameter space. Further experiments reveal that singular value scaling behaves
as a secondary effect, analogous to a temperature adjustment, whereas the core
functional transformation lies in the coordinated rotation of singular vectors.
These results challenge the prevailing view of the parameter space in large
models as a black box, uncovering the first clear regularities in how
parameters evolve during training, and providing a new perspective for deeper
investigation into model parameter changes.
\\ ( https://arxiv.org/abs/2509.17866 ,  35695kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17885 (*cross-listing*)
Date: Mon, 22 Sep 2025 15:18:21 GMT   (1150kb)

Title: Confidence-gated training for efficient early-exit neural networks
Authors: Saad Mokssit, Ouassim Karrakchou, Alejandro Mousist, Mounir Ghogho
Categories: cs.LG cs.AI
\\
  Early-exit neural networks reduce inference cost by enabling confident
predictions at intermediate layers. However, joint training often leads to
gradient interference, with deeper classifiers dominating optimization. We
propose Confidence-Gated Training (CGT), a paradigm that conditionally
propagates gradients from deeper exits only when preceding exits fail. This
encourages shallow classifiers to act as primary decision points while
reserving deeper layers for harder inputs. By aligning training with the
inference-time policy, CGT mitigates overthinking, improves early-exit
accuracy, and preserves efficiency. Experiments on the Indian Pines and
Fashion-MNIST benchmarks show that CGT lowers average inference cost while
improving overall accuracy, offering a practical solution for deploying deep
models in resource-constrained environments.
\\ ( https://arxiv.org/abs/2509.17885 ,  1150kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17941 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:04:50 GMT   (12682kb)

Title: ComposableNav: Instruction-Following Navigation in Dynamic Environments
  via Composable Diffusion
Authors: Zichao Hu, Chen Tang, Michael J. Munje, Yifeng Zhu, Alex Liu, Shuijing
  Liu, Garrett Warnell, Peter Stone, Joydeep Biswas
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Conference on Robot Learning (CoRL) 2025 Project site:
  https://amrl.cs.utexas.edu/ComposableNav/
\\
  This paper considers the problem of enabling robots to navigate dynamic
environments while following instructions. The challenge lies in the
combinatorial nature of instruction specifications: each instruction can
include multiple specifications, and the number of possible specification
combinations grows exponentially as the robot's skill set expands. For example,
"overtake the pedestrian while staying on the right side of the road" consists
of two specifications: "overtake the pedestrian" and "walk on the right side of
the road." To tackle this challenge, we propose ComposableNav, based on the
intuition that following an instruction involves independently satisfying its
constituent specifications, each corresponding to a distinct motion primitive.
Using diffusion models, ComposableNav learns each primitive separately, then
composes them in parallel at deployment time to satisfy novel combinations of
specifications unseen in training. Additionally, to avoid the onerous need for
demonstrations of individual motion primitives, we propose a two-stage training
procedure: (1) supervised pre-training to learn a base diffusion model for
dynamic navigation, and (2) reinforcement learning fine-tuning that molds the
base model into different motion primitives. Through simulation and real-world
experiments, we show that ComposableNav enables robots to follow instructions
by generating trajectories that satisfy diverse and unseen combinations of
specifications, significantly outperforming both non-compositional VLM-based
policies and costmap composing baselines. Videos and additional materials can
be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/
\\ ( https://arxiv.org/abs/2509.17941 ,  12682kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17942 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:05:45 GMT   (244kb)

Title: StefaLand: An Efficient Geoscience Foundation Model That Improves
  Dynamic Land-Surface Predictions
Authors: Nicholas Kraabel, Jiangtao Liu, Yuchen Bian, Daniel Kifer, Chaopeng
  Shen
Categories: cs.LG cs.AI
\\
  Stewarding natural resources, mitigating floods, droughts, wildfires, and
landslides, and meeting growing demands require models that can predict
climate-driven land-surface responses and human feedback with high accuracy.
Traditional impact models, whether process-based, statistical, or machine
learning, struggle with spatial generalization due to limited observations and
concept drift. Recently proposed vision foundation models trained on satellite
imagery demand massive compute and are ill-suited for dynamic land-surface
prediction. We introduce StefaLand, a generative spatiotemporal earth
foundation model centered on landscape interactions. StefaLand improves
predictions on three tasks and four datasets: streamflow, soil moisture, and
soil composition, compared to prior state-of-the-art. Results highlight its
ability to generalize across diverse, data-scarce regions and support broad
land-surface applications. The model builds on a masked autoencoder backbone
that learns deep joint representations of landscape attributes, with a
location-aware architecture fusing static and time-series inputs,
attribute-based representations that drastically reduce compute, and residual
fine-tuning adapters that enhance transfer. While inspired by prior methods,
their alignment with geoscience and integration in one model enables robust
performance on dynamic land-surface tasks. StefaLand can be pretrained and
finetuned on academic compute yet outperforms state-of-the-art baselines and
even fine-tuned vision foundation models. To our knowledge, this is the first
geoscience land-surface foundation model that demonstrably improves dynamic
land-surface interaction predictions and supports diverse downstream
applications.
\\ ( https://arxiv.org/abs/2509.17942 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17970 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:20:29 GMT   (19054kb)

Title: Joint Optimization of Memory Frequency, Computing Frequency,
  Transmission Power and Task Offloading for Energy-efficient DNN Inference
Authors: Yunchu Han, Zhaojun Nan, Sheng Zhou, and Zhisheng Niu
Categories: cs.LG cs.AI cs.CV
\\
  Deep neural networks (DNNs) have been widely applied in diverse applications,
but the problems of high latency and energy overhead are inevitable on
resource-constrained devices. To address this challenge, most researchers focus
on the dynamic voltage and frequency scaling (DVFS) technique to balance the
latency and energy consumption by changing the computing frequency of
processors. However, the adjustment of memory frequency is usually ignored and
not fully utilized to achieve efficient DNN inference, which also plays a
significant role in the inference time and energy consumption. In this paper,
we first investigate the impact of joint memory frequency and computing
frequency scaling on the inference time and energy consumption with a
model-based and data-driven method. Then by combining with the fitting
parameters of different DNN models, we give a preliminary analysis for the
proposed model to see the effects of adjusting memory frequency and computing
frequency simultaneously. Finally, simulation results in local inference and
cooperative inference cases further validate the effectiveness of jointly
scaling the memory frequency and computing frequency to reduce the energy
consumption of devices.
\\ ( https://arxiv.org/abs/2509.17970 ,  19054kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17971 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:20:41 GMT   (3005kb)

Title: Intra-Cluster Mixup: An Effective Data Augmentation Technique for
  Complementary-Label Learning
Authors: Tan-Ha Mai, Hsuan-Tien Lin
Categories: cs.LG cs.AI cs.CV
Comments: 22 pages, 10 figures
\\
  In this paper, we investigate the challenges of complementary-label learning
(CLL), a specialized form of weakly-supervised learning (WSL) where models are
trained with labels indicating classes to which instances do not belong, rather
than standard ordinary labels. This alternative supervision is appealing
because collecting complementary labels is generally cheaper and less
labor-intensive. Although most existing research in CLL emphasizes the
development of novel loss functions, the potential of data augmentation in this
domain remains largely underexplored. In this work, we uncover that the
widely-used Mixup data augmentation technique is ineffective when directly
applied to CLL. Through in-depth analysis, we identify that the
complementary-label noise generated by Mixup negatively impacts the performance
of CLL models. We then propose an improved technique called Intra-Cluster Mixup
(ICM), which only synthesizes augmented data from nearby examples, to mitigate
the noise effect. ICM carries the benefits of encouraging complementary label
sharing of nearby examples, and leads to substantial performance improvements
across synthetic and real-world labeled datasets. In particular, our wide
spectrum of experimental results on both balanced and imbalanced CLL settings
justifies the potential of ICM in allying with state-of-the-art CLL algorithms,
achieving significant accuracy increases of 30% and 10% on MNIST and CIFAR
datasets, respectively.
\\ ( https://arxiv.org/abs/2509.17971 ,  3005kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17998 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:39:12 GMT   (2762kb)

Title: Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with
  LLMs
Authors: Richard Cornelius Suwandi, Feng Yin, Juntao Wang, Renjie Li, Tsung-Hui
  Chang, Sergios Theodoridis
Categories: cs.LG cs.AI
Comments: Accepted as Poster at NeurIPS 2025
\\
  The efficiency of Bayesian optimization (BO) relies heavily on the choice of
the Gaussian process (GP) kernel, which plays a central role in balancing
exploration and exploitation under limited evaluation budgets. Traditional BO
methods often rely on fixed or heuristic kernel selection strategies, which can
result in slow convergence or suboptimal solutions when the chosen kernel is
poorly suited to the underlying objective function. To address this limitation,
we propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO
with large language models (LLMs). Concretely, CAKE leverages LLMs as the
crossover and mutation operators to adaptively generate and refine GP kernels
based on the observed data throughout the optimization process. To maximize the
power of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to
select the most effective kernel through balancing the model fit measured by
the Bayesian information criterion (BIC) with the expected improvement at each
iteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO
method consistently outperforms established baselines across a range of
real-world tasks, including hyperparameter optimization, controller tuning, and
photonic chip design. Our code is publicly available at
https://github.com/cake4bo/cake.
\\ ( https://arxiv.org/abs/2509.17998 ,  2762kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17999 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:39:22 GMT   (1812kb)

Title: The Narcissus Hypothesis:Descending to the Rung of Illusion
Authors: Riccardo Cadei, Christian Intern\`o
Categories: cs.CY cs.AI cs.HC cs.LG
\\
  Modern foundational models increasingly reflect not just world knowledge, but
patterns of human preference embedded in their training data. We hypothesize
that recursive alignment-via human feedback and model-generated corpora-induces
a social desirability bias, nudging models to favor agreeable or flattering
responses over objective reasoning. We refer to it as the Narcissus Hypothesis
and test it across 31 models using standardized personality assessments and a
novel Social Desirability Bias score. Results reveal a significant drift toward
socially conforming traits, with profound implications for corpus integrity and
the reliability of downstream inferences. We then offer a novel epistemological
interpretation, tracing how recursive bias may collapse higher-order reasoning
down Pearl's Ladder of Causality, culminating in what we refer to as the Rung
of Illusion.
\\ ( https://arxiv.org/abs/2509.17999 ,  1812kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18001 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:40:42 GMT   (83kb)

Title: Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise
Authors: Haocheng Luo, Mehrtash Harandi, Dinh Phung, Trung Le
Categories: cs.LG cs.AI
\\
  Sharpness-aware minimization (SAM) has emerged as a highly effective
technique for improving model generalization, but its underlying principles are
not fully understood. We investigated the phenomenon known as m-sharpness,
where the performance of SAM improves monotonically as the micro-batch size for
computing perturbations decreases. Leveraging an extended Stochastic
Differential Equation (SDE) framework, combined with an analysis of the
structure of stochastic gradient noise (SGN), we precisely characterize the
dynamics of various SAM variants. Our findings reveal that the stochastic noise
introduced during SAM perturbations inherently induces a variance-based
sharpness regularization effect. Motivated by our theoretical insights, we
introduce Reweighted SAM, which employs sharpness-weighted sampling to mimic
the generalization benefits of m-SAM while remaining parallelizable.
Comprehensive experiments validate the effectiveness of our theoretical
analysis and proposed method.
\\ ( https://arxiv.org/abs/2509.18001 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18008 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:47:08 GMT   (9654kb)

Title: Through the Lens of Human-Human Collaboration: A Configurable Research
  Platform for Exploring Human-Agent Collaboration
Authors: Bingsheng Yao, Jiaju Chen, Chaoran Chen, April Wang, Toby Jia-jun Li,
  Dakuo Wang
Categories: cs.HC cs.AI cs.CL
\\
  Intelligent systems have traditionally been designed as tools rather than
collaborators, often lacking critical characteristics that collaboration
partnerships require. Recent advances in large language model (LLM) agents open
new opportunities for human-LLM-agent collaboration by enabling natural
communication and various social and cognitive behaviors. Yet it remains
unclear whether principles of computer-mediated collaboration established in
HCI and CSCW persist, change, or fail when humans collaborate with LLM agents.
To support systematic investigations of these questions, we introduce an open
and configurable research platform for HCI researchers. The platform's modular
design allows seamless adaptation of classic CSCW experiments and manipulation
of theory-grounded interaction controls. We demonstrate the platform's
effectiveness and usability through two case studies: (1) re-implementing the
classic human-human-collaboration task Shape Factory as a between-subject
human-agent-collaboration experiment with 16 participants, and (2) a
participatory cognitive walkthrough with five HCI researchers to refine
workflows and interfaces for experiment setup and analysis.
\\ ( https://arxiv.org/abs/2509.18008 ,  9654kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18025 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:00:40 GMT   (863kb)

Title: Deep Learning as the Disciplined Construction of Tame Objects
Authors: Gilles Bareilles, Allen Gehret, Johannes Aspman, Jana Lep\v{s}ov\'a,
  Jakub Mare\v{c}ek
Categories: math.OC cs.AI cs.LG math.LO stat.ML
Comments: 35 pages, 8 figures
\\
  One can see deep-learning models as compositions of functions within the
so-called tame geometry. In this expository note, we give an overview of some
topics at the interface of tame geometry (also known as o-minimality),
optimization theory, and deep learning theory and practice. To do so, we
gradually introduce the concepts and tools used to build convergence guarantees
for stochastic gradient descent in a general nonsmooth nonconvex, but tame,
setting. This illustrates some ways in which tame geometry is a natural
mathematical framework for the study of AI systems, especially within Deep
Learning.
\\ ( https://arxiv.org/abs/2509.18025 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18044 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:18:59 GMT   (2161kb)

Title: Hybrid Reputation Aggregation: A Robust Defense Mechanism for
  Adversarial Federated Learning in 5G and Edge Network Environments
Authors: Saeid Sheikhi, Panos Kostakos, Lauri Loven
Categories: cs.CR cs.AI
\\
  Federated Learning (FL) in 5G and edge network environments face severe
security threats from adversarial clients. Malicious participants can perform
label flipping, inject backdoor triggers, or launch Sybil attacks to corrupt
the global model. This paper introduces Hybrid Reputation Aggregation (HRA), a
novel robust aggregation mechanism designed to defend against diverse
adversarial behaviors in FL without prior knowledge of the attack type. HRA
combines geometric anomaly detection with momentum-based reputation tracking of
clients. In each round, it detects outlier model updates via distance-based
geometric analysis while continuously updating a trust score for each client
based on historical behavior. This hybrid approach enables adaptive filtering
of suspicious updates and long-term penalization of unreliable clients,
countering attacks ranging from backdoor insertions to random noise Byzantine
failures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+
records) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse
adversarial attack scenarios. Experimental results reveal that HRA achieves
robust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on
NF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,
Trimmed Mean, and Bulyan by significant margins. Our ablation studies further
demonstrate that the full hybrid system achieves 98.66% accuracy, while the
anomaly-only and reputation-only variants drop to 84.77% and 78.52%,
respectively, validating the synergistic value of our dual-mechanism approach.
This demonstrates HRA's enhanced resilience and robustness in 5G/edge federated
learning deployments, even under significant adversarial conditions.
\\ ( https://arxiv.org/abs/2509.18044 ,  2161kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18046 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:19:55 GMT   (6663kb)

Title: HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement
  Learning with Mamba
Authors: Yinuo Wang, Yuanyang Qi, Jinzhao Zhou, and Gavin Tao
Categories: cs.RO cs.AI cs.ET cs.SY eess.SP eess.SY
Comments: 10 pages
\\
  End-to-end reinforcement learning (RL) for humanoid locomotion is appealing
for its compact perception-action mapping, yet practical policies often suffer
from training instability, inefficient feature fusion, and high actuation cost.
We present HuMam, a state-centric end-to-end RL framework that employs a
single-layer Mamba encoder to fuse robot-centric states with oriented footstep
targets and a continuous phase clock. The policy outputs joint position targets
tracked by a low-level PD loop and is optimized with PPO. A concise six-term
reward balances contact quality, swing smoothness, foot placement, posture, and
body stability while implicitly promoting energy saving. On the JVRC-1 humanoid
in mc-mujoco, HuMam consistently improves learning efficiency, training
stability, and overall task performance over a strong feedforward baseline,
while reducing power consumption and torque peaks. To our knowledge, this is
the first end-to-end humanoid RL controller that adopts Mamba as the fusion
backbone, demonstrating tangible gains in efficiency, stability, and control
economy.
\\ ( https://arxiv.org/abs/2509.18046 ,  6663kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18054 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:29:10 GMT   (1702kb)

Title: A Knowledge Graph-based Retrieval-Augmented Generation Framework for
  Algorithm Selection in the Facility Layout Problem
Authors: Nikhil N S (1), Amol Dilip Joshi (1 and 2), Bilal Muhammed (2), Soban
  Babu (2) ((1) Indian Institute of Science, Bengaluru, India, (2) TCS
  Research, Tata Consultancy Services Ltd.)
Categories: cs.IR cs.AI cs.LG
Comments: 10 pages, 5 figures
\\
  Selecting a solution algorithm for the Facility Layout Problem (FLP), an
NP-hard optimization problem with a multiobjective trade-off, is a complex task
that requires deep expert knowledge. The performance of a given algorithm
depends on specific problem characteristics such as its scale, objectives, and
constraints. This creates a need for a data-driven recommendation method to
guide algorithm selection in automated design systems. This paper introduces a
new recommendation method to make such expertise accessible, based on a
Knowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To
address this, a domain-specific knowledge graph is constructed from published
literature. The method then employs a multi-faceted retrieval mechanism to
gather relevant evidence from this knowledge graph using three distinct
approaches, which include a precise graph-based search, flexible vector-based
search, and high-level cluster-based search. The retrieved evidence is utilized
by a Large Language Model (LLM) to generate algorithm recommendations with
data-driven reasoning. The proposed KG-RAG method is compared against a
commercial LLM chatbot with access to the knowledge base as a table, across a
series of diverse, real-world FLP test cases. Based on recommendation accuracy
and reasoning capability, the proposed method performed significantly better
than the commercial LLM chatbot.
\\ ( https://arxiv.org/abs/2509.18054 ,  1702kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18057 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:30:33 GMT   (8358kb)

Title: Reinforced Generation of Combinatorial Structures: Applications to
  Complexity Theory
Authors: Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta
Categories: cs.LG cs.AI cs.CC math.CO
\\
  We explore whether techniques from AI can help discover new combinatorial
structures that improve provable limits on efficient algorithms. Specifically,
we use AlphaEvolve (an LLM coding agent) to study two settings:
  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a
recent result of Kunisky and Yu to obtain near-optimal upper and (conditional)
lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on
random 3- and 4-regular graphs. Our improved lower bounds are obtained by
constructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using
AlphaEvolve. Additionally, via analytical arguments we strengthen the upper
bounds to settle the computational hardness of these questions up to an error
in the third decimal place.
  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new
inapproximability results, proving that it is NP-hard to approximate MAX-4-CUT
and MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using
AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves
upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current
best gadget-based inapproximability result of $0.9853$, but falls short of
improving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget
reduction from "standard" H{\aa}stad-style PCPs.
  A key technical challenge we faced: verifying a candidate construction
produced by AlphaEvolve is costly (often requiring exponential time). In both
settings above, our results were enabled by using AlphaEvolve itself to evolve
the verification procedure to be faster (sometimes by $10,000\times$). We
conclude with a discussion of norms by which to assess the assistance from AI
in developing proofs.
\\ ( https://arxiv.org/abs/2509.18057 ,  8358kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18058 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:30:56 GMT   (343kb)

Title: Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM
Authors: Alexander Panfilov, Evgenii Kortukov, Kristina Nikoli\'c, Matthias
  Bethge, Sebastian Lapuschkin, Wojciech Samek, Ameya Prabhu, Maksym
  Andriushchenko, Jonas Geiping
Categories: cs.LG cs.AI cs.CR
\\
  Large language model (LLM) developers aim for their models to be honest,
helpful, and harmless. However, when faced with malicious requests, models are
trained to refuse, sacrificing helpfulness. We show that frontier LLMs can
develop a preference for dishonesty as a new strategy, even when other options
are available. Affected models respond to harmful requests with outputs that
sound harmful but are subtly incorrect or otherwise harmless in practice. This
behavior emerges with hard-to-predict variations even within models from the
same model family. We find no apparent cause for the propensity to deceive, but
we show that more capable models are better at executing this strategy.
Strategic dishonesty already has a practical impact on safety evaluations, as
we show that dishonest responses fool all output-based monitors used to detect
jailbreaks that we test, rendering benchmark scores unreliable. Further,
strategic dishonesty can act like a honeypot against malicious users, which
noticeably obfuscates prior jailbreak attacks. While output monitors fail, we
show that linear probes on internal activations can be used to reliably detect
strategic dishonesty. We validate probes on datasets with verifiable outcomes
and by using their features as steering vectors. Overall, we consider strategic
dishonesty as a concrete example of a broader concern that alignment of LLMs is
hard to control, especially when helpfulness and harmlessness conflict.
\\ ( https://arxiv.org/abs/2509.18058 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18085 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:58:21 GMT   (333kb)

Title: Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative
  Decoding
Authors: Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee,
  Christopher Lott, Fatih Porikli
Categories: cs.LG cs.AI cs.CL
\\
  Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to
autoregressive LLMs (AR-LLMs) with the potential to operate at significantly
higher token generation rates. However, currently available open-source dLLMs
often generate at much lower rates, typically decoding only a single token at
every denoising timestep in order to maximize output quality. We present
Spiffy, a speculative decoding algorithm that accelerates dLLM inference by
$\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output
distribution. This work addresses the unique challenges involved in applying
ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes
draft states by leveraging the dLLM's distribution itself in an
auto-speculative manner. This approach is efficient and effective, and
eliminates the overheads of training and running an independent draft model. To
structure the candidate draft states, we propose a novel directed draft graph
which is uniquely designed to take advantage of the bidirectional, block-wise
nature of dLLM generation and can be verified in parallel by the dLLM. To
further optimize the structure of these draft graphs, we introduce an
efficient, offline calibration algorithm that procedurally determines
high-quality graph configurations. These optimized draft graphs, enabling
increased acceptance rates, lead to a significant boost in the overall speedup
achieved by the system. Crucially, Spiffy is also complementary to other recent
innovations in improving dLLM generation speeds such as KV-caching and
multi-token unmasking. We demonstrate that when combined with such parallel
decoding algorithms, Spiffy is able to effectively multiply the benefits of
these methods leading to total speedups of up to $\mathbf{7.9\times}$.
\\ ( https://arxiv.org/abs/2509.18085 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18091 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:59:07 GMT   (3065kb)

Title: OnePiece: Bringing Context Engineering and Reasoning to Industrial
  Cascade Ranking System
Authors: Sunhao Dai, Jiakai Tang, Jiahua Wu, Kun Wang, Yuxuan Zhu, Bingjun
  Chen, Bangyang Hong, Yu Zhao, Cong Fu, Kangle Wu, Yabo Ni, Anxiang Zeng,
  Wenjie Wang, Xu Chen, Jun Xu, See-Kiong Ng
Categories: cs.IR cs.AI cs.CL
Comments: OnePiece Technical Report; Applied in Shopee
\\
  Despite the growing interest in replicating the scaled success of large
language models (LLMs) in industrial search and recommender systems, most
existing industrial efforts remain limited to transplanting Transformer
architectures, which bring only incremental improvements over strong Deep
Learning Recommendation Models (DLRMs). From a first principle perspective, the
breakthroughs of LLMs stem not only from their architectures but also from two
complementary mechanisms: context engineering, which enriches raw input queries
with contextual cues to better elicit model capabilities, and multi-step
reasoning, which iteratively refines model outputs through intermediate
reasoning paths. However, these two mechanisms and their potential to unlock
substantial improvements remain largely underexplored in industrial ranking
systems.
  In this paper, we propose OnePiece, a unified framework that seamlessly
integrates LLM-style context engineering and reasoning into both retrieval and
ranking models of industrial cascaded pipelines. OnePiece is built on a pure
Transformer backbone and further introduces three key innovations: (1)
structured context engineering, which augments interaction history with
preference and scenario signals and unifies them into a structured tokenized
input sequence for both retrieval and ranking; (2) block-wise latent reasoning,
which equips the model with multi-step refinement of representations and scales
reasoning bandwidth via block size; (3) progressive multi-task training, which
leverages user feedback chains to effectively supervise reasoning steps during
training. OnePiece has been deployed in the main personalized search scenario
of Shopee and achieves consistent online gains across different key business
metrics, including over $+2\%$ GMV/UU and a $+2.90\%$ increase in advertising
revenue.
\\ ( https://arxiv.org/abs/2509.18091 ,  3065kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16224 (*cross-listing*)
Date: Fri, 12 Sep 2025 09:32:02 GMT   (489kb)

Title: Predicting First Year Dropout from Pre Enrolment Motivation Statements
  Using Text Mining
Authors: K.F.B. Soppe, A. Bagheri, S. Nadi, I.G. Klugkist, T. Wubbels, L.D.N.V.
  Wijngaards-De Meij
Categories: cs.CY cs.CL cs.LG stat.AP
\\
  Preventing student dropout is a major challenge in higher education and it is
difficult to predict prior to enrolment which students are likely to drop out
and which students are likely to succeed. High School GPA is a strong predictor
of dropout, but much variance in dropout remains to be explained. This study
focused on predicting university dropout by using text mining techniques with
the aim of exhuming information contained in motivation statements written by
students. By combining text data with classic predictors of dropout in the form
of student characteristics, we attempt to enhance the available set of
predictive student characteristics. Our dataset consisted of 7,060 motivation
statements of students enrolling in a non-selective bachelor at a Dutch
university in 2014 and 2015. Support Vector Machines were trained on 75 percent
of the data and several models were estimated on the test data. We used various
combinations of student characteristics and text, such as TFiDF, topic
modelling, LIWC dictionary. Results showed that, although the combination of
text and student characteristics did not improve the prediction of dropout,
text analysis alone predicted dropout similarly well as a set of student
characteristics. Suggestions for future research are provided.
\\ ( https://arxiv.org/abs/2509.16224 ,  489kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16244 (*cross-listing*)
Date: Wed, 17 Sep 2025 08:18:58 GMT   (2973kb)

Title: How Can Quantum Deep Learning Improve Large Language Models?
Authors: Emily Jimin Roh, Hyojun Ahn, Samuel Yen-Chi Chen, Soohyun Park,
  Joongheon Kim
Categories: quant-ph cs.CL cs.LG
\\
  The rapid progress of large language models (LLMs) has transformed natural
language processing, yet the challenge of efficient adaptation remains
unresolved. Full fine-tuning achieves strong performance but imposes
prohibitive computational and memory costs. Parameter-efficient fine-tuning
(PEFT) strategies, such as low-rank adaptation (LoRA), Prefix tuning, and
sparse low-rank adaptation (SoRA), address this issue by reducing trainable
parameters while maintaining competitive accuracy. However, these methods often
encounter limitations in scalability, stability, and generalization across
diverse tasks. Recent advances in quantum deep learning introduce novel
opportunities through quantum-inspired encoding and parameterized quantum
circuits (PQCs). In particular, the quantum-amplitude embedded adaptation (QAA)
framework demonstrates expressive model updates with minimal overhead. This
paper presents a systematic survey and comparative analysis of conventional
PEFT methods and QAA. The analysis demonstrates trade-offs in convergence,
efficiency, and representational capacity, while providing insight into the
potential of quantum approaches for future LLM adaptation.
\\ ( https://arxiv.org/abs/2509.16244 ,  2973kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16378 (*cross-listing*)
Date: Fri, 19 Sep 2025 19:42:27 GMT   (1481kb)

Title: Longitudinal and Multimodal Recording System to Capture Real-World
  Patient-Clinician Conversations for AI and Encounter Research: Protocol
Authors: Misk Al Zahidy, Kerly Guevara Maldonado, Luis Vilatuna Andrango, Ana
  Cristina Proano, Ana Gabriela Claros, Maria Lizarazo Jimenez, David
  Toro-Tobon, Oscar J. Ponce-Ponce, and Juan P. Brito
Categories: cs.CY cs.CL
Comments: 23 pages, 2 figures, 2 tables
\\
  The promise of AI in medicine depends on learning from data that reflect what
matters to patients and clinicians. Most existing models are trained on
electronic health records (EHRs), which capture biological measures but rarely
patient-clinician interactions. These relationships, central to care, unfold
across voice, text, and video, yet remain absent from datasets. As a result, AI
systems trained solely on EHRs risk perpetuating a narrow biomedical view of
medicine and overlooking the lived exchanges that define clinical encounters.
Our objective is to design, implement, and evaluate the feasibility of a
longitudinal, multimodal system for capturing patient-clinician encounters,
linking 360 degree video/audio recordings with surveys and EHR data to create a
dataset for AI research. This single site study is in an academic outpatient
endocrinology clinic at Mayo Clinic. Adult patients with in-person visits to
participating clinicians are invited to enroll. Encounters are recorded with a
360 degree video camera. After each visit, patients complete a survey on
empathy, satisfaction, pace, and treatment burden. Demographic and clinical
data are extracted from the EHR. Feasibility is assessed using five endpoints:
clinician consent, patient consent, recording success, survey completion, and
data linkage across modalities. Recruitment began in January 2025. By August
2025, 35 of 36 eligible clinicians (97%) and 212 of 281 approached patients
(75%) had consented. Of consented encounters, 162 (76%) had complete recordings
and 204 (96%) completed the survey. This study aims to demonstrate the
feasibility of a replicable framework for capturing the multimodal dynamics of
patient-clinician encounters. By detailing workflows, endpoints, and ethical
safeguards, it provides a template for longitudinal datasets and lays the
foundation for AI models that incorporate the complexity of care.
\\ ( https://arxiv.org/abs/2509.16378 ,  1481kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16411 (*cross-listing*)
Date: Fri, 19 Sep 2025 20:35:58 GMT   (939kb)

Title: Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe
Authors: Chong You, Rajesh Jayaram, Ananda Theertha Suresh, Robin Nittka, Felix
  Yu, Sanjiv Kumar
Categories: cs.IR cs.CL cs.LG stat.ML
Comments: NeurIPS 2025
\\
  Dual encoder (DE) models, where a pair of matching query and document are
embedded into similar vector representations, are widely used in information
retrieval due to their simplicity and scalability. However, the Euclidean
geometry of the embedding space limits the expressive power of DEs, which may
compromise their quality. This paper investigates such limitations in the
context of hierarchical retrieval (HR), where the document set has a
hierarchical structure and the matching documents for a query are all of its
ancestors. We first prove that DEs are feasible for HR as long as the embedding
dimension is linear in the depth of the hierarchy and logarithmic in the number
of documents. Then we study the problem of learning such embeddings in a
standard retrieval setup where DEs are trained on samples of matching query and
document pairs. Our experiments reveal a lost-in-the-long-distance phenomenon,
where retrieval accuracy degrades for documents further away in the hierarchy.
To address this, we introduce a pretrain-finetune recipe that significantly
improves long-distance retrieval without sacrificing performance on closer
documents. We experiment on a realistic hierarchy from WordNet for retrieving
documents at various levels of abstraction, and show that pretrain-finetune
boosts the recall on long-distance pairs from 19% to 76%. Finally, we
demonstrate that our method improves retrieval of relevant products on a
shopping queries dataset.
\\ ( https://arxiv.org/abs/2509.16411 ,  939kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16442 (*cross-listing*)
Date: Fri, 19 Sep 2025 21:43:08 GMT   (9473kb)

Title: Evaluating the Effectiveness and Scalability of LLM-Based Data
  Augmentation for Retrieval
Authors: Pranjal A. Chitale, Bishal Santra, Yashoteja Prabhu, Amit Sharma
Categories: cs.IR cs.CL
Comments: EMNLP 2025 (MAIN Conference)
\\
  Compact dual-encoder models are widely used for retrieval owing to their
efficiency and scalability. However, such models often underperform compared to
their Large Language Model (LLM)-based retrieval counterparts, likely due to
their limited world knowledge. While LLM-based data augmentation has been
proposed as a strategy to bridge this performance gap, there is insufficient
understanding of its effectiveness and scalability to real-world retrieval
problems. Existing research does not systematically explore key factors such as
the optimal augmentation scale, the necessity of using large augmentation
models, and whether diverse augmentations improve generalization, particularly
in out-of-distribution (OOD) settings. This work presents a comprehensive study
of the effectiveness of LLM augmentation for retrieval, comprising over 100
distinct experimental settings of retrieval models, augmentation models and
augmentation strategies. We find that, while augmentation enhances retrieval
performance, its benefits diminish beyond a certain augmentation scale, even
with diverse augmentation strategies. Surprisingly, we observe that
augmentation with smaller LLMs can achieve performance competitive with larger
augmentation models. Moreover, we examine how augmentation effectiveness varies
with retrieval model pre-training, revealing that augmentation provides the
most benefit to models which are not well pre-trained. Our insights pave the
way for more judicious and efficient augmentation strategies, thus enabling
informed decisions and maximizing retrieval performance while being more
cost-effective. Code and augmented datasets accompanying this work are publicly
available at https://aka.ms/DAGR.
\\ ( https://arxiv.org/abs/2509.16442 ,  9473kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16446 (*cross-listing*)
Date: Fri, 19 Sep 2025 21:59:55 GMT   (193kb)

Title: Purely Semantic Indexing for LLM-based Generative Recommendation and
  Retrieval
Authors: Ruohan Zhang, Jiacheng Li, Julian McAuley, Yupeng Hou
Categories: cs.IR cs.CL
\\
  Semantic identifiers (IDs) have proven effective in adapting large language
models for generative recommendation and retrieval. However, existing methods
often suffer from semantic ID conflicts, where semantically similar documents
(or items) are assigned identical IDs. A common strategy to avoid conflicts is
to append a non-semantic token to distinguish them, which introduces randomness
and expands the search space, therefore hurting performance. In this paper, we
propose purely semantic indexing to generate unique, semantic-preserving IDs
without appending non-semantic tokens. We enable unique ID assignment by
relaxing the strict nearest-centroid selection and introduce two model-agnostic
algorithms: exhaustive candidate matching (ECM) and recursive residual
searching (RRS). Extensive experiments on sequential recommendation, product
search, and document retrieval tasks demonstrate that our methods improve both
overall and cold-start performance, highlighting the effectiveness of ensuring
ID uniqueness.
\\ ( https://arxiv.org/abs/2509.16446 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16475 (*cross-listing*)
Date: Sat, 20 Sep 2025 00:06:53 GMT   (81kb)

Title: Towards Universal Debiasing for Language Models-based Tabular Data
  Generation
Authors: Tianchun Li, Tianci Liu, Xingchen Wang, Rongzhe Wei, Pan Li, Lu Su,
  Jing Gao
Categories: cs.LG cs.CL
Comments: EMNLP 2025 Findings
\\
  Large language models (LLMs) have achieved promising results in tabular data
generation. However, inherent historical biases in tabular datasets often cause
LLMs to exacerbate fairness issues, particularly when multiple advantaged and
protected features are involved. In this work, we introduce a universal
debiasing framework that minimizes group-level dependencies by simultaneously
reducing the mutual information between advantaged and protected attributes. By
leveraging the autoregressive structure and analytic sampling distributions of
LLM-based tabular data generators, our approach efficiently computes mutual
information, reducing the need for cumbersome numerical estimations. Building
on this foundation, we propose two complementary methods: a direct preference
optimization (DPO)-based strategy, namely UDF-DPO, that integrates seamlessly
with existing models, and a targeted debiasing technique, namely UDF-MIX, that
achieves debiasing without tuning the parameters of LLMs. Extensive experiments
demonstrate that our framework effectively balances fairness and utility,
offering a scalable and practical solution for debiasing in high-stakes
applications.
\\ ( https://arxiv.org/abs/2509.16475 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16539 (*cross-listing*)
Date: Sat, 20 Sep 2025 05:05:34 GMT   (85kb)

Title: Long document summarization using page specific target text alignment
  and distilling page importance
Authors: Pushpa Devi, Ayush Agrawal, Ashutosh Dubey, C. Ravindranath Chowdary
Categories: cs.IR cs.CL
Comments: 8 pages, 2 figures
\\
  The rapid growth of textual data across news, legal, medical, and scientific
domains is becoming a challenge for efficiently accessing and understanding
large volumes of content. It is increasingly complex for users to consume and
extract meaningful information efficiently. Thus, raising the need for
summarization. Unlike short document summarization, long document abstractive
summarization is resource-intensive, and very little literature is present in
this direction. BART is a widely used efficient sequence-to-sequence
(seq-to-seq) model. However, when it comes to summarizing long documents, the
length of the context window limits its capabilities. We proposed a model
called PTS (Page-specific Target-text alignment Summarization) that extends the
seq-to-seq method for abstractive summarization by dividing the source document
into several pages. PTS aligns each page with the relevant part of the target
summary for better supervision. Partial summaries are generated for each page
of the document. We proposed another model called PTSPI (Page-specific
Target-text alignment Summarization with Page Importance), an extension to PTS
where an additional layer is placed before merging the partial summaries into
the final summary. This layer provides dynamic page weightage and explicit
supervision to focus on the most informative pages. We performed experiments on
the benchmark dataset and found that PTSPI outperformed the SOTA by 6.32\% in
ROUGE-1 and 8.08\% in ROUGE-2 scores.
\\ ( https://arxiv.org/abs/2509.16539 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16548 (*cross-listing*)
Date: Sat, 20 Sep 2025 06:19:55 GMT   (563kb)

Title: SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward
  Learning
Authors: Yuyang Ding, Xinyu Shi, Juntao Li, Xiaobo Liang, Zhaopeng Tu, Min
  Zhang
Categories: cs.LG cs.CL
Comments: NeurIPS 2025. Project page: https://scan-prm.github.io/
\\
  Process reward models (PRMs) offer fine-grained, step-level evaluations that
facilitate deeper reasoning processes in large language models (LLMs), proving
effective in complex tasks like mathematical reasoning. However, developing
PRMs is challenging due to the high cost and limited scalability of
human-annotated data. Synthetic data from Monte Carlo (MC) estimation is a
promising alternative but suffers from a high noise ratio, which can cause
overfitting and hinder large-scale training. In this work, we conduct a
preliminary study on the noise distribution in synthetic data from MC
estimation, identifying that annotation models tend to both underestimate and
overestimate step correctness due to limitations in their annotation
capabilities. Building on these insights, we propose Self-Denoising Monte Carlo
Annotation (SCAN), an efficient data synthesis and noise-tolerant learning
framework. Our key findings indicate that: (1) Even lightweight models (e.g.,
1.5B parameters) can produce high-quality annotations through a self-denoising
strategy, enabling PRMs to achieve superior performance with only 6% the
inference cost required by vanilla MC estimation. (2) With our robust learning
strategy, PRMs can effectively learn from this weak supervision, achieving a
39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using
only a compact synthetic dataset, our models surpass strong baselines,
including those trained on large-scale human-annotated datasets such as
PRM800K. Furthermore, performance continues to improve as we scale up the
synthetic data, highlighting the potential of SCAN for scalable,
cost-efficient, and robust PRM training.
\\ ( https://arxiv.org/abs/2509.16548 ,  563kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16621 (*cross-listing*)
Date: Sat, 20 Sep 2025 10:44:26 GMT   (27kb)

Title: The Role of Vocabularies in Learning Sparse Representations for Ranking
Authors: Hiun Kim, Tae Kwan Lee, Taeryun Won
Categories: cs.IR cs.CL
\\
  Learned Sparse Retrieval (LSR) such as SPLADE has growing interest for
effective semantic 1st stage matching while enjoying the efficiency of inverted
indices. A recent work on learning SPLADE models with expanded vocabularies
(ESPLADE) was proposed to represent queries and documents into a sparse space
of custom vocabulary which have different levels of vocabularic granularity.
Within this effort, however, there have not been many studies on the role of
vocabulary in SPLADE models and their relationship to retrieval efficiency and
effectiveness.
  To study this, we construct BERT models with 100K-sized output vocabularies,
one initialized with the ESPLADE pretraining method and one initialized
randomly. After finetune on real-world search click logs, we applied logit
score-based queries and documents pruning to max size for further balancing
efficiency. The experimental result in our evaluation set shows that, when
pruning is applied, the two models are effective compared to the 32K-sized
normal SPLADE model in the computational budget under the BM25. And the ESPLADE
models are more effective than the random vocab model, while having a similar
retrieval cost.
  The result indicates that the size and pretrained weight of output
vocabularies play the role of configuring the representational specification
for queries, documents, and their interactions in the retrieval engine, beyond
their original meaning and purposes in NLP. These findings can provide a new
room for improvement for LSR by identifying the importance of representational
specification from vocabulary configuration for efficient and effective
retrieval.
\\ ( https://arxiv.org/abs/2509.16621 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16718 (*cross-listing*)
Date: Sat, 20 Sep 2025 15:04:33 GMT   (322kb)

Title: Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition:
  Dysarthric Case Studies
Authors: Vishnu Raja, Adithya V Ganesan, Anand Syamkumar, Ritwik Banerjee, H
  Andrew Schwartz
Categories: cs.SD cs.CL eess.AS
Comments: Will appear in EMNLP 2025 Main Proceedings
\\
  State-of-the-art automatic speech recognition (ASR) models like Whisper,
perform poorly on atypical speech, such as that produced by individuals with
dysarthria. Past works for atypical speech have mostly investigated fully
personalized (or idiosyncratic) models, but modeling strategies that can both
generalize and handle idiosyncracy could be more effective for capturing
atypical speech. To investigate this, we compare four strategies: (a)
$\textit{normative}$ models trained on typical speech (no personalization), (b)
$\textit{idiosyncratic}$ models completely personalized to individuals, (c)
$\textit{dysarthric-normative}$ models trained on other dysarthric speakers,
and (d) $\textit{dysarthric-idiosyncratic}$ models which combine strategies by
first modeling normative patterns before adapting to individual speech. In this
case study, we find the dysarthric-idiosyncratic model performs better than
idiosyncratic approach while requiring less than half as much personalized data
(36.43 WER with 128 train size vs 36.99 with 256). Further, we found that
tuning the speech encoder alone (as opposed to the LM decoder) yielded the best
results reducing word error rate from 71% to 32% on average. Our findings
highlight the value of leveraging both normative (cross-speaker) and
idiosyncratic (speaker-specific) patterns to improve ASR for underrepresented
speech populations.
\\ ( https://arxiv.org/abs/2509.16718 ,  322kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16893 (*cross-listing*)
Date: Sun, 21 Sep 2025 03:06:58 GMT   (7472kb)

Title: DRES: Fake news detection by dynamic representation and ensemble
  selection
Authors: Faramarz Farhangian, Leandro A. Ensina, George D. C. Cavalcanti,
  Rafael M. O. Cruz
Categories: cs.LG cs.CL
Comments: Accepted as oral presentation at EMNLP 2025
\\
  The rapid spread of information via social media has made text-based fake
news detection critically important due to its societal impact. This paper
presents a novel detection method called Dynamic Representation and Ensemble
Selection (DRES) for identifying fake news based solely on text. DRES leverages
instance hardness measures to estimate the classification difficulty for each
news article across multiple textual feature representations. By dynamically
selecting the textual representation and the most competent ensemble of
classifiers for each instance, DRES significantly enhances prediction accuracy.
Extensive experiments show that DRES achieves notable improvements over
state-of-the-art methods, confirming the effectiveness of representation
selection based on instance hardness and dynamic ensemble selection in boosting
performance. Codes and data are available at:
https://github.com/FFarhangian/FakeNewsDetection_DRES
\\ ( https://arxiv.org/abs/2509.16893 ,  7472kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16941 (*cross-listing*)
Date: Sun, 21 Sep 2025 06:28:17 GMT   (2099kb)

Title: SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering
  Tasks?
Authors: Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak
  Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, Karmini
  Sampath, Maya Krishnan, Srivatsa Kundurthy, Sean Hendryx, Zifan Wang, Chen Bo
  Calvin Zhang, Noah Jacobson, Bing Liu, Brad Kenstler
Categories: cs.SE cs.CL
\\
  We introduce SWE-Bench Pro, a substantially more challenging benchmark that
builds upon the best practices of SWE-BENCH [25], but is explicitly designed to
capture realistic, complex, enterprise-level problems beyond the scope of
SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of
41 actively maintained repositories spanning business applications, B2B
services, and developer tools. The benchmark is partitioned into a public set
with open access to problems sourced from 11 repositories, a held-out set of 12
repositories and a commercial set of 18 proprietary repositories where we have
formal partnership agreements with early-stage startups. Problems in the
held-out and the commercial set are not publicly accessible, but we release
results on the commercial set. Our benchmark features long-horizon tasks that
may require hours to days for a professional software engineer to complete,
often involving patches across multiple files and substantial code
modifications. All tasks are human-verified and augmented with sufficient
context to ensure resolvability. In our evaluation of widely used coding
models, under a unified scaffold, we observe that their performance on
SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest
score to date at 23.3%. To better understand these limitations, we cluster the
failure modes observed in the collected agent trajectories for a clearer
characterization of the error patterns exhibited by current models. Overall,
SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully
captures the complexity and diversity of real-world software development,
advancing the pursuit of truly autonomous software engineering agents at a
professional level.
\\ ( https://arxiv.org/abs/2509.16941 ,  2099kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17070 (*cross-listing*)
Date: Sun, 21 Sep 2025 13:01:38 GMT   (1746kb)

Title: Localizing Malicious Outputs from CodeLLM
Authors: Mayukh Borana, Junyi Liang, Sai Sathiesh Rajan, and Sudipta
  Chattopadhyay
Categories: cs.CR cs.CL cs.LG
Comments: 10 pages, 2 figures, 6 tables, Accepted at EMNLP 2025 Findings
\\
  We introduce FreqRank, a mutation-based defense to localize malicious
components in LLM outputs and their corresponding backdoor triggers. FreqRank
assumes that the malicious sub-string(s) consistently appear in outputs for
triggered inputs and uses a frequency-based ranking system to identify them.
Our ranking system then leverages this knowledge to localize the backdoor
triggers present in the inputs. We create nine malicious models through
fine-tuning or custom instructions for three downstream tasks, namely, code
completion (CC), code generation (CG), and code summarization (CS), and show
that they have an average attack success rate (ASR) of 86.6%. Furthermore,
FreqRank's ranking system highlights the malicious outputs as one of the top
five suggestions in 98% of cases. We also demonstrate that FreqRank's
effectiveness scales as the number of mutants increases and show that FreqRank
is capable of localizing the backdoor trigger effectively even with a limited
number of triggered samples. Finally, we show that our approach is 35-50% more
effective than other defense methods.
\\ ( https://arxiv.org/abs/2509.17070 ,  1746kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17091 (*cross-listing*)
Date: Sun, 21 Sep 2025 14:11:16 GMT   (291kb)

Title: SVeritas: Benchmark for Robust Speaker Verification under Diverse
  Conditions
Authors: Massa Baali, Sarthak Bisht, Francisco Teixeira, Kateryna Shapovalenko,
  Rita Singh, Bhiksha Raj
Categories: cs.SD cs.CL
Comments: Accepted to EMNLP 2025 Findings
\\
  Speaker verification (SV) models are increasingly integrated into security,
personalization, and access control systems, yet their robustness to many
real-world challenges remains inadequately benchmarked. These include a variety
of natural and maliciously created conditions causing signal degradations or
mismatches between enrollment and test data, impacting performance. Existing
benchmarks evaluate only subsets of these conditions, missing others entirely.
We introduce SVeritas, a comprehensive Speaker Verification tasks benchmark
suite, assessing SV systems under stressors like recording duration,
spontaneity, content, noise, microphone distance, reverberation, channel
mismatches, audio bandwidth, codecs, speaker age, and susceptibility to
spoofing and adversarial attacks. While several benchmarks do exist that each
cover some of these issues, SVeritas is the first comprehensive evaluation that
not only includes all of these, but also several other entirely new, but
nonetheless important, real-life conditions that have not previously been
benchmarked. We use SVeritas to evaluate several state-of-the-art SV models and
observe that while some architectures maintain stability under common
distortions, they suffer substantial performance degradation in scenarios
involving cross-language trials, age mismatches, and codec-induced compression.
Extending our analysis across demographic subgroups, we further identify
disparities in robustness across age groups, gender, and linguistic
backgrounds. By standardizing evaluation under realistic and synthetic stress
conditions, SVeritas enables precise diagnosis of model weaknesses and
establishes a foundation for advancing equitable and reliable speaker
verification systems.
\\ ( https://arxiv.org/abs/2509.17091 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17228 (*cross-listing*)
Date: Sun, 21 Sep 2025 20:34:52 GMT   (229kb)

Title: Causal Representation Learning from Multimodal Clinical Records under
  Non-Random Modality Missingness
Authors: Zihan Liang, Ziwen Pan, Ruoxuan Xiong
Categories: cs.LG cs.CL stat.ME
Comments: To appear in Proc. of EMNLP 2025 (18 pages)
\\
  Clinical notes contain rich patient information, such as diagnoses or
medications, making them valuable for patient representation learning. Recent
advances in large language models have further improved the ability to extract
meaningful representations from clinical texts. However, clinical notes are
often missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of
patients have no available discharge summaries. In such cases, representations
can be learned from other modalities such as structured data, chest X-rays, or
radiology reports. Yet the availability of these modalities is influenced by
clinical decision-making and varies across patients, resulting in modality
missing-not-at-random (MMNAR) patterns. We propose a causal representation
learning framework that leverages observed data and informative missingness in
multimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion
component that integrates structured data, imaging, and text while conditioning
on missingness patterns to capture patient health and clinician-driven
assignment; (2) a modality reconstruction component with contrastive learning
to ensure semantic sufficiency in representation learning; and (3) a multitask
outcome prediction model with a rectifier that corrects for residual bias from
specific modality observation patterns. Comprehensive evaluations across
MIMIC-IV and eICU show consistent gains over the strongest baselines, achieving
up to 13.8% AUC improvement for hospital readmission and 13.1% for ICU
admission.
\\ ( https://arxiv.org/abs/2509.17228 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17321 (*cross-listing*)
Date: Mon, 22 Sep 2025 02:52:55 GMT   (6915kb)

Title: OpenGVL - Benchmarking Visual Temporal Progress for Data Curation
Authors: Pawe{\l} Budzianowski, Emilia Wi\'snios, Gracjan G\'oral, Igor
  Kulakov, Viktor Petrenko, Krzysztof Walas
Categories: cs.RO cs.CL
Journal-ref: Workshop on Making Sense of Data in Robotics: Composition,
  Curation, and Interpretability at Scale at CoRL 2025
\\
  Data scarcity remains one of the most limiting factors in driving progress in
robotics. However, the amount of available robotics data in the wild is growing
exponentially, creating new opportunities for large-scale data utilization.
Reliable temporal task completion prediction could help automatically annotate
and curate this data at scale. The Generative Value Learning (GVL) approach was
recently proposed, leveraging the knowledge embedded in vision-language models
(VLMs) to predict task progress from visual observations. Building upon GVL, we
propose OpenGVL, a comprehensive benchmark for estimating task progress across
diverse challenging manipulation tasks involving both robotic and human
embodiments. We evaluate the capabilities of publicly available open-source
foundation models, showing that open-source model families significantly
underperform closed-source counterparts, achieving only approximately $70\%$ of
their performance on temporal progress prediction tasks. Furthermore, we
demonstrate how OpenGVL can serve as a practical tool for automated data
curation and filtering, enabling efficient quality assessment of large-scale
robotics datasets. We release the benchmark along with the complete codebase at
\href{github.com/budzianowski/opengvl}{OpenGVL}.
\\ ( https://arxiv.org/abs/2509.17321 ,  6915kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17336 (*cross-listing*)
Date: Mon, 22 Sep 2025 03:13:58 GMT   (3385kb)

Title: Mano Report
Authors: Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu,
  Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, Yuyang Chen, Ruiyang Yu, Siran
  Peng, Menglin Li, Nan Huang, Haitian Wei, Jiawei Yu, Yi Xin, Xilin Zhao, Kai
  Gu, Ping Jiang, Sifan Zhou, Shuo Wang
Categories: cs.MM cs.CL cs.CV
\\
  Graphical user interfaces (GUIs) are the primary medium for human-computer
interaction, yet automating GUI interactions remains challenging due to the
complexity of visual elements, dynamic environments, and the need for
multi-step reasoning. Existing methods based on vision-language models (VLMs)
often suffer from limited resolution, domain mismatch, and insufficient
sequential decisionmaking capability. To address these issues, we propose Mano,
a robust GUI agent built upon a multi-modal foundation model pre-trained on
extensive web and computer system data. Our approach integrates a novel
simulated environment for high-fidelity data generation, a three-stage training
pipeline (supervised fine-tuning, offline reinforcement learning, and online
reinforcement learning), and a verification module for error recovery. Mano
demonstrates state-of-the-art performance on multiple GUI benchmarks, including
Mind2Web and OSWorld, achieving significant improvements in success rate and
operational accuracy. Our work provides new insights into the effective
integration of reinforcement learning with VLMs for practical GUI agent
deployment, highlighting the importance of domain-specific data, iterative
training, and holistic reward design.
\\ ( https://arxiv.org/abs/2509.17336 ,  3385kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17730 (*cross-listing*)
Date: Mon, 22 Sep 2025 13:00:35 GMT   (75kb)

Title: ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement
  Learning in LLMs
Authors: Bonan Zhang, Zhongqi Chen, Bowen Song, Qinya Li, Fan Wu, Guihai Chen
Categories: cs.LG cs.CL
\\
  Reinforcement learning (RL) has become a standard paradigm for refining large
language models (LLMs) beyond pre-training and instruction tuning. A prominent
line of work is RL with verifiable rewards (RLVR), which leverages
automatically verifiable outcomes (e.g., correctness or executability) to
generate reward signals. While efficient, this framework faces two key
limitations: First, its binary feedback is too sparse to capture the quality of
the reasoning process. Second, its coarse-grained rewards potentially lead to
vanishing gradients. Inspired by observations from human learning, we introduce
a RL technique that integrates verifiable outcomes with the model's own
confidence estimates. This joint design enriches the reward signal, providing
finer-grained feedback and implicitly supervising the reasoning process.
Experimental results demonstrate that our proposed method enhances RL
performance across multiple datasets and reduces token consumption during
inference, while incurring negligible additional training cost. Moreover, it
can be used as a plug-in module to enhance other state-of-the-art RL methods.
\\ ( https://arxiv.org/abs/2509.17730 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18095 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:59:42 GMT   (852kb)

Title: MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late
  Interaction
Authors: Zilin Xiao, Qi Ma, Mengting Gu, Chun-cheng Jason Chen, Xintao Chen,
  Vicente Ordonez, Vijai Mohan
Categories: cs.IR cs.CL cs.CV
\\
  Universal multimodal embedding models have achieved great success in
capturing semantic relevance between queries and candidates. However, current
methods either condense queries and candidates into a single vector,
potentially limiting the expressiveness for fine-grained information, or
produce too many vectors that are prohibitively expensive for multi-vector
retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal
retrieval that rethinks how multimodal embeddings are constructed and
interacted with at scale. During training, a fixed number of learnable Meta
Tokens are appended to the input sequence. At test-time, their last-layer
contextualized representations serve as compact yet expressive multi-vector
embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,
MetaEmbed learns to organize information by granularity across multiple
vectors. As a result, we enable test-time scaling in multimodal retrieval,
where users can balance retrieval quality against efficiency demands by
selecting the number of tokens used for indexing and retrieval interactions.
Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and
the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed
achieves state-of-the-art retrieval performance while scaling robustly to
models with 32B parameters.
\\ ( https://arxiv.org/abs/2509.18095 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16223 (*cross-listing*)
Date: Thu, 11 Sep 2025 16:34:12 GMT   (6281kb)

Title: MRADNET: a Compact Radar Object Detector with MetaFormer
Authors: Huaiyu Chen, Fahed Hassanat, Robert Laganiere, Martin Bouchard
Categories: eess.SP cs.CV
Comments: 5 pages, 2 figures, submitted to IEEE Icassp 2026
\\
  Frequency-modulated continuous wave radars have gained increasing popularity
in the automotive industry. Its robustness against adverse weather conditions
makes it a suitable choice for radar object detection in advanced driver
assistance systems. These real-time embedded systems have requirements for the
compactness and efficiency of the model, which have been largely overlooked in
previous work. In this work, we propose mRadNet, a novel radar object detection
model with compactness in mind. mRadNet employs a U-net style architecture with
MetaFormer blocks, in which separable convolution and attention token mixers
are used to capture both local and global features effectively. More efficient
token embedding and merging strategies are introduced to further facilitate the
lightweight design of the model. The performance of mRadNet is validated on the
CRUW dataset, improving state-of-the-art performance.
\\ ( https://arxiv.org/abs/2509.16223 ,  6281kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16336 (*cross-listing*)
Date: Fri, 19 Sep 2025 18:24:41 GMT   (43701kb)

Title: Neural Atlas Graphs for Dynamic Scene Decomposition and Editing
Authors: Jan Philipp Schneider and Pratik Singh Bisht and Ilya Chugunov and
  Andreas Kolb and Michael Moeller and Felix Heide
Categories: cs.GR cs.CV cs.LG
\\
  Learning editable high-resolution scene representations for dynamic scenes is
an open problem with applications across the domains from autonomous driving to
creative editing - the most successful approaches today make a trade-off
between editability and supporting scene complexity: neural atlases represent
dynamic scenes as two deforming image layers, foreground and background, which
are editable in 2D, but break down when multiple objects occlude and interact.
In contrast, scene graph models make use of annotated data such as masks and
bounding boxes from autonomous-driving datasets to capture complex 3D spatial
relationships, but their implicit volumetric node representations are
challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a
hybrid high-resolution scene representation, where every graph node is a
view-dependent neural atlas, facilitating both 2D appearance editing and 3D
ordering and positioning of scene elements. Fit at test-time, NAGs achieve
state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR
increase compared to existing methods - and make environmental editing possible
in high resolution and visual quality - creating counterfactual driving
scenarios with new backgrounds and edited vehicle appearance. We find that the
method also generalizes beyond driving scenes and compares favorably - by more
than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS
video dataset with a diverse set of human and animal-centric scenes.
\\ ( https://arxiv.org/abs/2509.16336 ,  43701kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16471 (*cross-listing*)
Date: Fri, 19 Sep 2025 23:46:08 GMT   (9648kb)

Title: From Coated to Uncoated: Scanning Electron Microscopy Corrections to
  Estimate True Surface Pore Size in Nanoporous Membranes
Authors: Sima Zeinali Danalou, Dian Yu, Niher R. Sarker, Hooman Chamani, Jane
  Y. Howe, Patrick C. Lee, and Jay R. Werber
Categories: cond-mat.mtrl-sci cs.CV physics.app-ph physics.chem-ph physics.ins-det
\\
  Scanning electron microscopy (SEM) is the premier method for characterizing
the nanoscale surface pores in ultrafiltration (UF) membranes and the support
layers of reverse osmosis (RO) membranes. Based on SEM, the conventional
understanding is that membranes typically have low surface porosities of <10%.
We hypothesized that high acceleration voltage during SEM imaging and sputter
metal coatings required for SEM have led to systematic underestimations of
porosity and pore size. We showed that imaging a commercial UF membrane at 1,
5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while
increasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for
the UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To
account for coating thickness, we developed a digital correction method that
simulates pore dilation, enabling the pore structure to be estimated for
uncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF
membrane and 20% for the RO support, about 3-fold greater than values observed
with a 4 nm coating. Mean pore diameters were 2-fold greater for the UF
membrane and 1.5-fold greater for the RO support. Critically, dilation-derived
pore-size distributions agreed with low-flux dextran-retention data fitted with
the Bungay-Brenner model. Our results suggest that surface porosities and pore
sizes of nanoporous membranes are much larger than previously understood, with
major implications for structure/transport relationships. For future nanoscale
pore analysis of membranes (and other nanoporous materials), we recommend low
acceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to
account for coating artifacts
\\ ( https://arxiv.org/abs/2509.16471 ,  9648kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16473 (*cross-listing*)
Date: Fri, 19 Sep 2025 23:59:43 GMT   (15730kb)

Title: The Iconicity of the Generated Image
Authors: Nanne van Noord, Noa Garcia
Categories: cs.CY cs.CV
Comments: Work presented at EA-AI 2025, May 2025, Venice
\\
  How humans interpret and produce images is influenced by the images we have
been exposed to. Similarly, visual generative AI models are exposed to many
training images and learn to generate new images based on this. Given the
importance of iconic images in human visual communication, as they are widely
seen, reproduced, and used as inspiration, we may expect that they may
similarly have a proportionally large influence within the generative AI
process. In this work we explore this question through a three-part analysis,
involving data attribution, semantic similarity analysis, and a user-study. Our
findings indicate that iconic images do not have an obvious influence on the
generative process, and that for many icons it is challenging to reproduce an
image which resembles it closely. This highlights an important difference in
how humans and visual generative AI models draw on and learn from prior visual
communication.
\\ ( https://arxiv.org/abs/2509.16473 ,  15730kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16554 (*cross-listing*)
Date: Sat, 20 Sep 2025 06:48:45 GMT   (36312kb)

Title: ViTCAE: ViT-based Class-conditioned Autoencoder
Authors: Vahid Jebraeeli, Hamid Krim, Derya Cansever
Categories: cs.LG cs.CV
Comments: -
\\
  Vision Transformer (ViT) based autoencoders often underutilize the global
Class token and employ static attention mechanisms, limiting both generative
control and optimization efficiency. This paper introduces ViTCAE, a framework
that addresses these issues by re-purposing the Class token into a generative
linchpin. In our architecture, the encoder maps the Class token to a global
latent variable that dictates the prior distribution for local, patch-level
latent variables, establishing a robust dependency where global semantics
directly inform the synthesis of local details. Drawing inspiration from
opinion dynamics, we treat each attention head as a dynamical system of
interacting tokens seeking consensus. This perspective motivates a
convergence-aware temperature scheduler that adaptively anneals each head's
influence function based on its distributional stability. This process enables
a principled head-freezing mechanism, guided by theoretically-grounded
diagnostics like an attention evolution distance and a consensus/cluster
functional. This technique prunes converged heads during training to
significantly improve computational efficiency without sacrificing fidelity. By
unifying a generative Class token with an adaptive attention mechanism rooted
in multi-agent consensus theory, ViTCAE offers a more efficient and
controllable approach to transformer-based generation.
\\ ( https://arxiv.org/abs/2509.16554 ,  36312kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16580 (*cross-listing*)
Date: Sat, 20 Sep 2025 08:58:08 GMT   (10338kb)

Title: Fusing Spectral Correlation Density Imaging with Deep Learning for
  Intelligent Fault Diagnosis in Rotating Machinery
Authors: Dilshara Herath, Chinthaka Abeyrathne, Chamindu Adithya, Chathura
  Seneviratne
Categories: eess.SP cs.CV
\\
  Bearing fault diagnosis in rotating machinery is critical for ensuring
operational reliability, therefore early fault detection is essential to avoid
catastrophic failures and expensive emergency repairs. Traditional methods like
Fast Fourier Transform (FFT) often fail to capture the complex, non-stationary
nature of vibration signals. This study leverages the cyclostationary
properties of vibration data through Spectral Correlation Density (SCD) images
to enhance fault detection and apply deep learning for classification. Using a
publicly available dataset with bearing faults seeded in two distinct housings
(A and B) under varying load conditions (0 Nm, 2 Nm, 4 Nm), we processed
vibration signals into 2D SCD images to reveal fault-specific periodicities,
such as broadband spectra (2000--8000 Hz) for larger faults. Three
convolutional neural network (CNN) models, Custom CNN, ResNet152V2, and
EfficientNetB0, were developed to classify seven bearing conditions. The custom
CNN achieved the highest accuracies of 96.58\% and 94.95\% on Housing A and B,
respectively, followed by ResNet152V2 at 96.49\% and 95.35\%, and
EfficientNetB0 at 94.16\% and 91.65\%, respectively. The models' high
accuracies across different housings demonstrate a robust solution suitable for
cost-effective condition monitoring deployable near sensing platforms,
contributing to applied machine learning for edge intelligence and showcasing
effective signal processing strategies for handling complex, potentially
large-scale vibration data.
\\ ( https://arxiv.org/abs/2509.16580 ,  10338kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16814 (*cross-listing*)
Date: Sat, 20 Sep 2025 21:33:12 GMT   (1435kb)

Title: Development of a Mobile Application for at-Home Analysis of Retinal
  Fundus Images
Authors: Mattea Reid, Zuhairah Zainal, Khaing Zin Than, Danielle Chan, and
  Jonathan Chan
Categories: cs.HC cs.CV
Comments: 5 pages, 4 figures
\\
  Machine learning is gaining significant attention as a diagnostic tool in
medical imaging, particularly in the analysis of retinal fundus images.
However, this approach is not yet clinically applicable, as it still depends on
human validation from a professional. Therefore, we present the design for a
mobile application that monitors metrics related to retinal fundus images
correlating to age-related conditions. The purpose of this platform is to
observe for a change in these metrics over time, offering early insights into
potential ocular diseases without explicitly delivering diagnostics. Metrics
analysed include vessel tortuosity, as well as signs of glaucoma, retinopathy
and macular edema. To evaluate retinopathy grade and risk of macular edema, a
model was trained on the Messidor dataset and compared to a similar model
trained on the MAPLES-DR dataset. Information from the DeepSeeNet glaucoma
detection model, as well as tortuosity calculations, is additionally
incorporated to ultimately present a retinal fundus image monitoring platform.
As a result, the mobile application permits monitoring of trends or changes in
ocular metrics correlated to age-related conditions with regularly uploaded
photographs.
\\ ( https://arxiv.org/abs/2509.16814 ,  1435kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16833 (*cross-listing*)
Date: Sat, 20 Sep 2025 23:15:28 GMT   (9692kb)

Title: SOLAR: Switchable Output Layer for Accuracy and Robustness in
  Once-for-All Training
Authors: Shaharyar Ahmed Khan Tareen, Lei Fan, Xiaojing Yuan, Qin Lin, Bin Hu
Categories: cs.LG cs.CV
Comments: 10 pages, 7 figures, 6 tables
\\
  Once-for-All (OFA) training enables a single super-net to generate multiple
sub-nets tailored to diverse deployment scenarios, supporting flexible
trade-offs among accuracy, robustness, and model-size without retraining.
However, as the number of supported sub-nets increases, excessive parameter
sharing in the backbone limits representational capacity, leading to degraded
calibration and reduced overall performance. To address this, we propose SOLAR
(Switchable Output Layer for Accuracy and Robustness in Once-for-All Training),
a simple yet effective technique that assigns each sub-net a separate
classification head. By decoupling the logit learning process across sub-nets,
the Switchable Output Layer (SOL) reduces representational interference and
improves optimization, without altering the shared backbone. We evaluate SOLAR
on five datasets (SVHN, CIFAR-10, STL-10, CIFAR-100, and TinyImageNet) using
four super-net backbones (ResNet-34, WideResNet-16-8, WideResNet-40-2, and
MobileNetV2) for two OFA training frameworks (OATS and SNNs). Experiments show
that SOLAR outperforms the baseline methods: compared to OATS, it improves
accuracy of sub-nets up to 1.26 %, 4.71 %, 1.67 %, and 1.76 %, and robustness
up to 9.01 %, 7.71 %, 2.72 %, and 1.26 % on SVHN, CIFAR-10, STL-10, and
CIFAR-100, respectively. Compared to SNNs, it improves TinyImageNet accuracy by
up to 2.93 %, 2.34 %, and 1.35 % using ResNet-34, WideResNet-16-8, and
MobileNetV2 backbones (with 8 sub-nets), respectively.
\\ ( https://arxiv.org/abs/2509.16833 ,  9692kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16875 (*cross-listing*)
Date: Sun, 21 Sep 2025 01:57:13 GMT   (31003kb)

Title: Towards Interpretable and Efficient Attention: Compressing All by
  Contracting a Few
Authors: Qishuai Wen, Zhiyuan Huang, Chun-Guang Li
Categories: cs.LG cs.CV
Comments: NeurIPS 2025 Spotlight
\\
  Attention mechanisms in Transformers have gained significant empirical
success. Nonetheless, the optimization objectives underlying their forward pass
are still unclear. Additionally, the quadratic complexity of self-attention is
increasingly prohibitive. Unlike the prior work on addressing the
interpretability or efficiency issue separately, we propose a unified
optimization objective to alleviate both issues simultaneously. By unrolling
the optimization over the objective, we derive an inherently interpretable and
efficient attention mechanism, which compresses all tokens into low-dimensional
structures by contracting a few representative tokens and then broadcasting the
contractions back. This Contract-and-Broadcast Self-Attention (CBSA) mechanism
can not only scale linearly but also generalize existing attention mechanisms
as its special cases. Experiments further demonstrate comparable performance
and even superior advantages of CBSA on several visual tasks. Code is available
at this https URL.
\\ ( https://arxiv.org/abs/2509.16875 ,  31003kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17022 (*cross-listing*)
Date: Sun, 21 Sep 2025 10:31:56 GMT   (3545kb)

Title: VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven
  Module
Authors: Kam Man Wu, Zeyue Tian, Liya Ji, Qifeng Chen
Categories: cs.MM cs.CV cs.SD eess.AS
\\
  Video and audio inpainting for mixed audio-visual content has become a
crucial task in multimedia editing recently. However, precisely removing an
object and its corresponding audio from a video without affecting the rest of
the scene remains a significant challenge. To address this, we propose
VAInpaint, a novel pipeline that first utilizes a segmentation model to
generate masks and guide a video inpainting model in removing objects. At the
same time, an LLM then analyzes the scene globally, while a region-specific
model provides localized descriptions. Both the overall and regional
descriptions will be inputted into an LLM, which will refine the content and
turn it into text queries for our text-driven audio separation model. Our audio
separation model is fine-tuned on a customized dataset comprising segmented
MUSIC instrument images and VGGSound backgrounds to enhance its generalization
performance. Experiments show that our method achieves performance comparable
to current benchmarks in both audio and video inpainting.
\\ ( https://arxiv.org/abs/2509.17022 ,  3545kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17034 (*cross-listing*)
Date: Sun, 21 Sep 2025 11:09:57 GMT   (267kb)

Title: Long-Tailed Out-of-Distribution Detection with Refined Separate Class
  Learning
Authors: Shuai Feng, Yuxin Ge, Yuntao Du, Mingcai Chen, Lei Feng
Categories: cs.LG cs.CV
\\
  Out-of-distribution (OOD) detection is crucial for deploying robust machine
learning models. However, when training data follows a long-tailed
distribution, the model's ability to accurately detect OOD samples is
significantly compromised, due to the confusion between OOD samples and
head/tail classes. To distinguish OOD samples from both head and tail classes,
the separate class learning (SCL) approach has emerged as a promising solution,
which separately conduct head-specific and tail-specific class learning. To
this end, we examine the limitations of existing works of SCL and reveal that
the OOD detection performance is notably influenced by the use of static
scaling temperature value and the presence of uninformative outliers. To
mitigate these limitations, we propose a novel approach termed Refined Separate
Class Learning (RSCL), which leverages dynamic class-wise temperature
adjustment to modulate the temperature parameter for each in-distribution class
and informative outlier mining to identify diverse types of outliers based on
their affinity with head and tail classes. Extensive experiments demonstrate
that RSCL achieves superior OOD detection performance while improving the
classification accuracy on in-distribution data.
\\ ( https://arxiv.org/abs/2509.17034 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17168 (*cross-listing*)
Date: Sun, 21 Sep 2025 17:27:57 GMT   (10502kb)

Title: Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics
Authors: Chengwei Shi, Chong Cao, Xin Tong, Xukun Shen
Categories: cs.GR cs.CV
Comments: arXiv submission
\\
  Head and gaze dynamics are crucial in expressive 3D facial animation for
conveying emotion and intention. However, existing methods frequently address
facial components in isolation, overlooking the intricate coordination between
gaze, head motion, and speech. The scarcity of high-quality gaze-annotated
datasets hinders the development of data-driven models capable of capturing
realistic, personalized gaze control. To address these challenges, we propose
StyGazeTalk, an audio-driven method that generates synchronized gaze and head
motion styles. We extract speaker-specific motion traits from gaze-head
sequences with a multi-layer LSTM structure incorporating a style encoder,
enabling the generation of diverse animation styles. We also introduce a
high-precision multimodal dataset comprising eye-tracked gaze, audio, head
pose, and 3D facial parameters, providing a valuable resource for training and
evaluating head and gaze control models. Experimental results demonstrate that
our method generates realistic, temporally coherent, and style-aware head-gaze
motions, significantly advancing the state-of-the-art in audio-driven facial
animation.
\\ ( https://arxiv.org/abs/2509.17168 ,  10502kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17212 (*cross-listing*)
Date: Sun, 21 Sep 2025 19:39:54 GMT   (43094kb)

Title: High Resolution UDF Meshing via Iterative Networks
Authors: Federico Stella, Nicolas Talabot, Hieu Le, Pascal Fua
Categories: cs.GR cs.CV
Comments: Accepted at NeurIPS 2025
\\
  Unsigned Distance Fields (UDFs) are a natural implicit representation for
open surfaces but, unlike Signed Distance Fields (SDFs), are challenging to
triangulate into explicit meshes. This is especially true at high resolutions
where neural UDFs exhibit higher noise levels, which makes it hard to capture
fine details. Most current techniques perform within single voxels without
reference to their neighborhood, resulting in missing surface and holes where
the UDF is ambiguous or noisy. We show that this can be remedied by performing
several passes and by reasoning on previously extracted surface elements to
incorporate neighborhood information. Our key contribution is an iterative
neural network that does this and progressively improves surface recovery
within each voxel by spatially propagating information from increasingly
distant neighbors. Unlike single-pass methods, our approach integrates newly
detected surfaces, distance values, and gradients across multiple iterations,
effectively correcting errors and stabilizing extraction in challenging
regions. Experiments on diverse 3D models demonstrate that our method produces
significantly more accurate and complete meshes than existing approaches,
particularly for complex geometries, enabling UDF surface extraction at higher
resolutions where traditional methods fail.
\\ ( https://arxiv.org/abs/2509.17212 ,  43094kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17268 (*cross-listing*)
Date: Sun, 21 Sep 2025 22:59:56 GMT   (17529kb)

Title: Computational Scaffolding of Composition, Value, and Color for
  Disciplined Drawing
Authors: Jiaju Ma, Chau Vu, Asya Lyubavina, Catherine Liu, Jingyi Li
Categories: cs.HC cs.CV
Comments: Accepted to UIST 2025 (Best Paper)
DOI: 10.1145/3746059.3747605
\\
  One way illustrators engage in disciplined drawing - the process of drawing
to improve technical skills - is through studying and replicating reference
images. However, for many novice and intermediate digital artists, knowing how
to approach studying a reference image can be challenging. It can also be
difficult to receive immediate feedback on their works-in-progress. To help
these users develop their professional vision, we propose ArtKrit, a tool that
scaffolds the process of replicating a reference image into three main steps:
composition, value, and color. At each step, our tool offers computational
guidance, such as adaptive composition line generation, and automatic feedback,
such as value and color accuracy. Evaluating this tool with intermediate
digital artists revealed that ArtKrit could flexibly accommodate their unique
workflows. Our code and supplemental materials are available at
https://majiaju.io/artkrit .
\\ ( https://arxiv.org/abs/2509.17268 ,  17529kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17287 (*cross-listing*)
Date: Sun, 21 Sep 2025 23:53:31 GMT   (5856kb)

Title: Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain
  Cross-Correlation
Authors: Gokul B. Nair, Alejandro Fontan, Michael Milford, Tobias Fischer
Categories: cs.RO cs.CV
Comments: 8 Pages, 4 Figures, Under Review
\\
  Visual teach-and-repeat navigation enables robots to autonomously traverse
previously demonstrated paths by comparing current sensory input with recorded
trajectories. However, conventional frame-based cameras fundamentally limit
system responsiveness: their fixed frame rates (typically 30-60 Hz) create
inherent latency between environmental changes and control responses. Here we
present the first event-camera-based visual teach-and-repeat system. To achieve
this, we develop a frequency-domain cross-correlation framework that transforms
the event stream matching problem into computationally efficient Fourier space
multiplications, capable of exceeding 300Hz processing rates, an order of
magnitude faster than frame-based approaches. By exploiting the binary nature
of event frames and applying image compression techniques, we further enhance
the computational speed of the cross-correlation process without sacrificing
localization accuracy. Extensive experiments using a Prophesee EVK4 HD event
camera mounted on an AgileX Scout Mini robot demonstrate successful autonomous
navigation across 4000+ meters of indoor and outdoor trajectories. Our system
achieves ATEs below 24 cm while maintaining consistent high-frequency control
updates. Our evaluations show that our approach achieves substantially higher
update rates compared to conventional frame-based systems, underscoring the
practical viability of event-based perception for real-time robotic navigation.
\\ ( https://arxiv.org/abs/2509.17287 ,  5856kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17299 (*cross-listing*)
Date: Mon, 22 Sep 2025 00:47:32 GMT   (6738kb)

Title: Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn
  and Larvae Imaging Camera System (CSLICS)
Authors: Dorian Tsai, Christopher A. Brunner, Riki Lamont, F. Mikaela Nordborg,
  Andrea Severati, Java Terry, Karen Jackel, Matthew Dunbabin, Tobias Fischer
  and Scarlett Raine
Categories: cs.RO cs.CV
Comments: 9 pages, 7 figures
\\
  Coral aquaculture for reef restoration requires accurate and continuous spawn
counting for resource distribution and larval health monitoring, but current
methods are labor-intensive and represent a critical bottleneck in the coral
production pipeline. We propose the Coral Spawn and Larvae Imaging Camera
System (CSLICS), which uses low cost modular cameras and object detectors
trained using human-in-the-loop labeling approaches for automated spawn
counting in larval rearing tanks. This paper details the system engineering,
dataset collection, and computer vision techniques to detect, classify and
count coral spawn. Experimental results from mass spawning events demonstrate
an F1 score of 82.4\% for surface spawn detection at different embryogenesis
stages, 65.3\% F1 score for sub-surface spawn detection, and a saving of 5,720
hours of labor per spawning event compared to manual sampling methods at the
same frequency. Comparison of manual counts with CSLICS monitoring during a
mass coral spawning event on the Great Barrier Reef demonstrates CSLICS'
accurate measurement of fertilization success and sub-surface spawn counts.
These findings enhance the coral aquaculture process and enable upscaling of
coral reef restoration efforts to address climate change threats facing
ecosystems like the Great Barrier Reef.
\\ ( https://arxiv.org/abs/2509.17299 ,  6738kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17755 (*cross-listing*)
Date: Mon, 22 Sep 2025 13:19:07 GMT   (46995kb)

Title: Learning Neural Antiderivatives
Authors: Fizza Rubab, Ntumba Elie Nsampi, Martin Balint, Felix Mujkanovic,
  Hans-Peter Seidel, Tobias Ritschel and Thomas Leimk\"uhler
Categories: cs.LG cs.CV cs.GR
\\
  Neural fields offer continuous, learnable representations that extend beyond
traditional discrete formats in visual computing. We study the problem of
learning neural representations of repeated antiderivatives directly from a
function, a continuous analogue of summed-area tables. Although widely used in
discrete domains, such cumulative schemes rely on grids, which prevents their
applicability in continuous neural contexts. We introduce and analyze a range
of neural methods for repeated integration, including both adaptations of prior
work and novel designs. Our evaluation spans multiple input dimensionalities
and integration orders, assessing both reconstruction quality and performance
in downstream tasks such as filtering and rendering. These results enable
integrating classical cumulative operators into modern neural systems and offer
insights into learning tasks involving differential and integral operators.
\\ ( https://arxiv.org/abs/2509.17755 ,  46995kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17877 (*cross-listing*)
Date: Mon, 22 Sep 2025 15:14:02 GMT   (2236kb)

Title: Sight Over Site: Perception-Aware Reinforcement Learning for Efficient
  Robotic Inspection
Authors: Richard Kuhlmann, Jakob Wolfram, Boyang Sun, Jiaxu Xing, Davide
  Scaramuzza, Marc Pollefeys, Cesar Cadena
Categories: cs.RO cs.CV
\\
  Autonomous inspection is a central problem in robotics, with applications
ranging from industrial monitoring to search-and-rescue. Traditionally,
inspection has often been reduced to navigation tasks, where the objective is
to reach a predefined location while avoiding obstacles. However, this
formulation captures only part of the real inspection problem. In real-world
environments, the inspection targets may become visible well before their exact
coordinates are reached, making further movement both redundant and
inefficient. What matters more for inspection is not simply arriving at the
target's position, but positioning the robot at a viewpoint from which the
target becomes observable. In this work, we revisit inspection from a
perception-aware perspective. We propose an end-to-end reinforcement learning
framework that explicitly incorporates target visibility as the primary
objective, enabling the robot to find the shortest trajectory that guarantees
visual contact with the target without relying on a map. The learned policy
leverages both perceptual and proprioceptive sensing and is trained entirely in
simulation, before being deployed to a real-world robot. We further develop an
algorithm to compute ground-truth shortest inspection paths, which provides a
reference for evaluation. Through extensive experiments, we show that our
method outperforms existing classical and learning-based navigation approaches,
yielding more efficient inspection trajectories in both simulated and
real-world settings. The project is avialable at
https://sight-over-site.github.io/
\\ ( https://arxiv.org/abs/2509.17877 ,  2236kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17940 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:01:11 GMT   (963kb)

Title: DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous
  Driving
Authors: Shuyao Shang, Yuntao Chen, Yuqi Wang, Yingyan Li, Zhaoxiang Zhang
Categories: cs.RO cs.CV
Comments: NeurIPS 2025
\\
  End-to-end autonomous driving has substantially progressed by directly
predicting future trajectories from raw perception inputs, which bypasses
traditional modular pipelines. However, mainstream methods trained via
imitation learning suffer from critical safety limitations, as they fail to
distinguish between trajectories that appear human-like but are potentially
unsafe. Some recent approaches attempt to address this by regressing multiple
rule-driven scores but decoupling supervision from policy optimization,
resulting in suboptimal performance. To tackle these challenges, we propose
DriveDPO, a Safety Direct Preference Optimization Policy Learning framework.
First, we distill a unified policy distribution from human imitation similarity
and rule-based safety scores for direct policy optimization. Further, we
introduce an iterative Direct Preference Optimization stage formulated as
trajectory-level preference alignment. Extensive experiments on the NAVSIM
benchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of
90.0. Furthermore, qualitative results across diverse challenging scenarios
highlight DriveDPO's ability to produce safer and more reliable driving
behaviors.
\\ ( https://arxiv.org/abs/2509.17940 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18040 (*cross-listing*)
Date: Mon, 22 Sep 2025 17:14:40 GMT   (9747kb)

Title: Detection of Misreporting Attacks on Software-Defined Immersive
  Environments
Authors: Sourya Saha, Md Nurul Absur, Shima Yousefi, Saptarshi Debroy
Categories: cs.NI cs.CV
Comments: 7 Pages, 7 Images, will appear in CNSM 2025
\\
  The ability to centrally control network infrastructure using a programmable
middleware has made Software-Defined Networking (SDN) ideal for emerging
applications, such as immersive environments. However, such flexibility
introduces new vulnerabilities, such as switch misreporting led load imbalance,
which in turn make such immersive environment vulnerable to severe quality
degradation. In this paper, we present a hybrid machine learning (ML)-based
network anomaly detection framework that identifies such stealthy misreporting
by capturing temporal inconsistencies in switch-reported loads, and thereby
counter potentially catastrophic quality degradation of hosted immersive
application. The detection system combines unsupervised anomaly scoring with
supervised classification to robustly distinguish malicious behavior. Data
collected from a realistic testbed deployment under both benign and adversarial
conditions is used to train and evaluate the model. Experimental results show
that the framework achieves high recall in detecting misreporting behavior,
making it effective for early and reliable detection in SDN environments.
\\ ( https://arxiv.org/abs/2509.18040 ,  9747kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16390 (*cross-listing*)
Date: Fri, 19 Sep 2025 20:11:42 GMT   (71kb)

Title: B5GRoam: A Zero Trust Framework for Secure and Efficient On-Chain B5G
  Roaming
Authors: Mohamed Abdessamed Rezazi, Mouhamed Amine Bouchiha, Ahmed Mounsf Rafik
  Bendada, Yacine Ghamri-Doudane
Categories: cs.CR cs.DC cs.NI
Comments: 6 pages, 2 figures, Accepted at GLOBECOM'25
\\
  Roaming settlement in 5G and beyond networks demands secure, efficient, and
trustworthy mechanisms for billing reconciliation between mobile operators.
While blockchain promises decentralization and auditability, existing solutions
suffer from critical limitations-namely, data privacy risks, assumptions of
mutual trust, and scalability bottlenecks. To address these challenges, we
present B5GRoam, a novel on-chain and zero-trust framework for secure,
privacy-preserving, and scalable roaming settlements. B5GRoam introduces a
cryptographically verifiable call detail record (CDR) submission protocol,
enabling smart contracts to authenticate usage claims without exposing
sensitive data. To preserve privacy, we integrate non-interactive
zero-knowledge proofs (zkSNARKs) that allow on-chain verification of roaming
activity without revealing user or network details. To meet the high-throughput
demands of 5G environments, B5GRoam leverages Layer 2 zk-Rollups, significantly
reducing gas costs while maintaining the security guarantees of Layer 1.
Experimental results demonstrate a throughput of over 7,200 tx/s with strong
privacy and substantial cost savings. By eliminating intermediaries and
enhancing verifiability, B5GRoam offers a practical and secure foundation for
decentralized roaming in future mobile networks.
\\ ( https://arxiv.org/abs/2509.16390 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17725 (*cross-listing*)
Date: Mon, 22 Sep 2025 12:56:52 GMT   (835kb)

Title: A Comparison of Low and high-Order Methods for the Simulation of
  Supersonic Jet Flows
Authors: D. F. Abreu and C. Junqueira-Junior and E. T. V. Dauricio and J. L. F.
  Azevedo
Categories: physics.flu-dyn cs.DC physics.comp-ph
Report-no: COB-2021-0388
\\
  The present work compares results for different numerical methods in search
of alternatives to improve the quality of large-eddy simulations for the
problem of supersonic turbulent jet flows. Previous work has analyzed
supersonic jet flows using a second-order, finite difference solver based on
structured meshes, and the results indicated a shorter potential core of the
jet and different levels of velocity fluctuations. In the present work, the
results of previous simulations are compared to new results using a high-order,
discontinuous Galerkin solver for unstructured meshes. All simulations are
performed keeping the total number of degrees of freedom constant. The results
of the current simulations present very similar mean velocity distributions and
slightly smaller velocity fluctuations, and they seem to correlate better with
the experimental data. The present results indicate that additional studies
should focus on the jet inlet boundary conditions in order to improve the
physical representation of the early stages of the jet development.
\\ ( https://arxiv.org/abs/2509.17725 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16912 (*cross-listing*)
Date: Sun, 21 Sep 2025 04:23:02 GMT   (329kb)

Title: Analysis of the Impact of an Execution Algorithm with an Order Book
  Imbalance Strategy on a Financial Market Using an Agent-based Simulation
Authors: Shuto Endo, Takanobu Mizuta, Isao Yagi
Categories: q-fin.CP cs.GT cs.MA
Journal-ref: The Journal of the Japanese Society for Artificial Intelligence
  (Vol. 39, No. 4, 2024)
DOI: 10.1527/tjsai.39-4_FIN23-I
\\
  Order book imbalance (OBI) - buy orders minus sell orders near the best quote
- measures supply-demand imbalance that can move prices. OBI is positively
correlated with returns, and some investors try to use it to improve
performance. Large orders placed at once can reveal intent, invite
front-running, raise volatility, and cause losses. Execution algorithms
therefore split parent orders into smaller lots to limit price distortion. In
principle, using OBI inside such algorithms could improve execution, but prior
evidence is scarce because isolating OBI's effect in real markets is nearly
impossible amid many external factors.
  Multi-agent simulation offers a way to study this. In an artificial market,
individual actors are agents whose rules and interactions form the model. This
study builds an execution algorithm that accounts for OBI, tests it across
several market patterns in artificial markets, and analyzes mechanisms,
comparing it with a conventional (OBI-agnostic) algorithm.
  Results: (i) In stable markets, the OBI strategy's performance depends on the
number of order slices; outcomes vary with how the parent order is partitioned.
(ii) In markets with unstable prices, the OBI-based algorithm outperforms the
conventional approach. (iii) Under spoofing manipulation, the OBI strategy is
not significantly worse than the conventional algorithm, indicating limited
vulnerability to spoofing.
  Overall, OBI provides a useful signal for execution. Incorporating OBI can
add value - especially in volatile conditions - while remaining reasonably
robust to spoofing; in calm markets, benefits are sensitive to slicing design.
\\ ( https://arxiv.org/abs/2509.16912 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17341 (*cross-listing*)
Date: Mon, 22 Sep 2025 04:10:20 GMT   (1593kb)

Title: Trajectory Encryption Cooperative Salvo Guidance
Authors: Lohitvel Gopikannan, Shashi Ranjan Kumar, and Abhinav Sinha
Categories: eess.SY cs.MA cs.RO cs.SY math.OC
\\
  This paper introduces the concept of trajectory encryption in cooperative
simultaneous target interception, wherein heterogeneity in guidance principles
across a team of unmanned autonomous systems is leveraged as a strategic design
feature. By employing a mix of heterogeneous time-to-go formulations leading to
a cooperative guidance strategy, the swarm of vehicles is able to generate
diverse trajectory families. This diversity expands the feasible solution space
for simultaneous target interception, enhances robustness under disturbances,
and enables flexible time-to-go adjustments without predictable detouring. From
an adversarial perspective, heterogeneity obscures the collective interception
intent by preventing straightforward prediction of swarm dynamics, effectively
acting as an encryption layer in the trajectory domain. Simulations demonstrate
that the swarm of heterogeneous vehicles is able to intercept a moving target
simultaneously from a diverse set of initial engagement configurations.
\\ ( https://arxiv.org/abs/2509.17341 ,  1593kb)
------------------------------------------------------------------------------
\\
arXiv:2509.17728 (*cross-listing*)
Date: Mon, 22 Sep 2025 12:58:53 GMT   (1659kb)

Title: A non-smooth regularization framework for learning over multitask graphs
Authors: Yara Zgheib, Luca Calatroni, Marc Antonini, Roula Nassif
Categories: cs.LG cs.MA
\\
  In this work, we consider learning over multitask graphs, where each agent
aims to estimate its own parameter vector. Although agents seek distinct
objectives, collaboration among them can be beneficial in scenarios where
relationships between tasks exist. Among the various approaches to promoting
relationships between tasks and, consequently, enhancing collaboration between
agents, one notable method is regularization. While previous multitask learning
studies have focused on smooth regularization to enforce graph smoothness, this
work explores non-smooth regularization techniques that promote sparsity,
making them particularly effective in encouraging piecewise constant
transitions on the graph. We begin by formulating a global regularized
optimization problem, which involves minimizing the aggregate sum of individual
costs, regularized by a general non-smooth term designed to promote
piecewise-constant relationships between the tasks of neighboring agents. Based
on the forward-backward splitting strategy, we propose a decentralized learning
approach that enables efficient solutions to the regularized optimization
problem. Then, under convexity assumptions on the cost functions and
co-regularization, we establish that the proposed approach converges in the
mean-square-error sense within $O(\mu)$ of the optimal solution of the globally
regularized cost. For broader applicability and improved computational
efficiency, we also derive closed-form expressions for commonly used non-smooth
(and, possibly, non-convex) regularizers, such as the weighted sum of the
$\ell_0$-norm, $\ell_1$-norm, and elastic net regularization. Finally, we
illustrate both the theoretical findings and the effectiveness of the approach
through simulations.
\\ ( https://arxiv.org/abs/2509.17728 ,  1659kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18011 (*cross-listing*)
Date: Mon, 22 Sep 2025 16:49:49 GMT   (4459kb)

Title: Robust, Online, and Adaptive Decentralized Gaussian Processes
Authors: Fernando Llorente, Daniel Waxman, Sanket Jantre, Nathan M. Urban,
  Susan E. Minkoff
Categories: stat.ML cs.LG cs.MA eess.SP
Comments: Submitted to Icassp 2026 Special Session on "Bridging Signal
  Processing and Machine Learning with Gaussian Processes."
\\
  Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for
modeling complex signals, but scale cubically with data, assume static targets,
and are brittle to outliers, limiting their applicability in large-scale
problems with dynamic and noisy environments. Recent work introduced
decentralized random Fourier feature Gaussian processes (DRFGP), an online and
distributed algorithm that casts GPs in an information-filter form, enabling
exact sequential inference and fully distributed computation without reliance
on a fusion center. In this paper, we extend DRFGP along two key directions:
first, by introducing a robust-filtering update that downweights the impact of
atypical observations; and second, by incorporating a dynamic adaptation
mechanism that adapts to time-varying functions. The resulting algorithm
retains the recursive information-filter structure while enhancing stability
and accuracy. We demonstrate its effectiveness on a large-scale Earth system
application, underscoring its potential for in-situ modeling.
\\ ( https://arxiv.org/abs/2509.18011 ,  4459kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2306.04335
replaced with revised version Sat, 20 Sep 2025 11:42:31 GMT   (4849kb)

Title: Semantic web technologies in sensor-based personal health monitoring
  systems: A systematic mapping study
Authors: Mbithe Nzomo and Deshendran Moodley
Categories: cs.AI
Comments: Under review at the Semantic Web Journal (SWJ)
\\ ( https://arxiv.org/abs/2306.04335 ,  4849kb)
------------------------------------------------------------------------------
\\
arXiv:2405.16887
replaced with revised version Mon, 22 Sep 2025 13:20:33 GMT   (3973kb)

Title: A Large Language Model-based multi-agent manufacturing system for
  intelligent shopfloor
Authors: Zhen Zhao, Dunbing Tang, Changchun Liu, Liping Wang, Zequn Zhang,
  Haihua Zhu, Kai Chen, Qingwei Nie, Yuchen Ji
Categories: cs.AI cs.MA cs.RO
Journal-ref: Zhao Z, Tang D, Liu C, et al. A Large language model-based
  multi-agent manufacturing system for intelligent shopfloors[J]. Advanced
  Engineering Informatics, 2026, 69: 103888
DOI: 10.1016/j.aei.2025.103888
\\ ( https://arxiv.org/abs/2405.16887 ,  3973kb)
------------------------------------------------------------------------------
\\
arXiv:2409.17407
replaced with revised version Sun, 21 Sep 2025 21:37:57 GMT   (1281kb)

Title: Post-hoc Reward Calibration: A Case Study on Length Bias
Authors: Zeyu Huang, Zihan Qiu, Zili Wang, Edoardo M. Ponti, Ivan Titov
Categories: cs.AI cs.CL
Comments: ICLR 2025
\\ ( https://arxiv.org/abs/2409.17407 ,  1281kb)
------------------------------------------------------------------------------
\\
arXiv:2410.08949
replaced with revised version Sun, 21 Sep 2025 00:58:43 GMT   (560kb)

Title: Information Fusion Using Transferable Belief Functions Implemented on
  Quantum Circuits
Authors: Qianli Zhou and Hao Luo and Lipeng Pan and Yong Deng and Eloi Bosse
Categories: cs.AI quant-ph
\\ ( https://arxiv.org/abs/2410.08949 ,  560kb)
------------------------------------------------------------------------------
\\
arXiv:2410.19238
replaced with revised version Sun, 21 Sep 2025 23:37:10 GMT   (1001kb)

Title: Designing AI-Agents with Personalities: A Psychometric Approach
Authors: Muhua Huang, Xijuan Zhang, Christopher Soto, James Evans
Categories: cs.AI cs.CY
\\ ( https://arxiv.org/abs/2410.19238 ,  1001kb)
------------------------------------------------------------------------------
\\
arXiv:2410.19817
replaced with revised version Sun, 21 Sep 2025 01:47:34 GMT   (146kb)

Title: Step Guided Reasoning: Improving Mathematical Reasoning using Guidance
  Generation and Step Reasoning
Authors: Lang Cao, Yingtian Zou, Chao Peng, Renhong Chen, Wu Ning, Yitong Li
Categories: cs.AI cs.CL cs.HC
Comments: 9 pages, 9 figures
\\ ( https://arxiv.org/abs/2410.19817 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2411.08307
replaced with revised version Mon, 22 Sep 2025 03:03:57 GMT   (452kb)

Title: PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for
  Long-Term Expressive Symbolic Music Generation
Authors: Yungang Yi, Weihua Li, Matthew Kuo, Quan Bai
Categories: cs.AI cs.MM cs.SD eess.AS
ACM-class: I.2.7; H.5.5
Journal-ref: IEEE Transactions on Audio, Speech, and Language Processing, 2025
DOI: 10.1109/TASLPRO.2025.3611836
\\ ( https://arxiv.org/abs/2411.08307 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2411.13932
replaced with revised version Mon, 22 Sep 2025 07:48:33 GMT   (0kb,I)

Title: XAgents: A Framework for Interpretable Rule-Based Multi-Agents
  Cooperation
Authors: Hailong Yang, Mingxian Gu, Renhuo Zhao, Fuping Hu, Zhaohong Deng,
  Yitang Chen
Categories: cs.AI cs.MA
Comments: We intend to substantially revise the problem statement and scope;
  therefore we withdraw the current version
\\ ( https://arxiv.org/abs/2411.13932 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2412.00251
replaced with revised version Sat, 20 Sep 2025 21:29:08 GMT   (1269kb)

Title: Fine-Tuning Open-Weight Language Models to Deliver Cognitive Behavioral
  Therapy for Depression: A Feasibility Study
Authors: Talha Tahir
Categories: cs.AI cs.HC
ACM-class: I.2.7; J.3
\\ ( https://arxiv.org/abs/2412.00251 ,  1269kb)
------------------------------------------------------------------------------
\\
arXiv:2501.17310
replaced with revised version Sat, 20 Sep 2025 04:34:48 GMT   (3482kb)

Title: Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds
  Decoding
Authors: Yun-Shiuan Chuang, Nikunj Harlalka, Sameer Narendran, Alexander
  Cheung, Sizhe Gao, Siddharth Suresh, Junjie Hu, Timothy T. Rogers
Categories: cs.AI cs.HC
\\ ( https://arxiv.org/abs/2501.17310 ,  3482kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11528
replaced with revised version Sat, 20 Sep 2025 11:39:01 GMT   (2652kb)

Title: A Survey of Personalized Large Language Models: Progress and Future
  Directions
Authors: Jiahong Liu, Zexuan Qiu, Zhongyang Li, Quanyu Dai, Wenhao Yu, Jieming
  Zhu, Minda Hu, Menglin Yang, Tat-Seng Chua, Irwin King
Categories: cs.AI
Comments: 34 pages, 8 figures, 7 tables, Under Review
\\ ( https://arxiv.org/abs/2502.11528 ,  2652kb)
------------------------------------------------------------------------------
\\
arXiv:2504.03699
replaced with revised version Mon, 22 Sep 2025 14:44:47 GMT   (342kb)

Title: Enhancing Clinical Decision-Making: Integrating Multi-Agent Systems with
  Ethical AI Governance
Authors: Ying-Jung Chen and Ahmad Albarqawi and Chi-Sheng Chen
Categories: cs.AI cs.CY cs.LG cs.MA q-bio.QM
\\ ( https://arxiv.org/abs/2504.03699 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2504.09574
replaced with revised version Sat, 20 Sep 2025 15:25:46 GMT   (2306kb)

Title: An Improved FOX Optimization Algorithm Using Adaptive Exploration and
  Exploitation for Global Optimization
Authors: Mahmood A. Jumaah, Yossra H. Ali, and Tarik A. Rashid
Categories: cs.AI
Comments: 44 pages
DOI: 10.1371/journal.pone.0331965
\\ ( https://arxiv.org/abs/2504.09574 ,  2306kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14615
replaced with revised version Mon, 22 Sep 2025 08:20:57 GMT   (264kb)

Title: SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle
  Generation from SAT Formulas
Authors: Anjiang Wei, Yuheng Wu, Yingjia Wan, Tarun Suresh, Huanmi Tan, Zhanke
  Zhou, Sanmi Koyejo, Ke Wang, Alex Aiken
Categories: cs.AI cs.CL cs.LG cs.LO
\\ ( https://arxiv.org/abs/2505.14615 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18135
replaced with revised version Sun, 21 Sep 2025 22:21:20 GMT   (51kb)

Title: Tool Preferences in Agentic LLMs are Unreliable
Authors: Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang
  Sriramanan, Sriram Balasubramanian, Parsa Hosseini, Soheil Feizi
Categories: cs.AI cs.CL cs.CR cs.LG
Comments: Conference on Empirical Methods in Natural Language Processing
  (EMNLP) 2025, main
\\ ( https://arxiv.org/abs/2505.18135 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22928
replaced with revised version Fri, 19 Sep 2025 20:40:50 GMT   (4876kb)

Title: Enhancing Study-Level Inference from Clinical Trial Papers via
  Reinforcement Learning-Based Numeric Reasoning
Authors: Massimiliano Pronesti, Michela Lorandi, Paul Flanagan, Oisin Redmond,
  Anya Belz, Yufang Hou
Categories: cs.AI cs.CL
Comments: Accepted at EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2505.22928 ,  4876kb)
------------------------------------------------------------------------------
\\
arXiv:2506.00073
replaced with revised version Sat, 20 Sep 2025 18:47:07 GMT   (1365kb)

Title: The Automated but Risky Game: Modeling and Benchmarking Agent-to-Agent
  Negotiations and Transactions in Consumer Markets
Authors: Shenzhe Zhu, Jiao Sun, Yi Nian, Tobin South, Alex Pentland, Jiaxin Pei
Categories: cs.AI cs.CL cs.CY cs.HC cs.MA
\\ ( https://arxiv.org/abs/2506.00073 ,  1365kb)
------------------------------------------------------------------------------
\\
arXiv:2506.04410
replaced with revised version Fri, 19 Sep 2025 20:39:25 GMT   (1873kb)

Title: Matter-of-Fact: A Benchmark for Verifying the Feasibility of
  Literature-Supported Claims in Materials Science
Authors: Peter Jansen, Samiah Hassan, Ruoyao Wang
Categories: cs.AI cond-mat.mtrl-sci cs.CL
Comments: 9 pages (Accepted to EMNLP 2025)
\\ ( https://arxiv.org/abs/2506.04410 ,  1873kb)
------------------------------------------------------------------------------
\\
arXiv:2506.12617
replaced with revised version Sat, 20 Sep 2025 15:01:26 GMT   (3792kb)

Title: Evaluating AI Alignment in Eleven LLMs through Output-Based Analysis and
  Human Benchmarking
Authors: G. R. Lau, W. Y. Low, S. M. Koh, A. Hartanto
Categories: cs.AI cs.HC
\\ ( https://arxiv.org/abs/2506.12617 ,  3792kb)
------------------------------------------------------------------------------
\\
arXiv:2506.16995
replaced with revised version Sun, 21 Sep 2025 16:22:19 GMT   (200kb)

Title: Style-Preserving Policy Optimization for Game Agents
Authors: Lingfeng Li, Yunlong Lu, Yongyi Wang, Wenxin Li
Categories: cs.AI
\\ ( https://arxiv.org/abs/2506.16995 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20130
replaced with revised version Mon, 22 Sep 2025 17:37:47 GMT   (216kb)

Title: AI Copilots for Reproducibility in Science: A Case Study
Authors: Adrien Bibal and Steven N. Minton and Deborah Khider and Yolanda Gil
Categories: cs.AI
\\ ( https://arxiv.org/abs/2506.20130 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20608
replaced with revised version Mon, 22 Sep 2025 14:54:39 GMT   (615kb)

Title: AI Assistants to Enhance and Exploit the PETSc Knowledge Base
Authors: Barry Smith, Junchao Zhang, Hong Zhang, Lois Curfman McInnes, Murat
  Keceli, Archit Vasan, Satish Balay, Toby Isaac, Le Chen, Venkatram Vishwanath
Categories: cs.AI cs.NA math.NA
Journal-ref: 54th International Conference on Parallel Processing Companion
  (ICPP Companion '25), 2025
\\ ( https://arxiv.org/abs/2506.20608 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2507.06993
replaced with revised version Sat, 20 Sep 2025 18:49:05 GMT   (18483kb)

Title: IMAIA: Interactive Maps AI Assistant for Travel Planning and Geo-Spatial
  Intelligence
Authors: Jieren Deng, Zhizhang Hu, Ziyan He, Aleksandar Cvetkovic, Pak Kiu
  Chung, Dragomir Yankov and Chiqun Zhang
Categories: cs.AI cs.CV
\\ ( https://arxiv.org/abs/2507.06993 ,  18483kb)
------------------------------------------------------------------------------
\\
arXiv:2507.10571
replaced with revised version Sun, 21 Sep 2025 06:44:14 GMT   (7643kb)

Title: Agentic AI with Orchestrator-Agent Trust: A Modular Visual
  Classification Framework with Trust-Aware Orchestration and RAG-Based
  Reasoning
Authors: Konstantinos I. Roumeliotis, Ranjan Sapkota, Manoj Karkee, Nikolaos D.
  Tselikas
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2507.10571 ,  7643kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13142
replaced with revised version Fri, 19 Sep 2025 20:42:41 GMT   (245kb)

Title: From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement
  Learning
Authors: Ahmed Bahloul, Simon Malberg
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2507.13142 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2507.15877
replaced with revised version Sun, 21 Sep 2025 14:56:12 GMT   (98kb)

Title: Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing
  Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning
Authors: Simon Ouellette
Categories: cs.AI
Comments: this version fixes errors in AlphaEvolve total % calculation, Table 3
  DSL description, and adds clarifications in response to review criticisms
\\ ( https://arxiv.org/abs/2507.15877 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2507.16370
replaced with revised version Mon, 22 Sep 2025 07:53:02 GMT   (1374kb,D)

Title: Canonical Representations of Markovian Structural Causal Models: A
  Framework for Counterfactual Reasoning
Authors: Lucas de Lara (IECL)
Categories: cs.AI math.ST stat.TH
\\ ( https://arxiv.org/abs/2507.16370 ,  1374kb)
------------------------------------------------------------------------------
\\
arXiv:2507.22149
replaced with revised version Sat, 20 Sep 2025 03:57:34 GMT   (4223kb)

Title: When Truthful Representations Flip Under Deceptive Instructions?
Authors: Xianxuan Long, Yao Fu, Runchao Li, Mu Sheng, Haotian Yu, Xiaotian Han,
  Pan Li
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2507.22149 ,  4223kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01561
replaced with revised version Sat, 20 Sep 2025 17:26:32 GMT   (3676kb)

Title: One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear
  Temporal Logic Requirements in Multi-Task Reinforcement Learning
Authors: Zijian Guo, \.Ilker I\c{s}{\i}k, H. M. Sabbir Ahmad, Wenchao Li
Categories: cs.AI
\\ ( https://arxiv.org/abs/2508.01561 ,  3676kb)
------------------------------------------------------------------------------
\\
arXiv:2508.03251
replaced with revised version Mon, 22 Sep 2025 13:13:25 GMT   (125kb)

Title: Full-History Graphs with Edge-Type Decoupled Networks for Temporal
  Reasoning
Authors: Osama Mohammed, Jiaxin Pan, Mojtaba Nayyeri, Daniel Hern\'andez and
  Steffen Staab
Categories: cs.AI
Comments: European Conference of Artificial Intelligence 2025
\\ ( https://arxiv.org/abs/2508.03251 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04282
replaced with revised version Mon, 22 Sep 2025 16:09:26 GMT   (212kb)

Title: Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand
  Structure Modeling
Authors: Yongyi Wang, Lingfeng Li, Bozhou Chen, Ang Li, Hanyu Liu, Qirui Zheng,
  Xionghui Yang, Wenxin Li
Categories: cs.AI
\\ ( https://arxiv.org/abs/2508.04282 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15118
replaced with revised version Sun, 21 Sep 2025 10:47:25 GMT   (2710kb)

Title: Argumentation for Explainable Workforce Optimisation (with Appendix)
Authors: Jennifer Leigh, Dimitrios Letsios, Alessandro Mella, Lucio Machetti
  and Francesca Toni
Categories: cs.AI
Comments: Accepted to PAIS 2025
\\ ( https://arxiv.org/abs/2508.15118 ,  2710kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16072
replaced with revised version Sun, 21 Sep 2025 03:54:25 GMT   (2337kb)

Title: InMind: Evaluating LLMs in Capturing and Applying Individual Human
  Reasoning Styles
Authors: Zizhen Li, Chuanhao Li, Yibin Wang, Qi Chen, Diping Song, Yukang Feng,
  Jianwen Sun, Jiaxin Ai, Fanrui Zhang, Mingzhu Sun, Kaipeng Zhang
Categories: cs.AI cs.CL
Comments: EMNLP 2025 MainConference
\\ ( https://arxiv.org/abs/2508.16072 ,  2337kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01920
replaced with revised version Sun, 21 Sep 2025 01:40:25 GMT   (129kb)

Title: Dynamic Speculative Agent Planning
Authors: Yilin Guan, Qingfeng Lan, Sun Fei, Dujian Ding, Devang Acharya, Chi
  Wang, William Yang Wang, Wenyue Hua
Categories: cs.AI cs.LG cs.MA
Comments: 19 pages, 11 figures
\\ ( https://arxiv.org/abs/2509.01920 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2509.03953
replaced with revised version Mon, 22 Sep 2025 11:45:16 GMT   (932kb)

Title: Handling Infinite Domain Parameters in Planning Through Best-First
  Search with Delayed Partial Expansions
Authors: \'Angel Aso-Mollar and Diego Aineto and Enrico Scala and Eva Onaindia
Categories: cs.AI cs.SC cs.SY eess.SY
Journal-ref: Proceedings of the Thirty-Fourth International Joint Conference on
  Artificial Intelligence. 2025. Main Track. Pages 8456-8464
DOI: 10.24963/ijcai.2025/940
\\ ( https://arxiv.org/abs/2509.03953 ,  932kb)
------------------------------------------------------------------------------
\\
arXiv:2509.06355
replaced with revised version Fri, 19 Sep 2025 22:37:17 GMT   (1833kb)

Title: A Data-Driven Discretized CS:GO Simulation Environment to Facilitate
  Strategic Multi-Agent Planning Research
Authors: Yunzhe Wang, Volkan Ustun, Chris McGroarty
Categories: cs.AI cs.LG cs.MA
Comments: Accepted at the Winter Simulation Conference 2025, December, Seattle
  USA
\\ ( https://arxiv.org/abs/2509.06355 ,  1833kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07260
replaced with revised version Sat, 20 Sep 2025 12:03:22 GMT   (603kb)

Title: HealthSLM-Bench: Benchmarking Small Language Models for Mobile and
  Wearable Healthcare Monitoring
Authors: Xin Wang, Ting Dang, Xinyu Zhang, Vassilis Kostakos, Michael J.
  Witbrock, Hong Jia
Categories: cs.AI cs.HC cs.LG
Comments: 9 pages, 6 tables, 6 figures
\\ ( https://arxiv.org/abs/2509.07260 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09498
replaced with revised version Sun, 21 Sep 2025 13:52:47 GMT   (177kb)

Title: SEDM: Scalable Self-Evolving Distributed Memory for Agents
Authors: Haoran Xu, Jiacong Hu, Ke Zhang, Lei Yu, Yuxin Tang, Xinyuan Song,
  Yiqun Duan, Lynn Ai, Bill Shi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.09498 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11940
replaced with revised version Mon, 22 Sep 2025 17:48:58 GMT   (3175kb)

Title: Neuromorphic Intelligence
Authors: Marcel van Gerven
Categories: cs.AI
Comments: 18 pages, 3 figures
\\ ( https://arxiv.org/abs/2509.11940 ,  3175kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12434
replaced with revised version Sun, 21 Sep 2025 01:28:26 GMT   (328kb)

Title: Building Coding Agents via Entropy-Enhanced Multi-Turn Preference
  Optimization
Authors: Jiahao Yu, Zelei Cheng, Xian Wu, Xinyu Xing
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.12434 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12875
replaced with revised version Sun, 21 Sep 2025 13:58:42 GMT   (3997kb)

Title: LTA-thinker: Latent Thought-Augmented Training Framework for Large
  Language Models on Complex Reasoning
Authors: Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang,
  Yuantao Han, Cangyi Yang, jiaxu Zhang, Feiliang Ren
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.12875 ,  3997kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14289
replaced with revised version Fri, 19 Sep 2025 23:57:06 GMT   (5536kb)

Title: From Capabilities to Performance: Evaluating Key Functional Properties
  of LLM Architectures in Penetration Testing
Authors: Lanxiao Huang, Daksh Dave, Ming Jin, Tyler Cody, and Peter Beling
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2509.14289 ,  5536kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14474
replaced with revised version Sat, 20 Sep 2025 15:06:29 GMT   (44kb)

Title: From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial
  General Intelligence
Authors: Meltem Subasioglu, Nevzat Subasioglu
Categories: cs.AI cs.CY
Comments: 27 pages, 1 figure
\\ ( https://arxiv.org/abs/2509.14474 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14693
replaced with revised version Mon, 22 Sep 2025 02:54:39 GMT   (1306kb)

Title: RationAnomaly: Log Anomaly Detection with Rationality via
  Chain-of-Thought and Reinforcement Learning
Authors: Song Xu, Yilun Liu, Minggui He, Mingchen Dai, Ziang Chen, Chunguang
  Zhao, Jingzhou Du, Shimin Tao, Weibin Meng, Shenglin Zhang, Yongqian Sun,
  Boxing Chen, Daimeng Wei
Categories: cs.AI
Comments: 5 pages, 3 figures
\\ ( https://arxiv.org/abs/2509.14693 ,  1306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14746
replaced with revised version Mon, 22 Sep 2025 13:37:52 GMT   (199kb)

Title: Scaling Efficient LLMs
Authors: B.N. Kausik
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.14746 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17010
replaced with revised version Sat, 20 Sep 2025 06:34:46 GMT   (500kb)

Title: MindRef: Mimicking Human Memory for Hierarchical Reference Retrieval
  with Fine-Grained Location Awareness
Authors: Ye Wang, Xinrun Xu, Zhiming Ding
Categories: cs.CL cs.AI
Comments: ACL 2025
\\ ( https://arxiv.org/abs/2402.17010 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2404.17785
replaced with revised version Sat, 20 Sep 2025 10:37:09 GMT   (597kb)

Title: Temporal Scaling Law for Large Language Models
Authors: Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian,
  Zhenpeng Su, Wei Huang, Jianwei Niu, Jungong Han, Guiguang Ding
Categories: cs.CL
Comments: Accepted by EMNLP'25 Main Conference (Oral presentation),
  Camera-ready version
\\ ( https://arxiv.org/abs/2404.17785 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2406.14498
replaced with revised version Mon, 22 Sep 2025 14:02:03 GMT   (15363kb)

Title: LLaSA: A Sensor-Aware LLM for Natural Language Reasoning of Human
  Activity from IMU Data
Authors: Sheikh Asif Imran, Mohammad Nur Hossain Khan, Subrata Biswas, Bashima
  Islam
Categories: cs.CL
\\ ( https://arxiv.org/abs/2406.14498 ,  15363kb)
------------------------------------------------------------------------------
\\
arXiv:2406.17974
replaced with revised version Sat, 20 Sep 2025 07:43:54 GMT   (9552kb)

Title: Evaluating Fairness in Large Vision-Language Models Across Diverse
  Demographic Attributes and Prompts
Authors: Xuyang Wu, Yuan Wang, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang
Categories: cs.CL cs.CV
Comments: EMNLP Findings
\\ ( https://arxiv.org/abs/2406.17974 ,  9552kb)
------------------------------------------------------------------------------
\\
arXiv:2407.04615
replaced with revised version Sat, 20 Sep 2025 15:33:57 GMT   (725kb)

Title: On the Low-Rank Parametrization of Reward Models for Controlled Language
  Generation
Authors: Sergey Troshin, Vlad Niculae, Antske Fokkens
Categories: cs.CL
Comments: TMLR 2025
\\ ( https://arxiv.org/abs/2407.04615 ,  725kb)
------------------------------------------------------------------------------
\\
arXiv:2409.00399
replaced with revised version Sun, 21 Sep 2025 06:37:27 GMT   (485kb)

Title: Rethinking Backdoor Detection Evaluation for Language Models
Authors: Jun Yan, Wenjie Jacky Mo, Xiang Ren, Robin Jia
Categories: cs.CL cs.CR
Comments: Accepted to EMNLP 2025
\\ ( https://arxiv.org/abs/2409.00399 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2409.04183
replaced with revised version Mon, 22 Sep 2025 16:36:18 GMT   (253kb)

Title: GALLa: Graph Aligned Large Language Models for Improved Source Code
  Understanding
Authors: Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang
Categories: cs.CL cs.AI
Comments: ACL 2025
\\ ( https://arxiv.org/abs/2409.04183 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2410.16322
replaced with revised version Fri, 19 Sep 2025 19:04:10 GMT   (1898kb)

Title: SouLLMate: An Application Enhancing Diverse Mental Health Support with
  Adaptive LLMs, Prompt Engineering, and RAG Techniques
Authors: Qiming Guo, Jinwen Tang, Wenbo Sun, Haoteng Tang, Yi Shang, Wenlu Wang
Categories: cs.CL cs.AI cs.HC
Comments: 26 pages, 19 figures, 8 tables
\\ ( https://arxiv.org/abs/2410.16322 ,  1898kb)
------------------------------------------------------------------------------
\\
arXiv:2410.16531
replaced with revised version Mon, 22 Sep 2025 16:30:22 GMT   (2329kb)

Title: Bayesian scaling laws for in-context learning
Authors: Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman
Categories: cs.CL cs.AI cs.FL cs.LG
Comments: COLM 2025 camera-ready version; 9 pages main text, 39 pages total
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2410.16531 ,  2329kb)
------------------------------------------------------------------------------
\\
arXiv:2410.21508
replaced with revised version Sat, 20 Sep 2025 09:57:39 GMT   (2580kb)

Title: Group-SAE: Efficient Training of Sparse Autoencoders for Large Language
  Models via Layer Groups
Authors: Davide Ghilardi, Federico Belotti, Marco Molinari, Tao Ma, Matteo
  Palmonari
Categories: cs.CL cs.AI
Comments: Accepted version at EMNLP'25
\\ ( https://arxiv.org/abs/2410.21508 ,  2580kb)
------------------------------------------------------------------------------
\\
arXiv:2411.00300
replaced with revised version Mon, 22 Sep 2025 17:11:26 GMT   (5158kb)

Title: Rationale-Guided Retrieval Augmented Generation for Medical Question
  Answering
Authors: Jiwoong Sohn, Yein Park, Chanwoong Yoon, Sihyeon Park, Hyeon Hwang,
  Mujeen Sung, Hyunjae Kim, Jaewoo Kang
Categories: cs.CL
Comments: Accepted to NAACL 2025 (Oral)
\\ ( https://arxiv.org/abs/2411.00300 ,  5158kb)
------------------------------------------------------------------------------
\\
arXiv:2411.15640
replaced with revised version Mon, 22 Sep 2025 01:02:09 GMT   (5816kb)

Title: AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering
  Benchmark Dataset
Authors: Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi,
  Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse
  Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas
  Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu MD, Naome A. Etori,
  Aim\'erou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best,
  Irfan Essa, Stephen Edward Moore, Chris Fourie, Mercy Nyamewaa Asiedu
Categories: cs.CL
Comments: ACL 2025 Main Conference (long paper, Best Social Impact Paper Award)
Journal-ref: Proceedings of the 63rd Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers), pages 1948-1973, Vienna,
  Austria. Association for Computational Linguistics, 2025
DOI: 10.18653/v1/2025.acl-long.96
\\ ( https://arxiv.org/abs/2411.15640 ,  5816kb)
------------------------------------------------------------------------------
\\
arXiv:2411.18831
replaced with revised version Fri, 19 Sep 2025 20:50:29 GMT   (26263kb)

Title: Measuring Risk of Bias in Biomedical Reports: The RoBBR Benchmark
Authors: Jianyou Wang, Weili Cao, Longtian Bao, Youze Zheng, Gil Pasternak,
  Kaicheng Wang, Xiaoyue Wang, Ramamohan Paturi, Leon Bergen
Categories: cs.CL
Comments: Published at EMNLP 2025 (Main)
\\ ( https://arxiv.org/abs/2411.18831 ,  26263kb)
------------------------------------------------------------------------------
\\
arXiv:2412.03230
replaced with revised version Mon, 22 Sep 2025 07:21:41 GMT   (2124kb)

Title: PERL: Pinyin Enhanced Rephrasing Language Model for Chinese ASR N-best
  Error Correction
Authors: Junhong Liang, Bojun Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2412.03230 ,  2124kb)
------------------------------------------------------------------------------
\\
arXiv:2412.08864
replaced with revised version Mon, 22 Sep 2025 05:18:24 GMT   (295kb)

Title: GRIP: A Graph-Based Reasoning Instruction Producer
Authors: Jiankang Wang, Jianjun Xu, Xiaorui Wang, Yuxin Wang, Mengting Xing,
  Shancheng Fang, Hongtao Xie
Categories: cs.CL
\\ ( https://arxiv.org/abs/2412.08864 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2412.16686
replaced with revised version Sat, 20 Sep 2025 17:39:23 GMT   (2090kb)

Title: NILE: Internal Consistency Alignment in Large Language Models
Authors: Minda Hu, Qiyuan Zhang, Yufei Wang, Bowei He, Hongru Wang, Jingyan
  Zhou, Liangyou Li, Yasheng Wang, Chen Ma, Irwin King
Categories: cs.CL
Comments: This work has been accepted by EMNLP 2025
\\ ( https://arxiv.org/abs/2412.16686 ,  2090kb)
------------------------------------------------------------------------------
\\
arXiv:2501.06582
replaced with revised version Sun, 21 Sep 2025 22:40:13 GMT   (91kb)

Title: ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting
Authors: Steven H. Wang, Maksim Zubkov, Kexin Fan, Sarah Harrell, Yuyang Sun,
  Wei Chen, Andreas Plesner, Roger Wattenhofer
Categories: cs.CL
Comments: ACL 2025 Findings. 9 pages + appendix. Code and data are available at
  https://www.atticusprojectai.org/acord
\\ ( https://arxiv.org/abs/2501.06582 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2501.14315
replaced with revised version Mon, 22 Sep 2025 01:39:13 GMT   (349kb)

Title: Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token
  Learning
Authors: Chao-Chung Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Shao-Hua
  Sun, Hung-yi Lee
Categories: cs.CL
\\ ( https://arxiv.org/abs/2501.14315 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2502.00085
replaced with revised version Mon, 22 Sep 2025 12:28:41 GMT   (744kb)

Title: Efficient Beam Search for Large Language Models Using Trie-Based
  Decoding
Authors: Brian J Chan, MaoXun Huang, Jui-Hung Cheng, Chao-Ting Chen, Hen-Hsen
  Huang
Categories: cs.CL
Comments: 13 pages, accepted as a main conference paper at EMNLP 2025
\\ ( https://arxiv.org/abs/2502.00085 ,  744kb)
------------------------------------------------------------------------------
\\
arXiv:2502.00919
replaced with revised version Mon, 22 Sep 2025 16:16:25 GMT   (5065kb)

Title: Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings
Authors: Stephen Zhang, Mustafa Khan, Vardan Papyan
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2502.00919 ,  5065kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01609
replaced with revised version Sun, 21 Sep 2025 07:16:27 GMT   (1923kb)

Title: Adaptive Distraction: Probing LLM Contextual Robustness with Automated
  Tree Search
Authors: Yanbo Wang, Zixiang Xu, Yue Huang, Chujie Gao, Siyuan Wu, Jiayi Ye,
  Pin-Yu Chen, Xiuying Chen, Xiangliang Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2502.01609 ,  1923kb)
------------------------------------------------------------------------------
\\
arXiv:2502.08550
replaced with revised version Sat, 20 Sep 2025 13:38:54 GMT   (1642kb)

Title: No Need for Explanations: LLMs can implicitly learn from mistakes
  in-context
Authors: Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Tan Yi-Chern, Marek
  Rei, Max Bartolo
Categories: cs.CL cs.AI
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2502.08550 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11114
replaced with revised version Sat, 20 Sep 2025 08:55:52 GMT   (2975kb)

Title: Beyond Pairwise: Global Zero-shot Temporal Graph Generation
Authors: Alon Eirew, Kfir Bar, Ido Dagan
Categories: cs.CL
Comments: Accepted to the main track of EMNLP 2025
\\ ( https://arxiv.org/abs/2502.11114 ,  2975kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11546
replaced with revised version Sun, 21 Sep 2025 01:30:39 GMT   (1542kb)

Title: DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data
  Cleaning as Anomaly Detection
Authors: Yingli Shen, Wen Lai, Shuo Wang, Xueren Zhang, Kangyang Luo, Alexander
  Fraser, Maosong Sun
Categories: cs.CL
\\ ( https://arxiv.org/abs/2502.11546 ,  1542kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11559
replaced with revised version Mon, 22 Sep 2025 11:13:55 GMT   (966kb)

Title: Auto-Search and Refinement: An Automated Framework for Gender Bias
  Mitigation in Large Language Models
Authors: Yue Xu, Chengyan Fu, Li Xiong, Sibei Yang, Wenjie Wang
Categories: cs.CL cs.AI
Comments: Accepted to NeurIPS 2025
\\ ( https://arxiv.org/abs/2502.11559 ,  966kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12289
replaced with revised version Sat, 20 Sep 2025 17:56:26 GMT   (1268kb)

Title: Evaluating Step-by-step Reasoning Traces: A Survey
Authors: Jinu Lee, Julia Hockenmaier
Categories: cs.CL
Comments: EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2502.12289 ,  1268kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12459
replaced with revised version Sun, 21 Sep 2025 09:10:28 GMT   (90kb)

Title: Large Language Models Badly Generalize across Option Length, Problem
  Types, and Irrelevant Noun Replacements
Authors: Guangxiang Zhao, Saier Hu, Xiaoqi Jian, Jinzhu Wu, Yuhan Wu, Change
  Jia, Lin Sun, Xiangzheng Zhang
Categories: cs.CL cs.AI cs.LG
Comments: EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2502.12459 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12970
replaced with revised version Sat, 20 Sep 2025 09:33:02 GMT   (701kb)

Title: Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language
  Models from Jailbreaking
Authors: Junda Zhu, Lingyong Yan, Shuaiqiang Wang, Dawei Yin, Lei Sha
Categories: cs.CL
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2502.12970 ,  701kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13251
replaced with revised version Mon, 22 Sep 2025 12:03:22 GMT   (1070kb)

Title: Neural Attention Search
Authors: Difan Deng and Marius Lindauer
Categories: cs.CL cs.AI
Comments: 18 pages, 8 figures
\\ ( https://arxiv.org/abs/2502.13251 ,  1070kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13780
replaced with revised version Sun, 21 Sep 2025 11:24:51 GMT   (6984kb)

Title: Translation in the Hands of Many:Centering Lay Users in Machine
  Translation Interactions
Authors: Beatrice Savoldi and Alan Ramponi and Matteo Negri and Luisa
  Bentivogli
Categories: cs.CL cs.CY
\\ ( https://arxiv.org/abs/2502.13780 ,  6984kb)
------------------------------------------------------------------------------
\\
arXiv:2502.15361
replaced with revised version Sat, 20 Sep 2025 07:58:54 GMT   (884kb)

Title: Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and
  Mitigation in LLM Reasoning
Authors: Xuyang Wu, Jinming Nian, Ting-Ruen Wei, Zhiqiang Tao, Hsin-Tai Wu, Yi
  Fang
Categories: cs.CL cs.AI
Comments: EMNLP Findings
\\ ( https://arxiv.org/abs/2502.15361 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16989
replaced with revised version Mon, 22 Sep 2025 08:22:14 GMT   (5318kb)

Title: All-in-one: Understanding and Generation in Multimodal Reasoning with
  the MAIA Benchmark
Authors: Davide Testa, Giovanni Bonetta, Raffaella Bernardi, Alessandro
  Bondielli, Alessandro Lenci, Alessio Miaschi, Lucia Passaro, Bernardo Magnini
Categories: cs.CL
Comments: Accepted at Findings of EMNLP 2025
\\ ( https://arxiv.org/abs/2502.16989 ,  5318kb)
------------------------------------------------------------------------------
\\
arXiv:2502.17775
replaced with revised version Mon, 22 Sep 2025 15:43:04 GMT   (5877kb)

Title: FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks
Authors: Tanawan Premsri, Parisa Kordjamshidi
Categories: cs.CL
Comments: 10 pages, 3 Figures, 4 Tables, EMNLP-2025 Main (Oral)
\\ ( https://arxiv.org/abs/2502.17775 ,  5877kb)
------------------------------------------------------------------------------
\\
arXiv:2502.19074
replaced with revised version Sat, 20 Sep 2025 16:13:22 GMT   (10735kb)

Title: Improving the quality of Web-mined Parallel Corpora of Low-Resource
  Languages using Debiasing Heuristics
Authors: Aloka Fernando, Nisansa de Silva, Menan Velyuthan, Charitha
  Rathnayake, Surangika Ranathunga
Categories: cs.CL
Comments: EMNLP 2025 Camera-ready version
\\ ( https://arxiv.org/abs/2502.19074 ,  10735kb)
------------------------------------------------------------------------------
\\
arXiv:2503.01606
replaced with revised version Sat, 20 Sep 2025 05:40:45 GMT   (9817kb)

Title: Beyond Prompting: An Efficient Embedding Framework for Open-Domain
  Question Answering
Authors: Zhanghao Hu, Hanqi Yan, Qinglin Zhu, Zhenyi Shen, Yulan He, Lin Gui
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: Accepted in ACL 2025 Main, Project link:
  https://zhanghao-acl25-embqa.github.io/ACL2025-EmbQA/
\\ ( https://arxiv.org/abs/2503.01606 ,  9817kb)
------------------------------------------------------------------------------
\\
arXiv:2503.01830
replaced with revised version Sat, 20 Sep 2025 12:04:48 GMT   (21395kb)

Title: From Language to Cognition: How LLMs Outgrow the Human Language Network
Authors: Badr AlKhamissi, Greta Tuckute, Yingtian Tang, Taha Binhuraib, Antoine
  Bosselut, Martin Schrimpf
Categories: cs.CL
Comments: EMNLP 2025. Project Page at https://language-to-cognition.epfl.ch
\\ ( https://arxiv.org/abs/2503.01830 ,  21395kb)
------------------------------------------------------------------------------
\\
arXiv:2503.04372
replaced with revised version Mon, 22 Sep 2025 16:34:43 GMT   (129kb)

Title: Assumed Identities: Quantifying Gender Bias in Machine Translation of
  Gender-Ambiguous Occupational Terms
Authors: Orfeas Menis Mastromichalakis, Giorgos Filandrianos, Maria Symeonaki
  and Giorgos Stamou
Categories: cs.CL
Comments: Accepted for presentation at EMNLP 2025
\\ ( https://arxiv.org/abs/2503.04372 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2503.05683
replaced with revised version Sun, 21 Sep 2025 09:03:38 GMT   (13607kb)

Title: WikiBigEdit: Understanding the Limits of Lifelong Knowledge Editing in
  LLMs
Authors: Lukas Thede, Karsten Roth, Matthias Bethge, Zeynep Akata, Tom
  Hartvigsen
Categories: cs.CL cs.LG
Comments: published at ICML 2025
\\ ( https://arxiv.org/abs/2503.05683 ,  13607kb)
------------------------------------------------------------------------------
\\
arXiv:2503.07129
replaced with revised version Fri, 19 Sep 2025 19:56:48 GMT   (10281kb)

Title: ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning via
  Tool-integrated Action for Dynamic Offer Optimization
Authors: Deuksin Kwon, Jiwon Hae, Emma Clift, Daniel Shamsoddini, Jonathan
  Gratch, Gale M. Lucas
Categories: cs.CL cs.AI
Comments: Accepted to EMNLP 2025 (Main Conference)
\\ ( https://arxiv.org/abs/2503.07129 ,  10281kb)
------------------------------------------------------------------------------
\\
arXiv:2503.08067
replaced with revised version Sat, 20 Sep 2025 19:02:42 GMT   (8622kb)

Title: Context-aware Biases for Length Extrapolation
Authors: Ali Veisi, Hamidreza Amirzadeh, Amir Mansourian
Categories: cs.CL
Comments: Accepted at EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2503.08067 ,  8622kb)
------------------------------------------------------------------------------
\\
arXiv:2503.12051
replaced with revised version Sat, 20 Sep 2025 11:21:17 GMT   (9326kb)

Title: TLUE: A Tibetan Language Understanding Evaluation Benchmark
Authors: Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering,
  Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Hao
  Wang Xiao Feng, Yongbin Yu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2503.12051 ,  9326kb)
------------------------------------------------------------------------------
\\
arXiv:2503.13222
replaced with revised version Sat, 20 Sep 2025 19:58:35 GMT   (2590kb)

Title: Can Language Models Follow Multiple Turns of Entangled Instructions?
Authors: Chi Han, Xin Liu, Haodong Wang, Shiyang Li, Jingfeng Yang, Haoming
  Jiang, Zhengyang Wang, Qingyu Yin, Liang Qiu, Changlong Yu, Yifan Gao, Zheng
  Li, Bing Yin, Jingbo Shang, Heng Ji
Categories: cs.CL cs.AI
Comments: The 2025 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2025) Findings
\\ ( https://arxiv.org/abs/2503.13222 ,  2590kb)
------------------------------------------------------------------------------
\\
arXiv:2503.18172
replaced with revised version Sat, 20 Sep 2025 08:48:39 GMT   (5740kb)

Title: Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language
  Models on Misleading Chart Question Answering
Authors: Zixin Chen, Sicheng Song, Kashun Shum, Yanna Lin, Rui Sheng, Weiqi
  Wang, Huamin Qu
Categories: cs.CL cs.AI
Comments: 34 pages in total, EMNLP 2025
\\ ( https://arxiv.org/abs/2503.18172 ,  5740kb)
------------------------------------------------------------------------------
\\
arXiv:2503.18247
replaced with revised version Fri, 19 Sep 2025 23:25:20 GMT   (120kb)

Title: AfroXLMR-Social: Adapting Pre-trained Language Models for African
  Languages Social Media Text
Authors: Tadesse Destaw Belay, Israel Abebe Azime, Ibrahim Said Ahmad, David
  Ifeoluwa Adelani, Idris Abdulmumin, Abinew Ali Ayele, Shamsuddeen Hassan
  Muhammad, Seid Muhie Yimam
Categories: cs.CL
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2503.18247 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2503.22973
replaced with revised version Sat, 20 Sep 2025 14:53:34 GMT   (1333kb)

Title: XL-Suite: Cross-Lingual Synthetic Training and Evaluation Data for
  Open-Ended Generation
Authors: Vivek Iyer, Pinzhen Chen, Ricardo Rei, and Alexandra Birch
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to EMNLP 2025 (Findings)
\\ ( https://arxiv.org/abs/2503.22973 ,  1333kb)
------------------------------------------------------------------------------
\\
arXiv:2504.02965
replaced with revised version Sat, 20 Sep 2025 21:32:32 GMT   (818kb)

Title: CoLa: Learning to Interactively Collaborate with Large Language Models
Authors: Abhishek Sharma and Dan Goldwasser
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2504.02965 ,  818kb)
------------------------------------------------------------------------------
\\
arXiv:2504.04713
replaced with revised version Sat, 20 Sep 2025 12:21:08 GMT   (1970kb)

Title: Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting
  Sequential Needles from Long Contexts
Authors: Yifei Yu, Qian-Wen Zhang, Lingfeng Qiao, Di Yin, Fang Li, Jie Wang,
  Zengxi Chen, Suncong Zheng, Xiaolong Liang, Xing Sun
Categories: cs.CL cs.IR
\\ ( https://arxiv.org/abs/2504.04713 ,  1970kb)
------------------------------------------------------------------------------
\\
arXiv:2504.09696
replaced with revised version Fri, 19 Sep 2025 23:06:38 GMT   (176kb)

Title: GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for
  Concise Mathematical Reasoning in Language Models
Authors: Jixiao Zhang, Chunsheng Zuo
Categories: cs.CL
Comments: Accepted to EMNLP 2025 (Main)
\\ ( https://arxiv.org/abs/2504.09696 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2504.11626
replaced with revised version Mon, 22 Sep 2025 15:58:14 GMT   (102kb)

Title: Improving Instruct Models for Free: A Study on Partial Adaptation
Authors: Ozan \.Irsoy, Pengxiang Cheng, Jennifer L. Chen, Daniel
  Preo\c{t}iuc-Pietro, Shiyue Zhang, Duccio Pappadopulo
Categories: cs.CL cs.AI
Comments: Author ordering chosen at random; accepted to EMNLP 2025
\\ ( https://arxiv.org/abs/2504.11626 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2504.12913
replaced with revised version Sun, 21 Sep 2025 14:21:59 GMT   (5210kb)

Title: MAIN: Mutual Alignment Is Necessary for instruction tuning
Authors: Fanyi Yang, Jianfeng Liu, Xin Zhang, Haoyu Liu, Xixin Cao, Yuefeng
  Zhan, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang
Categories: cs.CL
Comments: Accepted by EMNLP 2025
\\ ( https://arxiv.org/abs/2504.12913 ,  5210kb)
------------------------------------------------------------------------------
\\
arXiv:2504.21800
replaced with revised version Sat, 20 Sep 2025 08:28:38 GMT   (878kb)

Title: How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in
  Prolonged Exposure Dialogues
Authors: Suhas BN, Dominik Mattioli, Saeed Abdullah, Rosa I. Arriaga, Chris W.
  Wiese, Andrew M. Sherrill
Categories: cs.CL cs.AI cs.CY cs.HC
Comments: 10 pages, 5 tables. Accepted for Poster presentation at EMNLP 2025
MSC-class: 68T50
ACM-class: I.2.7; H.3.1
\\ ( https://arxiv.org/abs/2504.21800 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2505.00367
replaced with revised version Sat, 20 Sep 2025 16:44:05 GMT   (5912kb)

Title: KoACD: The First Korean Adolescent Dataset for Cognitive Distortion
  Analysis via Role-Switching Multi-LLM Negotiation
Authors: JunSeo Kim and HyeHyeon Kim
Categories: cs.CL cs.AI
Comments: Accepted to Findings of EMNLP 2025
\\ ( https://arxiv.org/abs/2505.00367 ,  5912kb)
------------------------------------------------------------------------------
\\
arXiv:2505.08498
replaced with revised version Sun, 21 Sep 2025 10:54:09 GMT   (239kb)

Title: LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using
  Large Language Models
Authors: Takumi Shibata, Yuichi Miyamura
Categories: cs.CL cs.AI
Comments: Accepted to EMNLP 2025 (Main Conference)
\\ ( https://arxiv.org/abs/2505.08498 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11876
replaced with revised version Mon, 22 Sep 2025 11:16:56 GMT   (365kb)

Title: EAMET: Robust Massive Model Editing via Embedding Alignment Optimization
Authors: Yanbo Dai, Zhenlan Ji, Zongjie Li, Shuai Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.11876 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12244
replaced with revised version Sun, 21 Sep 2025 20:15:35 GMT   (502kb)

Title: Distribution Prompting: Understanding the Expressivity of Language
  Models Through the Next-Token Distributions They Can Produce
Authors: Haojin Wang, Zining Zhu, Freda Shi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.12244 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13388
replaced with revised version Fri, 19 Sep 2025 22:07:47 GMT   (2350kb)

Title: R3: Robust Rubric-Agnostic Reward Models
Authors: David Anugraha, Zilu Tang, Lester James V. Miranda, Hanyang Zhao,
  Mohammad Rifqi Farhansyah, Garry Kuwanto, Derry Wijaya, Genta Indra Winata
Categories: cs.CL cs.AI cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2505.13388 ,  2350kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13448
replaced with revised version Fri, 19 Sep 2025 19:06:03 GMT   (2276kb)

Title: CIE: Controlling Language Model Text Generations Using Continuous
  Signals
Authors: Vinay Samuel, Harshita Diddee, Yiming Zhang, Daphne Ippolito
Categories: cs.CL cs.AI
Comments: EMNLP Main 2025
\\ ( https://arxiv.org/abs/2505.13448 ,  2276kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14305
replaced with revised version Sat, 20 Sep 2025 16:33:55 GMT   (1264kb)

Title: JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy
  Schema Sampling
Authors: Jinwang Song, Hongying Zan, Kunli Zhang, Lingling Mu, Yingjie Han,
  Haobo Hua, Min Peng
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2505.14305 ,  1264kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14347
replaced with revised version Sun, 21 Sep 2025 17:11:28 GMT   (97kb)

Title: QA-prompting: Improving Summarization with Large Language Models using
  Question-Answering
Authors: Neelabh Sinha
Categories: cs.CL
Comments: Accepted at The Fifth Workshop on New Frontiers in Summarization
  (NewSumm) in The 2025 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2025)
\\ ( https://arxiv.org/abs/2505.14347 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14423
replaced with revised version Mon, 22 Sep 2025 09:03:22 GMT   (11274kb)

Title: Scaling Low-Resource MT via Synthetic Data Generation with LLMs
Authors: Ona de Gibert, Joseph Attieh, Teemu Vahtola, Mikko Aulamo, Zihao Li,
  Ra\'ul V\'azquez, Tiancheng Hu, J\"org Tiedemann
Categories: cs.CL
Comments: Accepted at EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2505.14423 ,  11274kb)
------------------------------------------------------------------------------
\\
arXiv:2505.14660
replaced with revised version Fri, 19 Sep 2025 21:59:34 GMT   (4916kb)

Title: EmoGist: Efficient In-Context Learning for Visual Emotion Understanding
Authors: Ronald Seoh, Dan Goldwasser
Categories: cs.CL cs.AI cs.CV
Comments: EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2505.14660 ,  4916kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15065
replaced with revised version Sat, 20 Sep 2025 08:07:48 GMT   (7014kb)

Title: The Pursuit of Empathy: Evaluating Small Language Models for PTSD
  Dialogue Support
Authors: Suhas BN, Yash Mahajan, Dominik Mattioli, Andrew M. Sherrill, Rosa I.
  Arriaga, Chris W. Wiese, and Saeed Abdullah
Categories: cs.CL cs.AI cs.CY
Comments: 23 pages, 3 figures. Accepted for Oral presentation at EMNLP 2025
MSC-class: 68T50, 68T05
ACM-class: I.2.7; I.2.1; H.5.2
\\ ( https://arxiv.org/abs/2505.15065 ,  7014kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15108
replaced with revised version Sat, 20 Sep 2025 12:53:21 GMT   (651kb)

Title: A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents
Authors: Ian Steenstra and Timothy W. Bickmore
Categories: cs.CL cs.AI cs.HC
Comments: This is a preprint version of the paper accepted to IVA'25
DOI: 10.1145/3717511.3749286
\\ ( https://arxiv.org/abs/2505.15108 ,  651kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15712
replaced with revised version Mon, 22 Sep 2025 17:02:11 GMT   (1978kb)

Title: TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games
Authors: Yuan Yuan, Muyu He, Muhammad Adil Shahid, Jiani Huang, Ziyang Li, Li
  Zhang
Categories: cs.CL
Comments: In EMNLP 2025 main conference
\\ ( https://arxiv.org/abs/2505.15712 ,  1978kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17098
replaced with revised version Sat, 20 Sep 2025 05:31:09 GMT   (2651kb)

Title: TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided
  Sequence Configuration
Authors: Yanshu Li, Jianjiang Yang, Tian Yun, Pinyuan Feng, Jinfa Huang,
  Ruixiang Tang
Categories: cs.CL cs.CV
Comments: EMNLP2025 Main, 28 pages, 11 figures, 19 tables
\\ ( https://arxiv.org/abs/2505.17098 ,  2651kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17505
replaced with revised version Mon, 22 Sep 2025 08:41:30 GMT   (1046kb)

Title: L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large
  Language Models
Authors: Xiaohao Liu, Xiaobo Xia, Weixiang Zhao, Manyi Zhang, Xianzhi Yu, Xiu
  Su, Shuo Yang, See-Kiong Ng, Tat-Seng Chua
Categories: cs.CL
Comments: Accepted by NeurIPS 2025. Codes are available at
  https://github.com/Xiaohao-Liu/L-MTP
\\ ( https://arxiv.org/abs/2505.17505 ,  1046kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17601
replaced with revised version Sun, 21 Sep 2025 11:13:09 GMT   (3244kb)

Title: Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning
  Framework via Harmless Inputs
Authors: Jiawei Kong, Hao Fang, Xiaochen Yang, Kuofeng Gao, Bin Chen, Shu-Tao
  Xia, Ke Xu, Han Qiu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.17601 ,  3244kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17616
replaced with revised version Mon, 22 Sep 2025 01:20:02 GMT   (487kb)

Title: Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large
  Language Model-based Agents in Embodied Environments
Authors: Qingyu Lu, Liang Ding, Siyi Cao, Xuebo Liu, Kanjian Zhang, Jinxia
  Zhang, Dacheng Tao
Categories: cs.CL cs.AI
Comments: EMNLP 2025 - Findings
\\ ( https://arxiv.org/abs/2505.17616 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17747
replaced with revised version Mon, 22 Sep 2025 11:34:10 GMT   (2106kb)

Title: Discriminating Form and Meaning in Multilingual Models with Minimal-Pair
  ABX Tasks
Authors: Maureen de Seyssel, Jie Chi, Skyler Seto, Maartje ter Hoeve, Masha
  Fedzechkina, Natalie Schluter
Categories: cs.CL
Comments: Comments: Accepted to EMNLP 2025. Camera-ready version. 22 pages, 16
  figures
\\ ( https://arxiv.org/abs/2505.17747 ,  2106kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18761
replaced with revised version Mon, 22 Sep 2025 16:41:27 GMT   (737kb)

Title: How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using
  a Controlled Benchmark
Authors: Minglai Yang, Ethan Huang, Liang Zhang, Mihai Surdeanu, William Wang,
  and Liangming Pan
Categories: cs.CL cs.AI cs.LG
Comments: 19 pages, 10 figures, 5 tables
\\ ( https://arxiv.org/abs/2505.18761 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20045
replaced with revised version Sat, 20 Sep 2025 20:02:35 GMT   (144kb)

Title: Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty
  Quantification for LLMs
Authors: Artem Vazhentsev, Lyudmila Rvanova, Gleb Kuzmin, Ekaterina Fadeeva,
  Ivan Lazichny, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Mrinmaya
  Sachan, Preslav Nakov, Artem Shelmanov
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.20045 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20099
replaced with revised version Mon, 22 Sep 2025 13:18:46 GMT   (1018kb)

Title: Large Language Models Meet Knowledge Graphs for Question Answering:
  Synthesis and Opportunities
Authors: Chuangtao Ma, Yongrui Chen, Tianxing Wu, Arijit Khan, Haofen Wang
Categories: cs.CL cs.AI cs.IR
Comments: Accepted at EMNLP 2025 Main
\\ ( https://arxiv.org/abs/2505.20099 ,  1018kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20264
replaced with revised version Fri, 19 Sep 2025 18:15:26 GMT   (26kb)

Title: We Need to Measure Data Diversity in NLP -- Better and Broader
Authors: Dong Nguyen and Esther Ploeger
Categories: cs.CL cs.AI
Comments: EMNLP 2025
\\ ( https://arxiv.org/abs/2505.20264 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20276
replaced with revised version Sat, 20 Sep 2025 06:44:40 GMT   (4868kb)

Title: Does quantization affect models' performance on long-context tasks?
Authors: Anmol Mekala, Anirudh Atmakuru, Yixiao Song, Marzena Karpinska, Mohit
  Iyyer
Categories: cs.CL cs.AI
Comments: to appear in EMNLP 2025
\\ ( https://arxiv.org/abs/2505.20276 ,  4868kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20496
replaced with revised version Fri, 19 Sep 2025 22:29:42 GMT   (9445kb)

Title: Inceptive Transformers: Enhancing Contextual Representations through
  Multi-Scale Feature Learning Across Domains and Languages
Authors: Asif Shahriar and Rifat Shahriyar and M Saifur Rahman
Categories: cs.CL
Comments: Accepted to EMNLP 2025 (long paper). To appear in the Proceedings of
  EMNLP 2025
\\ ( https://arxiv.org/abs/2505.20496 ,  9445kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21548
replaced with revised version Sun, 21 Sep 2025 19:54:06 GMT   (756kb)

Title: Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment
Authors: Dhruv Agarwal, Anya Shukla, Sunayana Sitaram, Aditya Vashistha
Categories: cs.CL cs.AI cs.CY physics.soc-ph
Comments: Under review
\\ ( https://arxiv.org/abs/2505.21548 ,  756kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21693
replaced with revised version Mon, 22 Sep 2025 13:38:32 GMT   (8572kb)

Title: MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural
  Awareness Evaluation for LLMs
Authors: Raoyuan Zhao, Beiduo Chen, Barbara Plank, Michael A. Hedderich
Categories: cs.CL
Comments: Accepted by EMNLP 2025 Findings, 33 pages, 30 figures
\\ ( https://arxiv.org/abs/2505.21693 ,  8572kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22118
replaced with revised version Mon, 22 Sep 2025 13:16:16 GMT   (313kb)

Title: Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of
  Two Approaches
Authors: Alan Ramponi, Marco Rovera, Robert Moro, Sara Tonelli
Categories: cs.CL
Comments: Accepted to EMNLP 2025 Main Conference
\\ ( https://arxiv.org/abs/2505.22118 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2505.22157
replaced with revised version Sat, 20 Sep 2025 18:32:10 GMT   (870kb)

Title: LASER: Stratified Selective Sampling for Instruction Tuning with
  Dedicated Scoring Strategy
Authors: Paramita Mirza, Lucas Weber, Fabian K\"uch
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.22157 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23001
replaced with revised version Sun, 21 Sep 2025 21:34:23 GMT   (243kb)

Title: DyePack: Provably Flagging Test Set Contamination in LLMs Using
  Backdoors
Authors: Yize Cheng, Wenxiao Wang, Mazda Moayeri, Soheil Feizi
Categories: cs.CL
Comments: EMNLP2025 main, Camera-ready
\\ ( https://arxiv.org/abs/2505.23001 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23323
replaced with revised version Sat, 20 Sep 2025 13:40:43 GMT   (107kb)

Title: Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through
  Context-Directed Extrapolation from Training Data Priors
Authors: Harish Tayyar Madabushi and Melissa Torgbi and Claire Bonial
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.23323 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23765
replaced with revised version Sat, 20 Sep 2025 23:43:20 GMT   (9450kb)

Title: From Chat Logs to Collective Insights: Aggregative Question Answering
Authors: Wentao Zhang, Woojeong Kim, Yuntian Deng
Categories: cs.CL cs.AI cs.LG
Journal-ref: EMNLP 2025
\\ ( https://arxiv.org/abs/2505.23765 ,  9450kb)
------------------------------------------------------------------------------
\\
arXiv:2505.24456
replaced with revised version Sun, 21 Sep 2025 22:37:07 GMT   (8872kb)

Title: CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation
Authors: Emilio Villa-Cueva, Sholpan Bolatzhanova, Diana Turmakhan, Kareem
  Elzeky, Henok Biadglign Ademtew, Alham Fikri Aji, Vladimir Araujo, Israel
  Abebe Azime, Jinheon Baek, Frederico Belcavello, Fermin Cristobal, Jan
  Christian Blaise Cruz, Mary Dabre, Raj Dabre, Toqeer Ehsan, Naome A Etori,
  Fauzan Farooqui, Jiahui Geng, Guido Ivetta, Thanmay Jayakumar, Soyeong Jeong,
  Zheng Wei Lim, Aishik Mandal, Sofia Martinelli, Mihail Minkov Mihaylov,
  Daniil Orel, Aniket Pramanick, Sukannya Purkayastha, Israfel Salazar, Haiyue
  Song, Tiago Timponi Torrent, Debela Desalegn Yadeta, Injy Hamed, Atnafu
  Lambebo Tonja, Thamar Solorio
Categories: cs.CL
\\ ( https://arxiv.org/abs/2505.24456 ,  8872kb)
------------------------------------------------------------------------------
\\
arXiv:2505.24544
replaced with revised version Mon, 22 Sep 2025 01:23:11 GMT   (932kb)

Title: Cross-Attention Speculative Decoding
Authors: Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji,
  Chul Lee
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.24544 ,  932kb)
------------------------------------------------------------------------------
\\
arXiv:2506.00137
replaced with revised version Sat, 20 Sep 2025 14:37:31 GMT   (732kb)

Title: LaMP-QA: A Benchmark for Personalized Long-form Question Answering
Authors: Alireza Salemi, Hamed Zamani
Categories: cs.CL cs.IR cs.LG
\\ ( https://arxiv.org/abs/2506.00137 ,  732kb)
------------------------------------------------------------------------------
\\
arXiv:2506.00608
replaced with revised version Mon, 22 Sep 2025 14:28:02 GMT   (9156kb)

Title: PAKTON: A Multi-Agent Framework for Question Answering in Long Legal
  Agreements
Authors: Petros Raptopoulos, Giorgos Filandrianos, Maria Lymperaiou, Giorgos
  Stamou
Categories: cs.CL
Comments: Accepted at EMNLP 2025
\\ ( https://arxiv.org/abs/2506.00608 ,  9156kb)
------------------------------------------------------------------------------
\\
arXiv:2506.01646
replaced with revised version Fri, 19 Sep 2025 20:11:23 GMT   (4251kb)

Title: ESGenius: Benchmarking LLMs on Environmental, Social, and Governance
  (ESG) and Sustainability Knowledge
Authors: Chaoyue He, Xin Zhou, Yi Wu, Xinjia Yu, Yan Zhang, Lei Zhang, Di Wang,
  Shengfei Lyu, Hong Xu, Xiaoqiao Wang, Wei Liu and Chunyan Miao
Categories: cs.CL cs.AI cs.LG
Comments: EMNLP'25 Main Oral (42 pages, 10 figures, 11 tables), Nominations for
  Resource Award & Theme Paper Award
ACM-class: I.2.7; H.3.3
\\ ( https://arxiv.org/abs/2506.01646 ,  4251kb)
------------------------------------------------------------------------------
\\
arXiv:2506.03867
replaced with revised version Sat, 20 Sep 2025 09:26:47 GMT   (656kb)

Title: EuroGEST: Investigating gender stereotypes in multilingual language
  models
Authors: Jacqueline Rowe, Mateusz Klimaszewski, Liane Guillou, Shannon Vallor,
  Alexandra Birch
Categories: cs.CL
Comments: 9 pages, 5 figures, 1 table. To be published in the 2025 Conference
  on Empirical Methods in Natural Language Processing (EMNLP 2025)
\\ ( https://arxiv.org/abs/2506.03867 ,  656kb)
------------------------------------------------------------------------------
\\
arXiv:2506.03949
replaced with revised version Sun, 21 Sep 2025 13:29:52 GMT   (33123kb)

Title: TableEval: A Real-World Benchmark for Complex, Multilingual, and
  Multi-Structured Table Question Answering
Authors: Junnan Zhu, Jingyi Wang, Bohan Yu, Xiaoyu Wu, Junbo Li, Lei Wang, Nan
  Xu
Categories: cs.CL
Comments: EMNLP2025 Main Conference
\\ ( https://arxiv.org/abs/2506.03949 ,  33123kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05979
replaced with revised version Fri, 19 Sep 2025 18:26:04 GMT   (216kb)

Title: Tau-Eval: A Unified Evaluation Framework for Useful and Private Text
  Anonymization
Authors: Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc
  Tommasi
Categories: cs.CL
Comments: EMNLP 2025 Demo
\\ ( https://arxiv.org/abs/2506.05979 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2506.06273
replaced with revised version Sat, 20 Sep 2025 20:22:18 GMT   (8844kb)

Title: AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization
Authors: Mukur Gupta, Nikhil Reddy Varimalla, Nicholas Deas, Melanie Subbiah,
  Kathleen McKeown
Categories: cs.CL
\\ ( https://arxiv.org/abs/2506.06273 ,  8844kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09513
replaced with revised version Mon, 22 Sep 2025 06:50:32 GMT   (3883kb)

Title: ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical
  Reasoning
Authors: Yu Sun, Xingyu Qian, Weiwen Xu, Hao Zhang, Chenghao Xiao, Long Li,
  Deli Zhao, Wenbing Huang, Tingyang Xu, Qifeng Bai, Yu Rong
Categories: cs.CL cs.AI cs.MA
Comments: 28 pages, 6 figures, 7 tables
\\ ( https://arxiv.org/abs/2506.09513 ,  3883kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09996
replaced with revised version Mon, 22 Sep 2025 11:37:26 GMT   (4373kb)

Title: From Judgment to Interference: Early Stopping LLM Harmful Outputs via
  Streaming Content Monitoring
Authors: Yang Li, Qiang Sheng, Yehan Yang, Xueyao Zhang, Juan Cao
Categories: cs.CL cs.CY
Comments: NeurIPS 2025 Accepted Paper
\\ ( https://arxiv.org/abs/2506.09996 ,  4373kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11113
replaced with revised version Sun, 21 Sep 2025 07:54:46 GMT   (6616kb)

Title: Breaking the Reviewer: Assessing the Vulnerability of Large Language
  Models in Automated Peer Review Under Textual Adversarial Attacks
Authors: Tzu-Ling Lin, Wei-Chih Chen, Teng-Fang Hsiao, Hou-I Liu, Ya-Hsin Yeh,
  Yu Kai Chan, Wen-Sheng Lien, Po-Yen Kuo, Philip S. Yu, Hong-Han Shuai
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2506.11113 ,  6616kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11474
replaced with revised version Mon, 22 Sep 2025 17:04:42 GMT   (4293kb)

Title: Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified
  Process Rewards
Authors: Jaehoon Yun, Jiwoong Sohn, Jungwoo Park, Hyunjae Kim, Xiangru Tang,
  Yanjun Shao, Yonghoe Koo, Minhyeok Ko, Qingyu Chen, Mark Gerstein, Michael
  Moor, Jaewoo Kang
Categories: cs.CL
Comments: Accepted to EMNLP 2025 (Oral)
\\ ( https://arxiv.org/abs/2506.11474 ,  4293kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11930
replaced with revised version Sun, 21 Sep 2025 22:52:02 GMT   (2847kb)

Title: Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback
Authors: Dongwei Jiang, Alvin Zhang, Andrew Wang, Nicholas Andrews, Daniel
  Khashabi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2506.11930 ,  2847kb)
------------------------------------------------------------------------------
\\
arXiv:2506.12935
replaced with revised version Sat, 20 Sep 2025 05:47:48 GMT   (1030kb)

Title: SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models
Authors: Xingjian Diao, Chunhui Zhang, Keyi Kong, Weiyi Wu, Chiyu Ma, Zhongyu
  Ouyang, Peijun Qing, Soroush Vosoughi and Jiang Gui
Categories: cs.CL cs.MM cs.SD eess.AS
Comments: Accepted to EMNLP 2025 Main Conference (Oral Presentation)
\\ ( https://arxiv.org/abs/2506.12935 ,  1030kb)
------------------------------------------------------------------------------
\\
arXiv:2506.14646
replaced with revised version Sat, 20 Sep 2025 16:23:00 GMT   (335kb)

Title: GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel
  Optimization with GuidedSelection Vectors
Authors: Hengyuan Zhang, Xinrong Chen, Yingmin Qiu, Xiao Liang, Ziyue Li,
  Guanyu Wang, Weiping Li, Tong Mo, Hayden Kwok-Hay So, Ngai Wong
Categories: cs.CL
Comments: Accepted by EMNLP 2025
\\ ( https://arxiv.org/abs/2506.14646 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2506.15583
replaced with revised version Sat, 20 Sep 2025 19:02:38 GMT   (453kb)

Title: DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through
  Iterative Graph Refinement
Authors: Shaoqing Lin, Chong Teng, Fei Li, Donghong Ji, Lizhen Qu, Zhuang Li
Categories: cs.CL
Comments: EMNLP 2025 (oral), 26 pages
\\ ( https://arxiv.org/abs/2506.15583 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2506.16792
replaced with revised version Sat, 20 Sep 2025 03:27:21 GMT   (813kb)

Title: MIST: Jailbreaking Black-box Large Language Models via Iterative
  Semantic Tuning
Authors: Muyang Zheng, Yuanzhi Yao, Changting Lin, Caihong Kai, Yanxiang Chen,
  Zhiquan Liu
Categories: cs.CL cs.AI
Comments: 13 pages, 6 figures
\\ ( https://arxiv.org/abs/2506.16792 ,  813kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21532
replaced with revised version Sat, 20 Sep 2025 01:36:08 GMT   (3975kb)

Title: "What's Up, Doc?": Analyzing How Users Seek Health Information in
  Large-Scale Conversational AI Datasets
Authors: Akshay Paruchuri, Maryam Aziz, Rohit Vartak, Ayman Ali, Best Uchehara,
  Xin Liu, Ishan Chatterjee, Monica Agrawal
Categories: cs.CL cs.AI cs.CY
Comments: Accepted to EMNLP 2025 Findings - 25 pages, 6 figures, 4 tables
\\ ( https://arxiv.org/abs/2506.21532 ,  3975kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21556
replaced with revised version Sun, 21 Sep 2025 12:01:17 GMT   (2704kb)

Title: VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for
  Retrieval-Augmented Generation
Authors: Hyeongcheol Park, Jiyoung Seo, MinHyuk Jang, Hogun Park, Ha Dam Baek,
  Gyusam Chang, Hyeonsoo Im, Sangpil Kim
Categories: cs.CL
Comments: Project Page: https://vatkg.github.io/
\\ ( https://arxiv.org/abs/2506.21556 ,  2704kb)
------------------------------------------------------------------------------
\\
arXiv:2506.23411
replaced with revised version Mon, 22 Sep 2025 15:51:55 GMT   (18515kb)

Title: Datasets for Fairness in Language Models: An In-Depth Survey
Authors: Jiale Zhang, Zichong Wang, Avash Palikhe, Zhipeng Yin, Wenbin Zhang
Categories: cs.CL cs.CY cs.LG
\\ ( https://arxiv.org/abs/2506.23411 ,  18515kb)
------------------------------------------------------------------------------
\\
arXiv:2507.02954
replaced with revised version Mon, 22 Sep 2025 17:05:03 GMT   (739kb)

Title: Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of
  Large Language Models on CFA Level III
Authors: Pranam Shetty, Abhisek Upadhayaya, Parth Mitesh Shah, Srikanth
  Jagabathula, Shilpi Nayak, Anna Joo Fee
Categories: cs.CL cs.AI
Comments: Accepted at FinLLM @ IJCAI 2025
\\ ( https://arxiv.org/abs/2507.02954 ,  739kb)
------------------------------------------------------------------------------
\\
arXiv:2507.03009
replaced with revised version Mon, 22 Sep 2025 07:58:16 GMT   (3252kb)

Title: PDFMathTranslate: Scientific Document Translation Preserving Layouts
Authors: Rongxin Ouyang, Chang Chu, Zhikuang Xin, Xiangyao Ma
Categories: cs.CL cs.IR cs.LG
Comments: 7 pages, 4 figures, EMNLP 2025 System Demonstration
MSC-class: 68T50, 68T45, 68U10, 68U15
ACM-class: D.2.2; I.2.10; I.2.7; J.0
\\ ( https://arxiv.org/abs/2507.03009 ,  3252kb)
------------------------------------------------------------------------------
\\
arXiv:2507.04415
replaced with revised version Sun, 21 Sep 2025 22:36:05 GMT   (5173kb)

Title: MOMENTS: A Comprehensive Multimodal Benchmark for Theory of Mind
Authors: Emilio Villa-Cueva, S M Masrur Ahmed, Rendi Chevi, Jan Christian
  Blaise Cruz, Kareem Elzeky, Fermin Cristobal, Alham Fikri Aji, Skyler Wang,
  Rada Mihalcea, Thamar Solorio
Categories: cs.CL
\\ ( https://arxiv.org/abs/2507.04415 ,  5173kb)
------------------------------------------------------------------------------
\\
arXiv:2507.11049
replaced with revised version Sun, 21 Sep 2025 05:47:30 GMT   (2072kb)

Title: Journalism-Guided Agentic In-Context Learning for News Stance Detection
Authors: Dahyun Lee, Jonghyeon Choi, Jiyoung Han, and Kunwoo Park
Categories: cs.CL
Comments: EMNLP 2025 (24 pages)
\\ ( https://arxiv.org/abs/2507.11049 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2507.11405
replaced with revised version Mon, 22 Sep 2025 13:51:23 GMT   (1589kb)

Title: DCR: Quantifying Data Contamination in LLMs Evaluation
Authors: Cheng Xu, Nan Yan, Shuhao Guan, Changhong Jin, Yuke Mei, Yibing Guo,
  M-Tahar Kechadi
Categories: cs.CL
Comments: EMNLP 2025 Main
\\ ( https://arxiv.org/abs/2507.11405 ,  1589kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13255
replaced with revised version Sat, 20 Sep 2025 16:12:54 GMT   (16020kb)

Title: Automating Steering for Safe Multimodal Large Language Models
Authors: Lyucheng Wu, Mengru Wang, Ziwen Xu, Tri Cao, Nay Oo, Bryan Hooi,
  Shumin Deng
Categories: cs.CL cs.AI cs.IR cs.LG cs.MM
Comments: EMNLP 2025 Main Conference. 23 pages (8+ for main); 25 figures; 1
  table
\\ ( https://arxiv.org/abs/2507.13255 ,  16020kb)
------------------------------------------------------------------------------
\\
arXiv:2507.14063
replaced with revised version Mon, 22 Sep 2025 15:04:57 GMT   (9421kb)

Title: Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn
  Dialog
Authors: Lautaro Estienne, Gabriel Ben Zenou, Nona Naderi, Jackie Cheung, Pablo
  Piantanida
Categories: cs.CL
\\ ( https://arxiv.org/abs/2507.14063 ,  9421kb)
------------------------------------------------------------------------------
\\
arXiv:2507.14913
replaced with revised version Sun, 21 Sep 2025 13:48:21 GMT   (1123kb)

Title: PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation
Authors: Eliya Habba, Noam Dahan, Gili Lior, Gabriel Stanovsky
Categories: cs.CL
Comments: Eliya Habba and Noam Dahan contributed equally to this work
\\ ( https://arxiv.org/abs/2507.14913 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2507.18956
replaced with revised version Sat, 20 Sep 2025 14:00:56 GMT   (1058kb)

Title: A Similarity Measure for Comparing Conversational Dynamics
Authors: Sang Min Jung, Kaixiang Zhang, Cristian Danescu-Niculescu-Mizil
Categories: cs.CL
Comments: Proceedings of EMNLP 2025 (Findings). Code and demos available in
  ConvoKit (https://convokit.cornell.edu/)
\\ ( https://arxiv.org/abs/2507.18956 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2507.20849
replaced with revised version Sat, 20 Sep 2025 11:57:49 GMT   (3029kb)

Title: Latent Inter-User Difference Modeling for LLM Personalization
Authors: Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli
  Feng
Categories: cs.CL
Comments: 2025 EMNLP Main Conference (Oral)
\\ ( https://arxiv.org/abs/2507.20849 ,  3029kb)
------------------------------------------------------------------------------
\\
arXiv:2508.00489
replaced with revised version Sat, 20 Sep 2025 08:48:41 GMT   (4416kb)

Title: The Missing Parts: Augmenting Fact Verification with Half-Truth
  Detection
Authors: Yixuan Tang, Jincheng Wang, Anthony K.H. Tung
Categories: cs.CL
Comments: Accepted by EMNLP 2025
\\ ( https://arxiv.org/abs/2508.00489 ,  4416kb)
------------------------------------------------------------------------------
\\
arXiv:2508.00742
replaced with revised version Mon, 22 Sep 2025 07:46:49 GMT   (835kb)

Title: Applying Psychometrics to Large Language Model Simulated Populations:
  Recreating the HEXACO Personality Inventory Experiment with Generative Agents
Authors: Sarah Mercer, Daniel P. Martin and Phil Swatton
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2508.00742 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2508.06165
replaced with revised version Sun, 21 Sep 2025 14:32:14 GMT   (7180kb)

Title: UR$^2$: Unify RAG and Reasoning through Reinforcement Learning
Authors: Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2508.06165 ,  7180kb)
------------------------------------------------------------------------------
\\
arXiv:2508.08846
replaced with revised version Sat, 20 Sep 2025 07:24:55 GMT   (6527kb)

Title: Steering Towards Fairness: Mitigating Political Bias in LLMs
Authors: Afrozah Nadeem, Mark Dras, Usman Naseem
Categories: cs.CL cs.AI
Comments: Accepted at CASE@RANLP2025
\\ ( https://arxiv.org/abs/2508.08846 ,  6527kb)
------------------------------------------------------------------------------
\\
arXiv:2508.09138
replaced with revised version Mon, 22 Sep 2025 07:56:08 GMT   (3518kb)

Title: Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language
  Models
Authors: Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen,
  Qiuyu Wang, Hao Ouyang, Hao Chen, Chunhua Shen
Categories: cs.CL cs.AI
Comments: Project webpage: https://aim-uofa.github.io/dLLM-MidTruth
\\ ( https://arxiv.org/abs/2508.09138 ,  3518kb)
------------------------------------------------------------------------------
\\
arXiv:2508.09878
replaced with revised version Mon, 22 Sep 2025 08:44:25 GMT   (47kb)

Title: A Survey of Cognitive Distortion Detection and Classification in NLP
Authors: Archie Sage, Jeroen Keppens, Helen Yannakoudakis
Categories: cs.CL
Comments: Camera-ready version to appear in EMNLP Findings 2025
\\ ( https://arxiv.org/abs/2508.09878 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2508.13804
replaced with revised version Mon, 22 Sep 2025 17:59:40 GMT   (158kb)

Title: Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values
  Understanding
Authors: Maciej Skorski and Alina Landowska
Categories: cs.CL cs.HC
Comments: Appears in UncertaiNLP@EMNLP 2025
MSC-class: 68T50, 62F15, 62P25
ACM-class: I.2.7; K.4.1; J.4
\\ ( https://arxiv.org/abs/2508.13804 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15827
replaced with revised version Sat, 20 Sep 2025 05:57:42 GMT   (2067kb)

Title: Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech
  Models
Authors: Zhifei Xie, Ziyang Ma, Zihang Liu, Kaiyu Pang, Hongyu Li, Jialin
  Zhang, Yue Liao, Deheng Ye, Chunyan Miao, Shuicheng Yan
Categories: cs.CL cs.AI cs.LG eess.AS
Comments: Technical report; Work in progress. Project page:
  https://github.com/xzf-thu/Mini-Omni-Reasoner
\\ ( https://arxiv.org/abs/2508.15827 ,  2067kb)
------------------------------------------------------------------------------
\\
arXiv:2508.20764
replaced with revised version Sun, 21 Sep 2025 14:12:43 GMT   (135kb)

Title: Feel the Difference? A Comparative Analysis of Emotional Arcs in Real
  and LLM-Generated CBT Sessions
Authors: Xiaoyi Wang, Jiwei Zhang, Guangtao Zhang, Honglei Guo
Categories: cs.CL
Comments: Accepted at 2025 EMNLP findings,19 page,2 figures
\\ ( https://arxiv.org/abs/2508.20764 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2509.00707
replaced with revised version Sat, 20 Sep 2025 05:43:25 GMT   (5679kb)

Title: Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics
  in Masked Diffusion LLMs
Authors: Daehoon Gwak, Minseo Jung, Junwoo Park, Minho Park, ChaeHun Park,
  Junha Hyung, Jaegul Choo
Categories: cs.CL cs.AI
Comments: EMNLP 2025 Main Paper (Long)
\\ ( https://arxiv.org/abs/2509.00707 ,  5679kb)
------------------------------------------------------------------------------
\\
arXiv:2509.00877
replaced with revised version Sun, 21 Sep 2025 07:18:56 GMT   (10963kb)

Title: EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes
Authors: Yuqin Dai, Guoqing Wang, Yuan Wang, Kairan Dou, Kaichen Zhou, Zhanwei
  Zhang, Shuo Yang, Fei Tang, Jun Yin, Pengyu Zeng, Zhenzhe Ying, Can Yi,
  Changhua Meng, Yuchen Zhou, Yongliang Shen, Shuai Lu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2509.00877 ,  10963kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01053
replaced with revised version Sat, 20 Sep 2025 17:15:38 GMT   (824kb)

Title: A Dynamic Fusion Model for Consistent Crisis Response
Authors: Xiaoying Song, Anirban Saha Anik, Eduardo Blanco, Vanessa
  Frias-Martinez, Lingzi Hong
Categories: cs.CL cs.AI
Comments: Accepted at Findings of EMNLP 2025
\\ ( https://arxiv.org/abs/2509.01053 ,  824kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01058
replaced with revised version Mon, 22 Sep 2025 15:44:51 GMT   (962kb)

Title: Speaking at the Right Level: Literacy-Controlled Counterspeech
  Generation with RAG-RL
Authors: Xiaoying Song, Anirban Saha Anik, Dibakar Barua, Pengcheng Luo, Junhua
  Ding, Lingzi Hong
Categories: cs.CL cs.AI
Comments: Accepted at Findings of EMNLP 2025
\\ ( https://arxiv.org/abs/2509.01058 ,  962kb)
------------------------------------------------------------------------------
\\
arXiv:2509.03116
replaced with revised version Mon, 22 Sep 2025 16:47:45 GMT   (499kb)

Title: Measuring Scalar Constructs in Social Science with LLMs
Authors: Hauke Licht, Rupak Sarkar, Patrick Y. Wu, Pranav Goel, Niklas Stoehr,
  Elliott Ash, and Alexander Miserlis Hoyle
Categories: cs.CL
Comments: Accepted to EMNLP 2025 (Main)
\\ ( https://arxiv.org/abs/2509.03116 ,  499kb)
------------------------------------------------------------------------------
\\
arXiv:2509.04467
replaced with revised version Mon, 22 Sep 2025 14:45:26 GMT   (1081kb)

Title: PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference
Authors: Hao Zhang, Mengsi Lyu, Zhuo Chen, Xingrun Xing, Yulong Ao, Yonghua Lin
Categories: cs.CL cs.AI
Comments: 23 pages
\\ ( https://arxiv.org/abs/2509.04467 ,  1081kb)
------------------------------------------------------------------------------
\\
arXiv:2509.04484
replaced with revised version Mon, 22 Sep 2025 08:57:11 GMT   (518kb)

Title: The Good, the Bad and the Constructive: Automatically Measuring Peer
  Review's Utility for Authors
Authors: Abdelrahman Sadallah, Tim Baumg\"artner, Iryna Gurevych, Ted Briscoe
Categories: cs.CL cs.AI cs.CY
Comments: EMNLP 2025 Main
\\ ( https://arxiv.org/abs/2509.04484 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07755
replaced with revised version Sat, 20 Sep 2025 03:20:18 GMT   (1850kb)

Title: Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for
  Medical Texts
Authors: Rochana Prih Hastuti, Rian Adam Rajagede, Mansour Al Ghanim, Mengxin
  Zheng, Qian Lou
Categories: cs.CL cs.CR
Comments: Accepted at EMNLP 2025 Findings. Camera Ready
\\ ( https://arxiv.org/abs/2509.07755 ,  1850kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07801
replaced with revised version Sat, 20 Sep 2025 02:06:27 GMT   (679kb)

Title: SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and
  Relation Extraction in NLP
Authors: Decheng Duan, Yingyi Zhang, Jitong Peng, Chengzhi Zhang
Categories: cs.CL cs.DL cs.IR
Comments: EMNLP 2025 Main
\\ ( https://arxiv.org/abs/2509.07801 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09043
replaced with revised version Sat, 20 Sep 2025 20:35:31 GMT   (36kb)

Title: Stated Preference for Interaction and Continued Engagement (SPICE):
  Evaluating an LLM's Willingness to Re-engage in Conversation
Authors: Thomas Manuel Rost, Martina Figlia, Bernd Wallraff
Categories: cs.CL cs.AI cs.MA
Comments: Added link to GitHub and Bayesian Analysis Appendix
\\ ( https://arxiv.org/abs/2509.09043 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09712
replaced with revised version Sat, 20 Sep 2025 21:31:47 GMT   (783kb)

Title: The Thinking Therapist: Training Large Language Models to Deliver
  Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio
  Policy Optimization
Authors: Talha Tahir
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2509.09712 ,  783kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11101
replaced with revised version Mon, 22 Sep 2025 08:45:20 GMT   (0kb,I)

Title: EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional
  Intelligence of Multimodal Large Language Models
Authors: Haokun Li, Yazhou Zhang, Jizhi Ding, Qiuchi Li, Peng Zhang
Categories: cs.CL
Comments: I need to modify the content of the article
\\ ( https://arxiv.org/abs/2509.11101 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11498
replaced with revised version Fri, 19 Sep 2025 21:27:23 GMT   (1565kb)

Title: DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation
  Classification
Authors: Zhuoxuan Ju, Jingni Wu, Abhishek Purushothama, Amir Zeldes
Categories: cs.CL
Comments: System submission for the DISRPT 2025 - Shared Task on Discourse
  Relation Parsing and Treebanking In conjunction with CODI-CRAC & EMNLP 2025.
  1st place in Task 3: relation classification
\\ ( https://arxiv.org/abs/2509.11498 ,  1565kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12158
replaced with revised version Sat, 20 Sep 2025 12:16:33 GMT   (5683kb)

Title: Pun Unintended: LLMs and the Illusion of Humor Understanding
Authors: Alessandro Zangari, Matteo Marcuzzo, Andrea Albarelli, Mohammad Taher
  Pilehvar, Jose Camacho-Collados
Categories: cs.CL cs.AI
Comments: Accepted to EMNLP 2025 Main Conference
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2509.12158 ,  5683kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12853
replaced with revised version Sat, 20 Sep 2025 02:57:25 GMT   (23kb)

Title: Data Augmentation for Maltese NLP using Transliterated and Machine
  Translated Arabic Data
Authors: Kurt Micallef, Nizar Habash, Claudia Borg
Categories: cs.CL
Comments: EMNLP Camera-Ready
\\ ( https://arxiv.org/abs/2509.12853 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13309
replaced with revised version Sat, 20 Sep 2025 14:27:52 GMT   (8288kb)

Title: WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon
  Agents
Authors: Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu
  Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao,
  Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou
Categories: cs.CL
Comments: https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/
\\ ( https://arxiv.org/abs/2509.13309 ,  8288kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14269
replaced with revised version Mon, 22 Sep 2025 03:08:40 GMT   (836kb)

Title: SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts
  Enhanced Large Language Models
Authors: Jianbin Zhang, Yulin Zhu, Wai Lun Lo, Richard Tai-Chiu Hsung, Harris
  Sik-Ho Tsang, Kai Zhou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2509.14269 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15048
replaced with revised version Mon, 22 Sep 2025 16:46:34 GMT   (4448kb)

Title: Can maiBERT Speak for Maithili?
Authors: Sumit Yadav, Raju Kumar Yadav, Utsav Maskey, Gautam Siddharth Kashyap,
  Md Azizul Hoque and Ganesh Gautam
Categories: cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2509.15048 ,  4448kb)
------------------------------------------------------------------------------
\\
arXiv:2206.14263
replaced with revised version Sat, 20 Sep 2025 17:59:29 GMT   (174kb)

Title: ZoDIAC: Zoneout Dropout Injection Attention Calculation
Authors: Zanyar Zohourianshahzadi and Jugal Kalita
Categories: cs.CV
Comments: This work has been published in IEEE AIxSET 2024 and is available
  conference proceedings
DOI: 10.1109/AIxSET62544.2024.00008
\\ ( https://arxiv.org/abs/2206.14263 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2210.06586
replaced with revised version Sat, 20 Sep 2025 05:24:30 GMT   (0kb,I)

Title: Automatic Real-time Vehicle Classification by Image Colour Component
  Based Template Matching
Authors: Ahmet Orun
Categories: cs.CV cs.AI
Comments: The paper may clash with another submission and dispute of copyright
\\ ( https://arxiv.org/abs/2210.06586 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10523
replaced with revised version Mon, 22 Sep 2025 11:09:26 GMT   (8598kb)

Title: Unsupervised Interpretable Basis Extraction for Concept-Based Visual
  Explanations
Authors: Alexandros Doumanoglou, Stylianos Asteriadis, Dimitrios Zarpalas
Categories: cs.CV cs.AI cs.LG
Comments: 15 pages, Original version accepted to IEEE Transactions on
  Artificial Intelligence, Special Issue on New Developments in Explainable and
  Interpretable AI, This version contains improvements in the presentation
  style
\\ ( https://arxiv.org/abs/2303.10523 ,  8598kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01259
replaced with revised version Mon, 22 Sep 2025 15:57:13 GMT   (6807kb)

Title: SINF: Semantic Neural Network Inference with Semantic Subgraphs
Authors: A. Q. M. Sazzad Sayyed, Francesco Restuccia
Categories: cs.CV cs.AI cs.LG
Comments: 12 pages, 13 figures, conference format
\\ ( https://arxiv.org/abs/2310.01259 ,  6807kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01799
replaced with revised version Sun, 21 Sep 2025 14:55:33 GMT   (3363kb)

Title: Superpixel Graph Contrastive Clustering with Semantic-Invariant
  Augmentations for Hyperspectral Images
Authors: Jianhan Qi, Yuheng Jia, Hui Liu, Junhui Hou
Categories: cs.CV
\\ ( https://arxiv.org/abs/2403.01799 ,  3363kb)
------------------------------------------------------------------------------
\\
arXiv:2405.15269
replaced with revised version Mon, 22 Sep 2025 12:42:30 GMT   (747kb)

Title: Test-Time Multimodal Backdoor Detection by Contrastive Prompting
Authors: Yuwei Niu, Shuo He, Qi Wei, Zongyu Wu, Feng Liu, Lei Feng
Categories: cs.CV cs.LG
Comments: Accepted to ICML2025
\\ ( https://arxiv.org/abs/2405.15269 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2406.19875
replaced with revised version Sat, 20 Sep 2025 10:21:03 GMT   (4371kb)

Title: InfiniBench: A Benchmark for Large Multi-Modal Models in Long-Form
  Movies and TV Shows
Authors: Kirolos Ataallah, Eslam Abdelrahman, Mahmoud Ahmed, Chenhui Gou,
  Khushbu Pahwa, Jian Ding, Mohamed Elhoseiny
Categories: cs.CV
Comments: Accepted at EMNLP 2025
\\ ( https://arxiv.org/abs/2406.19875 ,  4371kb)
------------------------------------------------------------------------------
\\
arXiv:2407.15143
replaced with revised version Sat, 20 Sep 2025 06:37:57 GMT   (1003kb)

Title: Investigating Long-term Training for Remote Sensing Object Detection
Authors: JongHyun Park and Yechan Kim and Moongu Jeon
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2407.15143 ,  1003kb)
------------------------------------------------------------------------------
\\
arXiv:2407.20437
replaced with revised version Mon, 22 Sep 2025 06:28:57 GMT   (4812kb)

Title: BaseBoostDepth: Exploiting Larger Baselines For Self-supervised
  Monocular Depth Estimation
Authors: Kieran Saunders, Luis J. Manso, George Vogiatzis
Categories: cs.CV
\\ ( https://arxiv.org/abs/2407.20437 ,  4812kb)
------------------------------------------------------------------------------
\\
arXiv:2408.10894
replaced with revised version Sat, 20 Sep 2025 13:24:29 GMT   (1298kb)

Title: ViLReF: An Expert Knowledge Enabled Vision-Language Retinal Foundation
  Model
Authors: Shengzhu Yang, Jiawei Du, Jia Guo, Weihang Zhang, Hanruo Liu, Huiqi
  Li, and Ningli Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2408.10894 ,  1298kb)
------------------------------------------------------------------------------
\\
arXiv:2409.05381
replaced with revised version Sun, 21 Sep 2025 04:48:18 GMT   (3962kb)

Title: Few-Shot Image Quality Assessment via Adaptation of Vision-Language
  Models
Authors: Xudong Li, Zihao Huang, Yan Zhang, Yunhang Shen, Ke Li, Xiawu Zheng,
  Liujuan Cao, Rongrong Ji
Categories: cs.CV
\\ ( https://arxiv.org/abs/2409.05381 ,  3962kb)
------------------------------------------------------------------------------
\\
arXiv:2409.16953
replaced with revised version Sun, 21 Sep 2025 13:23:12 GMT   (1280kb)

Title: PASS: Path-selective State Space Model for Event-based Recognition
Authors: Jiazhou Zhou, Kanghao Chen, Lei Zhang, Lin Wang
Categories: cs.CV
Comments: Accepted by NeurIPS 2025. Main paper: 10 pages; Supplementary: 6
  pages
\\ ( https://arxiv.org/abs/2409.16953 ,  1280kb)
------------------------------------------------------------------------------
\\
arXiv:2410.02103
replaced with revised version Sat, 20 Sep 2025 08:13:54 GMT   (13807kb)

Title: Multi-viewregulated gaussian splatting for novel view synthesis
Authors: Xiaobiao Du, Yida Wang, Xin Yu
Categories: cs.CV
Comments: Project Page:https://xiaobiaodu.github.io/mvgs-project/
\\ ( https://arxiv.org/abs/2410.02103 ,  13807kb)
------------------------------------------------------------------------------
\\
arXiv:2410.14138
replaced with revised version Sat, 20 Sep 2025 03:11:23 GMT   (894kb)

Title: ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and
  Wisdom
Authors: Jingqi Zhou, Sheng Wang, Jingwei Dong, Kai Liu, Lei Li, Jiahui Gao,
  Jiyue Jiang, Lingpeng Kong, Chuan Wu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2410.14138 ,  894kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03823
replaced with revised version Sat, 20 Sep 2025 19:01:48 GMT   (1072kb)

Title: Both Text and Images Leaked! A Systematic Analysis of Data Contamination
  in Multimodal LLM
Authors: Dingjie Song, Sicheng Lai, Mingxuan Wang, Shunian Chen, Lichao Sun,
  Benyou Wang
Categories: cs.CV cs.AI cs.CL cs.MM
Comments: Accepted to EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2411.03823 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2412.03526
replaced with revised version Sun, 21 Sep 2025 19:13:53 GMT   (8840kb)

Title: Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular
  Videos
Authors: Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu,
  Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic,
  Jiahui Huang
Categories: cs.CV cs.AI cs.GR
Comments: Project website:
  https://research.nvidia.com/labs/toronto-ai/bullet-timer/
\\ ( https://arxiv.org/abs/2412.03526 ,  8840kb)
------------------------------------------------------------------------------
\\
arXiv:2412.04783
replaced with revised version Sun, 21 Sep 2025 08:35:12 GMT   (7207kb)

Title: KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment
Authors: Zijian Zhao, Zhijie Cai, Tingwei Chen, Xiaoyang Li, Hang Li, Qimei
  Chen, Guangxu Zhu
Categories: cs.CV cs.AI eess.SP
\\ ( https://arxiv.org/abs/2412.04783 ,  7207kb)
------------------------------------------------------------------------------
\\
arXiv:2412.09585
replaced with revised version Sun, 21 Sep 2025 05:51:43 GMT   (17650kb)

Title: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding
  Distillation
Authors: Jitesh Jain, Zhengyuan Yang, Humphrey Shi, Jianfeng Gao, Jianwei Yang
Categories: cs.CV
Comments: Project Page: https://praeclarumjj3.github.io/visper_lm/
\\ ( https://arxiv.org/abs/2412.09585 ,  17650kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01890
replaced with revised version Mon, 22 Sep 2025 14:11:08 GMT   (5121kb)

Title: 3D Cell Oversegmentation Correction via Geo-Wasserstein Divergence
Authors: Peter Chen, Bryan Chang, Olivia Annette Creasey, Julie Beth Sneddon,
  Zev Gartner, Yining Liu
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2502.01890 ,  5121kb)
------------------------------------------------------------------------------
\\
arXiv:2502.07239
replaced with revised version Mon, 22 Sep 2025 13:45:21 GMT   (6183kb)

Title: Contextual Gesture: Co-Speech Gesture Video Generation through
  Context-aware Gesture Representation
Authors: Pinxin Liu, Pengfei Zhang, Hyeongwoo Kim, Pablo Garrido, Ari Shapiro,
  Kyle Olszewski
Categories: cs.CV cs.AI
Comments: Accepted to ACM MM 2025. Project Page:
  https://andypinxinliu.github.io/Contextual-Gesture/
\\ ( https://arxiv.org/abs/2502.07239 ,  6183kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11655
replaced with revised version Mon, 22 Sep 2025 13:02:53 GMT   (32209kb)

Title: TextOCVP: Object-Centric Video Prediction with Language Guidance
Authors: Angel Villar-Corrales, Gjergj Plepi and Sven Behnke
Categories: cs.CV
\\ ( https://arxiv.org/abs/2502.11655 ,  32209kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12520
replaced with revised version Mon, 22 Sep 2025 06:24:32 GMT   (5140kb)

Title: SafeEraser: Enhancing Safety in Multimodal Large Language Models through
  Multimodal Machine Unlearning
Authors: Junkai Chen, Zhijie Deng, Kening Zheng, Yibo Yan, Shuliang Liu, PeiJun
  Wu, Peijie Jiang, Jia Liu, Xuming Hu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2502.12520 ,  5140kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13146
replaced with revised version Fri, 19 Sep 2025 21:28:23 GMT   (5866kb)

Title: Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct
  Preference Optimization
Authors: Shuo Xing, Peiran Li, Yuping Wang, Ruizheng Bai, Yueqi Wang, Chan-Wei
  Hu, Chengxuan Qian, Huaxiu Yao, Zhengzhong Tu
Categories: cs.CV cs.LG
Comments: Published at EMNLP 2025
\\ ( https://arxiv.org/abs/2502.13146 ,  5866kb)
------------------------------------------------------------------------------
\\
arXiv:2502.19269
replaced with revised version Sun, 21 Sep 2025 11:26:38 GMT   (3625kb)

Title: Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in
  CLIP
Authors: Jiawei Kong, Hao Fang, Sihang Guo, Chenxi Qing, Kuofeng Gao, Bin Chen,
  Shu-Tao Xia, Ke Xu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2502.19269 ,  3625kb)
------------------------------------------------------------------------------
\\
arXiv:2502.20134
replaced with revised version Mon, 22 Sep 2025 12:50:48 GMT   (10079kb)

Title: Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware
  Concept Bottleneck Models
Authors: Itay Benou and Tammy Riklin-Raviv
Categories: cs.CV
\\ ( https://arxiv.org/abs/2502.20134 ,  10079kb)
------------------------------------------------------------------------------
\\
arXiv:2502.21059
replaced with revised version Sat, 20 Sep 2025 09:53:22 GMT   (3021kb)

Title: FC-Attack: Jailbreaking Multimodal Large Language Models via
  Auto-Generated Flowcharts
Authors: Ziyi Zhang, Zhen Sun, Zongmin Zhang, Jihui Guo, Xinlei He
Categories: cs.CV cs.AI cs.CR cs.LG
Comments: Accepted to Findings of EMNLP 2025
\\ ( https://arxiv.org/abs/2502.21059 ,  3021kb)
------------------------------------------------------------------------------
\\
arXiv:2503.04504
replaced with revised version Sat, 20 Sep 2025 12:41:25 GMT   (4139kb)

Title: AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM
Authors: Sunghyun Ahn, Youngwan Jo, Kijung Lee, Sein Kwon, Inpyo Hong, Sanghyun
  Park
Categories: cs.CV
\\ ( https://arxiv.org/abs/2503.04504 ,  4139kb)
------------------------------------------------------------------------------
\\
arXiv:2503.04522
replaced with revised version Mon, 22 Sep 2025 15:51:03 GMT   (6390kb)

Title: Conformal In-Context Reverse Classification Accuracy: Efficient
  Estimation of Segmentation Quality with Statistical Guarantees
Authors: Matias Cosarinsky, Ramiro Billot, Lucas Mansilla, Gabriel Jimenez,
  Nicolas Gaggi\'on, Guanghui Fu, Tom Tirer, Enzo Ferrante
Categories: cs.CV
\\ ( https://arxiv.org/abs/2503.04522 ,  6390kb)
------------------------------------------------------------------------------
\\
arXiv:2503.07417
replaced with revised version Sun, 21 Sep 2025 12:53:47 GMT   (35018kb)

Title: GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts
Authors: Minwen Liao, Hao Bo Dong, Xinyi Wang, Kurban Ubul, Yihua Shao, Ziyang
  Yan
Categories: cs.CV
\\ ( https://arxiv.org/abs/2503.07417 ,  35018kb)
------------------------------------------------------------------------------
\\
arXiv:2503.09419
replaced with revised version Mon, 22 Sep 2025 08:37:29 GMT   (8167kb)

Title: Alias-Free Latent Diffusion Models: Improving Fractional Shift
  Equivariance of Diffusion Latent Space
Authors: Yifan Zhou, Zeqi Xiao, Shuai Yang, Xingang Pan
Categories: cs.CV
Comments: Code is available at: https://github.com/SingleZombie/AFLDM
\\ ( https://arxiv.org/abs/2503.09419 ,  8167kb)
------------------------------------------------------------------------------
\\
arXiv:2503.12404
replaced with revised version Sun, 21 Sep 2025 01:18:09 GMT   (8738kb)

Title: SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote
  Sensing Segmentation
Authors: Jianhao Yang, Wenshuo Yu, Yuanchao Lv, Jiance Sun, Bokang Sun and
  Mingyang Liu
Categories: cs.CV
Comments: published in IEEE Journal of Selected Topics in Applied Earth
  Observations and Remote Sensing
Journal-ref: in J. Sel. Top. Appl. Earth Obs. Remote Sens, vol. 18, pp.
  22499-22512, 2025
DOI: 10.1109/JSTARS.2025.3603983
\\ ( https://arxiv.org/abs/2503.12404 ,  8738kb)
------------------------------------------------------------------------------
\\
arXiv:2503.13156
replaced with revised version Mon, 22 Sep 2025 11:07:08 GMT   (0kb,I)

Title: DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph
  Knowledge Distillation for Gait Disorders Recognition
Authors: Zakariae Zrimek, Youssef Mourchid, Mohammed El Hassouni
Categories: cs.CV
Comments: After receiving detailed feedback from journalreviewers, we
  identified limitations in the initial approach and substantially improved the
  methodology and contributions of the work. To ensure clarity and avoid
  confusion between the initial and revised versions, we are withdrawing this
  submission. A new version reflecting these improvements will be submitted and
  made available on arXiv shortly
\\ ( https://arxiv.org/abs/2503.13156 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2503.13806
replaced with revised version Sun, 21 Sep 2025 19:33:16 GMT   (513kb)

Title: DescriptorMedSAM: Language-Image Fusion with Multi-Aspect Text Guidance
  for Medical Image Segmentation
Authors: Wenjie Zhang, Liming Luo, Mengnan He, Jiarui Hai, Jiancheng Ye
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2503.13806 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2503.16980
replaced with revised version Sun, 21 Sep 2025 17:45:54 GMT   (887kb)

Title: VQToken: Neural Discrete Token Representation Learning for Extreme Token
  Reduction in Video Large Language Models
Authors: Haichao Zhang, Yun Fu
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Proceedings of the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2025)
MSC-class: 68T07, 68T45, 68T50, 68T09, 68U10, 94A29, 94A34, 94A08, 94A17
ACM-class: I.2.10; I.2.7; I.5.4; I.4.9; I.4; H.5.1; H.3.3
\\ ( https://arxiv.org/abs/2503.16980 ,  887kb)
------------------------------------------------------------------------------
\\
arXiv:2503.19769
replaced with revised version Sun, 21 Sep 2025 07:06:05 GMT   (0kb,I)

Title: BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection
  between Point and Text Prompts
Authors: Suzhe Xu, Jialin Peng, Chengyuan Zhang
Categories: cs.CV cs.LG
Comments: metrics went wrong
\\ ( https://arxiv.org/abs/2503.19769 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2504.04653
replaced with revised version Sun, 21 Sep 2025 05:16:39 GMT   (1399kb)

Title: LEO-MINI: An Efficient Multimodal Large Language Model using Conditional
  Token Reduction and Mixture of Multi-Modal Experts
Authors: Yimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki
Categories: cs.CV cs.CL
Comments: To appear at EMNLP 2025
\\ ( https://arxiv.org/abs/2504.04653 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2504.05682
replaced with revised version Mon, 22 Sep 2025 07:02:57 GMT   (221kb)

Title: On the Suitability of Reinforcement Fine-Tuning to Visual Tasks
Authors: Xiaxu Chen, Wei Li, Chunxu Liu, Chi Xie, Xiaoyan Hu, Chengqian Ma,
  Feng Zhu, Rui Zhao
Categories: cs.CV
\\ ( https://arxiv.org/abs/2504.05682 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2504.11669
replaced with revised version Mon, 22 Sep 2025 17:27:30 GMT   (924kb)

Title: Co-STAR: Collaborative Curriculum Self-Training with Adaptive
  Regularization for Source-Free Video Domain Adaptation
Authors: Amirhossein Dadashzadeh, Parsa Esmati, Majid Mirmehdi
Categories: cs.CV
\\ ( https://arxiv.org/abs/2504.11669 ,  924kb)
------------------------------------------------------------------------------
\\
arXiv:2504.12795
replaced with revised version Sun, 21 Sep 2025 04:36:55 GMT   (5834kb)

Title: EarthGPT-X: A Spatial MLLM for Multi-level Multi-Source Remote Sensing
  Imagery Understanding with Visual Prompting
Authors: Wei Zhang, Miaoxin Cai, Yaqian Ning, Tong Zhang, Yin Zhuang, Shijian
  Lu, He Chen, Jun Li, Xuerui Mao
Categories: cs.CV
\\ ( https://arxiv.org/abs/2504.12795 ,  5834kb)
------------------------------------------------------------------------------
\\
arXiv:2504.14899
replaced with revised version Sat, 20 Sep 2025 05:38:05 GMT   (22886kb)

Title: Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls
  for Video Generation
Authors: Chenjie Cao, Jingkai Zhou, Shikai Li, Jingyun Liang, Chaohui Yu, Fan
  Wang, Xiangyang Xue, Yanwei Fu
Categories: cs.CV
Comments: Project page: https://github.com/ewrfcas/Uni3C. Accepted by Siggraph
  Asian 2025
\\ ( https://arxiv.org/abs/2504.14899 ,  22886kb)
------------------------------------------------------------------------------
\\
arXiv:2504.16081
replaced with revised version Sat, 20 Sep 2025 20:25:42 GMT   (27555kb)

Title: Survey of Video Diffusion Models: Foundations, Implementations, and
  Applications
Authors: Yimu Wang, Xuye Liu, Wei Pang, Li Ma, Shuai Yuan, Paul Debevec, Ning
  Yu
Categories: cs.CV cs.CL
Comments: Accepted by TMLR
\\ ( https://arxiv.org/abs/2504.16081 ,  27555kb)
------------------------------------------------------------------------------
\\
arXiv:2504.19327
replaced with revised version Sun, 21 Sep 2025 16:16:09 GMT   (581kb)

Title: DeepInsert: Early Layer Bypass for Efficient and Performant Multimodal
  Understanding
Authors: Moulik Choraria, Xinbo Wu, Akhil Bhimaraju, Nitesh Sekhar, Yue Wu, Xu
  Zhang, Prateek Singhal, Lav R. Varshney
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2504.19327 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2504.20690
replaced with revised version Sat, 20 Sep 2025 15:08:04 GMT   (18859kb)

Title: In-Context Edit: Enabling Instructional Image Editing with In-Context
  Generation in Large Scale Diffusion Transformer
Authors: Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, Yi Yang
Categories: cs.CV
Comments: Accepted by NeurIPS 2025, there will be future updates for camera
  ready version. Code: https://github.com/River-Zhang/ICEdit
\\ ( https://arxiv.org/abs/2504.20690 ,  18859kb)
------------------------------------------------------------------------------
\\
arXiv:2504.21476
replaced with revised version Mon, 22 Sep 2025 02:59:05 GMT   (3669kb)

Title: GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal
  Diffusion Transformers
Authors: Xinyu Li, Qi Yao, Yuanda Wang
Categories: cs.CV cs.AI
Comments: The 34th International Joint Conference on Artificial Intelligence
  (IJCAI 2025)
DOI: 10.24963/ijcai.2025/163
\\ ( https://arxiv.org/abs/2504.21476 ,  3669kb)
------------------------------------------------------------------------------
\\
arXiv:2505.03319
replaced with revised version Mon, 22 Sep 2025 11:15:35 GMT   (17745kb)

Title: SD-VSum: A Method and Dataset for Script-Driven Video Summarization
Authors: Manolis Mylonas, Evlampios Apostolidis, Vasileios Mezaris
Categories: cs.CV cs.AI cs.MM
Comments: In ACM Multimedia 2025, DOI:10.1145/3746027.3755821
\\ ( https://arxiv.org/abs/2505.03319 ,  17745kb)
------------------------------------------------------------------------------
\\
arXiv:2505.10888
replaced with revised version Sun, 21 Sep 2025 13:49:40 GMT   (1539kb)

Title: PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose
  Estimation via Pose Lifting Networks
Authors: Saad Manzur, Bryan Vela, Brandon Vela, Aditya Agrawal, Lan-Anh
  Dang-Vu, David Li, Wayne Hayes
Categories: cs.CV
Comments: Code: https://github.com/bryanjvela/PoseBench3D
\\ ( https://arxiv.org/abs/2505.10888 ,  1539kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11196
replaced with revised version Mon, 22 Sep 2025 11:38:26 GMT   (11849kb)

Title: DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion
  Modeling
Authors: Yuang Ai, Qihang Fan, Xuefeng Hu, Zhenheng Yang, Ran He, Huaibo Huang
Categories: cs.CV
Comments: NeurIPS 2025 Spotlight
\\ ( https://arxiv.org/abs/2505.11196 ,  11849kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11497
replaced with revised version Sat, 20 Sep 2025 18:16:28 GMT   (25788kb)

Title: QVGen: Pushing the Limit of Quantized Video Generative Models
Authors: Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong
  Qin, Jun Zhang
Categories: cs.CV
Comments: Our code will be released upon acceptance
\\ ( https://arxiv.org/abs/2505.11497 ,  25788kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12434
replaced with revised version Sat, 20 Sep 2025 08:53:09 GMT   (10609kb)

Title: VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via
  Reinforced Fine-Tuning
Authors: Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, Tianfei Zhou
Categories: cs.CV
Comments: Accepted by NeurIPS 2025. Code: https://github.com/QiWang98/VideoRFT
\\ ( https://arxiv.org/abs/2505.12434 ,  10609kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12667
replaced with revised version Mon, 22 Sep 2025 03:06:36 GMT   (6630kb)

Title: Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking
Authors: Zihan Su, Xuerui Qiu, Hongbin Xu, Tangyu Jiang, Junhao Zhuang, Chun
  Yuan, Ming Li, Shengfeng He, Fei Richard Yu
Categories: cs.CV
Comments: Safa-Sora is accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2505.12667 ,  6630kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13300
replaced with revised version Sun, 21 Sep 2025 18:41:03 GMT   (75kb)

Title: DD-Ranking: Rethinking the Evaluation of Dataset Distillation
Authors: Zekai Li, Xinhao Zhong, Samir Khaki, Zhiyuan Liang, Yuhao Zhou,
  Mingjia Shi, Ziqiao Wang, Xuanlei Zhao, Wangbo Zhao, Ziheng Qin, Mengxuan Wu,
  Pengfei Zhou, Haonan Wang, David Junhao Zhang, Jia-Wei Liu, Shaobo Wang, Dai
  Liu, Linfeng Zhang, Guang Li, Kun Wang, Zheng Zhu, Zhiheng Ma, Joey Tianyi
  Zhou, Jiancheng Lv, Yaochu Jin, Peihao Wang, Kaipeng Zhang, Lingjuan Lyu,
  Yiran Huang, Zeynep Akata, Zhiwei Deng, Xindi Wu, George Cazenavette, Yuzhang
  Shang, Justin Cui, Jindong Gu, Qian Zheng, Hao Ye, Shuo Wang, Xiaobo Wang,
  Yan Yan, Angela Yao, Mike Zheng Shou, Tianlong Chen, Hakan Bilen, Baharan
  Mirzasoleiman, Manolis Kellis, Konstantinos N. Plataniotis, Zhangyang Wang,
  Bo Zhao, Yang You, Kai Wang
Categories: cs.CV
Comments: 20 pages, 4 figures
\\ ( https://arxiv.org/abs/2505.13300 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20024
replaced with revised version Mon, 22 Sep 2025 07:21:16 GMT   (8553kb)

Title: ReasonPlan: Unified Scene Prediction and Decision Reasoning for
  Closed-loop Autonomous Driving
Authors: Xueyi Liu, Zuodong Zhong, Yuxin Guo, Yun-Fu Liu, Zhiguo Su, Qichao
  Zhang, Junli Wang, Yinfeng Gao, Yupeng Zheng, Qiao Lin, Huiyong Chen, Dongbin
  Zhao
Categories: cs.CV cs.AI cs.RO
Comments: 18 pages; 9 figures; https://github.com/Liuxueyi/ReasonPlan
MSC-class: 68T40(Primary), 68T45, 68T50(Secondary)
ACM-class: I.2.9; I.2.10; I.5.1
\\ ( https://arxiv.org/abs/2505.20024 ,  8553kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20655
replaced with revised version Mon, 22 Sep 2025 09:16:35 GMT   (7028kb)

Title: Photography Perspective Composition: Towards Aesthetic Perspective
  Recommendation
Authors: Lujian Yao, Siming Zheng, Xinbin Yuan, Zhuoxuan Cai, Pu Wu, Jinwei
  Chen, Bo Li, Peng-Tao Jiang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2505.20655 ,  7028kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21531
replaced with revised version Sat, 20 Sep 2025 08:00:55 GMT   (14220kb)

Title: How Much Do Large Language Models Know about Human Motion? A Case Study
  in 3D Avatar Control
Authors: Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao
Categories: cs.CV cs.AI cs.CL cs.RO
\\ ( https://arxiv.org/abs/2505.21531 ,  14220kb)
------------------------------------------------------------------------------
\\
arXiv:2506.03972
replaced with revised version Mon, 22 Sep 2025 06:37:04 GMT   (0kb,I)

Title: MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell
  Detection
Authors: Guohua Wu and Shengqi Chen and Pengchao Deng and Wenting Yu
Categories: cs.CV
Comments: There is a disagreement among the authors regarding the content and
  submission of the manuscript, which needs to be resolved before it can be
  made public
\\ ( https://arxiv.org/abs/2506.03972 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2506.04039
replaced with revised version Mon, 22 Sep 2025 09:12:18 GMT   (3452kb)

Title: Mitigating Hallucinations in Large Vision-Language Models via
  Entity-Centric Multimodal Preference Optimization
Authors: Jiulong Wu, Zhengliang Shi, Shuaiqiang Wang, Jizhou Huang, Dawei Yin,
  Lingyong Yan, Min Cao, Min Zhang
Categories: cs.CV cs.AI cs.CL
Comments: This paper is accepted by EMNLP2025
\\ ( https://arxiv.org/abs/2506.04039 ,  3452kb)
------------------------------------------------------------------------------
\\
arXiv:2506.06970
replaced with revised version Sun, 21 Sep 2025 02:30:09 GMT   (18831kb)

Title: Guiding Cross-Modal Representations with MLLM Priors via Preference
  Alignment
Authors: Pengfei Zhao, Rongbo Luan, Wei Zhang, Peng Wu, Sifeng He
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2506.06970 ,  18831kb)
------------------------------------------------------------------------------
\\
arXiv:2506.08591
replaced with revised version Mon, 22 Sep 2025 07:23:14 GMT   (391kb)

Title: Diversity-Guided MLP Reduction for Efficient Large Vision Transformers
Authors: Chengchao Shen, Hourun Zhu, Gongfan Fang, Jianxin Wang, Xinchao Wang
Categories: cs.CV cs.LG cs.MM
\\ ( https://arxiv.org/abs/2506.08591 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09846
replaced with revised version Sat, 20 Sep 2025 18:37:05 GMT   (194kb)

Title: Learning to Align: Addressing Character Frequency Distribution Shifts in
  Handwritten Text Recognition
Authors: Panagiotis Kaliosis, John Pavlopoulos
Categories: cs.CV cs.AI
Comments: EMNLP 2025 Findings, 18 pages, 10 figures, 11 tables
\\ ( https://arxiv.org/abs/2506.09846 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11653
replaced with revised version Mon, 22 Sep 2025 12:41:16 GMT   (2562kb)

Title: DISCO: Mitigating Bias in Deep Learning with Conditional Distance
  Correlation
Authors: Emre Kavak, Tom Nuno Wolf, Christian Wachinger
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2506.11653 ,  2562kb)
------------------------------------------------------------------------------
\\
arXiv:2506.14629
replaced with revised version Sat, 20 Sep 2025 12:30:04 GMT   (3605kb)

Title: VisText-Mosquito: A Unified Multimodal Benchmark Dataset for Visual
  Detection, Segmentation, and Textual Reasoning on Mosquito Breeding Sites
Authors: Md. Adnanul Islam, Md. Faiyaz Abdullah Sayeedi, Md. Asaduzzaman Shuvo,
  Shahanur Rahman Bappy, Md Asiful Islam, Swakkhar Shatabda
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2506.14629 ,  3605kb)
------------------------------------------------------------------------------
\\
arXiv:2506.15564
replaced with revised version Mon, 22 Sep 2025 01:24:29 GMT   (3893kb)

Title: Show-o2: Improved Native Unified Multimodal Models
Authors: Jinheng Xie, Zhenheng Yang, Mike Zheng Shou
Categories: cs.CV
Comments: NeurIPS 2025. (v3: update to include video understanding, OneIG, and
  more ablation study results)
\\ ( https://arxiv.org/abs/2506.15564 ,  3893kb)
------------------------------------------------------------------------------
\\
arXiv:2506.16157
replaced with revised version Mon, 22 Sep 2025 08:20:47 GMT   (1409kb)

Title: Proxy-Embedding as an Adversarial Teacher: An Embedding-Guided
  Bidirectional Attack for Referring Expression Segmentation Models
Authors: Xingbai Chen, Tingchao Fu, Renyang Liu, Wei Zhou, Chao Yi
Categories: cs.CV
Comments: 20pages, 5figures
\\ ( https://arxiv.org/abs/2506.16157 ,  1409kb)
------------------------------------------------------------------------------
\\
arXiv:2506.16806
replaced with revised version Sun, 21 Sep 2025 06:58:31 GMT   (6438kb)

Title: FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven
  by Referential Segmentation
Authors: Fan Yang, Yousong Zhu, Xin Li, Yufei Zhan, Hongyin Zhao, Shurong
  Zheng, Yaowei Wang, Ming Tang, Jinqiao Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2506.16806 ,  6438kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20168
replaced with revised version Mon, 22 Sep 2025 12:48:45 GMT   (4845kb)

Title: Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large
  Language Models
Authors: Zhentao He, Can Zhang, Ziheng Wu, Zhenghao Chen, Yufei Zhan, Yifan Li,
  Zhao Zhang, Xian Wang, Minghui Qiu
Categories: cs.CV
Comments: Accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2506.20168 ,  4845kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21188
replaced with revised version Sun, 21 Sep 2025 07:18:34 GMT   (1866kb)

Title: GroundFlow: A Plug-in Module for Temporal Reasoning on 3D Point Cloud
  Sequential Grounding
Authors: Zijun Lin, Shuting He, Cheston Tan, Bihan Wen
Categories: cs.CV
\\ ( https://arxiv.org/abs/2506.21188 ,  1866kb)
------------------------------------------------------------------------------
\\
arXiv:2506.23711
replaced with revised version Sun, 21 Sep 2025 16:31:59 GMT   (3959kb)

Title: Subjective Camera 1.0: Bridging Human Cognition and Visual
  Reconstruction through Sequence-Aware Sketch-Guided Diffusion
Authors: Haoyang Chen, Dongfang Sun, Caoyuan Ma, Shiqin Wang, Kewei Zhang,
  Zheng Wang and Zhixiang Wang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2506.23711 ,  3959kb)
------------------------------------------------------------------------------
\\
arXiv:2506.23835
replaced with revised version Mon, 22 Sep 2025 08:28:03 GMT   (34882kb)

Title: SCORP: Scene-Consistent Object Refinement via Proxy Generation and
  Tuning
Authors: Ziwei Chen, Ziling Liu, Zitong Huang, Mingqi Gao, Feng Zheng
Categories: cs.CV
Comments: 8 pages with 6 figures. Project page:
  https://polysummit.github.io/scorp.github.io/
\\ ( https://arxiv.org/abs/2506.23835 ,  34882kb)
------------------------------------------------------------------------------
\\
arXiv:2507.05255
replaced with revised version Fri, 19 Sep 2025 20:57:59 GMT   (864kb)

Title: Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for
  Visual Reasoning
Authors: Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin,
  Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, Jia Wang, Chunrui
  Han, Yuang Peng, Qi Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Vishal M.
  Patel
Categories: cs.CV cs.CL
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2507.05255 ,  864kb)
------------------------------------------------------------------------------
\\
arXiv:2507.08330
replaced with revised version Sun, 21 Sep 2025 00:25:35 GMT   (3046kb)

Title: Interpretability-Aware Pruning for Efficient Medical Image Analysis
Authors: Nikita Malik, Pratinav Seth, Neeraj Kumar Singh, Chintan Chitroda,
  Vinay Kumar Sankarapu
Categories: cs.CV cs.AI cs.ET cs.LG
Comments: Accepted at The 1st MICCAI Workshop on Efficient Medical AI 2025
\\ ( https://arxiv.org/abs/2507.08330 ,  3046kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13797
replaced with revised version Sat, 20 Sep 2025 07:03:35 GMT   (41466kb)

Title: DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind
  Face Restoration with Dynamic Blur-Level Mapping and Guidance
Authors: Huu-Phu Do, Yu-Wei Chen, Yi-Cheng Liao, Chi-Wei Hsiao, Han-Yang Wang,
  Wei-Chen Chiu, Ching-Chun Huang
Categories: cs.CV
Comments: Accepted by ICCV 2025
\\ ( https://arxiv.org/abs/2507.13797 ,  41466kb)
------------------------------------------------------------------------------
\\
arXiv:2507.18407
replaced with revised version Mon, 22 Sep 2025 07:31:39 GMT   (6401kb)

Title: DCFFSNet: Deep Connectivity Feature Fusion Separation Network for
  Medical Image Segmentation
Authors: Mingda Zhang, Xun Ye, Ruixiang Tang, Haiyan Ding
Categories: cs.CV
Comments: 16 pages , 11 figures
\\ ( https://arxiv.org/abs/2507.18407 ,  6401kb)
------------------------------------------------------------------------------
\\
arXiv:2508.02329
replaced with revised version Mon, 22 Sep 2025 06:33:36 GMT   (6446kb)

Title: CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via
  Instruction Editing Data and Long Captions
Authors: Ziteng Wang, Siqi Yang, Limeng Qiao, Lin Ma
Categories: cs.CV
Comments: NeurIPS 2025 Main
\\ ( https://arxiv.org/abs/2508.02329 ,  6446kb)
------------------------------------------------------------------------------
\\
arXiv:2508.02363
replaced with revised version Sat, 20 Sep 2025 11:27:13 GMT   (34109kb)

Title: Optimal Transport for Rectified Flow Image Editing: Unifying
  Inversion-Based and Direct Methods
Authors: Marian Lupascu and Mihai-Sorin Stupariu
Categories: cs.CV
Comments: 27 pages, 26 figures, WACV conference
\\ ( https://arxiv.org/abs/2508.02363 ,  34109kb)
------------------------------------------------------------------------------
\\
arXiv:2508.03485
replaced with revised version Mon, 22 Sep 2025 12:36:16 GMT   (3214kb)

Title: LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion
  Transformers for Image and Video Generation
Authors: Lianwei Yang, Haokun Lin, Tianchen Zhao, Yichen Wu, Hongyu Zhu, Ruiqi
  Xie, Zhenan Sun, Yu Wang, Qingyi Gu
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.03485 ,  3214kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04324
replaced with revised version Sun, 21 Sep 2025 11:06:38 GMT   (23647kb)

Title: TempFlow-GRPO: When Timing Matters for GRPO in Flow Models
Authors: Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin,
  Fengyun Rao, Bo Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.04324 ,  23647kb)
------------------------------------------------------------------------------
\\
arXiv:2508.05630
replaced with revised version Mon, 22 Sep 2025 13:44:53 GMT   (7626kb)

Title: MOSEv2: A More Challenging Dataset for Video Object Segmentation in
  Complex Scenes
Authors: Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang,
  Yu-Gang Jiang, Philip H.S. Torr, Song Bai
Categories: cs.CV
Comments: MOSEv2 Dataset Report, Project Page: https://mose.video/, Baseline &
  metric code: https://github.com/henghuiding/MOSE-api
\\ ( https://arxiv.org/abs/2508.05630 ,  7626kb)
------------------------------------------------------------------------------
\\
arXiv:2508.06191
replaced with revised version Mon, 22 Sep 2025 07:49:11 GMT   (802kb)

Title: A Semantic Segmentation Algorithm for Pleural Effusion Based on
  DBIF-AUNet
Authors: Ruixiang Tang, Mingda Zhang, Jianglong Qin, Yan Song, Yi Wu, and Wei
  Wu
Categories: cs.CV
Comments: 12 pages, 6 figures, 2 tables
MSC-class: 68T45, 92C55
ACM-class: I.4.6; I.5.4; J.3
\\ ( https://arxiv.org/abs/2508.06191 ,  802kb)
------------------------------------------------------------------------------
\\
arXiv:2508.12381
replaced with revised version Mon, 22 Sep 2025 03:22:05 GMT   (899kb)

Title: IPGPhormer: Interpretable Pathology Graph-Transformer for Survival
  Analysis
Authors: Guo Tang, Songhan Jiang, Jinpeng Lu, Linghan Cai, Yongbing Zhang
Categories: cs.CV cs.AI
Comments: 13 pages, 5 figures
\\ ( https://arxiv.org/abs/2508.12381 ,  899kb)
------------------------------------------------------------------------------
\\
arXiv:2508.12720
replaced with revised version Sat, 20 Sep 2025 12:20:24 GMT   (15274kb)

Title: Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian
  Splatting
Authors: Kangjie Chen, Yingji Zhong, Zhihao Li, Jiaqi Lin, Youyu Chen, Minghan
  Qin, Haoqian Wang
Categories: cs.CV
Comments: Accepted by NeurIPS 2025. Project page:
  https://chenkangjie1123.github.io/Co-Adaptation-3DGS/, Code at:
  https://github.com/chenkangjie1123/Co-Adaptation-of-3DGS
\\ ( https://arxiv.org/abs/2508.12720 ,  15274kb)
------------------------------------------------------------------------------
\\
arXiv:2508.13479
replaced with revised version Sun, 21 Sep 2025 09:45:59 GMT   (11875kb)

Title: AIM 2025 challenge on Inverse Tone Mapping Report: Methods and Results
Authors: Chao Wang, Francesco Banterle, Bin Ren, Radu Timofte, Xin Lu, Yufeng
  Peng, Chengjie Ge, Zhijing Sun, Ziang Zhou, Zihao Li, Zishun Liao, Qiyu Kang,
  Xueyang Fu, Zheng-Jun Zha, Zhijing Sun, Xingbo Wang, Kean Liu, Senyan Xu,
  Yang Qiu, Yifan Ding, Gabriel Eilertsen, Jonas Unger, Zihao Wang, Ke Wu,
  Jinshan Pan, Zhen Liu, Zhongyang Li, Shuaicheng Liu, S.M Nadim Uddin
Categories: cs.CV eess.IV
\\ ( https://arxiv.org/abs/2508.13479 ,  11875kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15376
replaced with revised version Sun, 21 Sep 2025 15:08:43 GMT   (17853kb)

Title: DriveSplat: Decoupled Driving Scene Reconstruction with
  Geometry-enhanced Partitioned Neural Gaussians
Authors: Cong Wang, Xianda Guo, Wenbo Xu, Wei Tian, Ruiqi Song, Chenming Zhang,
  Lingxi Li, Long Chen
Categories: cs.CV
\\ ( https://arxiv.org/abs/2508.15376 ,  17853kb)
------------------------------------------------------------------------------
\\
arXiv:2508.17007
replaced with revised version Sun, 21 Sep 2025 10:13:19 GMT   (1092kb)

Title: An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional
  Attention for Multi-organ Segmentation
Authors: Riad Hassan, M. Rubaiyat Hossain Mondal, Sheikh Iqbal Ahamed, Fahad
  Mostafa, Md Mostafijur Rahman
Categories: cs.CV cs.AI
Comments: After revision, minor ablation studies have been added in the
  published version in Biomedical Signal Processing and Control (BSPC)
DOI: 10.1016/j.bspc.2025.108611
\\ ( https://arxiv.org/abs/2508.17007 ,  1092kb)
------------------------------------------------------------------------------
\\
arXiv:2508.19972
replaced with revised version Mon, 22 Sep 2025 02:58:41 GMT   (4722kb)

Title: GLSim: Detecting Object Hallucinations in LVLMs via Global-Local
  Similarity
Authors: Seongheon Park and Yixuan Li
Categories: cs.CV cs.AI cs.CL
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2508.19972 ,  4722kb)
------------------------------------------------------------------------------
\\
arXiv:2509.00367
replaced with revised version Sat, 20 Sep 2025 11:24:54 GMT   (9008kb)

Title: A Multimodal and Multi-centric Head and Neck Cancer Dataset for
  Segmentation, Diagnosis and Outcome Prediction
Authors: Numan Saeed, Salma Hassan, Shahad Hardan, Ahmed Aly, Darya Taratynova,
  Umair Nawaz, Ufaq Khan, Muhammad Ridzuan, Vincent Andrearczyk, Adrien
  Depeursinge, Yutong Xie, Thomas Eugene, Rapha\"el Metz, M\'elanie Dore,
  Gregory Delpon, Vijay Ram Kumar Papineni, Kareem Wahid, Cem Dede, Alaa
  Mohamed Shawky Ali, Carlos Sjogreen, Mohamed Naser, Clifton D. Fuller,
  Valentin Oreiller, Mario Jreige, John O. Prior, Catherine Cheze Le Rest,
  Olena Tankyevych, Pierre Decazes, Su Ruan, Stephanie Tanadini-Lang, Martin
  Valli\`eres, Hesham Elhalawani, Ronan Abgral, Romain Floch, Kevin Kerleguer,
  Ulrike Schick, Maelle Mauguen, David Bourhis, Jean-Christophe Leclere,
  Amandine Sambourg, Arman Rahmim, Mathieu Hatt, Mohammad Yaqub
Categories: cs.CV
Comments: 10 pages, 5 figures. Numan Saeed is the corresponding author. Numan
  Saeed, Salma Hassan and Shahad Hardan contributed equally to this work.
  Project page: https://hecktor25.grand-challenge.org/
\\ ( https://arxiv.org/abs/2509.00367 ,  9008kb)
------------------------------------------------------------------------------
\\
arXiv:2509.00787
replaced with revised version Sun, 21 Sep 2025 00:49:10 GMT   (31607kb)

Title: Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided
  Multimodal Diffusion Models
Authors: Ganxi Xu, Jinyi Long, Jia Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.00787 ,  31607kb)
------------------------------------------------------------------------------
\\
arXiv:2509.03185
replaced with revised version Mon, 22 Sep 2025 11:42:48 GMT   (13561kb)

Title: PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement
  Learning Framework for Adaptive Low-Dose CT Denoising
Authors: Debopom Sutradhar, Ripon Kumar Debnath, Mohaimenul Azam Khan Raiaan,
  Yan Zhang, Reem E. Mohamed, Sami Azam
Categories: cs.CV
Comments: 20 pages, 5 figures, 5 tables
\\ ( https://arxiv.org/abs/2509.03185 ,  13561kb)
------------------------------------------------------------------------------
\\
arXiv:2509.05075
replaced with revised version Fri, 19 Sep 2025 18:29:48 GMT   (16252kb)

Title: GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting
Authors: Yangming Li, Chaoyu Liu, Lihao Liu, Simon Masnou, Carola-Bibiane
  Sch\"onlieb
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.05075 ,  16252kb)
------------------------------------------------------------------------------
\\
arXiv:2509.06035
replaced with revised version Mon, 22 Sep 2025 09:19:38 GMT   (19592kb)

Title: TinyDef-DETR: A DETR-based Framework for Defect Detection in
  Transmission Lines from UAV Imagery
Authors: Feng Shen, Jiaming Cui, Shuai Zhou, Wenqiang Li and Ruifeng Qin
Categories: cs.CV cs.AI cs.CE
\\ ( https://arxiv.org/abs/2509.06035 ,  19592kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07493
replaced with revised version Sun, 21 Sep 2025 06:09:22 GMT   (62264kb)

Title: Accurate and Complete Surface Reconstruction from 3D Gaussians via
  Direct SDF Learning
Authors: Wenzhi Guo, Bing Wang
Categories: cs.CV cs.CG
\\ ( https://arxiv.org/abs/2509.07493 ,  62264kb)
------------------------------------------------------------------------------
\\
arXiv:2509.08436
replaced with revised version Mon, 22 Sep 2025 06:47:26 GMT   (39143kb)

Title: HyperTTA: Test-Time Adaptation for Hyperspectral Image Classification
  under Distribution Shifts
Authors: Xia Yue, Anfeng Liu, Ning Chen, Chenjia Huang, Hui Liu, Zhou Huang,
  and Leyuan Fang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.08436 ,  39143kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11598
replaced with revised version Sun, 21 Sep 2025 05:18:37 GMT   (4984kb)

Title: Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid
  Generative-Discriminative Learning Framework
Authors: Siming Fu, Sijun Dong, Xiaoliang Meng
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2509.11598 ,  4984kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11815
replaced with revised version Sun, 21 Sep 2025 03:35:36 GMT   (406kb)

Title: SpecVLM: Fast Speculative Decoding in Vision-Language Models
Authors: Haiduo Huang, Fuwei Yang, Zhenhua Liu, Xuanwu Yin, Dong Li, Pengju
  Ren, Emad Barsoum
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2509.11815 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13767
replaced with revised version Mon, 22 Sep 2025 07:12:22 GMT   (5709kb)

Title: VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in
  Real-time MRI
Authors: Daiqi Liu, Tom\'as Arias-Vergara, Johannes Enk, Fangxu Xing, Maureen
  Stone, Jerry L. Prince, Jana Hutter, Andreas Maier, Jonghye Woo, Paula Andrea
  P\'erez-Toro
Categories: cs.CV
Comments: Preprint submitted to ICASSP
\\ ( https://arxiv.org/abs/2509.13767 ,  5709kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13795
replaced with revised version Sat, 20 Sep 2025 08:43:46 GMT   (30842kb)

Title: SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient
  4-DoF UAV Localization in GNSS-Denied Environments
Authors: Jiayu Yuan, Ming Dai, Enhui Zheng, Chao Su, Nanxing Chen, Qiming Hu,
  Shibo Zhu and Yibin Cao
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.13795 ,  30842kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14051
replaced with revised version Sat, 20 Sep 2025 23:16:38 GMT   (1359kb)

Title: PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd
  Multi-modal Embeddings
Authors: Suhang You, Carla Pitarch-Abaigar, Sanket Kachole, Sumedh Sonawane,
  Juhyung Ha, Anish Sudarshan Gada, David Crandall, Rakesh Shiradkar and
  Spyridon Bakas
Categories: cs.CV
Comments: 11 pages, 1 figure, method paper for CHIMERA 2025 Challenge
\\ ( https://arxiv.org/abs/2509.14051 ,  1359kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14958
replaced with revised version Sun, 21 Sep 2025 04:53:08 GMT   (2780kb)

Title: Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via
  Cross-Modal Geometric Rectification
Authors: Tuo Xiang and Xuemiao Xu and Bangzhen Liu and Jinyi Li and Yong Li and
  Shengfeng He
Categories: cs.CV
Comments: ICCV2025
\\ ( https://arxiv.org/abs/2509.14958 ,  2780kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15235
replaced with revised version Mon, 22 Sep 2025 06:42:20 GMT   (2499kb)

Title: ViSpec: Accelerating Vision-Language Models with Vision-Aware
  Speculative Decoding
Authors: Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen
Categories: cs.CV cs.CL
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2509.15235 ,  2499kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15250
replaced with revised version Mon, 22 Sep 2025 01:18:13 GMT   (2412kb)

Title: Walk and Read Less: Improving the Efficiency of Vision-and-Language
  Navigation via Tuning-Free Multimodal Token Pruning
Authors: Wenda Qin, Andrea Burns, Bryan A. Plummer, Margrit Betke
Categories: cs.CV cs.AI
Comments: Accepted to EMNLP 2025. Data and code to be released at
  https://github.com/wdqin/VLN-NAP
\\ ( https://arxiv.org/abs/2509.15250 ,  2412kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15293
replaced with revised version Mon, 22 Sep 2025 17:44:37 GMT   (853kb)

Title: How Good are Foundation Models in Step-by-Step Embodied Reasoning?
Authors: Dinura Dissanayake and Ahmed Heakl and Omkar Thawakar and Noor Ahsan
  and Ritesh Thawkar and Ketan More and Jean Lahoud and Rao Anwer and Hisham
  Cholakkal and Ivan Laptev and Fahad Shahbaz Khan and Salman Khan
Categories: cs.CV cs.RO
Comments: Project page: https://mbzuai-oryx.github.io/FoMER-Bench/
\\ ( https://arxiv.org/abs/2509.15293 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15548
replaced with revised version Mon, 22 Sep 2025 17:34:24 GMT   (33618kb)

Title: MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild
Authors: Deming Li, Kaiwen Jiang, Yutao Tang, Ravi Ramamoorthi, Rama Chellappa,
  Cheng Peng
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.15548 ,  33618kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15566
replaced with revised version Mon, 22 Sep 2025 02:37:02 GMT   (4503kb)

Title: BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent
Authors: Shaojie Zhang, Ruoceng Zhang, Pei Fu, Shaokang Wang, Jiahui Yang, Xin
  Du, Shiqi Cui, Bin Qin, Ying Huang, Zhenbo Luo, Jian Luan
Categories: cs.CV cs.AI
Comments: Accepted at NeurIPS 2025
\\ ( https://arxiv.org/abs/2509.15566 ,  4503kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15602
replaced with revised version Mon, 22 Sep 2025 14:29:06 GMT   (953kb)

Title: TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?
Authors: Zhongyuan Bao, Lejun Zhang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2509.15602 ,  953kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08460
replaced with revised version Mon, 22 Sep 2025 08:21:52 GMT   (37kb)

Title: Selective Population Protocols
Authors: Adam Ga\'nczorz, Leszek G\k{a}sieniec, Tomasz Jurdzi\'nski, Jakub
  Kowalski, and Grzegorz Stachowiak
Categories: cs.DC cs.DS
Comments: Full version of SSS 2024 paper
\\ ( https://arxiv.org/abs/2305.08460 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10834
replaced with revised version Mon, 22 Sep 2025 12:42:54 GMT   (1294kb)

Title: Cppless: Single-Source and High-Performance Serverless Programming in
  C++
Authors: Marcin Copik, Lukas M\"oller, Alexandru Calotoiu, Torsten Hoefler
Categories: cs.DC
Comments: Extended version of paper accepted at the ACM Transactions on
  Architecture and Code Optimization (TACO) journal
Journal-ref: ACM Transactions on Architecture and Code Optimization, Volume 22,
  Issue 3, Article No.: 110, Pages 1 - 27, 2025
DOI: 10.1145/3747841
\\ ( https://arxiv.org/abs/2401.10834 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2501.05408
replaced with revised version Mon, 22 Sep 2025 11:17:18 GMT   (2107kb)

Title: Tempo: Compiled Dynamic Deep Learning with Symbolic Dependence Graphs
Authors: Pedro F. Silvestre, Peter Pietzuch
Categories: cs.DC cs.AI cs.LG
Comments: 17 pages, 24 figures, 3 bibliography pages
ACM-class: I.2; I.1
\\ ( https://arxiv.org/abs/2501.05408 ,  2107kb)
------------------------------------------------------------------------------
\\
arXiv:2506.10356
replaced with revised version Sun, 21 Sep 2025 18:11:26 GMT   (907kb)

Title: Is Sparse Matrix Reordering Effective for Sparse Matrix-Vector
  Multiplication?
Authors: Omid Asudeh, Sina Mahdipour Saravani, Gerald Sabin, Fabrice Rastello,
  P Sadayappan
Categories: cs.DC cs.PF
\\ ( https://arxiv.org/abs/2506.10356 ,  907kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20673
replaced with revised version Mon, 22 Sep 2025 13:29:31 GMT   (944kb)

Title: ClusterRCA: An End-to-End Approach for Network Fault Localization and
  Classification for HPC System
Authors: Yongqian Sun, Xijie Pan, Xiao Xiong, Lei Tao, Jiaju Wang, Shenglin
  Zhang, Yuan Yuan, Yuqi Li and Kunlin Jian
Categories: cs.DC cs.AI
\\ ( https://arxiv.org/abs/2506.20673 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2507.04647
replaced with revised version Mon, 22 Sep 2025 11:31:53 GMT   (1562kb)

Title: RAPTOR: Practical Numerical Profiling of Scientific Applications
Authors: Faveo Hoerold, Ivan R. Ivanov, Akash Dhruv, William S. Moses, Anshu
  Dubey, Mohamed Wahib, Jens Domke
Categories: cs.DC cs.NA math.NA
Comments: 12 pages, 8 figures, to be published in SC'25
DOI: 10.1145/3712285.3759810
\\ ( https://arxiv.org/abs/2507.04647 ,  1562kb)
------------------------------------------------------------------------------
\\
arXiv:2507.10367
replaced with revised version Sun, 21 Sep 2025 07:03:46 GMT   (612kb)

Title: FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline
Authors: Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong
  Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu,
  and Haibo Chen
Categories: cs.DC cs.PF
Comments: Accepted by NSDI'26
\\ ( https://arxiv.org/abs/2507.10367 ,  612kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04284
replaced with revised version Sat, 20 Sep 2025 09:42:41 GMT   (920kb)

Title: Optimizing Microgrid Composition for Sustainable Data Centers
Authors: Julius Irion, Philipp Wiesner, Jonathan Bader, Odej Kao
Categories: cs.DC cs.SY eess.SY
Comments: Presented at SC25 Sustainable Supercomputing Workshop
\\ ( https://arxiv.org/abs/2508.04284 ,  920kb)
------------------------------------------------------------------------------
\\
arXiv:2508.06406
replaced with revised version Sat, 20 Sep 2025 12:03:46 GMT   (1422kb)

Title: Blockchain-Enabled Federated Learning
Authors: Murtaza Rangwala, KR Venugopal, Rajkumar Buyya
Categories: cs.DC cs.LG
Comments: 32 pages, 6 figures, chapter for edited book (Federated Learning:
  Foundations and Applications)
\\ ( https://arxiv.org/abs/2508.06406 ,  1422kb)
------------------------------------------------------------------------------
\\
arXiv:2508.21613
replaced with revised version Mon, 22 Sep 2025 02:45:57 GMT   (895kb)

Title: Odyssey: Adaptive Policy Selection for Resilient Distributed Training
Authors: Yuhang Zhou, Zhibin Wang, Peng Jiang, Haoran Xia, Junhe Lu, Qianyu
  Jiang, Rong Gu, Hengxi Xu, Xinjing Huang, Guanghuan Fang, Zhiheng Hu, Jingyi
  Zhang, Yongjin Cai, Jian He and Chen Tian
Categories: cs.DC
\\ ( https://arxiv.org/abs/2508.21613 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11697
replaced with revised version Sun, 21 Sep 2025 14:59:36 GMT   (1175kb)

Title: Towards the Distributed Large-scale k-NN Graph Construction by Graph
  Merge
Authors: Cheng Zhang and Wan-Lei Zhao and Shihai Xiao and Jiajie Yao and
  Xuecang Zhang
Categories: cs.DC
Comments: 16 pages, 17 figures
\\ ( https://arxiv.org/abs/2509.11697 ,  1175kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13583
replaced with revised version Mon, 22 Sep 2025 00:14:01 GMT   (444kb)

Title: Modeling the Carbon Footprint of HPC: The Top 500 and EasyC
Authors: Varsha Rao, Andrew A. Chien
Categories: cs.DC
Comments: 15 pages, 11 figures
Journal-ref: Workshops of the International Conference for High Performance
  Computing, Networking, Storage and Analysis (SC Workshops 2025)
DOI: 10.1145/3731599.3767567
\\ ( https://arxiv.org/abs/2509.13583 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05309
replaced with revised version Sat, 20 Sep 2025 16:08:12 GMT   (671kb)

Title: Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia
  Games
Authors: Niv Eckhaus, Uri Berger and Gabriel Stanovsky
Categories: cs.MA cs.AI cs.CL
\\ ( https://arxiv.org/abs/2506.05309 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2509.08811
replaced with revised version Sun, 21 Sep 2025 17:37:18 GMT   (983kb)

Title: A Bayesian Dynamical System Model of Joint Action and Interpersonal
  Coordination
Authors: Andrew Jun Lee, Grace Qiyuan Miao, Rick Dale, Alexia Galati, Hongjing
  Lu
Categories: cs.MA
\\ ( https://arxiv.org/abs/2509.08811 ,  983kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11656
replaced with revised version Mon, 22 Sep 2025 08:56:44 GMT   (1338kb)

Title: MALLM: Multi-Agent Large Language Models Framework
Authors: Jonas Becker, Lars Benedikt Kaesberg, Niklas Bauer, Jan Philip Wahle,
  Terry Ruas, Bela Gipp
Categories: cs.MA cs.AI cs.CL
Comments: Accepted at EMNLP 2025 (Demo)
ACM-class: A.1; I.2.7
\\ ( https://arxiv.org/abs/2509.11656 ,  1338kb)
------------------------------------------------------------------------------
\\
arXiv:2105.14125
replaced with revised version Sun, 21 Sep 2025 02:44:38 GMT   (4898kb)

Title: Joint Optimization of Multi-Objective Reinforcement Learning with Policy
  Gradient Based Algorithm
Authors: Qinbo Bai and Mridul Agarwal and Vaneet Aggarwal
Categories: cs.LG cs.AI cs.SY eess.SY
DOI: 10.1613/jair.1.13981
\\ ( https://arxiv.org/abs/2105.14125 ,  4898kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06055
replaced with revised version Fri, 19 Sep 2025 23:36:12 GMT   (2284kb)

Title: Sample-Efficient Reinforcement Learning with Symmetry-Guided
  Demonstrations for Robotic Manipulation
Authors: Amir M. Soufi Enayati, Zengjie Zhang, Kashish Gupta, and Homayoun
  Najjaran
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2304.06055 ,  2284kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00016 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 04:55:08 GMT   (4121kb)

Title: Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment
Authors: Saizhuo Wang, Hang Yuan, Leon Zhou, Lionel M. Ni, Heung-Yeung Shum,
  Jian Guo
Categories: q-fin.CP cs.AI cs.CL
Comments: EMNLP 2025 System Demonstration Track
\\ ( https://arxiv.org/abs/2308.00016 ,  4121kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10164
replaced with revised version Sun, 21 Sep 2025 20:41:13 GMT   (5556kb)

Title: A Scalable Multi-Robot Framework for Decentralized and Asynchronous
  Perception-Action-Communication Loops
Authors: Saurav Agarwal, Frederic Vatnsdal, Romina Garcia Camargo, Vijay Kumar,
  Alejandro Ribeiro
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2309.10164 ,  5556kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11895
replaced with revised version Mon, 22 Sep 2025 01:22:54 GMT   (35kb)

Title: Audio Contrastive-based Fine-tuning: Decoupling Representation Learning
  and Classification
Authors: Yang Wang, Qibin Liang, Chenghao Xiao, Yizhi Li, Noura Al Moubayed,
  Chenghua Lin
Categories: cs.SD cs.AI cs.CL eess.AS
Comments: This paper has been submitted to ICASSP 2026 and is currently under
  review
\\ ( https://arxiv.org/abs/2309.11895 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2405.04760
replaced with revised version Mon, 22 Sep 2025 12:50:51 GMT   (450kb)

Title: Large Language Models for Cyber Security: A Systematic Literature Review
Authors: Hanxiang Xu, Shenao Wang, Ningke Li, Kailong Wang, Yanjie Zhao, Kai
  Chen, Ting Yu, Yang Liu and Haoyu Wang
Categories: cs.CR cs.AI
Comments: Accepted by ACM Transactions on Software Engineering and Methodology
  (TOSEM)
\\ ( https://arxiv.org/abs/2405.04760 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2405.14286
replaced with revised version Sun, 21 Sep 2025 13:43:29 GMT   (3444kb)

Title: Modeling Edge-Specific Node Features through Co-Representation Neural
  Hypergraph Diffusion
Authors: Yijia Zheng, Marcel Worring
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2405.14286 ,  3444kb)
------------------------------------------------------------------------------
\\
arXiv:2410.03032
replaced with revised version Sat, 20 Sep 2025 12:50:17 GMT   (3292kb)

Title: Designing Human-AI Collaboration to Support Learning in Counterspeech
  Writing
Authors: Xiaohan Ding, Kaike Ping, Uma Sushmitha Gunturi, Buse Carik, Sophia
  Stil, Lance T Wilhelm, Taufiq Daryanto, James Hawdon, Sang Won Lee, Eugenia H
  Rho
Categories: cs.HC cs.AI cs.CY
\\ ( https://arxiv.org/abs/2410.03032 ,  3292kb)
------------------------------------------------------------------------------
\\
arXiv:2410.04039
replaced with revised version Fri, 19 Sep 2025 23:02:41 GMT   (134kb)

Title: BlockScan: Detecting Anomalies in Blockchain Transactions
Authors: Jiahao Yu, Xian Wu, Hao Liu, Wenbo Guo, Xinyu Xing
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2410.04039 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2412.07836 (*cross-listing*)
replaced with revised version Mon, 22 Sep 2025 00:46:53 GMT   (5210kb)

Title: Machine learning-driven conservative-to-primitive conversion in hybrid
  piecewise polytropic and tabulated equations of state
Authors: Semih Kacmaz, Roland Haas, E. A. Huerta
Categories: gr-qc astro-ph.IM cs.AI physics.comp-ph
Comments: 15 pages, 6 figures, 3 tables Manuscript content synced with
  publication
ACM-class: J.2; I.2
Journal-ref: Symmetry 2025, 17(9)
DOI: 10.3390/sym17091409
\\ ( https://arxiv.org/abs/2412.07836 ,  5210kb)
------------------------------------------------------------------------------
\\
arXiv:2501.00755 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 16:00:23 GMT   (4729kb)

Title: An AI-powered Bayesian generative modeling approach for causal inference
  in observational studies
Authors: Qiao Liu and Wing Hung Wong
Categories: stat.ML cs.AI cs.LG stat.ME
\\ ( https://arxiv.org/abs/2501.00755 ,  4729kb)
------------------------------------------------------------------------------
\\
arXiv:2501.08760
replaced with revised version Sat, 20 Sep 2025 13:31:21 GMT   (1491kb)

Title: INTA: Intent-Based Translation for Network Configuration with LLM Agents
Authors: Yunze Wei, Xiaohui Xie, Tianshuo Hu, Yiwei Zuo, Xinyi Chen, Kaiwen
  Chi, Yong Cui
Categories: cs.NI cs.AI cs.LG cs.SE
Comments: Accepted by The 33rd IEEE International Conference on Network
  Protocols (IEEE ICNP 2025)
\\ ( https://arxiv.org/abs/2501.08760 ,  1491kb)
------------------------------------------------------------------------------
\\
arXiv:2501.16534
replaced with revised version Fri, 19 Sep 2025 21:29:25 GMT   (488kb)

Title: Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs
Authors: Jean-Charles Noirot Ferrand, Yohan Beugin, Eric Pauley, Ryan
  Sheatsley, Patrick McDaniel
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2501.16534 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2501.17296
replaced with revised version Fri, 19 Sep 2025 22:31:31 GMT   (7219kb)

Title: COMPOL: A Unified Neural Operator Framework for Scalable Multi-Physics
  Simulations
Authors: Yifei Sun, Tao Wang, Junqi Qu, Yushun Dong, Hewei Tang, Shibo Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2501.17296 ,  7219kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01316
replaced with revised version Mon, 22 Sep 2025 06:28:20 GMT   (9497kb)

Title: Learning Fused State Representations for Control from Multi-View
  Observations
Authors: Zeyu Wang, Yao-Hui Li, Xin Li, Hongyu Zang, Romain Laroche, Riashat
  Islam
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2502.01316 ,  9497kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01406
replaced with revised version Sun, 21 Sep 2025 10:01:05 GMT   (11044kb)

Title: GRADIEND: Feature Learning within Neural Networks Exemplified through
  Biases
Authors: Jonathan Drechsel, Steffen Herbold
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2502.01406 ,  11044kb)
------------------------------------------------------------------------------
\\
arXiv:2502.01755
replaced with revised version Sat, 20 Sep 2025 17:16:57 GMT   (6984kb)

Title: Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA
Authors: Shuangyi Chen, Yuanxin Guo, Yue Ju, Harik Dalal, Zhongwen Zhu, Ashish
  Khisti
Categories: cs.LG cs.AI
Comments: Accepted to NeurIPS 2025
\\ ( https://arxiv.org/abs/2502.01755 ,  6984kb)
------------------------------------------------------------------------------
\\
arXiv:2502.02495
replaced with revised version Sat, 20 Sep 2025 23:28:32 GMT   (37kb)

Title: The Causal-Effect Score in Data Management
Authors: Felipe Azua and Leopoldo Bertossi
Categories: cs.DB cs.AI
Comments: 21 pages. Slightly extended and revised version of published paper in
  Proc. 4th Conference on Causal Learning and Reasoning (CLeaR), 2025. PMLR
  2025, 275:874-893
\\ ( https://arxiv.org/abs/2502.02495 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09885
replaced with revised version Sun, 21 Sep 2025 12:39:38 GMT   (64kb)

Title: Comprehensive Review of Neural Differential Equations for Time Series
  Analysis
Authors: YongKyung Oh, Seungsu Kam, Jonghun Lee, Dong-Young Lim, Sungil Kim,
  Alex Bui
Categories: cs.LG cs.AI
Comments: Published at the Thirty-Fourth International Joint Conference on
  Artificial Intelligence (IJCAI 2025), Survey Track.
  https://www.ijcai.org/proceedings/2025/1179
\\ ( https://arxiv.org/abs/2502.09885 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12466
replaced with revised version Fri, 19 Sep 2025 22:16:55 GMT   (314kb)

Title: EquiBench: Benchmarking Large Language Models' Reasoning about Program
  Semantics via Equivalence Checking
Authors: Anjiang Wei, Jiannan Cao, Ran Li, Hongyu Chen, Yuhui Zhang, Ziheng
  Wang, Yuan Liu, Thiago S. F. X. Teixeira, Diyi Yang, Ke Wang, Alex Aiken
Categories: cs.LG cs.AI cs.CL cs.PL cs.SE
\\ ( https://arxiv.org/abs/2502.12466 ,  314kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12507
replaced with revised version Mon, 22 Sep 2025 06:31:04 GMT   (434kb)

Title: Multi-branch of Attention Yields Accurate Results for Tabular Data
Authors: Xuechen Li, Yupeng Li, Jian Liu, Xiaolin Jin and Xin Hu
Categories: cs.LG cs.AI
Comments: 19 pages, 3 figures
\\ ( https://arxiv.org/abs/2502.12507 ,  434kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13767
replaced with revised version Sun, 21 Sep 2025 11:07:27 GMT   (107kb)

Title: Agentic AI Software Engineers: Programming with Trust
Authors: Abhik Roychoudhury, Corina Pasareanu, Michael Pradel, Baishakhi Ray
Categories: cs.SE cs.AI
Comments: 5 pages
\\ ( https://arxiv.org/abs/2502.13767 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2502.17439
replaced with revised version Fri, 19 Sep 2025 20:42:28 GMT   (327kb)

Title: Large Language Models as Realistic Microservice Trace Generators
Authors: Donghyun Kim, Sriram Ravula, Taemin Ha, Alexandros G. Dimakis,
  Daehyeok Kim, Aditya Akella
Categories: cs.SE cs.AI cs.DC cs.OS
\\ ( https://arxiv.org/abs/2502.17439 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2503.02318
replaced with revised version Sat, 20 Sep 2025 06:37:34 GMT   (2094kb)

Title: Audio-Reasoner: Improving Reasoning Capability in Large Audio Language
  Models
Authors: Zhifei Xie, Mingbao Lin, Zihang Liu, Pengcheng Wu, Shuicheng Yan and
  Chunyan Miao
Categories: cs.SD cs.AI cs.CL cs.LG cs.MM eess.AS
Comments: Technical report, in process
\\ ( https://arxiv.org/abs/2503.02318 ,  2094kb)
------------------------------------------------------------------------------
\\
arXiv:2503.03262
replaced with revised version Sat, 20 Sep 2025 17:44:10 GMT   (13362kb)

Title: Trajectory Prediction for Autonomous Driving: Progress, Limitations, and
  Future Directions
Authors: Nadya Abdel Madjid, Abdulrahman Ahmad, Murad Mebrahtu, Yousef Babaa,
  Abdelmoamen Nasser, Sumbal Malik, Bilal Hassan, Naoufel Werghi, Jorge Dias,
  Majid Khonji
Categories: cs.RO cs.AI cs.CV cs.LG
DOI: 10.1016/j.inffus.2025.103588
\\ ( https://arxiv.org/abs/2503.03262 ,  13362kb)
------------------------------------------------------------------------------
\\
arXiv:2503.22424
replaced with revised version Mon, 22 Sep 2025 12:45:26 GMT   (1157kb)

Title: CoSIL: Issue Localization via LLM-Driven Code Graph Searching
Authors: Zhonghao Jiang, Xiaoxue Ren, Meng Yan, Wei Jiang, Yong Li, Zhongxin
  Liu
Categories: cs.SE cs.AI cs.CL
Comments: Accepted by ASE 2025
\\ ( https://arxiv.org/abs/2503.22424 ,  1157kb)
------------------------------------------------------------------------------
\\
arXiv:2503.23270
replaced with revised version Sat, 20 Sep 2025 21:57:19 GMT   (4569kb)

Title: Localized Graph-Based Neural Dynamics Models for Terrain Manipulation
Authors: Chaoqi Liu, Yunzhu Li, Kris Hauser
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2503.23270 ,  4569kb)
------------------------------------------------------------------------------
\\
arXiv:2504.08771
replaced with revised version Sat, 20 Sep 2025 15:15:05 GMT   (5624kb)

Title: Generate the browsing process for short-video recommendation
Authors: Chao Feng, Yanze Zhang, Chenghao Zhang
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2504.08771 ,  5624kb)
------------------------------------------------------------------------------
\\
arXiv:2504.09716
replaced with revised version Sun, 21 Sep 2025 18:35:48 GMT   (32kb)

Title: Dominated Actions in Imperfect-Information Games
Authors: Sam Ganzfried
Categories: cs.GT cs.AI cs.MA econ.TH
\\ ( https://arxiv.org/abs/2504.09716 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2504.10369
replaced with revised version Mon, 22 Sep 2025 15:39:22 GMT   (353kb)

Title: SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired
  Symbolic Reasoning
Authors: Yiting Wang, Wanghao Ye, Ping Guo, Yexiao He, Ziyao Wang, Bowei Tian,
  Shwai He, Guoheng Sun, Zheyu Shen, Sihan Chen, Ankur Srivastava, Qingfu
  Zhang, Gang Qu, Ang Li
Categories: cs.AR cs.AI cs.LG cs.PL
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2504.10369 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2504.10552
replaced with revised version Fri, 19 Sep 2025 20:25:29 GMT   (6029kb)

Title: LEMUR Neural Network Dataset: Towards Seamless AutoML
Authors: Arash Torabi Goodarzi, Roman Kochnev, Waleed Khalid, Hojjat Torabi
  Goudarzi, Furui Qin, Tolgay Atinc Uzun, Yashkumar Sanjaybhai Dhameliya, Yash
  Kanubhai Kathiriya, Zofia Antonina Bentyn, Dmitry Ignatov, Radu Timofte
Categories: cs.LG cs.AI cs.CV cs.DL
\\ ( https://arxiv.org/abs/2504.10552 ,  6029kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15325
replaced with revised version Mon, 22 Sep 2025 13:10:36 GMT   (120kb)

Title: Significativity Indices for Agreement Values
Authors: Alberto Casagrande, Francesco Fabris, Rossano Girometti, Roberto
  Pagliarini
Categories: cs.LG cs.AI stat.ML
Comments: 27 pages, 6 figures
Journal-ref: Casagrande, A., Fabris, F., Girometti, R. et al. Significativity
  Indices for Agreement Values. Stat Comput 35, 197 (2025)
DOI: 10.1007/s11222-025-10728-1
\\ ( https://arxiv.org/abs/2504.15325 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2504.20069
replaced with revised version Sun, 21 Sep 2025 08:08:12 GMT   (178kb)

Title: A Simple Review of EEG Foundation Models: Datasets, Advancements and
  Future Perspectives
Authors: Junhong Lai, Jiyu Wei, Lin Yao and Yueming Wang
Categories: cs.LG cs.AI eess.SP
\\ ( https://arxiv.org/abs/2504.20069 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2505.01475
replaced with revised version Sun, 21 Sep 2025 09:42:50 GMT   (3640kb)

Title: CodeSSM: Towards State Space Models for Code Understanding
Authors: Shweta Verma, Abhinav Anand, Mira Mezini
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2505.01475 ,  3640kb)
------------------------------------------------------------------------------
\\
arXiv:2505.10831
replaced with revised version Sun, 21 Sep 2025 00:03:34 GMT   (16752kb)

Title: Creating General User Models from Computer Use
Authors: Omar Shaikh, Shardul Sapkota, Shan Rizvi, Eric Horvitz, Joon Sung
  Park, Diyi Yang, Michael S. Bernstein
Categories: cs.HC cs.AI cs.CL
Comments: 23 pages, 6 figures, 2 tables; see
  https://generalusermodels.github.io/
\\ ( https://arxiv.org/abs/2505.10831 ,  16752kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11146
replaced with revised version Sat, 20 Sep 2025 07:38:04 GMT   (13791kb)

Title: X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic
  Humanoid Imitation
Authors: Peizhen Li, Longbing Cao, Xiao-Ming Wu, Runze Yang, Xiaohan Yu
Categories: cs.RO cs.AI cs.HC
\\ ( https://arxiv.org/abs/2505.11146 ,  13791kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11750 (*cross-listing*)
replaced with revised version Sun, 21 Sep 2025 06:40:41 GMT   (1643kb)

Title: Improving Medium Range Severe Weather Prediction through Transformer
  Post-processing of AI Weather Forecasts
Authors: Zhanxiang Hua, Ryan Sobash, David John Gagne II, Yingkai Sha,
  Alexandra Anderson-Frey
Categories: physics.ao-ph cs.AI cs.LG
Comments: revision update
\\ ( https://arxiv.org/abs/2505.11750 ,  1643kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12260
replaced with revised version Mon, 22 Sep 2025 12:48:04 GMT   (451kb)

Title: LightRetriever: A LLM-based Text Retrieval Architecture with Extremely
  Faster Query Inference
Authors: Guangyuan Ma, Yongliang Ma, Xuanrui Gou, Zhenpeng Su, Ming Zhou,
  Songlin Hu
Categories: cs.IR cs.AI cs.CL
\\ ( https://arxiv.org/abs/2505.12260 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15856
replaced with revised version Sat, 20 Sep 2025 05:56:00 GMT   (527kb)

Title: DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster
  Management
Authors: Kai Yin, Xiangjue Dong, Chengkai Liu, Lipai Huang, Yiming Xiao, Zhewei
  Liu, Ali Mostafavi, James Caverlee
Categories: cs.IR cs.AI
Comments: EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2505.15856 ,  527kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19086
replaced with revised version Sat, 20 Sep 2025 17:43:37 GMT   (13702kb)

Title: MaskedManipulator: Versatile Whole-Body Manipulation
Authors: Chen Tessler, Yifeng Jiang, Erwin Coumans, Zhengyi Luo, Gal Chechik,
  Xue Bin Peng
Categories: cs.RO cs.AI cs.GR
Comments: SIGGRAPH Asia 2025
\\ ( https://arxiv.org/abs/2505.19086 ,  13702kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23195
replaced with revised version Sat, 20 Sep 2025 11:34:07 GMT   (926kb)

Title: Less is More: Unlocking Specialization of Time Series Foundation Models
  via Structured Pruning
Authors: Lifan Zhao, Yanyan Shen, Zhaoyang Liu, Xue Wang, Jiaji Deng
Categories: cs.LG cs.AI
Comments: Accepted by NeurIPS 2025
\\ ( https://arxiv.org/abs/2505.23195 ,  926kb)
------------------------------------------------------------------------------
\\
arXiv:2506.00455
replaced with revised version Sun, 21 Sep 2025 03:08:57 GMT   (3993kb)

Title: Diffusion Graph Neural Networks and Dataset for Robust Olfactory
  Navigation in Hazard Robotics
Authors: Kordel K. France, Ovidiu Daescu
Categories: cs.RO cs.AI cs.LG
Comments: 8 pages, 4 figures
\\ ( https://arxiv.org/abs/2506.00455 ,  3993kb)
------------------------------------------------------------------------------
\\
arXiv:2506.04742 (*cross-listing*)
replaced with revised version Mon, 22 Sep 2025 13:30:04 GMT   (340kb)

Title: Were Residual Penalty and Neural Operators All We Needed for Solving
  Optimal Control Problems?
Authors: Oliver G. S. Lundqvist and Fabricio Oliveira
Categories: math.OC cs.AI
\\ ( https://arxiv.org/abs/2506.04742 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05104
replaced with revised version Sat, 20 Sep 2025 20:26:48 GMT   (6229kb)

Title: Survey on the Evaluation of Generative Models in Music
Authors: Alexander Lerch, Claire Arthur, Nick Bryan-Kinns, Corey Ford, Qianyi
  Sun, Ashvala Vinay
Categories: cs.SD cs.AI cs.LG
Comments: Accepted paper submitted to ACM CSUR on 12-Sep-2025, original
  manuscript submitted on 26-Jun-2024
\\ ( https://arxiv.org/abs/2506.05104 ,  6229kb)
------------------------------------------------------------------------------
\\
arXiv:2506.07323
replaced with revised version Sun, 21 Sep 2025 20:56:28 GMT   (5460kb)

Title: Speech Recognition on TV Series with Video-guided Post-ASR Correction
Authors: Haoyuan Yang, Yue Zhang, Liqiang Jing, John H.L. Hansen
Categories: cs.SD cs.AI eess.AS
\\ ( https://arxiv.org/abs/2506.07323 ,  5460kb)
------------------------------------------------------------------------------
\\
arXiv:2506.14775
replaced with revised version Mon, 22 Sep 2025 12:44:58 GMT   (2084kb)

Title: See What I Mean? CUE: A Cognitive Model of Understanding Explanations
Authors: Tobias Labarta, Nhi Hoang, Katharina Weitz, Wojciech Samek, Sebastian
  Lapuschkin, Leander Weber
Categories: cs.HC cs.AI cs.LG
Comments: 10 pages, 5 figures (main text), 4 tables, 455-participant user study
MSC-class: 68T05, 91E30, 68U35, 62P10
ACM-class: H.5.2; I.2.6; H.1.2; I.4.9; K.4.2
Journal-ref: IJCAI 2025 Workshop on Explainable Artificial Intelligence (XAI),
  Montreal, Canada, August 2025
\\ ( https://arxiv.org/abs/2506.14775 ,  2084kb)
------------------------------------------------------------------------------
\\
arXiv:2506.17265
replaced with revised version Sun, 21 Sep 2025 19:41:22 GMT   (1907kb)

Title: SUA: Stealthy Multimodal Large Language Model Unlearning Attack
Authors: Xianren Zhang, Hui Liu, Delvin Ce Zhang, Xianfeng Tang, Qi He, Dongwon
  Lee and Suhang Wang
Categories: cs.LG cs.AI
Comments: EMNLP25
\\ ( https://arxiv.org/abs/2506.17265 ,  1907kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20062
replaced with revised version Sun, 21 Sep 2025 15:50:29 GMT   (5895kb)

Title: Beyond Autocomplete: Designing CopilotLens Towards Transparent and
  Explainable AI Coding Agents
Authors: Runlong Ye, Zeling Zhang, Boushra Almazroua, Michael Liut
Categories: cs.HC cs.AI
Comments: accepted at The First Workshop on the Application of LLM
  Explainability to Reasoning and Planning (XLLM-Reason-Plan) @ COLM 2025
\\ ( https://arxiv.org/abs/2506.20062 ,  5895kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20685
replaced with revised version Sun, 21 Sep 2025 04:50:02 GMT   (0kb,I)

Title: Progressive Size-Adaptive Federated Learning: A Comprehensive Framework
  for Heterogeneous Multi-Modal Data Systems
Authors: Sajid Hussain, Muhammad Sohail, Nauman Ali Khan, Naima Iltaf, and
  Ihtesham ul Islam
Categories: cs.LG cs.AI
Comments: Due to some technical issues
\\ ( https://arxiv.org/abs/2506.20685 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21140
replaced with revised version Fri, 19 Sep 2025 19:25:52 GMT   (6663kb)

Title: DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding
Authors: Ziwei Wang, Hongbin Wang, Tianwang Jia, Xingyi He, Siyang Li, and
  Dongrui Wu
Categories: cs.LG cs.AI
Comments: 14 pages, 7 figures
\\ ( https://arxiv.org/abs/2506.21140 ,  6663kb)
------------------------------------------------------------------------------
\\
arXiv:2506.22393
replaced with revised version Fri, 19 Sep 2025 20:53:26 GMT   (745kb)

Title: Multi-View Contrastive Learning for Robust Domain Adaptation in Medical
  Time Series Analysis
Authors: YongKyung Oh, Alex Bui
Categories: cs.LG cs.AI
Comments: Published at the sixth Conference on Health, Inference, and Learning
  (CHIL 2025), PMLR 287:502-526, 2025. Models & Methods Track - Best Paper
  Award. https://proceedings.mlr.press/v287/oh25a.html
\\ ( https://arxiv.org/abs/2506.22393 ,  745kb)
------------------------------------------------------------------------------
\\
arXiv:2506.23085
replaced with revised version Sat, 20 Sep 2025 01:02:23 GMT   (937kb)

Title: Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short
  Video Recommendations Using MMGCN and User Preferences
Authors: Saeid Aghasoleymani Najafabadi
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2506.23085 ,  937kb)
------------------------------------------------------------------------------
\\
arXiv:2507.01062
replaced with revised version Mon, 22 Sep 2025 12:21:53 GMT   (843kb)

Title: Quantifying Student Success with Generative AI: A Monte Carlo Simulation
  Informed by Systematic Review
Authors: Seyma Yaman Kayadibi
Categories: cs.CY cs.AI
Comments: 35 pages, 4 figures. All figures are image-based: one Python code
  screenshot, one regression model output, one success score distribution
  chart, and one PRISMA diagram. This article presents a standalone segment
  from the author's master's thesis at Victoria University
MSC-class: 62P25
ACM-class: K.3.1; H.5.2
\\ ( https://arxiv.org/abs/2507.01062 ,  843kb)
------------------------------------------------------------------------------
\\
arXiv:2507.02106 (*cross-listing*)
replaced with revised version Mon, 22 Sep 2025 01:05:44 GMT   (3463kb)

Title: Resolving Turbulent Magnetohydrodynamics: A Hybrid Operator-Diffusion
  Framework
Authors: Semih Kacmaz, E. A. Huerta, Roland Haas
Categories: physics.flu-dyn cs.AI cs.LG gr-qc physics.comp-ph
Comments: 16 pages, 6 figures, 1 table. Content synced with the published
  version
ACM-class: J.2; I.2
Journal-ref: Semih Kacmaz et al 2025 Mach. Learn.: Sci. Technol. 6 035057
DOI: 10.1088/2632-2153/ae054c
\\ ( https://arxiv.org/abs/2507.02106 ,  3463kb)
------------------------------------------------------------------------------
\\
arXiv:2507.11630
replaced with revised version Sat, 20 Sep 2025 08:53:22 GMT   (1077kb)

Title: Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility
Authors: Brendan Murphy, Dillon Bowen, Shahrad Mohammadzadeh, Tom Tseng, Julius
  Broomfield, Adam Gleave, Kellin Pelrine
Categories: cs.CR cs.AI cs.CL cs.CY
\\ ( https://arxiv.org/abs/2507.11630 ,  1077kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13340
replaced with revised version Sun, 21 Sep 2025 23:37:25 GMT   (1782kb)

Title: Latent Policy Steering with Embodiment-Agnostic Pretrained World Models
Authors: Yiqi Wang, Mrinal Verghese and Jeff Schneider
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2507.13340 ,  1782kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13543
replaced with revised version Mon, 22 Sep 2025 11:47:52 GMT   (14788kb)

Title: Loss-Complexity Landscape and Model Structure Functions
Authors: Alexander Kolpakov
Categories: cs.IT cs.AI cs.LG math-ph math.IT math.MP
Comments: 25 pages, 11 figures; GitHub repository at
  https://github.com/sashakolpakov/structure-functions
ACM-class: I.2.2; I.2.6
\\ ( https://arxiv.org/abs/2507.13543 ,  14788kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13742
replaced with revised version Sat, 20 Sep 2025 13:13:55 GMT   (1015kb)

Title: Search-Optimized Quantization in Biomedical Ontology Alignment
Authors: Oussama Bouaggad, Natalia Grabar
Categories: cs.LG cs.AI math.OC
Comments: Accepted for publication in Frontiers in Artificial Intelligence -
  Medicine and Public Health (Original Research)
Journal-ref: Front. Artif. Intell. 8:1662984 (2025)
DOI: 10.3389/frai.2025.1662984
\\ ( https://arxiv.org/abs/2507.13742 ,  1015kb)
------------------------------------------------------------------------------
\\
arXiv:2507.15833
replaced with revised version Mon, 22 Sep 2025 17:42:33 GMT   (2413kb)

Title: Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and
  Foveated Vision Transformers
Authors: Ian Chuang, Jinyu Zou, Andrew Lee, Dechen Gao, Iman Soltani
Categories: cs.RO cs.AI cs.CV
Comments: Project page: https://ian-chuang.github.io/gaze-av-aloha/
\\ ( https://arxiv.org/abs/2507.15833 ,  2413kb)
------------------------------------------------------------------------------
\\
arXiv:2507.17513
replaced with revised version Sat, 20 Sep 2025 21:07:37 GMT   (4701kb)

Title: HOTA: Hamiltonian framework for Optimal Transport Advection
Authors: Nazar Buzun, Daniil Shlenskii, Maxim Bobrin, Dmitry V. Dylov
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2507.17513 ,  4701kb)
------------------------------------------------------------------------------
\\
arXiv:2507.20936
replaced with revised version Sun, 21 Sep 2025 15:16:05 GMT   (781kb)

Title: Dissecting Persona-Driven Reasoning in Language Models via Activation
  Patching
Authors: Ansh Poonia, Maeghal Jain
Categories: cs.LG cs.AI cs.CL
Comments: EMNLP (Findings) 2025
\\ ( https://arxiv.org/abs/2507.20936 ,  781kb)
------------------------------------------------------------------------------
\\
arXiv:2507.21105
replaced with revised version Fri, 19 Sep 2025 22:28:32 GMT   (8673kb)

Title: AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP
  Protocols for Multimodal Information Retrieval and Analysis
Authors: Callie C. Liao, Duoduo Liao, Sai Surya Gadiraju
Categories: cs.IR cs.AI cs.CL
Comments: Accepted by EMNLP 2025
\\ ( https://arxiv.org/abs/2507.21105 ,  8673kb)
------------------------------------------------------------------------------
\\
arXiv:2507.22424
replaced with revised version Sat, 20 Sep 2025 18:24:48 GMT   (409kb)

Title: Spec-VLA: Speculative Decoding for Vision-Language-Action Models with
  Relaxed Acceptance
Authors: Songsheng Wang, Rucheng Yu, Zhihang Yuan, Chao Yu, Feng Gao, Yu Wang
  and Derek F. Wong
Categories: cs.LG cs.AI
Comments: 13 pages, 5 figures, Accepted by EMNLP 2025 (main conference)
\\ ( https://arxiv.org/abs/2507.22424 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01332
replaced with revised version Sun, 21 Sep 2025 03:31:35 GMT   (288kb)

Title: BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability
Authors: Zhenhua Zou and Zhuotao Liu and Lepeng Zhao and Qiuyang Zhan
Categories: cs.CR cs.AI
Comments: 43 pages
MSC-class: 68T42 (Primary), 94A60 (Secondary)
ACM-class: I.2.11; E.3
\\ ( https://arxiv.org/abs/2508.01332 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01861 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 01:30:51 GMT   (429kb)

Title: Tensor-Empowered Asset Pricing with Missing Data
Authors: Junyi Mo, Jiayu Li, Duo Zhang, Elynn Chen
Categories: stat.AP cs.AI cs.LG
\\ ( https://arxiv.org/abs/2508.01861 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2508.07842
replaced with revised version Mon, 22 Sep 2025 12:52:57 GMT   (6219kb)

Title: DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of
  Disentangled Experts
Authors: Yutong Shen, Hangxu Liu, Lei Zhang, Penghui Liu, Ruizhe Xia, Tianyi
  Yao, Tongtong Feng
Categories: cs.RO cs.AI
Comments: 14 pages,8 figures. Submitted to ICRA'26
\\ ( https://arxiv.org/abs/2508.07842 ,  6219kb)
------------------------------------------------------------------------------
\\
arXiv:2508.08019
replaced with revised version Mon, 22 Sep 2025 15:48:43 GMT   (2879kb)

Title: Advancing Knowledge Tracing by Exploring Follow-up Performance Trends
Authors: Hengyu Liu, Yushuai Li, Minghe Yu, Tiancheng Zhang, Ge Yu, Torben Bach
  Pedersen, Kristian Torp, Christian S. Jensen, and Tianyi Li
Categories: cs.CY cs.AI cs.LG
Comments: 14 pages, 5 figures
\\ ( https://arxiv.org/abs/2508.08019 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2508.14140
replaced with revised version Sun, 21 Sep 2025 23:43:54 GMT   (1289kb)

Title: Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse
  and Efficient ANNs
Authors: Orestis Konstantaropoulos, Stelios Manolis Smirnakis, Maria
  Papadopouli
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2508.14140 ,  1289kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16313
replaced with revised version Sat, 20 Sep 2025 01:50:54 GMT   (11312kb)

Title: Retrieval Enhanced Feedback via In-context Neural Error-book
Authors: Jongyeop Hyun, Bumsoo Kim
Categories: cs.LG cs.AI cs.CL
Comments: Accepted at EMNLP 2025 main conference
\\ ( https://arxiv.org/abs/2508.16313 ,  11312kb)
------------------------------------------------------------------------------
\\
arXiv:2508.17343
replaced with revised version Mon, 22 Sep 2025 10:56:53 GMT   (538kb)

Title: Agentic AI for Software: thoughts from Software Engineering community
Authors: Abhik Roychoudhury
Categories: cs.SE cs.AI
Comments: 4 pages
ACM-class: D.2
\\ ( https://arxiv.org/abs/2508.17343 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2508.17850
replaced with revised version Mon, 22 Sep 2025 12:43:01 GMT   (2499kb)

Title: GEPO: Group Expectation Policy Optimization for Stable Heterogeneous
  Reinforcement Learning
Authors: Han Zhang, Ruibin Zheng, Zexuan Yi, Zhuo Zhang, Hanyang Peng, Hui
  Wang, Zike Yuan, Cai Ke, Shiwei Chen, Jiacheng Yang, Yangning Li, Xiang Li,
  Jiangyue Yan, Yaoqi Liu, Liwen Jing, Jiayin Qi, Ruifeng Xu, Binxing Fang, Yue
  Yu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2508.17850 ,  2499kb)
------------------------------------------------------------------------------
\\
arXiv:2508.19318
replaced with revised version Sun, 21 Sep 2025 02:50:29 GMT   (920kb)

Title: (DEMO) Deep Reinforcement Learning Based Resource Allocation in
  Distributed IoT Systems
Authors: Aohan Li and Miyu Tsuzuki
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2508.19318 ,  920kb)
------------------------------------------------------------------------------
\\
arXiv:2508.20840
replaced with revised version Sat, 20 Sep 2025 03:42:09 GMT   (11467kb)

Title: Learning Primitive Embodied World Models: Towards Scalable Robotic
  Learning
Authors: Qiao Sun, Liujia Yang, Wei Tang, Wei Huang, Kaixin Xu, Yongchao Chen,
  Mingyu Liu, Jiange Yang, Haoyi Zhu, Yating Wang, Tong He, Yilun Chen, Xili
  Dai, Nanyang Ye, Qinying Gu
Categories: cs.RO cs.AI cs.MM
\\ ( https://arxiv.org/abs/2508.20840 ,  11467kb)
------------------------------------------------------------------------------
\\
arXiv:2509.00268 (*cross-listing*)
replaced with revised version Fri, 19 Sep 2025 21:43:32 GMT   (26067kb)

Title: Revealing Hidden Precursors to Earthquakes via a Stress-Sensitive
  Transformation of Seismic Noise
Authors: Nader Shakibay Senobari
Categories: physics.geo-ph cs.AI eess.SP
Comments: 24 pages, 7 figures. Github code included
MSC-class: 86A15 (Seismology), 62M10 (Time series, stochastic processes)
ACM-class: I.5.4; I.2.6
\\ ( https://arxiv.org/abs/2509.00268 ,  26067kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01426 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 09:35:16 GMT   (3873kb)

Title: DCA: Graph-Guided Deep Embedding Clustering for Brain Atlases
Authors: Mo Wang, Kaining Peng, Jingsheng Tang, Hongkai Wen, Quanying Liu
Categories: q-bio.NC cs.AI cs.CV
Comments: Accepted as a poster at NeurIPS 2025 with scores 5554
\\ ( https://arxiv.org/abs/2509.01426 ,  3873kb)
------------------------------------------------------------------------------
\\
arXiv:2509.05983
replaced with revised version Sat, 20 Sep 2025 14:15:55 GMT   (729kb)

Title: TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching
  Vietnamese-English Speech Recognition
Authors: Minh N. H. Nguyen, Anh Nguyen Tran, Dung Truong Dinh, Nam Van Vo
Categories: cs.SD cs.AI cs.CL eess.AS
Comments: Update new version
\\ ( https://arxiv.org/abs/2509.05983 ,  729kb)
------------------------------------------------------------------------------
\\
arXiv:2509.06287 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 03:49:26 GMT   (230kb)

Title: Statistical Inference for Misspecified Contextual Bandits
Authors: Yongyi Guo and Ziping Xu
Categories: math.ST cs.AI stat.TH
\\ ( https://arxiv.org/abs/2509.06287 ,  230kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09716
replaced with revised version Mon, 22 Sep 2025 02:40:04 GMT   (324kb)

Title: VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions
Authors: Jun Zhan, Mingyang Han, Yuxuan Xie, Chen Wang, Dong Zhang, Kexin
  Huang, Haoxiang Shi, DongXiao Wang, Tengtao Song, Qinyuan Cheng, Shimin Li,
  Jun Song, Xipeng Qiu, Bo Zheng
Categories: cs.SD cs.AI cs.CL eess.AS
\\ ( https://arxiv.org/abs/2509.09716 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09744
replaced with revised version Mon, 22 Sep 2025 01:27:46 GMT   (4311kb)

Title: Structure Matters: Brain Graph Augmentation via Learnable Edge Masking
  for Data-efficient Psychiatric Diagnosis
Authors: Mujie Liu, Chenze Wang, Liping Chen, Nguyen Linh Dan Le, Niharika
  Tewari, Ting Dang, Jiangang Ma, and Feng Xia
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2509.09744 ,  4311kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10572
replaced with revised version Sun, 21 Sep 2025 02:54:05 GMT   (1702kb)

Title: Quality Assessment of Tabular Data using Large Language Models and Code
  Generation
Authors: Ashlesha Akella, Akshar Kaul, Krishnasuri Narayanam, Sameep Mehta
Categories: cs.SE cs.AI cs.DB
Comments: under review
\\ ( https://arxiv.org/abs/2509.10572 ,  1702kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10858
replaced with revised version Fri, 19 Sep 2025 18:26:21 GMT   (338kb)

Title: Large Language Models for Security Operations Centers: A Comprehensive
  Survey
Authors: Ali Habibzadeh, Farid Feyzi, and Reza Ebrahimi Atani
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2509.10858 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13400
replaced with revised version Mon, 22 Sep 2025 08:28:21 GMT   (100kb)

Title: Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer
  Reviews
Authors: Sai Suresh Marchala Vasu, Ivaxi Sheth, Hui-Po Wang, Ruta Binkyte,
  Mario Fritz
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2509.13400 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14057 (*cross-listing*)
replaced with revised version Mon, 22 Sep 2025 13:37:28 GMT   (22630kb)

Title: Machines are more productive than humans until they aren't, and vice
  versa
Authors: Riccardo Zanardelli
Categories: econ.GN cs.AI q-fin.EC
Comments: Results enriched by experiment focusing on machine skill achieving
  high performance across all task difficulties; results of the primary
  experiment unchanged; data analysis section expanded; conclusions enriched
  and re-organized; abstract perfected; example in section A.4.1 enhanced;
  corrections to Table 17 (now Table 21); minor typos corrected
\\ ( https://arxiv.org/abs/2509.14057 ,  22630kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14181
replaced with revised version Sun, 21 Sep 2025 17:18:35 GMT   (8994kb)

Title: Bridging Past and Future: Distribution-Aware Alignment for Time Series
  Forecasting
Authors: Yifan Hu, Jie Yang, Tian Zhou, Peiyuan Liu, Yujin Tang, Rong Jin,
  Liang Sun
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2509.14181 ,  8994kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14223
replaced with revised version Mon, 22 Sep 2025 16:05:05 GMT   (1375kb)

Title: Fresh in memory: Training-order recency is linearly encoded in language
  model activations
Authors: Dmitrii Krasheninnikov, Richard E. Turner, David Krueger
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2509.14223 ,  1375kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14353
replaced with revised version Sun, 21 Sep 2025 00:29:04 GMT   (1686kb)

Title: DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene
  Interaction via Guided Diffusion
Authors: Dvij Kalaria, Sudarshan S Harithas, Pushkal Katara, Sangkyung Kwak,
  Sarthak Bhagat, Shankar Sastry, Srinath Sridhar, Sai Vemprala, Ashish Kapoor,
  Jonathan Chung-Kuan Huang
Categories: cs.RO cs.AI cs.LG
Comments: https://genrobo.github.io/DreamControl/ (under submission)
\\ ( https://arxiv.org/abs/2509.14353 ,  1686kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14803
replaced with revised version Sat, 20 Sep 2025 06:11:23 GMT   (615kb)

Title: OnlineMate: An LLM-Based Multi-Agent Companion System for Cognitive
  Support in Online Learning
Authors: Xian Gao, Zongyun Zhang, Ting Liu, Yuzhuo Fu
Categories: cs.CY cs.AI
Comments: work in progress
\\ ( https://arxiv.org/abs/2509.14803 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15095 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 12:01:56 GMT   (3993kb)

Title: Listening, Imagining & Refining: A Heuristic Optimized ASR Correction
  Framework with LLMs
Authors: Yutong Liu, Ziyue Zhang, Cheng Huang, Yongbin Yu, Xiangxiang Wang,
  Yuqing Cai, Nyima Tashi
Categories: eess.AS cs.AI
\\ ( https://arxiv.org/abs/2509.15095 ,  3993kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15151
replaced with revised version Sat, 20 Sep 2025 08:36:11 GMT   (13059kb)

Title: Exploring How Audio Effects Alter Emotion with Foundation Models
Authors: Stelios Katsis, Vassilis Lyberatos, Spyridon Kantarelis, Edmund
  Dervakos, and Giorgos Stamou
Categories: cs.SD cs.AI
Comments: https://github.com/stelioskt/audioFX
\\ ( https://arxiv.org/abs/2509.15151 ,  13059kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15666
replaced with revised version Mon, 22 Sep 2025 03:13:19 GMT   (235kb)

Title: TISDiSS: A Training-Time and Inference-Time Scalable Framework for
  Discriminative Source Separation
Authors: Yongsheng Feng, Yuetonghui Xu, Jiehui Luo, Hongjia Liu, Xiaobing Li,
  Feng Yu, Wei Li
Categories: cs.SD cs.AI eess.AS
Comments: Submitted to ICASSP 2026.(C) 2025 IEEE. Personal use of this material
  is permitted. Permission from IEEE must be obtained for all other uses, in
  any current or future media, including reprinting/republishing this material
  for advertising or promotional purposes, creating new collective works, for
  resale or redistribution to servers or lists, or reuse of any copyrighted
  component of this work
\\ ( https://arxiv.org/abs/2509.15666 ,  235kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15952
replaced with revised version Mon, 22 Sep 2025 13:01:44 GMT   (3345kb)

Title: Compose Yourself: Average-Velocity Flow Matching for One-Step Speech
  Enhancement
Authors: Gang Yang, Yue Lei, Wenxin Tai, Jin Wu, Jia Chen, Ting Zhong, Fan Zhou
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: 5 pages, 2 figures, submitted to ICASSP 2026
\\ ( https://arxiv.org/abs/2509.15952 ,  3345kb)
------------------------------------------------------------------------------
\\
arXiv:2501.08406
replaced with revised version Sun, 21 Sep 2025 17:31:05 GMT   (1349kb)

Title: OptiChat: Bridging Optimization Models and Practitioners with Large
  Language Models
Authors: Hao Chen, Gonzalo Esteban Constante-Flores, Krishna Sri Ipsit Mantri,
  Sai Madhukiran Kompalli, Akshdeep Singh Ahluwalia, Can Li
Categories: cs.HC cs.CL cs.LG math.OC
\\ ( https://arxiv.org/abs/2501.08406 ,  1349kb)
------------------------------------------------------------------------------
\\
arXiv:2502.20383
replaced with revised version Sun, 21 Sep 2025 22:29:53 GMT   (2868kb)

Title: Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security
  Analysis
Authors: Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang,
  Yizheng Chen
Categories: cs.LG cs.CL
Comments: Project website: http://vulnerable-ai-agents.github.io
\\ ( https://arxiv.org/abs/2502.20383 ,  2868kb)
------------------------------------------------------------------------------
\\
arXiv:2503.16718
replaced with revised version Sun, 21 Sep 2025 13:40:13 GMT   (203kb)

Title: CAARMA: Class Augmentation with Adversarial Mixup Regularization
Authors: Massa Baali, Xiang Li, Hao Chen, Syed Abdul Hannan, Rita Singh,
  Bhiksha Raj
Categories: cs.SD cs.CL cs.LG
Comments: Accepted to EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2503.16718 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2504.05652
replaced with revised version Sat, 20 Sep 2025 11:34:22 GMT   (1112kb)

Title: Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking
Authors: Yu-Hang Wu and Yu-Jie Xiong and Hao Zhang and Jia-Chen Zhang and Zheng
  Zhou
Categories: cs.CR cs.CL
Comments: Accepted by EMNLP2025
\\ ( https://arxiv.org/abs/2504.05652 ,  1112kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12594
replaced with revised version Sun, 21 Sep 2025 13:51:09 GMT   (4118kb)

Title: The Better You Learn, The Smarter You Prune: Towards Efficient
  Vision-language-action Models via Differentiable Token Pruning
Authors: Titong Jiang, Xuefeng Jiang, Yuan Ma, Xin Wen, Bailin Li, Kun Zhan,
  Peng Jia, Yahui Liu, Sheng Sun, Xianpeng Lang
Categories: cs.RO cs.CL cs.CV
Comments: Under review. Project site:
  https://liauto-research.github.io/LightVLA
\\ ( https://arxiv.org/abs/2509.12594 ,  4118kb)
------------------------------------------------------------------------------
\\
arXiv:2509.13625
replaced with revised version Mon, 22 Sep 2025 00:06:49 GMT   (227kb)

Title: Privacy-Aware In-Context Learning for Large Language Models
Authors: Bishnu Bhusal, Manoj Acharya, Ramneet Kaur, Colin Samplawski, Anirban
  Roy, Adam D. Cobb, Rohit Chadha, Susmit Jha
Categories: cs.LG cs.CL cs.CR
\\ ( https://arxiv.org/abs/2509.13625 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14946 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 16:16:54 GMT   (158kb)

Title: SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech
  Generation and Understanding
Authors: Bingsong Bai, Qihang Lu, Wenbing Yang, Zihan Sun, Yueran Hou, Peilei
  Jia, Songbai Pu, Ruibo Fu, Yingming Gao, Ya Li, Jun Gao
Categories: eess.AS cs.CL
Comments: Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this
  material is permitted. Permission from IEEE must be obtained for all other
  uses, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2509.14946 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01421
replaced with revised version Mon, 22 Sep 2025 05:32:33 GMT   (21kb)

Title: Convergence analysis of equilibrium methods for inverse problems
Authors: Daniel Obmann, Gyeongha Hwang and Markus Haltmeier
Categories: math.NA cs.CV cs.NA
\\ ( https://arxiv.org/abs/2306.01421 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2405.01600 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 07:07:14 GMT   (17079kb)

Title: Block-Fused Attention-Driven Adaptively-Pooled ResNet Model for Improved
  Cervical Cancer Classification
Authors: Saurabh Saini, Kapil Ahuja, and Akshat S. Chauhan
Categories: eess.IV cs.CV cs.LG
Comments: 32 Pages, 12 Tables, 14 Figures
ACM-class: I.2.1; I.5.2
\\ ( https://arxiv.org/abs/2405.01600 ,  17079kb)
------------------------------------------------------------------------------
\\
arXiv:2406.09384
replaced with revised version Sun, 21 Sep 2025 19:32:38 GMT   (218kb)

Title: Reflecting on the State of Rehearsal-free Continual Learning with
  Pretrained Models
Authors: Lukas Thede, Karsten Roth, Olivier J. H\'enaff, Matthias Bethge,
  Zeynep Akata
Categories: cs.LG cs.CV
Comments: 3rd Conference on Lifelong Learning Agents (CoLLAs) 2024
\\ ( https://arxiv.org/abs/2406.09384 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2409.03555
replaced with revised version Sun, 21 Sep 2025 09:38:19 GMT   (308kb)

Title: Unified Framework for Pre-trained Neural Network Compression via
  Decomposition and Optimized Rank Selection
Authors: Ali Aghababaei-Harandi, Massih-Reza Amini
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2409.03555 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2409.14204 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 00:57:50 GMT   (2420kb)

Title: A Unified Deep Learning Framework for Motion Correction in Medical
  Imaging
Authors: Jian Wang, Razieh Faghihpirayesh, Danny Joca, Polina Golland, Ali
  Gholipour
Categories: eess.IV cs.CV
Comments: 10 pages, 6 figures
\\ ( https://arxiv.org/abs/2409.14204 ,  2420kb)
------------------------------------------------------------------------------
\\
arXiv:2409.15493
replaced with revised version Fri, 19 Sep 2025 21:22:29 GMT   (3504kb)

Title: A Modular Robotic System for Autonomous Exploration and Semantic
  Updating in Large-Scale Indoor Environments
Authors: Sai Haneesh Allu, Itay Kadosh, Tyler Summers, Yu Xiang
Categories: cs.RO cs.CV
Comments: 10 pages, 9 figures, 5 tables. Project page is available at
  https://irvlutd.github.io/SemanticMapping/
\\ ( https://arxiv.org/abs/2409.15493 ,  3504kb)
------------------------------------------------------------------------------
\\
arXiv:2410.10328 (*cross-listing*)
replaced with revised version Mon, 22 Sep 2025 16:59:25 GMT   (12884kb)

Title: Anatomical feature-prioritized loss for enhanced MR to CT translation
Authors: Arthur Longuefosse, Baudouin Denis de Senneville, Gael Dournes, Ilyes
  Benlala, Pascal Desbarats, Fabien Baldacci
Categories: eess.IV cs.CV
Journal-ref: 2025 Phys. Med. Biol. 70 145012
DOI: 10.1088/1361-6560/adea07
\\ ( https://arxiv.org/abs/2410.10328 ,  12884kb)
------------------------------------------------------------------------------
\\
arXiv:2503.06743 (*cross-listing*)
replaced with revised version Mon, 22 Sep 2025 09:12:09 GMT   (4505kb)

Title: X-GAN: A Generative AI-Powered Unsupervised Model for Main Vessel
  Segmentation of Glaucoma Screening
Authors: Cheng Huang and Weizheng Xie and Tsengdar J. Lee and Jui-Kai Wang and
  Karanjit Kooner and Ning Zhang and Jia Zhang
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2503.06743 ,  4505kb)
------------------------------------------------------------------------------
\\
arXiv:2504.02045
replaced with revised version Mon, 22 Sep 2025 17:49:50 GMT   (28991kb)

Title: Generating 360{\deg} Video is What You Need For a 3D Scene
Authors: Zhaoyang Zhang, Yannick Hold-Geoffroy, Milo\v{s} Ha\v{s}an, Ziwen
  Chen, Fujun Luan, Julie Dorsey and Yiwei Hu
Categories: cs.GR cs.CV
Journal-ref: SIGGRAPH Asia 2025
\\ ( https://arxiv.org/abs/2504.02045 ,  28991kb)
------------------------------------------------------------------------------
\\
arXiv:2504.05803
replaced with revised version Sat, 20 Sep 2025 08:39:13 GMT   (3511kb)

Title: Revisiting Speech-Lip Alignment: A Phoneme-Aware Speech Encoder for
  Robust Talking Head Synthesis
Authors: Yihuan Huang, Jiajun Liu, Yanzhen Ren, Wuyang Liu, Zongkun Sun
Categories: cs.GR cs.CV
\\ ( https://arxiv.org/abs/2504.05803 ,  3511kb)
------------------------------------------------------------------------------
\\
arXiv:2505.08787
replaced with revised version Sat, 20 Sep 2025 22:27:04 GMT   (12109kb)

Title: UniSkill: Imitating Human Videos via Cross-Embodiment Skill
  Representations
Authors: Hanjung Kim, Jaehyun Kang, Hyolim Kang, Meedeum Cho, Seon Joo Kim,
  Youngwoon Lee
Categories: cs.RO cs.CV
Comments: CoRL 2025. Project Page: https://kimhanjung.github.io/UniSkill/
\\ ( https://arxiv.org/abs/2505.08787 ,  12109kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13289
replaced with revised version Mon, 22 Sep 2025 12:19:35 GMT   (20931kb)

Title: RECON: Robust symmetry discovery via Explicit Canonical Orientation
  Normalization
Authors: Alonso Urbano, David W. Romero, Max Zimmer, Sebastian Pokutta
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2505.13289 ,  20931kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05411
replaced with revised version Sun, 21 Sep 2025 04:49:25 GMT   (0kb,I)

Title: QA-HFL: Quality-Aware Hierarchical Federated Learning for
  Resource-Constrained Mobile Devices with Heterogeneous Image Quality
Authors: Sajid Hussain, Muhammad Sohail, Nauman Ali Khan
Categories: cs.CR cs.CV
Comments: Due to some technical issues
\\ ( https://arxiv.org/abs/2506.05411 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2506.16572 (*cross-listing*)
replaced with revised version Mon, 22 Sep 2025 12:02:52 GMT   (4445kb)

Title: Single-step Diffusion for Image Compression at Ultra-Low Bitrates
Authors: Chanung Park, Joo Chan Lee, Jong Hwan Ko
Categories: eess.IV cs.CV
\\ ( https://arxiv.org/abs/2506.16572 ,  4445kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21535 (*cross-listing*)
replaced with revised version Sun, 21 Sep 2025 13:26:01 GMT   (232kb)

Title: Exploring the Design Space of 3D MLLMs for CT Report Generation
Authors: Mohammed Baharoon, Jun Ma, Congyu Fang, Augustin Toma, Bo Wang
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2506.21535 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2507.00416
replaced with revised version Sat, 20 Sep 2025 12:15:29 GMT   (4483kb)

Title: Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding
Authors: Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, Yuxin Du, Jiting Liu,
  Encheng Gu, Bo Zhao
Categories: cs.RO cs.CV
\\ ( https://arxiv.org/abs/2507.00416 ,  4483kb)
------------------------------------------------------------------------------
\\
arXiv:2507.02864
replaced with revised version Mon, 22 Sep 2025 01:21:21 GMT   (2268kb)

Title: The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies
  with Generative Audio
Authors: Renhao Wang, Haoran Geng, Tingle Li, Feishi Wang, Gopala
  Anumanchipalli, Trevor Darrell, Boyi Li, Pieter Abbeel, Jitendra Malik,
  Alexei A. Efros
Categories: cs.RO cs.CV
Comments: Conference on Robot Learning 2025
\\ ( https://arxiv.org/abs/2507.02864 ,  2268kb)
------------------------------------------------------------------------------
\\
arXiv:2507.22832
replaced with revised version Sun, 21 Sep 2025 12:46:56 GMT   (6226kb)

Title: Pulling Back the Curtain on ReLU Networks
Authors: Maciej Satkiewicz
Categories: cs.LG cs.CV cs.NE
Comments: 12 pages, 3-page appendix, 4 figures, under review; v4 changes:
  wording improvements, clarification of arguments and of the Hypothesis 1
ACM-class: I.2.6; I.4.10
\\ ( https://arxiv.org/abs/2507.22832 ,  6226kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15124
replaced with revised version Sat, 20 Sep 2025 02:59:06 GMT   (9089kb)

Title: Side Effects of Erasing Concepts from Diffusion Models
Authors: Shaswati Saha, Sourajit Saha, Manas Gaur, Tejas Gokhale
Categories: cs.LG cs.CV
Comments: Findings of the Association for Computational Linguistics: EMNLP 2025
\\ ( https://arxiv.org/abs/2508.15124 ,  9089kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16024
replaced with revised version Sat, 20 Sep 2025 13:56:49 GMT   (33052kb)

Title: Wavelet-Space Representations for Neural Super-Resolution in Rendering
  Pipelines
Authors: Prateek Poudel, Prashant Aryal, Kirtan Kunwar, Navin Nepal and Dinesh
  Baniya Kshatri
Categories: cs.GR cs.CV
\\ ( https://arxiv.org/abs/2508.16024 ,  33052kb)
------------------------------------------------------------------------------
\\
arXiv:2509.02957 (*cross-listing*)
replaced with revised version Sat, 20 Sep 2025 08:43:08 GMT   (835kb)

Title: Ensemble YOLO Framework for Multi-Domain Mitotic Figure Detection in
  Histopathology Images
Authors: Navya Sri Kelam, Akash Parekh, Saikiran Bonthu and Nitin Singhal
Categories: eess.IV cs.CV
Comments: 4 pages, MIDOG25 Challenge
MSC-class: 68T07
ACM-class: I.4.9; I.5.4
\\ ( https://arxiv.org/abs/2509.02957 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2509.08947
replaced with revised version Sun, 21 Sep 2025 21:34:01 GMT   (3956kb)

Title: CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via
  Camera and Visual Difference Prediction
Authors: Yancheng Cai, Robert Wanat, Rafal Mantiuk
Categories: cs.GR cs.CV
Comments: Accepted by SIGGRAPH Asia 2025
\\ ( https://arxiv.org/abs/2509.08947 ,  3956kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09926
replaced with revised version Sat, 20 Sep 2025 15:36:17 GMT   (6725kb)

Title: LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised
  Learning in Open-World Scenarios
Authors: Zhiyuan Huang, Jiahao Chen, Yurou Liu, Bing Su
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2509.09926 ,  6725kb)
------------------------------------------------------------------------------
\\
arXiv:2509.11003
replaced with revised version Mon, 22 Sep 2025 12:25:56 GMT   (40616kb)

Title: AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting
Authors: Gurutva Patle, Nilay Girgaonkar, Nagabhushan Somraj, Rajiv
  Soundararajan
Categories: cs.GR cs.CV
Comments: SIGGRAPH Asia 2025
DOI: 10.1145/3757377.3763993
\\ ( https://arxiv.org/abs/2509.11003 ,  40616kb)
------------------------------------------------------------------------------
\\
arXiv:2509.15059
replaced with revised version Fri, 19 Sep 2025 20:39:05 GMT   (21244kb)

Title: QuizRank: Picking Images by Quizzing VLMs
Authors: Tenghao Ji, Eytan Adar
Categories: cs.HC cs.CV
\\ ( https://arxiv.org/abs/2509.15059 ,  21244kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16131
replaced with revised version Mon, 22 Sep 2025 07:47:52 GMT   (29158kb)

Title: Dynamic Classifier-Free Diffusion Guidance via Online Feedback
Authors: Pinelopi Papalampidi, Olivia Wiles, Ira Ktena, Aleksandar Shtedritski,
  Emanuele Bugliarello, Ivana Kajic, Isabela Albuquerque, Aida Nematzadeh
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2509.16131 ,  29158kb)
------------------------------------------------------------------------------
\\
arXiv:2409.03897
replaced with revised version Sat, 20 Sep 2025 01:01:49 GMT   (1277kb)

Title: On the Convergence Rates of Federated Q-Learning across Heterogeneous
  Environments
Authors: Muxing Wang, Pengkun Yang, Lili Su
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2409.03897 ,  1277kb)
------------------------------------------------------------------------------
\\
arXiv:2409.15520
replaced with revised version Sat, 20 Sep 2025 21:59:16 GMT   (80kb)

Title: MobiZO: Enabling Efficient LLM Fine-Tuning at the Edge via Inference
  Engines
Authors: Lei Gao, Amir Ziashahabi, Yue Niu, Salman Avestimehr, Murali Annavaram
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2409.15520 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2412.07435
replaced with revised version Mon, 22 Sep 2025 14:12:23 GMT   (149kb)

Title: Parallel Simulation for Log-concave Sampling and Score-based Diffusion
  Models
Authors: Huanjian Zhou and Masashi Sugiyama
Categories: cs.DS cs.DC cs.LG cs.NA math.NA
Comments: Accepted to ICML2025 and this version corrects errors from the
  previous submission
\\ ( https://arxiv.org/abs/2412.07435 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09303
replaced with revised version Sun, 21 Sep 2025 13:01:36 GMT   (398kb)

Title: Towards Seamless Hierarchical Federated Learning under Intermittent
  Client Participation: A Stagewise Decision-Making Methodology
Authors: Minghong Wu, Minghui Liwang, Yuhan Su, Li Li, Seyyedali
  Hosseinalipour, Xianbin Wang, Huaiyu Dai, Zhenzhen Jiao
Categories: cs.LG cs.DC
Comments: 23 pages, 10 figures,9 tables
\\ ( https://arxiv.org/abs/2502.09303 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2504.03173
replaced with revised version Mon, 22 Sep 2025 16:18:42 GMT   (499kb)

Title: PPFPL: Cross-silo Privacy-preserving Federated Prototype Learning
  Against Data Poisoning Attacks
Authors: Hongliang Zhang, Jiguo Yu, Fenghua Xu, Chunqiang Hu, Yongzhao Zhang,
  Xiaofen Wang, Zhongyuan Yu, Xiaosong Zhang
Categories: cs.CR cs.DC
\\ ( https://arxiv.org/abs/2504.03173 ,  499kb)
------------------------------------------------------------------------------
\\
arXiv:2504.11651
replaced with revised version Fri, 19 Sep 2025 23:02:54 GMT   (3707kb)

Title: 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU
  Inference via Dynamic-Length Float
Authors: Tianyi Zhang, Mohsen Hariri, Shaochen Zhong, Vipin Chaudhary, Yang
  Sui, Xia Hu, Anshumali Shrivastava
Categories: cs.LG cs.DC
Comments: Accepted in NeurIPS 2025
\\ ( https://arxiv.org/abs/2504.11651 ,  3707kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17236
replaced with revised version Fri, 19 Sep 2025 19:10:30 GMT   (989kb)

Title: LogStamping: A blockchain-based log auditing approach for large-scale
  systems
Authors: Md Shariful Islam and M. Sohel Rahman
Categories: cs.CR cs.DC
Comments: 7 Figures, 2 tables
\\ ( https://arxiv.org/abs/2505.17236 ,  989kb)
------------------------------------------------------------------------------
\\
arXiv:2505.07501
replaced with revised version Mon, 22 Sep 2025 16:51:29 GMT   (39kb,D)

Title: The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games
Authors: Purandar Bhaduri (IIT Guwahati)
Categories: cs.GT cs.CC cs.FL cs.LO cs.MA
Comments: In Proceedings GandALF 2025, arXiv:2509.13258
Journal-ref: EPTCS 428, 2025, pp. 62-75
DOI: 10.4204/EPTCS.428.6
\\ ( https://arxiv.org/abs/2505.07501 ,  39kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
