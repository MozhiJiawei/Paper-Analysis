# AIæ¨ç†åŠ é€ŸæŠ€æœ¯è®ºæ–‡åˆ†ææŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: 2025-10-16 10:21:43
åˆ†æè®ºæ–‡æ•°é‡: 28ç¯‡

## è®ºæ–‡æŠ€æœ¯ç®€æŠ¥

### 1. An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution

ç›¸å…³æœºæ„å‘å¸ƒäº†An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolutionè®ºæ–‡ï¼Œä½¿ç”¨æ¨¡æ¿æ„ŸçŸ¥åŠ¨æ€å·ç§¯æŠ€æœ¯ï¼Œè§£å†³äº†æ·±åº¦æ¨¡æ¿åŒ¹é…ä¸å¹³é¢å†…ä½å§¿ä¼°è®¡é—®é¢˜ï¼Œè¾¾æˆäº†é«˜æ•ˆçš„æ€§èƒ½æå‡ã€‚

### 2. BIOX-BRIDGE: MODEL BRIDGING FOR UNSUPERVISED CROSS-MODAL KNOWLEDGE TRANSFER ACROSS BIOSIGNALS

ç‰›æ´¥å¤§å­¦å‘å¸ƒäº†BIOX-BRIDGEè®ºæ–‡ï¼Œä½¿ç”¨è½»é‡çº§æ¡¥æ¥ç½‘ç»œï¼ˆå«å¯¹é½ä½ç½®é€‰æ‹©ç­–ç•¥å’ŒåŸå‹ç½‘ç»œï¼‰ï¼Œè§£å†³äº†æ— ç›‘ç£è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»ä¸­çŸ¥è¯†è’¸é¦å¯¼è‡´çš„è®¡ç®—å†…å­˜å¼€é”€å¤§é—®é¢˜ï¼Œè¾¾æˆå‡å°‘88-99%å¯è®­ç»ƒå‚æ•°ä¸”ä¿æŒæˆ–æå‡è¿ç§»æ€§èƒ½çš„æ•ˆæœ

### 3. Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency

Harvard Universityå‘å¸ƒäº†Budgeted Broadcastè®ºæ–‡ï¼Œä½¿ç”¨åŸºäºæœ¬åœ°æµé‡é¢„ç®—ï¼ˆé•¿æœŸæ¿€æ´»ç‡ä¸æ‰‡å‡ºä¹˜ç§¯ï¼‰çš„æ´»æ€§ä¾èµ–å‰ªæè§„åˆ™ï¼Œè§£å†³äº†ä¼ ç»Ÿå‰ªææ–¹æ³•ä»…æŒ‰æŸå¤±å½±å“ç§»é™¤å‚æ•°çš„å±€é™ï¼Œåœ¨ASRçš„Transformersã€äººè„¸è¯†åˆ«çš„ResNetsç­‰å¤šç§æ¨¡å‹ä¸Šç›¸åŒç¨€ç–åº¦ä¸‹æé«˜ç¼–ç ç†µã€å»ç›¸å…³æ€§å’Œå‡†ç¡®ç‡ï¼ˆæœ‰æ—¶è¶…å¯†é›†åŸºçº¿ï¼‰ï¼Œå¹¶åœ¨ç”µå­æ˜¾å¾®é•œå›¾åƒä¸Šè¾¾æˆSOTA F1å’ŒPR-AUCã€‚

### 4. ClustViT: Clustering-based Token Merging for Semantic Segmentation

å‘å¸ƒäº†ClustViTè®ºæ–‡ï¼Œä½¿ç”¨å«å¯è®­ç»ƒClusteræ¨¡å—ï¼ˆåŸºäºåˆ†å‰²æ©ç ä¼ªç°‡å¼•å¯¼åˆå¹¶ç›¸ä¼¼tokenï¼‰å’ŒRegeneratoræ¨¡å—ï¼ˆæ¢å¤ç»†èŠ‚ï¼‰çš„æ¶æ„ï¼Œè§£å†³äº†Vision Transformersåœ¨è¯­ä¹‰åˆ†å‰²ç­‰å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­å› äºŒæ¬¡æ³¨æ„åŠ›å¤æ‚åº¦å¯¼è‡´å®ç”¨æ€§å—é™çš„é—®é¢˜ï¼Œè¾¾æˆäº†å‡å°‘2.18å€GFLOPsã€æ¨ç†é€Ÿåº¦æå‡1.64å€ä¸”ä¿æŒç›¸å½“åˆ†å‰²ç²¾åº¦çš„æ•ˆæœã€‚

### 5. CONSTRAINED ADAPTIVE REJECTION SAMPLING

University of California San Diegoå‘å¸ƒäº†CONSTRAINED ADAPTIVE REJECTION SAMPLINGè®ºæ–‡ï¼Œä½¿ç”¨Constrained Adaptive Rejection Sampling (CARS)æŠ€æœ¯ï¼ˆé€šè¿‡trieè®°å½•å¹¶å‡å»è¿åçº¦æŸå»¶ç»­çš„æ¦‚ç‡è´¨é‡ä»¥è‡ªé€‚åº”æ’é™¤æ— æ•ˆå‰ç¼€ï¼‰ï¼Œè§£å†³äº†ç°æœ‰çº¦æŸç”Ÿæˆä¸­æœ‰æ•ˆæ€§ä¸åˆ†å¸ƒä¿çœŸåº¦ã€è®¡ç®—æ•ˆç‡çš„çŸ›ç›¾ï¼ˆè´ªå©ªæ–¹æ³•æ‰­æ›²åˆ†å¸ƒï¼Œæ‹’ç»é‡‡æ ·æ•ˆç‡ä½ï¼‰ï¼Œè¾¾æˆäº†æé«˜æ ·æœ¬æ•ˆç‡ï¼ˆå‡å°‘æ¯ä¸ªæœ‰æ•ˆæ ·æœ¬çš„LMå‰å‘ä¼ é€’æ¬¡æ•°ï¼‰ã€å¢å¼ºæ ·æœ¬å¤šæ ·æ€§ä¸”ä¸æ‰­æ›²åˆ†å¸ƒçš„æ•ˆæœã€‚

### 6. Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning

Max Planck Institute for Intelligent Systemsä¸Emory Universityå‘å¸ƒäº†Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoningè®ºæ–‡ï¼Œä½¿ç”¨ç³»ç»Ÿç ”ç©¶LLMå„å±‚åœ¨æ£€ç´¢ã€çŸ¥è¯†å’Œæ¨ç†ä¸­ä½œç”¨çš„æŠ€æœ¯ï¼Œè§£å†³äº†ç°æœ‰ç ”ç©¶å¯¹LLMæ·±å±‚ä½œç”¨çš„çª„åŒ–è¯„ä¼°é—®é¢˜ï¼Œè¾¾æˆäº†æ­ç¤ºLLMæ·±åº¦åˆ©ç”¨çš„å¼‚è´¨æ€§å’Œæƒ…å¢ƒä¾èµ–æ€§ï¼Œæ˜ç¡®æµ…å±‚è´Ÿè´£çŸ¥è¯†ä¸æ£€ç´¢ã€æ·±å±‚å¯¹æ¨ç†è‡³å…³é‡è¦ä¸”å¯é€šè¿‡è’¸é¦é‡å¡‘çš„æ•ˆæœã€‚

### 7. EXPLORING RESOLUTION-WISE SHARED ATTENTION IN HYBRID MAMBA-U-NETS FOR IMPROVED CROSS-CORPUS SPEECH ENHANCEMENT

Aalborg Universityå‘å¸ƒäº†ç›¸å…³è®ºæ–‡ï¼Œä½¿ç”¨RWSA-MambaUNetæ¨¡å‹ï¼ˆèåˆMambaã€å¤šå¤´æ³¨æ„åŠ›çš„æ··åˆU-Netç»“æ„ï¼Œé‡‡ç”¨åˆ†è¾¨ç‡çº§å…±äº«æ³¨æ„åŠ›ï¼‰ï¼Œè§£å†³äº†è·¨è¯­æ–™åº“è¯­éŸ³å¢å¼ºçš„æ³›åŒ–æ€§èƒ½é—®é¢˜ï¼Œè¾¾æˆäº†åœ¨ä¸¤ä¸ªåŸŸå¤–æµ‹è¯•é›†ä¸Šå®ç°æœ€å…ˆè¿›æ³›åŒ–æ€§èƒ½ï¼Œæœ€å°æ¨¡å‹è¶…è¶Šæ‰€æœ‰åŸºçº¿ä¸”å‚æ•°å’Œè®¡ç®—é‡æ˜¾è‘—é™ä½çš„æ•ˆæœ

### 8. FIDEDIFF: EFFICIENT DIFFUSION MODEL FOR HIGH-FIDELITY IMAGE MOTION DEBLURRING

Harvard Universityå’ŒShanghai Jiao Tong Universityå‘å¸ƒäº†FideDiffè®ºæ–‡ï¼Œä½¿ç”¨å•æ­¥æ‰©æ•£æ¨¡å‹åŠä¸€è‡´æ€§æ¨¡å‹ï¼ˆé€šè¿‡å°†è¿åŠ¨å»æ¨¡ç³Šé‡æ„ä¸ºæ‰©æ•£è¿‡ç¨‹ã€åŒ¹é…æ¨¡ç³Šè½¨è¿¹çš„è®­ç»ƒæ•°æ®é‡å»ºã€é›†æˆKernel ControlNetå’Œè‡ªé€‚åº”æ—¶é—´æ­¥é¢„æµ‹å¢å¼ºï¼‰ï¼Œè§£å†³äº†æ‰©æ•£æ¨¡å‹æ¨ç†æ—¶é—´é•¿å’Œä¿çœŸåº¦ä½çš„é—®é¢˜ï¼Œè¾¾æˆäº†åœ¨å…¨å‚è€ƒæŒ‡æ ‡ä¸Šæ€§èƒ½è¶…è¶Šå…ˆå‰æ‰©æ•£æ–¹æ³•å¹¶åŒ¹é…å…¶ä»–æœ€å…ˆè¿›æ¨¡å‹çš„æ•ˆæœ

### 9. HISPEC: HIERARCHICAL SPECULATIVE DECODING FOR LLMS

The University of Texas at Austinå‘å¸ƒäº†HISPECè®ºæ–‡ï¼Œä½¿ç”¨åˆ©ç”¨æ—©é€€å‡ºæ¨¡å‹è¿›è¡Œä½å¼€é”€ä¸­é—´éªŒè¯å¹¶é‡ç”¨å…³é”®èµ„æºçš„åˆ†å±‚æ¨æµ‹è§£ç æ¡†æ¶ï¼Œè§£å†³äº†LLMæ¨æµ‹è§£ç ä¸­éªŒè¯ç“¶é¢ˆåŠç°æœ‰ä¸­é—´éªŒè¯æ–¹æ³•çš„è®­ç»ƒå¼€é”€å¤§ã€å†…å­˜å¢åŠ å’Œç²¾åº¦å¦¥åé—®é¢˜ï¼Œè¾¾æˆäº†å¹³å‡ååé‡æå‡1.28å€ã€æœ€é«˜2.01å€ä¸”ä¸å½±å“ç²¾åº¦çš„æ•ˆæœã€‚

### 10. JaneEye: A 12-nm 2K-FPS 18.9-ÂµJ/Frame Event-based Eye Tracking Accelerator

Leiden Universityå’ŒDelft University of Technologyå‘å¸ƒäº†JaneEyeè®ºæ–‡ï¼Œä½¿ç”¨å¸¦æ–°å‹ConvJANETå±‚çš„è¶…è½»é‡ç¥ç»ç½‘ç»œæ¶æ„åŠ12nm ASICå®ç°çš„åŸºäºäº‹ä»¶çš„çœ¼åŠ¨è¿½è¸ªç¡¬ä»¶åŠ é€Ÿå™¨æŠ€æœ¯ï¼Œè§£å†³äº†ä¼ ç»Ÿå¸§åŸºçœ¼åŠ¨è¿½è¸ªéš¾ä»¥æ»¡è¶³XRé«˜ç²¾åº¦ã€ä½å»¶è¿Ÿã€é«˜èƒ½æ•ˆéœ€æ±‚çš„é—®é¢˜ï¼Œè¾¾æˆ2000 FPSã€18.9 ÂµJ/frameèƒ½æ•ˆåŠ2.45åƒç´ è¯¯å·®çš„æ•ˆæœã€‚

### 11. LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction

å‘å¸ƒäº†LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstructionè®ºæ–‡ï¼Œä½¿ç”¨è½»é‡çº§æ½œåœ¨LiDARè‡ªåŠ¨ç¼–ç å™¨ï¼ˆLiLa-Netï¼‰æŠ€æœ¯ï¼Œè§£å†³äº†ä»…ç”¨LiDARç‚¹äº‘æ—¶é€šè¿‡å‡å°‘ç¼–ç å™¨å±‚å’Œç®€åŒ–è·³è·ƒè¿æ¥é™ä½èµ„æºæ¶ˆè€—åŒæ—¶å®ç°å‡†ç¡®3Dç‚¹äº‘é‡å»ºçš„é—®é¢˜ï¼Œè¾¾æˆäº†æå‡é‡å»ºè´¨é‡ä¸”ä¸å½±å“æ€§èƒ½ã€å…·å¤‡å¼ºæ³›åŒ–èƒ½åŠ›èƒ½é‡å»ºæ— å…³ç‰©ä½“çš„æ•ˆæœã€‚

### 12. Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression

åç››é¡¿å¤§å­¦å‘å¸ƒäº†Local Linear Attentionè®ºæ–‡ï¼Œä½¿ç”¨LLAæ³¨æ„åŠ›æœºåˆ¶åŠFlashLLAç®—æ³•ï¼Œè§£å†³äº†çº¿æ€§å’ŒSoftmaxæ³¨æ„åŠ›åœ¨æµ‹è¯•æ—¶å›å½’ä¸­çš„è®¡ç®—å¤æ‚åº¦ä¸è¡¨è¾¾èƒ½åŠ›é—®é¢˜ï¼Œåœ¨æµ‹è¯•æ—¶è®­ç»ƒå’Œä¸Šä¸‹æ–‡å†…å­¦ä¹ ä¸­ä¼˜äºå¼ºåŸºçº¿å¹¶å±•ç°å¯æ‰©å±•æ€§ã€‚

### 13. Mamba Outpaces Reformer in Stock Prediction with Sentiments from Top Ten LLMs

ç¾å›½åŒ—å¾·å…‹è¨æ–¯å¤§å­¦å‘å¸ƒäº†Mambaåœ¨è‚¡ç¥¨é¢„æµ‹ä¸­è¶…è¶ŠReformerçš„è®ºæ–‡ï¼Œä½¿ç”¨ç»“åˆåå¤§LLMè¯­ä¹‰æƒ…æ„Ÿåˆ†æ•°ä¸åˆ†é’Ÿçº§è‚¡ç¥¨æ•°æ®çš„æ–°æ¡†æ¶åŠMambaæ¨¡å‹ï¼Œè§£å†³äº†è‚¡ç¥¨å¸‚åœºçŸ­æœŸé¢„æµ‹éš¾ï¼ˆé«˜æ³¢åŠ¨æ€§ã€æ–°é—»å½±å“ã€éçº¿æ€§æ—¶é—´åºåˆ—ï¼‰çš„é—®é¢˜ï¼Œè¾¾æˆäº†Mambaç›¸æ¯”Reformeré€Ÿåº¦æ›´å¿«ä¸”é¢„æµ‹æ›´å‡†ç¡®ï¼ˆLLaMA 3.3-70Bæ—¶MSEæœ€ä½0.137ï¼‰çš„æ•ˆæœ

### 14. Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving

é¦™æ¸¯åŸå¸‚å¤§å­¦å‘å¸ƒäº†Nav-EEè®ºæ–‡ï¼Œä½¿ç”¨å¯¼èˆªå¼•å¯¼çš„æ—©æœŸé€€å‡ºæ¡†æ¶ï¼ˆç¦»çº¿é¢„è®¡ç®—ä»»åŠ¡ç‰¹å®šé€€å‡ºå±‚å¹¶åœ¨çº¿åŸºäºå¯¼èˆªå…ˆéªŒåŠ¨æ€åº”ç”¨ï¼‰ï¼Œè§£å†³äº†è‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¨ç†å»¶è¿Ÿé«˜ä¸”æ—©æœŸé€€å‡ºæ³›åŒ–å—é™çš„é—®é¢˜ï¼Œè¾¾æˆäº†å‡†ç¡®ç‡æ¥è¿‘å…¨æ¨ç†çš„åŒæ—¶å°†å»¶è¿Ÿå‡å°‘é«˜è¾¾63.9%ï¼ˆå®è½¦é›†æˆä¸­å»¶è¿Ÿä»600msé™è‡³300msï¼‰çš„æ•ˆæœã€‚

### 15. OPTIMAL STOPPING VS BEST-OF-N FOR INFERENCE TIME OPTIMIZATION

University of Michiganç­‰æœºæ„å‘å¸ƒäº†ã€ŠOPTIMAL STOPPING VS BEST-OF-N FOR INFERENCE TIME OPTIMIZATIONã€‹è®ºæ–‡ï¼Œä½¿ç”¨åŸºäºæ½˜å¤šæ‹‰é­”ç›’é—®é¢˜çš„UCBé£æ ¼ç®—æ³•åŠBradley-Terryå¯å‘çš„è‡ªé€‚åº”æ¨ç†æ—¶é—´ä¼˜åŒ–æ–¹æ³•ï¼Œè§£å†³äº†LLMå¤šä»£ç”Ÿæˆæ—¶è¾“å‡ºè´¨é‡ä¸æ¨ç†æˆæœ¬çš„å¹³è¡¡é—®é¢˜ï¼Œè¾¾æˆäº†ä¸Best-of-Né‡‡æ ·ç›¸åŒæ€§èƒ½ä¸”å¹³å‡å‡å°‘15-35%ç”Ÿæˆæ¬¡æ•°çš„æ•ˆæœã€‚

### 16. Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution

å—äº¬å¤§å­¦å‘å¸ƒäº†Pure-Passè®ºæ–‡ï¼Œä½¿ç”¨Pure-Passåƒç´ çº§æ©è”½æœºåˆ¶ï¼ˆé€šè¿‡å›ºå®šé¢œè‰²ä¸­å¿ƒç‚¹åˆ†ç±»åƒç´ å®ç°ç»†ç²’åº¦ã€ç©ºé—´çµæ´»ä¸”è‡ªé€‚åº”çš„æ©è”½ï¼‰ï¼Œè§£å†³äº†ç°æœ‰è½»é‡çº§å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•é€‚åº”æ€§å·®ã€ç²—ç²’åº¦æ©è”½åŠç©ºé—´ä¸çµæ´»çš„é—®é¢˜ï¼Œè¾¾æˆåœ¨èŠ‚çœç±»ä¼¼è®¡ç®—é‡æ—¶ï¼Œé›†æˆäºATD-lightæ¨¡å‹çš„PP-ATD-lighté‡å»ºè´¨é‡å’Œå‚æ•°æ•ˆç‡ä¼˜äºCAMixer-ATD-lightçš„æ•ˆæœã€‚

### 17. ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning

Columbia University New Yorkå‘å¸ƒäº†ReSSFormerè®ºæ–‡ï¼Œä½¿ç”¨é›†æˆé€’å½’æ¨ç†è®°å¿†å•å…ƒã€è‡ªé€‚åº”ç¨€ç–æ³¨æ„åŠ›æ¨¡å—å’Œè‡ªç»„ç»‡ç¼–ç å™¨ç»“æ„çš„é€’å½’ç¨€ç–ç»“æ„åŒ–Transformerï¼Œè§£å†³äº†Transformerçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€è®¡ç®—æ•ˆç‡å’Œç»“æ„æ³›åŒ–æŒ‘æˆ˜ï¼Œåœ¨å¯æ¯”è®¡ç®—é‡å’Œå‚æ•°é¢„ç®—ä¸‹æŒç»­ä¼˜äºå¼ºåŸºçº¿ï¼Œå±•ç°å‡ºå¯æ‰©å±•æ€§ã€æ•ˆç‡å’Œç»“æ„çµæ´»æ€§ã€‚

### 18. RETHINKING THE SHAPE CONVENTION OF AN MLP

MediaTek Researchå‘å¸ƒäº†â€œRETHINKING THE SHAPE CONVENTION OF AN MLPâ€è®ºæ–‡ï¼Œä½¿ç”¨æå‡ºçš„wide-narrow-wideï¼ˆHourglassï¼‰MLPå—ï¼ˆè·³è·ƒè¿æ¥åœ¨æ‰©å±•ç»´åº¦æ“ä½œã€æ®‹å·®é€šè¿‡çª„ç“¶é¢ˆæµåŠ¨ï¼Œä¸”åˆå§‹æŠ•å½±å›ºå®šéšæœºåˆå§‹åŒ–ï¼‰ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»ŸMLPçš„narrow-wide-narrowè®¾è®¡ï¼Œåœ¨ç”Ÿæˆä»»åŠ¡ä¸Šç›¸æ¯”ä¼ ç»Ÿè®¾è®¡å®ç°äº†æ›´ä¼˜çš„æ€§èƒ½-å‚æ•°Paretoå‰æ²¿ã€‚

### 19. RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models

å‘å¸ƒäº†RSAVQè®ºæ–‡ï¼Œä½¿ç”¨å«Error Direction Sensitivity Guidance (EDSG)å’ŒWeight Channel Sensitivity Guidance (WCSG)çš„Riemannian Sensitivity-Aware Vector Quantizationæ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰å‘é‡é‡åŒ–åœ¨å¤§è¯­è¨€æ¨¡å‹æä½æ¯”ç‰¹é‡åŒ–ä¸­é¢ä¸´çš„æ— çº¦æŸæ–¹å‘è¯¯å·®å’Œæ¬¡ä¼˜æ¯”ç‰¹åˆ†é…é—®é¢˜ï¼Œåœ¨LLaMA-3 8Bçš„2ä½é‡åŒ–ä¸­ï¼Œæ¯”VPTQå’ŒQuIP#ç­‰åŸºçº¿å›°æƒ‘åº¦ï¼ˆPPLï¼‰æå‡0.4ï¼Œé›¶æ ·æœ¬å‡†ç¡®ç‡æå‡1.5ã€‚

### 20. Self-Forcing++: Towards Minute-Scale High-Quality Video Generation

University of Central Floridaå‘å¸ƒäº†Self-Forcing++è®ºæ–‡ï¼Œä½¿ç”¨åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çŸ¥è¯†é€šè¿‡è‡ªç”Ÿæˆé•¿è§†é¢‘é‡‡æ ·ç‰‡æ®µæŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹çš„æ–¹æ³•ï¼Œè§£å†³äº†é•¿è§†é¢‘ç”Ÿæˆä¸­æ•™å¸ˆæ¨¡å‹æ— æ³•åˆæˆé•¿è§†é¢‘å¯¼è‡´çš„å¤–æ¨è´¨é‡ä¸‹é™ä¸è¯¯å·®ç´¯ç§¯é—®é¢˜ï¼Œè¾¾æˆè§†é¢‘é•¿åº¦æ‰©å±•è‡³æ•™å¸ˆèƒ½åŠ›20å€ã€ç”Ÿæˆ4åˆ†15ç§’è§†é¢‘ï¼ˆæ¯”åŸºçº¿é•¿50å¤šå€ï¼‰ä¸”ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ˜¾è‘—æå‡çš„æ•ˆæœã€‚

### 21. SHIFT-INVARIANT ATTRIBUTE SCORING FOR KOLMOGOROV-ARNOLD NETWORKS VIA SHAPLEY VALUE

æ–°åŠ å¡å›½ç«‹å¤§å­¦å‘å¸ƒäº†å…³äºKolmogorov-Arnold Networksï¼ˆKANsï¼‰çš„è®ºæ–‡ï¼Œä½¿ç”¨ShapKANæ¡†æ¶ï¼ˆåŸºäºShapleyå€¼å½’å› çš„å¹³ç§»ä¸å˜èŠ‚ç‚¹é‡è¦æ€§è¯„ä¼°æŠ€æœ¯ï¼‰ï¼Œè§£å†³äº†KANç½‘ç»œå‰ªæä¸­ä¼ ç»Ÿå¹…åº¦æ–¹æ³•å¯¹è¾“å…¥åæ ‡åç§»æ•æ„Ÿçš„é—®é¢˜ï¼Œè¾¾æˆäº†ä¿ç•™çœŸå®èŠ‚ç‚¹é‡è¦æ€§å¹¶å®ç°æœ‰æ•ˆç½‘ç»œå‹ç¼©ã€æå‡å¯è§£é‡Šæ€§çš„æ•ˆæœã€‚

### 22. Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction

ç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†Sparse Query Attention (SQA)è®ºæ–‡ï¼Œé€šè¿‡å‡å°‘æŸ¥è¯¢å¤´çš„ç¨€ç–æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œè§£å†³äº†Transformerä¸­MHAçš„äºŒæ¬¡è®¡ç®—å¤æ‚åº¦åŠè®­ç»ƒç­‰åœºæ™¯çš„FLOPsç“¶é¢ˆï¼Œåœ¨è®¡ç®—å¯†é›†åœºæ™¯ä¸­å®ç°ååé‡æå‡é«˜è¾¾3å€ä¸”å¯¹æ¨¡å‹è´¨é‡å½±å“æå°ã€‚

### 23. SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs

Los Alamos National Laboratoryå‘å¸ƒäº†SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEsè®ºæ–‡ï¼Œä½¿ç”¨è½»é‡çº§æ®‹å·®U-Netæ¶æ„åŠè‡ªå›å½’é¢„è®­ç»ƒç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰PDEåŸºç¡€æ¨¡å‹å‚æ•°ä¸è®¡ç®—å¼€é”€é«˜çš„é—®é¢˜ï¼Œè¾¾æˆäº†åœ¨å¤šæ ·ä¸‹æ¸¸PDEä»»åŠ¡ä¸Šå®ç°SOTAæ³›åŒ–ä¸”å‚æ•°æ˜¾è‘—å‡å°‘ã€å¾®è°ƒæ•°æ®éœ€æ±‚ä½çš„æ•ˆæœã€‚

### 24. SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs

æ­¦æ±‰ç†å·¥å¤§å­¦ä¸ä¸­å›½äººæ°‘å¤§å­¦å‘å¸ƒçš„SSTAGè®ºæ–‡ï¼Œé‡‡ç”¨ç»“æ„æ„ŸçŸ¥çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³äº†æ–‡æœ¬å±æ€§å›¾ä¸­ç»“æ„ä¸æ–‡æœ¬ä¿¡æ¯èåˆä¸è¶³çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ–‡æœ¬å±æ€§å›¾çš„è¡¨ç¤ºå­¦ä¹ æ€§èƒ½ã€‚

### 25. THE DISPARATE IMPACTS OF SPECULATIVE DECODING

University of Virginiaå‘å¸ƒäº†THE DISPARATE IMPACTS OF SPECULATIVE DECODINGè®ºæ–‡ï¼Œä½¿ç”¨é’ˆå¯¹æŠ•æœºè§£ç çš„ç¼“è§£ç­–ç•¥ï¼Œè§£å†³äº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸ŠåŠ é€Ÿä¸å‡ï¼ˆå°¤å…¶å¯¹æ¬ æ‹Ÿåˆã€ä»£è¡¨æ€§ä¸è¶³ä»»åŠ¡åŠ é€Ÿå‡å°‘ï¼‰çš„é—®é¢˜ï¼Œè¾¾æˆäº†å¹³å‡12%çš„å…¬å¹³æ€§æŒ‡æ ‡æå‡æ•ˆæœã€‚

### 26. THE UNSEEN FRONTIER: PUSHING THE LIMITS OF LLM SPARSITY WITH SURROGATE-FREE ADMM

å‘å¸ƒäº†ã€ŠTHE UNSEEN FRONTIER: PUSHING THE LIMITS OF LLM SPARSITY WITH SURROGATE-FREE ADMMã€‹è®ºæ–‡ï¼Œä½¿ç”¨ELSAæŠ€æœ¯ï¼ˆåŸºäºæ— ä»£ç†ADMMçš„çº¦æŸä¼˜åŒ–ï¼‰ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤§è¯­è¨€æ¨¡å‹ç¨€ç–åŒ–ä¸­éš¾ä»¥çªç ´50-60%ç¨€ç–åº¦ä¸”ä¸ä¸¥é‡é™ä½ç²¾åº¦çš„é—®é¢˜ï¼Œè¾¾æˆäº†é«˜è¾¾90%çš„æç«¯ç¨€ç–åº¦åŒæ—¶ä¿æŒé«˜æ¨¡å‹ä¿çœŸåº¦ï¼Œåœ¨LLaMA-2-7Bä¸Š90%ç¨€ç–åº¦æ—¶å›°æƒ‘åº¦æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•ä½7.8å€ï¼Œå¹¶æ¨å‡ºå¯æ‰©å±•è‡³27Bå¤§æ¨¡å‹çš„é‡åŒ–å˜ä½“ELSA-Lã€‚

### 27. Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction

çˆ±è·åå·ç«‹å¤§å­¦è‰¾å§†æ–¯åˆ†æ ¡å‘å¸ƒäº†ã€Šç«¯åˆ°ç«¯ç¥ç»å‹ç¼©ä¸é‡å»ºçš„è¶…é«˜æ•ˆè§£ç ã€‹è®ºæ–‡ï¼Œä½¿ç”¨åœ¨å¸¦çŸ¢é‡é‡åŒ–çš„è‡ªç¼–ç å™¨ä¸­èå…¥ä½ç§©è¡¨ç¤ºçš„æŠ€æœ¯ï¼Œè§£å†³äº†ç¥ç»å‹ç¼©ä¸­å·ç§¯åŸºè§£ç å™¨é‡å»ºæ—¶çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œè¾¾æˆäº†æ˜¾è‘—é™ä½è§£ç è®¡ç®—å¼€é”€ã€æ¶ˆé™¤è§£ç å™¨è®¡ç®—ç“¶é¢ˆå¹¶ä¿æŒå›¾åƒé«˜ä¿çœŸåº¦çš„æ•ˆæœã€‚

### 28. VideoNSA: Native Sparse Attention Scales Video Understanding VIDEONSA: NATIVE SPARSE ATTENTION SCALES VIDEO UNDERSTANDING

çº½çº¦å¤§å­¦ã€æ™®æ—æ–¯é¡¿å¤§å­¦å‘å¸ƒäº†VideoNSAè®ºæ–‡ï¼Œä½¿ç”¨ç¡¬ä»¶æ„ŸçŸ¥æ··åˆæ³¨æ„åŠ›ï¼ˆæ–‡æœ¬å¯†é›†+è§†é¢‘Native Sparse Attentionï¼‰æŠ€æœ¯ï¼Œè§£å†³äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è§†é¢‘ç†è§£å—ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼ˆé”™è¿‡å…³é”®å¸§ã€é•¿æ—¶é—´å°ºåº¦è¿è´¯æ€§å·®ï¼‰çš„é—®é¢˜ï¼Œè¾¾æˆäº†é•¿è§†é¢‘ç†è§£ã€æ—¶é—´æ¨ç†å’Œç©ºé—´åŸºå‡†æ€§èƒ½æå‡å¹¶å¯é æ‰©å±•åˆ°128K tokensçš„æ•ˆæœã€‚

## è®ºæ–‡è¯¦ç»†ä¿¡æ¯

### 1. An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution

**ä¸»è¦æœºæ„**: 
**ä½œè€…æ•°é‡**: 6äºº

**æ‘˜è¦**:


### 2. BIOX-BRIDGE: MODEL BRIDGING FOR UNSUPERVISED CROSS-MODAL KNOWLEDGE TRANSFER ACROSS BIOSIGNALS

**ä¸»è¦æœºæ„**: University of Oxford, Department of Engineering Science
**ä½œè€…æ•°é‡**: 4äºº

**æ‘˜è¦**:
Biosignals offer valuable insights into the physiological states of the human body. Although biosignal modalities differ in functionality, signal fidelity, sensor comfort, and cost, they are often intercorrelated, reflecting the holistic and interconnected nature of human physiology. This opens up the possibility of performing the same tasks using alternative biosignal modalities, thereby improving the accessibility, usability, and adaptability of health monitoring systems. However, the limited availability of large labeled datasets presents challenges for training models tailored to specific tasks and modalities of interest. Unsupervised cross-modal knowledge transfer offers a promising solution by leveraging knowledge from an existing modality to support model training for a new modality. Existing methods are typically based on knowledge distillation, which requires running a teacher model alongside student model training, resulting in high computational and memory overhead. This challenge is further exacerbated by the recent development of foundation models that demonstrate superior performance and generalization across tasks at the cost of large model sizes. To this end, we explore a new framework for unsupervised cross-modal knowledge transfer of biosignals by training a lightweight bridge network to align the intermediate representations and enable information flow between foundation models and across modalities. Specifically, we introduce an efficient strategy for selecting alignment positions where the bridge should be constructed, along with a flexible prototype network as the bridge architecture. Extensive experiments across multiple biosignal modalities, tasks, and datasets show that BioX-Bridge reduces the number of trainable parameters by 88-99% while maintaining or even improving transfer performance compared to state-of-the-art methods.

### 3. Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency

**ä¸»è¦æœºæ„**: Harvard University
**ä½œè€…æ•°é‡**: 6äºº

**æ‘˜è¦**:
Most pruning methods remove parameters ranked by impact on loss (e.g., magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each unit a local traffic budget-the product of its long-term on-rate ğ‘ ğ‘– and fan-out ğ‘˜ ğ‘–. A constrained-entropy analysis shows that maximizing coding entropy under a global traffic budget yields a selectivity-audience balance, log 1-ğ‘ ğ‘– ğ‘ ğ‘– = ğ›½ğ‘˜ ğ‘–. BB enforces this balance with simple local actuators that prune either fan-in (to lower activity) or fan-out (to reduce broadcast). In practice, BB increases coding entropy and decorrelation and improves accuracy at matched sparsity across Transformers for ASR, ResNets for face identification, and 3D U-Nets for synapse prediction, sometimes exceeding dense baselines. On electron microscopy images, it attains state-of-the-art F1 and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests a path towards learning more diverse and efficient representations.

### 4. ClustViT: Clustering-based Token Merging for Semantic Segmentation

**ä¸»è¦æœºæ„**: 
**ä½œè€…æ•°é‡**: 3äºº

**æ‘˜è¦**:
Vision Transformers can achieve high accuracy and strong generalization across various contexts, but their practical applicability on real-world robotic systems is limited due to their quadratic attention complexity. Recent works have focused on dynamically merging tokens according to the image complexity. Token merging works well for classification but is less suited to dense prediction. We propose ClustViT, where we expand upon the Vision Transformer (ViT) backbone and address semantic segmentation. Within our architecture, a trainable Cluster module merges similar tokens along the network guided by pseudo-clusters from segmentation masks. Subsequently, a Regenerator module restores fine details for downstream heads. Our approach achieves up to 2.18Ã— fewer GFLOPs and 1.64Ã— faster inference on three different datasets, with comparable segmentation accuracy. Our code and models will be made publicly available.

### 5. CONSTRAINED ADAPTIVE REJECTION SAMPLING

**ä¸»è¦æœºæ„**: University of Warsaw, University of California San Diego
**ä½œè€…æ•°é‡**: 4äºº

**æ‘˜è¦**:
Language Models (LMs) are increasingly used in applications where generated outputs must satisfy strict semantic or syntactic constraints. Existing approaches to constrained generation fall along a spectrum: greedy constrained decoding methods enforce validity during decoding but distort the LM's distribution, while rejection sampling (RS) preserves fidelity but wastes computation by discarding invalid outputs. Both extremes are problematic in domains such as program fuzzing, where both validity and diversity of samples are essential. We present Constrained Adaptive Rejection Sampling (CARS), an approach that strictly improves the sample-efficiency of RS without distributional distortion. CARS begins with unconstrained LM sampling and adaptively rules out constraint-violating continuations by recording them in a trie and subtracting their probability mass from future draws. This adaptive pruning ensures that prefixes proven invalid are never revisited, acceptance rates improve monotonically, and the resulting samples exactly follow the constrained distribution. In experiments on a variety of domains-e.g., program fuzzing and molecular generation-CARS consistently achieves higher efficiency-measured in the number of LM forward passes per valid sample-while also producing stronger sample diversity than both GCD and methods that approximate the LM's distribution.

### 6. Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning

**ä¸»è¦æœºæ„**: Emory University, University of Surrey, Max Planck Institute for Intelligent Systems, Hong Kong Polytechic University, University of Tuebingen
**ä½œè€…æ•°é‡**: 5äºº

**æ‘˜è¦**:
Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers-yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.

### 7. EXPLORING RESOLUTION-WISE SHARED ATTENTION IN HYBRID MAMBA-U-NETS FOR IMPROVED CROSS-CORPUS SPEECH ENHANCEMENT

**ä¸»è¦æœºæ„**: Aalborg University, Department of Electronic Systems
**ä½œè€…æ•°é‡**: 4äºº

**æ‘˜è¦**:
Recent advances in speech enhancement have shown that models combining Mamba and attention mechanisms yield superior crosscorpus generalization performance. At the same time, integrating Mamba in a U-Net structure has yielded state-of-the-art enhancement performance, while reducing both model size and computational complexity. Inspired by these insights, we propose RWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and multi-head attention in a U-Net structure for improved crosscorpus performance. Resolution-wise shared attention (RWSA) refers to layerwise attention-sharing across corresponding time-and frequency resolutions. Our best-performing RWSA-MambaUNet model achieves state-of-the-art generalization performance on two out-of-domain test sets. Notably, our smallest model surpasses all baselines on the out-of-domain DNS 2020 test set in terms of PESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM v2 test set in terms of SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and a fraction of the FLOPs.

### 8. FIDEDIFF: EFFICIENT DIFFUSION MODEL FOR HIGH-FIDELITY IMAGE MOTION DEBLURRING

**ä¸»è¦æœºæ„**: Harvard University, Shanghai Jiao Tong University
**ä½œè€…æ•°é‡**: 6äºº

**æ‘˜è¦**:
Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusionbased methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to highfidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at https://github.com/xyLiu339/FideDiff.

### 9. HISPEC: HIERARCHICAL SPECULATIVE DECODING FOR LLMS

**ä¸»è¦æœºæ„**: The University of Texas at Austin, Department of Electrical and Computer Engineering
**ä½œè€…æ•°é‡**: 3äºº

**æ‘˜è¦**:
Speculative decoding accelerates LLM inference by using a smaller draft model to speculate tokens that a larger target model verifies. Verification is often the bottleneck (e.g. verification is 4Ã— slower than token generation when a 3B model speculates for a 70B target model), but most prior works focus only on accelerating drafting. "Intermediate" verification reduces verification time by discarding inaccurate draft tokens early, but existing methods incur substantial training overheads in incorporating the intermediate verifier, increase the memory footprint to orchestrate the intermediate verification step, and compromise accuracy by relying on approximate heuristics. We propose Hierarchical Speculative Decoding (HiSpec), a framework for highthroughput speculative decoding that exploits early-exit (EE) models for lowoverhead intermediate verification. EE models allow tokens to exit early by skipping layer traversal and are explicitly trained so that hidden states at selected layers can be interpreted, making them uniquely suited for intermediate verification without drastically increasing compute and memory overheads. To improve resource-efficiency even further, we design a methodology that enables HiSpec to re-use key-value caches and hidden states between the draft, intermediate verifier, and target models. To maintain accuracy, HiSpec periodically validates the draft tokens accepted by the intermediate verifier against the target model. Our evaluations using various representative benchmarks and models show that HiSpec improves throughput by 1.28Ã— on average and by up to 2.01Ã— compared to the baseline single-layer speculation without compromising accuracy.

### 10. JaneEye: A 12-nm 2K-FPS 18.9-ÂµJ/Frame Event-based Eye Tracking Accelerator

**ä¸»è¦æœºæ„**: Leiden University, Delft University of Technology, Leiden Institute of Advanced Computer Science (LIACS), Department of Microelectronics
**ä½œè€…æ•°é‡**: 4äºº

**æ‘˜è¦**:
Eye tracking has become a key technology for gaze-based interactions in Extended Reality (XR). However, conventional frame-based eye-tracking systems often fall short of XR's stringent requirements for high accuracy, low latency, and energy efficiency. Event cameras present a compelling alternative, offering ultra-high temporal resolution and low power consumption. In this paper, we present JaneEye, an energy-efficient eventbased eye-tracking hardware accelerator designed specifically for wearable devices, leveraging sparse, high-temporal-resolution event data. We introduce an ultra-lightweight neural network architecture featuring a novel ConvJANET layer, which simplifies the traditional ConvLSTM by retaining only the forget gate, thereby halving computational complexity without sacrificing temporal modeling capability. Our proposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+ dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To further enhance hardware efficiency, we employ custom linear approximations of activation functions (hardsigmoid and hardtanh) and fixed-point quantization. Through software-hardware co-design, our 12-nm ASIC implementation operates at 400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames Per Second (FPS)) at an energy efficiency of 18.9 ÂµJ/frame. JaneEye sets a new benchmark in low-power, highperformance eye-tracking solutions suitable for integration into next-generation XR wearables.

### 11. LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction

**ä¸»è¦æœºæ„**: 
**ä½œè€…æ•°é‡**: 5äºº

**æ‘˜è¦**:
This work proposed a 3D autoencoder architecture, named LiLa-Net, which encodes efficient features from real traffic environments, employing only the LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle, equipped with Velodyne LiDAR. The system leverage skip connections concept to improve the performance without using extensive resources as the state-of-the-art architectures. Key changes include reducing the number of encoder layers and simplifying the skip connections, while still producing an efficient and representative latent space which allows to accurately reconstruct the original point cloud. Furthermore, an effective balance has been achieved between the information carried by the skip connections and the latent encoding, leading to improved reconstruction quality without compromising performance. Finally, the model demonstrates strong generalization capabilities, successfully reconstructing objects unrelated to the original traffic environment.

### 12. Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression

**ä¸»è¦æœºæ„**: University of Washington, Northwestern University
**ä½œè€…æ•°é‡**: 6äºº

**æ‘˜è¦**:
Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater computational cost-has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the Î˜(n 2 d) and Î˜(nd 2) complexity. We then introduce FlashLLA, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models. Code is available at https://github.com/Yifei-Zuo/Flash-LLA.

### 13. Mamba Outpaces Reformer in Stock Prediction with Sentiments from Top Ten LLMs

**ä¸»è¦æœºæ„**: University of North Texas USA
**ä½œè€…æ•°é‡**: 2äºº

**æ‘˜è¦**:
The stock market is extremely difficult to predict in the short term due to high market volatility, changes caused by news, and the nonlinear nature of the financial time series. This research proposes a novel framework for improving minute-level prediction accuracy using semantic sentiment scores from ten different large language models (LLMs) combined with minute interval intraday stock price data. We systematically constructed a time-aligned dataset of AAPL news articles and 1-minute Apple Inc. (AAPL) stock prices for the dates of April 4 to May 2, 2025. The sentiment analysis was achieved using the DeepSeek-V3, GPT variants, LLaMA, Claude, Gemini, Qwen, and Mistral models through their APIs. Each article obtained sentiment scores from all ten LLMs, which were scaled to a [0, 1] range and combined with prices and technical indicators like RSI, ROC, and Bollinger Band Width. Two state-of-the-art such as Reformer and Mamba were trained separately on the dataset using the sentiment scores produced by each LLM as input. Hyper parameters were optimized by means of Optuna and were evaluated through a 3-day evaluation period. Reformer had mean squared error (MSE) or the evaluation metrics, and it should be noted that Mamba performed not only faster but also better than Reformer for every LLM across the 10 LLMs tested. Mamba performed best with LLaMA 3.3-70B, with the lowest error of 0.137. While Reformer could capture broader trends within the data, the model appeared to over smooth sudden changes by the LLMs. This study highlights the potential of integrating LLM-based semantic analysis paired with efficient temporal modeling to enhance real-time financial forecasting.

### 14. Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving

**ä¸»è¦æœºæ„**: Department of Computer Science, City University of Hong Kong
**ä½œè€…æ•°é‡**: 7äºº

**æ‘˜è¦**:
Vision-Language Models (VLMs) are increasingly applied in autonomous driving for unified perception and reasoning, but high inference latency hinders real-time deployment. Early-exit reduces latency by terminating inference at intermediate layers, yet its task-dependent nature limits generalization across diverse scenarios. We observe that this limitation aligns with autonomous driving: navigation systems can anticipate upcoming contexts (e.g., intersections, traffic lights), indicating which tasks will be required. We propose Nav-EE, a navigation-guided early-exit framework that precomputes task-specific exit layers offline and dynamically applies them online based on navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE achieves accuracy comparable to full inference while reducing latency by up to 63.9%. Real-vehicle integration with Autoware Universe further demonstrates reduced inference latency (600 ms to 300 ms), supporting faster decision-making in complex scenarios. These results suggest that coupling navigation foresight with early-exit offers a viable path toward efficient deployment of large models in autonomous systems. Code and data are available at our anonymous repository: https://anonymous.4open.science/r/Nav-EE-BBC4 *Equal contribution

### 15. OPTIMAL STOPPING VS BEST-OF-N FOR INFERENCE TIME OPTIMIZATION

**ä¸»è¦æœºæ„**: University of Southern, University of Michigan
**ä½œè€…æ•°é‡**: 5äºº

**æ‘˜è¦**:
Large language model (LLM) generation often requires balancing output quality against inference cost, especially when using multiple generations. We introduce a new framework for inference-time optimization based on the classical Pandora's Box problem. Viewing each generation as opening a costly "box" with random reward, we develop algorithms that decide when to stop generating without knowing the underlying reward distribution. Our first contribution is a UCB-style Pandora's Box algorithm, which achieves performance that is provably close to Weitzman's algorithm, the optimal strategy when the distribution is known. We further adapt this method to practical LLM settings by addressing reward scaling across prompts via a Bradley-Terry inspired transformation. This leads to an adaptive inference-time optimization method that normalizes rewards and learns stopping thresholds on the fly. Experiments on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs, show that our adaptive strategy can obtain the same performance as non-adaptive Best-of-N sampling while requiring 15-35% fewer generations on average. Our results establish a principled bridge between optimal stopping theory and inference-time scaling, providing both theoretical performance bounds and practical efficiency gains for LLM deployment.

### 16. Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution

**ä¸»è¦æœºæ„**: State Key Laboratory for Novel Software Technology Nanjing University
**ä½œè€…æ•°é‡**: 4äºº

**æ‘˜è¦**:
Image Super-Resolution (SR) aims to reconstruct highresolution images from low-resolution counterparts, but the computational complexity of deep learning-based methods often hinders practical deployment. CAMixer is the pioneering work to integrate the advantages of existing lightweight SR methods and proposes a content-aware mixer to route token mixers of varied complexities according to the difficulty of content recovery. However, several limitations remain, such as poor adaptability, coarse-grained masking and spatial inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking mechanism that identifies pure pixels and exempts them from expensive computations. PP utilizes fixed color center points to classify pixels into distinct categories, enabling fine-grained, spatially flexible masking while maintaining adaptive flexibility. Integrated into the state-of-the-art ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency when saving a similar amount of computation.

### 17. ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning

**ä¸»è¦æœºæ„**: Hebei Institute of Communications Shijiazhuang, Columbia University New York
**ä½œè€…æ•°é‡**: 4äºº

**æ‘˜è¦**:
While Transformer architectures have demonstrated impressive scalability across domains, they continue to face challenges in longcontext reasoning, computational efficiency, and structural generalization-largely due to rigid layer stacking, dense attention, and reliance on positional encodings. We present ReSSFormer, a Recursive Sparse Structured Transformer that integrates three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM) for efficient and focused context selection, and Self-Organizing Encoder Structure (SOES) for position-free structure induction. ReSSFormer replaces conventional depth stacking with recurrent inference, substitutes full attention with tokenand expert-level sparsity, and models latent token topology directly from content. Across language modeling, multi-hop QA, and structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines under comparable FLOPs and parameter budgets, highlighting its scalability, efficiency, and structural flexibility. CCS Concepts â€¢ Computing methodologies â†’ Natural language generation.

### 18. RETHINKING THE SHAPE CONVENTION OF AN MLP

**ä¸»è¦æœºæ„**: MediaTek Research
**ä½œè€…æ•°é‡**: 4äºº

**æ‘˜è¦**:
Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.

### 19. RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models

**ä¸»è¦æœºæ„**: 
**ä½œè€…æ•°é‡**: 5äºº

**æ‘˜è¦**:
Large language models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their exponentially increasing parameters pose significant challenges for deployment on resourceconstrained devices. Vector Quantization (VQ) shows great promise for low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key challenges: unconstrained direction error and suboptimal bit allocation. In this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit quantization for LLMs. RSAVQ introduces two geometry-driven innovations that effectively mitigate above limitations: (1) Error Direction Sensitivity Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced Riemannian metric to project quantization errors onto low-sensitivity directions in the parameter space. Specifically, this projection is performed along the negative natural gradient direction, which effectively suppresses error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which constructs a channel-wise sensitivity metric via FIM curvature analysis to dynamically guide bit resource allocation. The approach facilitates a globally optimal quantization solution within prescribed bit constraints. Experiments demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in 2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by 0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a practical solution for constrained environments and a theoretical bridge between information geometry and the quantization of neural networks, advancing efficient deep learning.

### 20. Self-Forcing++: Towards Minute-Scale High-Quality Video Generation

**ä¸»è¦æœºæ„**: University of Central Florida
**ä½œè€…æ•°é‡**: 11äºº

**æ‘˜è¦**:
Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20Ã— beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/.

### 21. SHIFT-INVARIANT ATTRIBUTE SCORING FOR KOLMOGOROV-ARNOLD NETWORKS VIA SHAPLEY VALUE

**ä¸»è¦æœºæ„**: National University of Singapore
**ä½œè€…æ•°é‡**: 4äºº

**æ‘˜è¦**:
For many real-world applications, understanding feature-outcome relationships is as crucial as achieving high predictive accuracy. While traditional neural networks excel at prediction, their black-box nature obscures underlying functional relationships. Kolmogorov-Arnold Networks (KANs) address this by employing learnable spline-based activation functions on edges, enabling recovery of symbolic representations while maintaining competitive performance. However, KAN's architecture presents unique challenges for network pruning. Conventional magnitude-based methods become unreliable due to sensitivity to input coordinate shifts. We propose ShapKAN, a pruning framework using Shapley value attribution to assess node importance in a shift-invariant manner. Unlike magnitudebased approaches, ShapKAN quantifies each node's actual contribution, ensuring consistent importance rankings regardless of input parameterization. Extensive experiments on synthetic and real-world datasets demonstrate that ShapKAN preserves true node importance while enabling effective network compression. Our approach improves KAN's interpretability advantages, facilitating deployment in resource-constrained environments.

### 22. Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction

**ä¸»è¦æœºæ„**: 
**ä½œè€…æ•°é‡**: 1äºº

**æ‘˜è¦**:
The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents a significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains a critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), a novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by a factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and a family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only a minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, a context in which its computational advantages are maximized, suggesting its potential as a powerful tool for building more efficient and scalable models.

### 23. SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs

**ä¸»è¦æœºæ„**: Space Remote Sensing and Data Science, Computing and Artificial Intelligence Division (CAI), Los Alamos National Laboratory
**ä½œè€…æ•°é‡**: 5äºº

**æ‘˜è¦**:
We introduce Small PDE U-Net Solver (SPUS), a compact and efficient foundation model (FM) designed as a unified neural operator for solving a wide range of partial differential equations (PDEs). Unlike existing state-of-the-art PDE FMs-primarily based on large complex transformer architectures with high computational and parameter overhead-SPUS leverages a lightweight residual U-Net-based architecture that has been largely underexplored as a foundation model architecture in this domain. To enable effective learning in this minimalist framework, we utilize a simple yet powerful auto-regressive pretraining strategy which closely replicates the behavior of numerical solvers to learn the underlying physics. SPUS is pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6 challenging unseen downstream PDEs spanning various physical systems. Experimental results demonstrate that SPUS using residual U-Net based architecture achieves state-of-the-art generalization on these downstream tasks while requiring significantly fewer parameters and minimal fine-tuning data, highlighting its potential as a highly parameter-efficient FM for solving diverse PDE systems.

### 24. SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs

**ä¸»è¦æœºæ„**: Wuhan University of Technology, Renmin University of China, Institute of Information Engineering, CAS School of Cyberspace Security
**ä½œè€…æ•°é‡**: 13äºº

**æ‘˜è¦**:


### 25. THE DISPARATE IMPACTS OF SPECULATIVE DECODING

**ä¸»è¦æœºæ„**: Hofstra University, University of Virginia
**ä½œè€…æ•°é‡**: 5äºº

**æ‘˜è¦**:
The practice of speculative decoding, whereby inference is probabilistically supported by a smaller, cheaper, "drafter" model, has become a standard technique for systematically reducing the decoding time of large language models. This paper conducts an analysis of speculative decoding through the lens of its potential disparate speed-up rates across tasks. Crucially, the paper shows that speed-up gained from speculative decoding is not uniformly distributed across tasks, consistently diminishing for under-fit, and often underrepresented tasks. To better understand this phenomenon, we derive an analysis to quantify this observed "unfairness" and draw attention to the factors that motivate such disparate speed-ups to emerge. Further, guided by these insights, the paper proposes a mitigation strategy designed to reduce speed-up disparities and validates the approach across several model pairs, revealing on average a 12% improvement in our fairness metric.

### 26. THE UNSEEN FRONTIER: PUSHING THE LIMITS OF LLM SPARSITY WITH SURROGATE-FREE ADMM

**ä¸»è¦æœºæ„**: 
**ä½œè€…æ•°é‡**: 6äºº

**æ‘˜è¦**:
Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs). Despite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60%) without severely degrading model accuracy. This work breaks through the current impasse, presenting a principled and effective method called ELSA, which achieves extreme sparsity levels of up to 90% while retaining high model fidelity. This is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation. ELSA tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM. Our extensive experiments across a wide range of models and scales show that ELSA achieves substantial improvements over existing methods; e.g., it achieves 7.8Ë†less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity. Furthermore, we present ELSA-L , a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees. These results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.

### 27. Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction

**ä¸»è¦æœºæ„**: Iowa State University Ames, Department of Electrical and Computer Engineering
**ä½œè€…æ•°é‡**: 2äºº

**æ‘˜è¦**:
Image compression and reconstruction are crucial for various digital applications. While contemporary neural compression methods achieve impressive compression rates, the adoption of such technology has been largely hindered by the complexity and large computational costs of the convolution-based decoders during data reconstruction. To address the decoder bottleneck in neural compression, we develop a new compression-reconstruction framework based on incorporating low-rank representation in an autoencoder with vector quantization. We demonstrated that performing a series of computationally efficient low-rank operations on the learned latent representation of images can efficiently reconstruct the data with high quality. Our approach dramatically reduces the computational overhead in the decoding phase of neural compression/reconstruction, essentially eliminating the decoder compute bottleneck while maintaining high fidelity of image outputs.

### 28. VideoNSA: Native Sparse Attention Scales Video Understanding VIDEONSA: NATIVE SPARSE ATTENTION SCALES VIDEO UNDERSTANDING

**ä¸»è¦æœºæ„**: New York University, University of California, Princeton University, Lambda
**ä½œè€…æ•°é‡**: 8äºº

**æ‘˜è¦**:
Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks.
