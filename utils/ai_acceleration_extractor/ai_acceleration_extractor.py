from typing import Dict, Optional, List, Tuple
import os
import csv
from datetime import datetime
from utils.pdf_extractor import extract_paper_abstract
from utils.doubao_api import call_doubao


class _KeywordConfig:
    """å…³é”®è¯é…ç½®ç±»ï¼Œç®¡ç†æ‰€æœ‰åŒ¹é…å…³é”®è¯"""
    
    def __init__(self):
        # æ ¸å¿ƒæ¨ç†åŠ é€Ÿå…³é”®è¯ï¼ˆæœ€é«˜æƒé‡ï¼š6åˆ†ï¼‰
        self.core_keywords = {
            "inference_optimization": [
                "inference acceleration", "inference optimization", "inference speedup",
                "model acceleration", "neural acceleration", "ai acceleration",
                "inference latency", "inference throughput", "inference efficiency",
                "model serving optimization", "inference engine", "serving optimization",
                "rapid inference", "accelerated inference", "inference time optimization",
                "acceleration of diffusion", "accelerating multimodal", 
                "lazy learning for acceleration", "fast training and inference",
                "neural network acceleration", "deep learning acceleration",
                "efficient inference", "fast inference", "optimized inference",
                "low-latency inference", "real-time inference", "fast decoding",
                "inference performance", "serving performance", "model efficiency",
                "training-free acceleration", "accelerating llm", "llm acceleration"
            ],
            "quantization_compression": [
                "quantization", "pruning", "model compression", "weight pruning",
                "activation quantization", "weight quantization", "int8", "fp16", "bf16",
                "low-precision", "mixed precision", "post-training quantization", "qat",
                "quantization-aware training", "bit-width optimization", "sparse models",
                "structured pruning", "unstructured pruning", "magnitude pruning",
                "arbitrary-bit quantized", "quantized inference acceleration",
                "neural compression", "network compression", "model size reduction",
                "quantized model", "compressed model", "low-bit quantization",
                "extremely low bit", "mixed precision quantization", "quantized inference",
                "numerical pruning", "weight compression", "model miniaturization"
            ],
            "distillation_acceleration": [
                "knowledge distillation", "model distillation", "teacher-student",
                "student model", "teacher model", "distillation training",
                "lightweight model", "compact model", "model miniaturization",
                "neural distillation", "attention distillation", "cross-modal distillation",
                "multi-teacher distillation", "self-distillation", "contrastive distillation"
            ],
            "spiking_acceleration": [
                "spiking neural network", "spiking neural networks", "snn", "snns",
                "spiking neurons", "spiking transformer", "spiking", "neuromorphic",
                "spike-based", "event-driven", "temporal coding", "rate coding",
                "leaky integrate-and-fire", "spiking convolution", "spiking attention",
                "spike2former", "efficient spiking", "spiking yolo"
            ],
            "moe_acceleration": [
                "mixture of experts", "moe", "sparse moe", "dense moe", "switch transformer",
                "expert routing", "expert selection", "gating network", "routing network",
                "sparse activation", "expert parallelism", "moe scaling", "expert capacity",
                "load balancing", "auxiliary loss", "routing loss", "expert dropout",
                "top-k routing", "expert utilization", "moe efficiency", "sparse expert",
                "expert load balancing", "dynamic routing", "adaptive routing",
                "expert sparsity", "moe optimization", "expert caching", "expert scheduling",
                "communication-efficient mixture", "communication-efficient moe"
            ]
        }

        # é«˜ç›¸å…³æ¨ç†æŠ€æœ¯ï¼ˆæƒé‡ï¼š5åˆ†ï¼‰
        self.high_relevance_keywords = {
            "inference_techniques": [
                "early exit", "dynamic inference", "adaptive inference",
                "speculative decoding", "parallel decoding", "batch inference",
                "kv cache", "kv caching", "attention optimization", 
                "dynamic batching", "continuous batching", "tensor parallelism",
                "adaptive attention", "conditional execution", "skip connections",
                "progressive inference", "multi-exit", "conditional computation",
                "spatial-temporal visual token trimming", "visual token trimming",
                "token reduction", "visual tokens withdrawal", "token merging",
                "attention head pruning", "layer skipping", "adaptive computation",
                "computational efficiency", "efficient attention", "sparse attention",
                "linear attention", "fast attention", "multi-branch self-drafting",
                "self-drafting", "adaptive step", "step selection", "adaptive guidance",
                "training-free acceleration", "sublayer skipping", "adaptive skip"
            ],
            "hardware_acceleration": [
                "gpu acceleration", "tpu optimization", "fpga implementation",
                "tensorcore", "cuda optimization", "hardware-aware optimization",
                "edge deployment", "mobile inference", "embedded inference",
                "hardware efficiency", "memory-efficient inference",
                "hardware-software co-design", "accelerator design", "on-device inference",
                "edge ai", "mobile ai", "real-time processing", "low-power inference"
            ],
            "frameworks_engines": [
                "tensorrt", "onnx runtime", "tvm", "openvino", "tensorflow lite",
                "pytorch mobile", "vllm", "triton inference", "tensorrt-llm",
                "deepspeed", "fastertransformer", "lightllm", "flash attention",
                "xformers", "optimum", "neural magic", "ort", "tensorrt inference"
            ],
            "diffusion_acceleration": [
                "diffusion acceleration", "fast diffusion", "few steps diffusion",
                "diffusion distillation", "diffusion optimization", "step selection",
                "adaptive step", "flash diffusion", "accelerating diffusion",
                "efficient diffusion", "rapid diffusion", "diffusion speedup",
                "acceleration of diffusion transformers", "diffusion pruning",
                "diffusion quantization", "diffusion caching", "lazy learning",
                "lazydit", "adaptive guidance", "fast generative", "generative acceleration"
            ],
            "multimodal_acceleration": [
                "extending mamba", "multimodal acceleration", "efficient multimodal",
                "accelerating multimodal", "multimodal optimization",
                "vision-language acceleration", "multimodal inference optimization",
                "cobra", "mamba", "state space model", "ssm", "linear complexity",
                "efficient mllm", "multimodal efficiency", "visual token withdrawal",
                "boosting multimodal", "efficient vlm", "lightweight multimodal",
                "elastic visual experts", "vision language optimization"
            ],
            "compression_efficiency": [
                "text compression", "lossless compression", "low-complexity",
                "compression ratio", "entropy coding", "learned compression",
                "data compression", "neural compression", "efficient compression",
                "compression performance", "compression algorithm", "video compression",
                "neural block compression", "bit-operation acceleration"
            ]
        }

        # ä¸­ç­‰ç›¸å…³æŠ€æœ¯ï¼ˆæƒé‡ï¼š3åˆ†ï¼‰
        self.medium_relevance_keywords = {
            "optimization_techniques": [
                "kernel fusion", "operator fusion", "graph optimization",
                "memory optimization", "computational optimization", "parameter efficiency",
                "flops reduction", "latency reduction", "throughput improvement",
                "runtime optimization", "performance optimization", "energy efficiency",
                "look-up table", "lut", "deformable", "multi-frame",
                "gradient checkpointing", "activation checkpointing", "cache optimization",
                "memory efficiency", "compute efficiency", "computational cost reduction",
                "resource optimization", "system optimization", "deployment optimization"
            ],
            "model_architectures": [
                "efficient transformer", "lightweight neural network", "mobile model",
                "compact architecture", "efficient architecture", 
                "binary neural network", "efficient attention", "linear attention",
                "lightweight transformer", "mobile transformer", "efficient convolution",
                "depthwise separable", "mobile nets", "efficient nets",
                "mamba", "state space model", "ssm", "linear complexity",
                "parallel optimal position search", "popos", "scalable model",
                "elastic model", "adaptive model", "dynamic model"
            ],
            "serving_deployment": [
                "model serving", "deployment optimization", "production deployment",
                "scalable inference", "high-throughput serving", "low-latency serving",
                "real-time deployment", "streaming inference",
                "online inference", "efficient deployment", "inference server",
                "scalable deployment", "cloud inference", "edge serving",
                "distributed inference", "parallel inference"
            ],
            "vision_language_efficiency": [
                "efficient vision language", "multimodal efficiency",
                "vision language optimization", "vlm efficiency",
                "efficient vlm", "lightweight multimodal", "fast multimodal",
                "efficient mllm", "multimodal inference", "visual token", 
                "adaptive multimodal", "multimodal compression", "cross-modal efficiency",
                "visual efficiency", "language model efficiency"
            ],
            "specialized_acceleration": [
                "privacy-utility-scalable", "offsite-tuning", "rank compression",
                "layer replace", "dynamic layer", "selective compression",
                "scalable optimization", "efficient tuning", "parameter efficiency",
                "fast training", "rapid training", "accelerated training", 
                "efficient learning", "fast adaptation", "quick adaptation",
                "parameter-efficient", "resource-efficient", "compute-efficient"
            ],
            "system_efficiency": [
                "communication-efficient", "bandwidth efficient", "network optimization",
                "distributed efficiency", "parallel efficiency", "concurrent processing",
                "asynchronous processing", "pipeline optimization", "batch optimization",
                "throughput optimization", "latency optimization", "response optimization",
                "scalable system", "high-performance system", "efficient system",
                "fast system", "optimized system", "streamlined system"
            ]
        }

        # æ”¯æ’‘å…³é”®è¯ï¼ˆæƒé‡ï¼š1åˆ†ï¼Œå¢åŠ æƒé‡ï¼‰ - å¤§å¹…å‡å°‘é€šç”¨è¯æ±‡
        self.supporting_keywords = {
            "performance_metrics": [
                "inference latency", "inference throughput", "inference speed", 
                "model efficiency", "computational efficiency", "inference performance",
                "serving performance", "deployment performance", "acceleration ratio",
                "speedup ratio", "memory efficiency", "compute efficiency",
                "inference cost", "computational cost", "serving cost",
                "processing time", "response time", "latency reduction",
                "throughput improvement", "performance gain", "efficiency gain",
                "speed improvement", "acceleration gain", "optimization gain"
            ],
            "model_types": [
                "large language model", "transformer model", "neural network model",
                "foundation model", "generative model", "vision language model", 
                "multimodal model", "diffusion model", "state space model", 
                "diffusion transformer", "language model inference", "autoregressive model",
                "seq2seq model", "encoder-decoder model", "attention model",
                "transformer-based model", "neural architecture", "deep model"
            ],
            "specific_efficiency": [
                "fast inference", "rapid inference", "accelerated model",
                "optimized inference", "lightweight inference", "efficient model",
                "compressed model", "pruned model", "quantized model",
                "high-performance", "low-latency", "real-time",
                "efficient implementation", "optimized implementation",
                "fast implementation", "rapid implementation", "accelerated implementation",
                "streamlined implementation", "enhanced performance", "improved efficiency",
                "performance boost", "efficiency boost", "speed boost"
            ],
            "efficiency_terms": [
                "efficient", "effectiveness", "fast", "rapid", "quick", "swift",
                "accelerated", "optimized", "streamlined", "enhanced", "improved",
                "boosted", "upgraded", "advanced", "superior", "high-performance",
                "lightweight", "compact", "minimal", "reduced", "low-cost",
                "resource-saving", "time-saving", "cost-effective", "performance-oriented"
            ]
        }


class _KeywordMatcher:
    """å…³é”®è¯åŒ¹é…å™¨ï¼Œè´Ÿè´£åŒ¹é…å’Œè¯„åˆ†é€»è¾‘"""
    
    def __init__(self):
        self._config = _KeywordConfig()
    
    def _match_keyword_category(self, title: str, abstract: str, keywords: Dict[str, List[str]], 
                               priority: str, weight: int) -> Tuple[List[Dict], int]:
        """åŒ¹é…ç‰¹å®šç±»åˆ«çš„å…³é”®è¯"""
        matched_keywords = []
        category_count = 0
        
        for category, category_keywords in keywords.items():
            for keyword in category_keywords:
                keyword_lower = keyword.lower()
                
                # æ›´ç²¾ç¡®çš„å…³é”®è¯åŒ¹é…
                title_match = self._precise_keyword_match(title, keyword_lower)
                abstract_match = self._precise_keyword_match(abstract, keyword_lower)
                
                if title_match or abstract_match:
                    category_count += weight  # ä½¿ç”¨æƒé‡è€Œä¸æ˜¯ç®€å•è®¡æ•°
                    matched_keywords.append({
                        'keyword': keyword,
                        'category': category,
                        'priority': priority,
                        'weight': weight,
                        'in_title': title_match,
                        'in_abstract': abstract_match
                    })
        
        return matched_keywords, category_count
    
    def _precise_keyword_match(self, text: str, keyword: str) -> bool:
        """ç²¾ç¡®çš„å…³é”®è¯åŒ¹é…"""
        text_lower = text.lower()
        
        # å¯¹äºå•ä¸ªè¯çš„å…³é”®è¯ï¼Œä½¿ç”¨è¯è¾¹ç•ŒåŒ¹é…
        if ' ' not in keyword:
            import re
            pattern = r'\b' + re.escape(keyword) + r'\b'
            return bool(re.search(pattern, text_lower))
        else:
            # å¯¹äºçŸ­è¯­ï¼Œæ£€æŸ¥å®Œæ•´çŸ­è¯­åŒ¹é…
            return (f" {keyword} " in f" {text_lower} " or 
                   text_lower.startswith(f"{keyword} ") or 
                   text_lower.endswith(f" {keyword}") or
                   text_lower == keyword)
    
    def match_keywords(self, title: str, abstract: str) -> Dict:
        """æ‰§è¡Œå…³é”®è¯åŒ¹é…"""
        # å®‰å…¨å¤„ç†è¾“å…¥å‚æ•°
        title = title or ""
        abstract = abstract or ""
        title_lower = title.lower()
        abstract_lower = abstract.lower()
        
        # åŒ¹é…å„ç±»åˆ«å…³é”®è¯
        all_matched_keywords = []
        total_score = 0
        
        # æ ¸å¿ƒå…³é”®è¯åŒ¹é…
        core_keywords, core_count = self._match_keyword_category(
            title_lower, abstract_lower, self._config.core_keywords, 'core', 6)
        all_matched_keywords.extend(core_keywords)
        
        # é«˜ç›¸å…³å…³é”®è¯åŒ¹é…
        high_keywords, high_count = self._match_keyword_category(
            title_lower, abstract_lower, self._config.high_relevance_keywords, 'high', 5)
        all_matched_keywords.extend(high_keywords)
        
        # ä¸­ç­‰ç›¸å…³å…³é”®è¯åŒ¹é…
        medium_keywords, medium_count = self._match_keyword_category(
            title_lower, abstract_lower, self._config.medium_relevance_keywords, 'medium', 3)
        all_matched_keywords.extend(medium_keywords)
        
        # æ”¯æ’‘å…³é”®è¯åŒ¹é…
        supporting_keywords, supporting_count = self._match_keyword_category(
            title_lower, abstract_lower, self._config.supporting_keywords, 'supporting', 1)
        all_matched_keywords.extend(supporting_keywords)
        
        # è®¡ç®—æ€»åˆ†å’Œåˆ†ç±»ä¿¡æ¯
        title_keywords = []
        abstract_keywords = []
        
        for kw_info in all_matched_keywords:
            total_score += kw_info['weight']
            
            if kw_info['in_title']:
                title_keywords.append(kw_info['keyword'])
                # æ ‡é¢˜å…³é”®è¯é¢å¤–åŠ æƒ
                if kw_info['priority'] == 'core':
                    total_score += 3
                elif kw_info['priority'] == 'high':
                    total_score += 2
                elif kw_info['priority'] == 'medium':
                    total_score += 1
            
            if kw_info['in_abstract']:
                abstract_keywords.append(kw_info['keyword'])
        
        return {
            'matched_keywords': all_matched_keywords,
            'keyword_count': total_score,
            'title_keywords': title_keywords,
            'abstract_keywords': abstract_keywords,
            'core_match_count': core_count,
            'high_match_count': high_count,
            'medium_match_count': medium_count,
            'supporting_match_count': supporting_count
        }


class _MatchDecisionEngine:
    """åŒ¹é…å†³ç­–å¼•æ“ï¼Œè´Ÿè´£æœ€ç»ˆçš„åŒ¹é…åˆ¤æ–­"""
    
    @staticmethod
    def should_match(match_result: Dict, threshold: int = 5) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ¹é…"""
        total_score = match_result.get('keyword_count', 0)
        
        # ç®€å•çš„é˜ˆå€¼åˆ¤æ–­ï¼šæƒé‡å¤§äºç­‰äº5æ—¶åŒ¹é…æˆåŠŸ
        return total_score >= threshold


class _LLMJudge:
    """å¤§æ¨¡å‹åˆ¤åˆ«å™¨ï¼Œä½¿ç”¨è±†åŒ…APIè¿›è¡Œè®ºæ–‡ç›¸å…³æ€§åˆ¤æ–­"""
    
    def __init__(self):
        # å…¬å…±å‰ç¼€æ¨¡æ¿ï¼ŒåŒ…å«æ ‡é¢˜å’Œæ‘˜è¦ä¿¡æ¯
        self.common_prefix = """ä»¥ä¸‹æ˜¯ä¸€ç¯‡è®ºæ–‡çš„åŸºæœ¬ä¿¡æ¯ï¼š

æ ‡é¢˜ï¼š{title}

æ‘˜è¦ï¼š{abstract}

"""

        # æ€»ç»“ä»»åŠ¡çš„å…·ä½“æŒ‡ä»¤
        self.summary_task = """è¯·ç”¨ä¸­æ–‡ä¸€å¥è¯æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ï¼Œå°½å¯èƒ½åŒ…å«è®ºæ–‡çš„å®éªŒæ•°æ®ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜ã€‚
"""

        # ç›¸å…³æ€§åˆ¤æ–­ä»»åŠ¡çš„å…·ä½“æŒ‡ä»¤
        self.relevance_task = """è¯·åˆ¤æ–­è¿™ç¯‡è®ºæ–‡æ˜¯å¦ä¸AIæ¨ç†åŠ é€Ÿç›¸å…³ã€‚AIæ¨ç†åŠ é€ŸåŒ…æ‹¬ä½†ä¸é™äºï¼š
- é‡åŒ–ã€å‰ªæã€è’¸é¦ã€æ—©é€€ç­‰æ¨¡å‹å‹ç¼©æŠ€æœ¯
- åŠ¨æ€æ¨ç†ã€æŠ•æœºè§£ç ã€é‡‡æ ·ä¼˜åŒ–ç­‰ç®—æ³•æŠ€æœ¯
- GPU/TPU/FPGAç­‰ç¡¬ä»¶åŠ é€Ÿ
- æ¨¡å‹å¹¶è¡Œæ¨ç†ä¼˜åŒ–ï¼Œé€šä¿¡ä¼˜åŒ–
- æ¨ç†å¼•æ“ä¼˜åŒ–ã€servingä¼˜åŒ–
- å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–ã€å¤šæ¨¡æ€æ¨ç†åŠ é€Ÿ
- æ‰©æ•£æ¨¡å‹åŠ é€Ÿã€ç”Ÿæˆæ¨¡å‹æ¨ç†ä¼˜åŒ–
- é«˜æ•ˆæ¨¡å‹ç»“æ„çš„ç ”ç©¶

è¯·å›ç­”"ç›¸å…³"æˆ–"ä¸ç›¸å…³"ï¼Œå¹¶ç®€è¦è¯´æ˜åŸå› ï¼ˆä¸è¶…è¿‡30ä¸ªå­—ï¼‰ã€‚æ ¼å¼ï¼šç›¸å…³/ä¸ç›¸å…³ - åŸå› """

        # ç¿»è¯‘ä»»åŠ¡çš„å…·ä½“æŒ‡ä»¤  
        self.translation_task = """è¯·å°†ä¸Šè¿°è‹±æ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼Œè¦æ±‚ç¿»è¯‘å‡†ç¡®ã€é€šé¡ºã€ä¸“ä¸šã€‚

è¯·ç›´æ¥ç»™å‡ºä¸­æ–‡ç¿»è¯‘ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜ã€‚"""
    
    def get_summary(self, title: str, abstract: str) -> str:
        """è·å–è®ºæ–‡çš„ä¸€å¥è¯æ€»ç»“"""
        try:
            # ä½¿ç”¨å…¬å…±å‰ç¼€ + ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤
            full_prompt = self.common_prefix.format(title=title, abstract=abstract) + self.summary_task
            
            messages = [
                {
                    "role": "user",
                    "content": full_prompt
                }
            ]
            
            result = call_doubao(messages)
            if result.get("success"):
                return result.get("content", "").strip()
            else:
                return f"æ€»ç»“ç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                
        except Exception as e:
            return f"æ€»ç»“ç”Ÿæˆå‡ºé”™: {str(e)}"
    
    def judge_relevance(self, title: str, abstract: str) -> str:
        """åˆ¤æ–­è®ºæ–‡æ˜¯å¦ä¸AIæ¨ç†åŠ é€Ÿç›¸å…³"""
        try:
            # ä½¿ç”¨å…¬å…±å‰ç¼€ + ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤
            full_prompt = self.common_prefix.format(title=title, abstract=abstract) + self.relevance_task
            
            messages = [
                {
                    "role": "user", 
                    "content": full_prompt
                }
            ]
            
            result = call_doubao(messages)
            if result.get("success"):
                return result.get("content", "").strip()
            else:
                return f"ç›¸å…³æ€§åˆ¤æ–­å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                
        except Exception as e:
            return f"ç›¸å…³æ€§åˆ¤æ–­å‡ºé”™: {str(e)}"
    
    def translate_abstract(self, title: str, abstract: str) -> str:
        """å°†è‹±æ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡"""
        try:
            # ä½¿ç”¨å…¬å…±å‰ç¼€ + ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤ï¼Œç¡®ä¿æ ‡é¢˜å’Œæ‘˜è¦ä¿¡æ¯ä¸€è‡´
            full_prompt = self.common_prefix.format(title=title, abstract=abstract) + self.translation_task
            
            messages = [
                {
                    "role": "user",
                    "content": full_prompt
                }
            ]
            
            result = call_doubao(messages)
            if result.get("success"):
                return result.get("content", "").strip()
            else:
                return f"ç¿»è¯‘å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                
        except Exception as e:
            return f"ç¿»è¯‘å‡ºé”™: {str(e)}"


class _PaperAnalyzer:
    """è®ºæ–‡åˆ†æå™¨ï¼Œå¤„ç†å•ç¯‡è®ºæ–‡çš„åˆ†æé€»è¾‘"""
    
    def __init__(self, enable_llm_judge: bool = True):
        self._matcher = _KeywordMatcher()
        self._decision_engine = _MatchDecisionEngine()
        self._llm_judge = _LLMJudge() if enable_llm_judge else None
        self.enable_llm_judge = enable_llm_judge
    
    def analyze_paper(self, pdf_path: str) -> Optional[Dict]:
        """åˆ†æå•ç¯‡è®ºæ–‡ï¼ŒåŒ…å«é”™è¯¯æ¢å¤æœºåˆ¶"""
        filename = os.path.basename(pdf_path)
        
        try:
            # PDFæå–é˜¶æ®µ
            try:
                paper_info = extract_paper_abstract(pdf_path)
            except Exception as e:
                print(f"    âŒ PDFæå–å¤±è´¥: {str(e)}")
                return None
            
            if not paper_info or not paper_info.get('title') or not paper_info.get('abstract'):
                print(f"    âŒ æ— æ³•æå–æœ‰æ•ˆçš„æ ‡é¢˜æˆ–æ‘˜è¦")
                return None
            
            # å…³é”®è¯åŒ¹é…é˜¶æ®µ
            try:
                match_result = self._matcher.match_keywords(
                    paper_info['title'], paper_info['abstract']
                )
            except Exception as e:
                print(f"    âš ï¸ å…³é”®è¯åŒ¹é…å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤åŒ¹é…ç»“æœ")
                match_result = {
                    'matched_keywords': [],
                    'keyword_count': 0,
                    'title_keywords': [],
                    'abstract_keywords': [],
                    'core_match_count': 0,
                    'high_match_count': 0,
                    'medium_match_count': 0,
                    'supporting_match_count': 0
                }
            
            # ç¡®ä¿match_resultåŒ…å«å¿…è¦çš„å­—æ®µ
            if not isinstance(match_result, dict):
                match_result = {
                    'matched_keywords': [],
                    'keyword_count': 0,
                    'title_keywords': [],
                    'abstract_keywords': [],
                    'core_match_count': 0,
                    'high_match_count': 0,
                    'medium_match_count': 0,
                    'supporting_match_count': 0
                }
            
            # åˆ¤æ–­æ˜¯å¦åŒ¹é…
            is_match = self._decision_engine.should_match(match_result)
            match_result['is_match'] = is_match
            
            # åˆå§‹åŒ–å¤§æ¨¡å‹åˆ¤åˆ«ç»“æœ
            llm_summary = ""
            llm_relevance = ""
            chinese_abstract = ""
            
            # å¤§æ¨¡å‹åˆ¤åˆ«é˜¶æ®µï¼ˆå¯é€‰ä¸”å…è®¸å¤±è´¥ï¼‰
            if is_match and self.enable_llm_judge and self._llm_judge:
                print(f"    ğŸ¤– æ‰§è¡Œå¤§æ¨¡å‹åˆ¤åˆ«...")
                
                # æ€»ç»“ç”Ÿæˆ
                try:
                    llm_summary = self._llm_judge.get_summary(
                        paper_info['title'], paper_info['abstract']
                    )
                    print(f"    âœ“ å¤§æ¨¡å‹æ€»ç»“: {llm_summary}")
                except Exception as e:
                    print(f"    âš ï¸ å¤§æ¨¡å‹æ€»ç»“å¤±è´¥: {str(e)}")
                    llm_summary = "å¤§æ¨¡å‹æ€»ç»“ç”Ÿæˆå¤±è´¥"
                
                # ç›¸å…³æ€§åˆ¤æ–­
                try:
                    llm_relevance = self._llm_judge.judge_relevance(
                        paper_info['title'], paper_info['abstract']
                    )
                    print(f"    âœ“ å¤§æ¨¡å‹ç›¸å…³æ€§åˆ¤æ–­: {llm_relevance}")
                except Exception as e:
                    print(f"    âš ï¸ å¤§æ¨¡å‹ç›¸å…³æ€§åˆ¤æ–­å¤±è´¥: {str(e)}")
                    llm_relevance = "å¤§æ¨¡å‹ç›¸å…³æ€§åˆ¤æ–­å¤±è´¥"
                
                # æ‘˜è¦ç¿»è¯‘
                try:
                    print(f"    ğŸŒ æ‰§è¡Œæ‘˜è¦ç¿»è¯‘...")
                    chinese_abstract = self._llm_judge.translate_abstract(
                        paper_info['title'], paper_info['abstract']
                    )
                    print(f"    âœ“ ç¿»è¯‘å®Œæˆ: {chinese_abstract[:100]}...")
                except Exception as e:
                    print(f"    âš ï¸ æ‘˜è¦ç¿»è¯‘å¤±è´¥: {str(e)}")
                    chinese_abstract = "æ‘˜è¦ç¿»è¯‘å¤±è´¥"
            
            # å°†å¤§æ¨¡å‹åˆ¤åˆ«ç»“æœå’Œç¿»è¯‘ç»“æœæ·»åŠ åˆ°match_resultä¸­
            match_result['llm_summary'] = llm_summary
            match_result['llm_relevance'] = llm_relevance
            match_result['chinese_abstract'] = chinese_abstract
            
            paper_info['filename'] = filename
            paper_info['match_info'] = match_result
            
            return paper_info
            
        except Exception as e:
            print(f"    âŒ åˆ†æè®ºæ–‡æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
            return None


class _ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨ï¼Œè´Ÿè´£ç”Ÿæˆå„ç§è¾“å‡ºæŠ¥å‘Šï¼Œæ”¯æŒå¢é‡å†™å…¥"""
    
    def __init__(self, output_dir: str):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç»“æœæ–‡ä»¶å¤¹
        result_folder_name = f"ai_acceleration_analysis_{self.timestamp}"
        self.output_dir = os.path.join(output_dir, result_folder_name)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            print(f"åˆ›å»ºç»“æœè¾“å‡ºæ–‡ä»¶å¤¹: {self.output_dir}")
        
        # åˆå§‹åŒ–å¢é‡å†™å…¥æ–‡ä»¶
        self._init_incremental_files()
    
    def _init_incremental_files(self):
        """åˆå§‹åŒ–å¢é‡å†™å…¥çš„æ–‡ä»¶"""
        # AIç›¸å…³è®ºæ–‡æ–‡ä»¶
        self.ai_papers_txt_file = os.path.join(self.output_dir, "ai_inference_related_papers.txt")
        self.ai_papers_csv_file = os.path.join(self.output_dir, "ai_inference_related_papers.csv")
        
        # éAIç›¸å…³è®ºæ–‡æ–‡ä»¶
        self.non_ai_papers_txt_file = os.path.join(self.output_dir, "non_ai_inference_papers.txt")
        self.non_ai_papers_csv_file = os.path.join(self.output_dir, "non_ai_inference_papers.csv")
        
        # åˆå§‹åŒ–æ–‡æœ¬æ–‡ä»¶
        with open(self.ai_papers_txt_file, 'w', encoding='utf-8') as f:
            f.write("AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡åˆ—è¡¨\n")
            f.write("=" * 50 + "\n\n")
        
        with open(self.non_ai_papers_txt_file, 'w', encoding='utf-8') as f:
            f.write("éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡åˆ—è¡¨\n")
            f.write("=" * 50 + "\n\n")
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶å¤´
        ai_headers = [
            'åºå·', 'æ ‡é¢˜', 'æ–‡ä»¶å', 'ä½œè€…', 'ç»„ç»‡', 'åŒ¹é…åˆ†æ•°',
            'æ ¸å¿ƒå…³é”®å­—æ•°', 'é«˜ç›¸å…³å…³é”®å­—æ•°', 'ä¸­ç­‰å…³é”®å­—æ•°', 'æ”¯æ’‘å…³é”®å­—æ•°',
            'æ ‡é¢˜å…³é”®å­—', 'æ‘˜è¦å…³é”®å­—æ•°é‡', 'å¤§æ¨¡å‹æ€»ç»“', 'å¤§æ¨¡å‹ç›¸å…³æ€§åˆ¤æ–­', 
            'ä¸­æ–‡æ‘˜è¦ç¿»è¯‘', 'æ‘˜è¦é¢„è§ˆ'
        ]
        
        non_ai_headers = [
            'åºå·', 'æ ‡é¢˜', 'æ–‡ä»¶å', 'ä½œè€…', 'ç»„ç»‡', 'åŒ¹é…åˆ†æ•°',
            'å¤§æ¨¡å‹æ€»ç»“', 'å¤§æ¨¡å‹ç›¸å…³æ€§åˆ¤æ–­', 'ä¸­æ–‡æ‘˜è¦ç¿»è¯‘', 'æ‘˜è¦é¢„è§ˆ'
        ]
        
        with open(self.ai_papers_csv_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(ai_headers)
        
        with open(self.non_ai_papers_csv_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(non_ai_headers)
        
        # è®¡æ•°å™¨ï¼Œç”¨äºCSVåºå·
        self.ai_paper_count = 0
        self.non_ai_paper_count = 0
        
        # è¿›åº¦æ–‡ä»¶ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ ï¼ˆå¦‚éœ€è¦ï¼‰
        self.progress_file = os.path.join(self.output_dir, "analysis_progress.txt")
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            f.write(f"åˆ†æå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("AIç›¸å…³è®ºæ–‡æ•°: 0\n")
            f.write("éAIç›¸å…³è®ºæ–‡æ•°: 0\n")
            f.write("å¤„ç†å¤±è´¥æ•°: 0\n")
    
    def _format_paper_info(self, paper: Dict, show_details: bool = True) -> str:
        """æ ¼å¼åŒ–å•ç¯‡è®ºæ–‡ä¿¡æ¯"""
        lines = []
        lines.append(f"   æ–‡ä»¶å: {paper['filename']}")
        
        if paper['authors']:
            authors_str = ", ".join([author.get('name', '') for author in paper['authors'] if author.get('name')])
            lines.append(f"   ä½œè€…: {authors_str}")
            
            # æ”¶é›†å¹¶å»é‡ç»„ç»‡ä¿¡æ¯
            all_affiliations = set()
            for author in paper['authors']:
                if author.get('affiliation'):
                    affiliations = author.get('affiliation', [])
                    for affiliation in affiliations:
                        if affiliation and affiliation.strip():
                            all_affiliations.add(affiliation.strip())
            
            if all_affiliations:
                sorted_affiliations = sorted(list(all_affiliations))
                lines.append(f"   ç»„ç»‡: {'; '.join(sorted_affiliations)}")
        
        if 'match_info' in paper and show_details:
            match_info = paper['match_info']
            lines.append(f"   åŒ¹é…åˆ†æ•°: {match_info['keyword_count']}")
            
            # å…³é”®å­—åˆ†å¸ƒ
            core_count = match_info.get('core_match_count', 0)
            high_count = match_info.get('high_match_count', 0)
            medium_count = match_info.get('medium_match_count', 0)
            supporting_count = match_info.get('supporting_match_count', 0)
            lines.append(f"   å…³é”®å­—åˆ†å¸ƒ - æ ¸å¿ƒ:{core_count}, é«˜ç›¸å…³:{high_count}, ä¸­ç­‰:{medium_count}, æ”¯æ’‘:{supporting_count}")
            
            if match_info['title_keywords']:
                lines.append(f"   æ ‡é¢˜å…³é”®å­—: {', '.join(match_info['title_keywords'])}")
                
            # å¤§æ¨¡å‹åˆ¤åˆ«ç»“æœ
            if match_info.get('llm_summary'):
                lines.append(f"   å¤§æ¨¡å‹æ€»ç»“: {match_info['llm_summary']}")
            if match_info.get('llm_relevance'):
                lines.append(f"   å¤§æ¨¡å‹ç›¸å…³æ€§åˆ¤æ–­: {match_info['llm_relevance']}")
            if match_info.get('chinese_abstract'):
                lines.append(f"   ä¸­æ–‡æ‘˜è¦ç¿»è¯‘: {match_info['chinese_abstract']}")
        
        if paper['abstract']:
            abstract_preview = paper['abstract'][:2000] + "..." if len(paper['abstract']) > 2000 else paper['abstract']
            lines.append(f"   æ‘˜è¦: {abstract_preview}")
        
        return '\n'.join(lines)
    
    def append_ai_paper(self, paper: Dict):
        """å¢é‡å†™å…¥å•ç¯‡AIç›¸å…³è®ºæ–‡"""
        try:
            self.ai_paper_count += 1
            
            # å†™å…¥æ–‡æœ¬æ–‡ä»¶
            with open(self.ai_papers_txt_file, 'a', encoding='utf-8') as f:
                f.write(f"{self.ai_paper_count}. æ ‡é¢˜: {paper['title']}\n")
                f.write(self._format_paper_info(paper))
                f.write("\n" + "-" * 80 + "\n\n")
            
            # å†™å…¥CSVæ–‡ä»¶
            self._append_ai_paper_csv(paper)
            
        except Exception as e:
            print(f"    è­¦å‘Š: å†™å…¥AIç›¸å…³è®ºæ–‡å¤±è´¥: {str(e)}")
    
    def append_non_ai_paper(self, paper: Dict):
        """å¢é‡å†™å…¥å•ç¯‡éAIç›¸å…³è®ºæ–‡"""
        try:
            self.non_ai_paper_count += 1
            
            # å†™å…¥æ–‡æœ¬æ–‡ä»¶
            with open(self.non_ai_papers_txt_file, 'a', encoding='utf-8') as f:
                f.write(f"{self.non_ai_paper_count}. æ ‡é¢˜: {paper['title']}\n")
                f.write(self._format_paper_info(paper, show_details=False))
                
                # æ·»åŠ åˆ†æ•°ä¿¡æ¯
                if 'match_info' in paper:
                    match_info = paper['match_info']
                    f.write(f"\n   åŒ¹é…åˆ†æ•°ä¸è¶³ (åˆ†æ•°: {match_info['keyword_count']})")
                
                f.write("\n" + "-" * 80 + "\n\n")
            
            # å†™å…¥CSVæ–‡ä»¶
            self._append_non_ai_paper_csv(paper)
            
        except Exception as e:
            print(f"    è­¦å‘Š: å†™å…¥éAIç›¸å…³è®ºæ–‡å¤±è´¥: {str(e)}")
    
    def _append_ai_paper_csv(self, paper: Dict):
        """å¢é‡å†™å…¥AIç›¸å…³è®ºæ–‡åˆ°CSV"""
        match_info = paper.get('match_info', {})
        
        # å¤„ç†ä½œè€…ä¿¡æ¯
        authors_str = ""
        organizations_str = ""
        if paper.get('authors'):
            authors = [author.get('name', '') for author in paper['authors'] if author.get('name')]
            authors_str = "; ".join(authors)
            
            # æ”¶é›†ç»„ç»‡ä¿¡æ¯
            all_affiliations = set()
            for author in paper['authors']:
                if author.get('affiliation'):
                    for affiliation in author.get('affiliation', []):
                        if affiliation and affiliation.strip():
                            all_affiliations.add(affiliation.strip())
            organizations_str = "; ".join(sorted(list(all_affiliations)))
        
        # å¤„ç†å…³é”®å­—ä¿¡æ¯
        title_keywords = "; ".join(match_info.get('title_keywords', []))
        abstract_keywords_count = len(match_info.get('abstract_keywords', []))
        
        # æ‘˜è¦é¢„è§ˆ
        abstract_preview = ""
        if paper.get('abstract'):
            abstract_preview = paper['abstract'][:2000] + "..." if len(paper['abstract']) > 2000 else paper['abstract']
            abstract_preview = abstract_preview.replace('\n', ' ').replace('\r', ' ')
        
        # ä¸­æ–‡æ‘˜è¦ç¿»è¯‘
        chinese_abstract = match_info.get('chinese_abstract', '')
        if chinese_abstract:
            chinese_abstract = chinese_abstract.replace('\n', ' ').replace('\r', ' ')
        
        row = [
            self.ai_paper_count,
            paper.get('title', ''),
            paper.get('filename', ''),
            authors_str,
            organizations_str,
            match_info.get('keyword_count', 0),
            match_info.get('core_match_count', 0),
            match_info.get('high_match_count', 0),
            match_info.get('medium_match_count', 0),
            match_info.get('supporting_match_count', 0),
            title_keywords,
            abstract_keywords_count,
            match_info.get('llm_summary', ''),
            match_info.get('llm_relevance', ''),
            chinese_abstract,
            abstract_preview
        ]
        
        with open(self.ai_papers_csv_file, 'a', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(row)
    
    def _append_non_ai_paper_csv(self, paper: Dict):
        """å¢é‡å†™å…¥éAIç›¸å…³è®ºæ–‡åˆ°CSV"""
        match_info = paper.get('match_info', {})
        
        # å¤„ç†ä½œè€…ä¿¡æ¯
        authors_str = ""
        organizations_str = ""
        if paper.get('authors'):
            authors = [author.get('name', '') for author in paper['authors'] if author.get('name')]
            authors_str = "; ".join(authors)
            
            # æ”¶é›†ç»„ç»‡ä¿¡æ¯
            all_affiliations = set()
            for author in paper['authors']:
                if author.get('affiliation'):
                    for affiliation in author.get('affiliation', []):
                        if affiliation and affiliation.strip():
                            all_affiliations.add(affiliation.strip())
            organizations_str = "; ".join(sorted(list(all_affiliations)))
        
        # æ‘˜è¦é¢„è§ˆ
        abstract_preview = ""
        if paper.get('abstract'):
            abstract_preview = paper['abstract'][:2000] + "..." if len(paper['abstract']) > 2000 else paper['abstract']
            abstract_preview = abstract_preview.replace('\n', ' ').replace('\r', ' ')
        
        # ä¸­æ–‡æ‘˜è¦ç¿»è¯‘
        chinese_abstract = match_info.get('chinese_abstract', '')
        if chinese_abstract:
            chinese_abstract = chinese_abstract.replace('\n', ' ').replace('\r', ' ')
        
        row = [
            self.non_ai_paper_count,
            paper.get('title', ''),
            paper.get('filename', ''),
            authors_str,
            organizations_str,
            match_info.get('keyword_count', 0),
            match_info.get('llm_summary', ''),
            match_info.get('llm_relevance', ''),
            chinese_abstract,
            abstract_preview
        ]
        
        with open(self.non_ai_papers_csv_file, 'a', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(row)
    
    def update_progress(self, error_count: int = 0):
        """æ›´æ–°è¿›åº¦æ–‡ä»¶"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                f.write(f"æœ€åæ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"AIç›¸å…³è®ºæ–‡æ•°: {self.ai_paper_count}\n")
                f.write(f"éAIç›¸å…³è®ºæ–‡æ•°: {self.non_ai_paper_count}\n")
                f.write(f"å¤„ç†å¤±è´¥æ•°: {error_count}\n")
                f.write(f"æ€»å¤„ç†æ•°: {self.ai_paper_count + self.non_ai_paper_count}\n")
        except:
            pass  # å¿½ç•¥è¿›åº¦æ–‡ä»¶æ›´æ–°é”™è¯¯
    
    def save_ai_papers(self, papers: List[Dict], output_filename: str = "ai_inference_related_papers.txt"):
        """ä¿å­˜AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡"""
        output_file = os.path.join(self.output_dir, output_filename)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡åˆ—è¡¨\n")
            f.write("=" * 50 + "\n\n")
            
            for i, paper in enumerate(papers, 1):
                f.write(f"{i}. æ ‡é¢˜: {paper['title']}\n")
                f.write(self._format_paper_info(paper))
                f.write("\n" + "-" * 80 + "\n\n")
        
        print(f"AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡å·²ä¿å­˜åˆ° {output_file}")
    
    def save_non_ai_papers(self, papers: List[Dict], output_filename: str = "non_ai_inference_papers.txt"):
        """ä¿å­˜éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡"""
        output_file = os.path.join(self.output_dir, output_filename)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡åˆ—è¡¨\n")
            f.write("=" * 50 + "\n\n")
            
            for i, paper in enumerate(papers, 1):
                f.write(f"{i}. æ ‡é¢˜: {paper['title']}\n")
                f.write(self._format_paper_info(paper, show_details=False))
                
                # æ·»åŠ åˆ†æ•°ä¿¡æ¯
                if 'match_info' in paper:
                    match_info = paper['match_info']
                    f.write(f"\n   åŒ¹é…åˆ†æ•°ä¸è¶³ (åˆ†æ•°: {match_info['keyword_count']})")
                
                f.write("\n" + "-" * 80 + "\n\n")
        
        print(f"éAIæ¨ç†åŠ é€Ÿè®ºæ–‡ç»“æœå·²ä¿å­˜åˆ° {output_file}")
    
    def generate_statistics(self, papers: List[Dict], output_filename: str = "match_statistics.txt"):
        """ç”ŸæˆåŒ¹é…ç»Ÿè®¡æŠ¥å‘Š"""
        if not papers:
            print("æ²¡æœ‰è®ºæ–‡æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š")
            return
        
        output_file = os.path.join(self.output_dir, output_filename)
        
        # æ”¶é›†ç»Ÿè®¡æ•°æ®
        category_stats = {}
        keyword_stats = {}
        priority_stats = {}
        
        for paper in papers:
            if 'match_info' not in paper:
                continue
            
            for kw_info in paper['match_info']['matched_keywords']:
                category = kw_info['category']
                keyword = kw_info['keyword']
                priority = kw_info.get('priority', 'unknown')
                
                category_stats[category] = category_stats.get(category, 0) + 1
                keyword_stats[keyword] = keyword_stats.get(keyword, 0) + 1
                priority_stats[priority] = priority_stats.get(priority, 0) + 1
        
        # å†™å…¥ç»Ÿè®¡æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡åŒ¹é…ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"æ€»åŒ¹é…è®ºæ–‡æ•°: {len(papers)}\n\n")
            
            # ä¼˜å…ˆçº§ç»Ÿè®¡
            f.write("å…³é”®å­—ä¼˜å…ˆçº§ç»Ÿè®¡:\n")
            f.write("-" * 30 + "\n")
            for priority in ['core', 'high', 'medium', 'supporting']:
                if priority in priority_stats:
                    f.write(f"{priority}: {priority_stats[priority]} æ¬¡\n")
            f.write("\n")
            
            # ç±»åˆ«ç»Ÿè®¡
            f.write("å…³é”®å­—ç±»åˆ«ç»Ÿè®¡:\n")
            f.write("-" * 30 + "\n")
            for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
                f.write(f"{category}: {count} æ¬¡\n")
            f.write("\n")
            
            # æœ€å¸¸è§å…³é”®å­—
            f.write("æœ€å¸¸åŒ¹é…çš„å…³é”®å­— (å‰20ä¸ª):\n")
            f.write("-" * 30 + "\n")
            for keyword, count in sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True)[:20]:
                f.write(f"{keyword}: {count} æ¬¡\n")
        
        print(f"åŒ¹é…ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ° {output_file}")
    
    def save_ai_papers_csv(self, papers: List[Dict], output_filename: str = "ai_inference_related_papers.csv"):
        """ä¿å­˜AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡ä¸ºCSVæ ¼å¼"""
        output_file = os.path.join(self.output_dir, output_filename)
        
        if not papers:
            print("æ²¡æœ‰AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡æ•°æ®ï¼Œæ— æ³•ç”ŸæˆCSVæ–‡ä»¶")
            return
        
        with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            headers = [
                'åºå·', 'æ ‡é¢˜', 'æ–‡ä»¶å', 'ä½œè€…', 'ç»„ç»‡', 'åŒ¹é…åˆ†æ•°',
                'æ ¸å¿ƒå…³é”®å­—æ•°', 'é«˜ç›¸å…³å…³é”®å­—æ•°', 'ä¸­ç­‰å…³é”®å­—æ•°', 'æ”¯æ’‘å…³é”®å­—æ•°',
                'æ ‡é¢˜å…³é”®å­—', 'æ‘˜è¦å…³é”®å­—æ•°é‡', 'å¤§æ¨¡å‹æ€»ç»“', 'å¤§æ¨¡å‹ç›¸å…³æ€§åˆ¤æ–­', 
                'ä¸­æ–‡æ‘˜è¦ç¿»è¯‘', 'æ‘˜è¦é¢„è§ˆ'
            ]
            writer.writerow(headers)
            
            # å†™å…¥æ•°æ®
            for i, paper in enumerate(papers, 1):
                match_info = paper.get('match_info', {})
                
                # å¤„ç†ä½œè€…ä¿¡æ¯
                authors_str = ""
                organizations_str = ""
                if paper.get('authors'):
                    authors = [author.get('name', '') for author in paper['authors'] if author.get('name')]
                    authors_str = "; ".join(authors)
                    
                    # æ”¶é›†ç»„ç»‡ä¿¡æ¯
                    all_affiliations = set()
                    for author in paper['authors']:
                        if author.get('affiliation'):
                            for affiliation in author.get('affiliation', []):
                                if affiliation and affiliation.strip():
                                    all_affiliations.add(affiliation.strip())
                    organizations_str = "; ".join(sorted(list(all_affiliations)))
                
                # å¤„ç†å…³é”®å­—ä¿¡æ¯
                title_keywords = "; ".join(match_info.get('title_keywords', []))
                abstract_keywords_count = len(match_info.get('abstract_keywords', []))
                
                # æ‘˜è¦é¢„è§ˆ
                abstract_preview = ""
                if paper.get('abstract'):
                    abstract_preview = paper['abstract'][:2000] + "..." if len(paper['abstract']) > 2000 else paper['abstract']
                    # æ¸…ç†æ‘˜è¦ä¸­çš„æ¢è¡Œç¬¦å’Œé€—å·ï¼Œé¿å…CSVæ ¼å¼é—®é¢˜
                    abstract_preview = abstract_preview.replace('\n', ' ').replace('\r', ' ')
                
                # ä¸­æ–‡æ‘˜è¦ç¿»è¯‘
                chinese_abstract = match_info.get('chinese_abstract', '')
                if chinese_abstract:
                    # æ¸…ç†ç¿»è¯‘ä¸­çš„æ¢è¡Œç¬¦ï¼Œé¿å…CSVæ ¼å¼é—®é¢˜
                    chinese_abstract = chinese_abstract.replace('\n', ' ').replace('\r', ' ')
                
                row = [
                    i,
                    paper.get('title', ''),
                    paper.get('filename', ''),
                    authors_str,
                    organizations_str,
                    match_info.get('keyword_count', 0),
                    match_info.get('core_match_count', 0),
                    match_info.get('high_match_count', 0),
                    match_info.get('medium_match_count', 0),
                    match_info.get('supporting_match_count', 0),
                    title_keywords,
                    abstract_keywords_count,
                    match_info.get('llm_summary', ''),
                    match_info.get('llm_relevance', ''),
                    chinese_abstract,
                    abstract_preview
                ]
                writer.writerow(row)
        
        print(f"AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡CSVæ–‡ä»¶å·²ä¿å­˜åˆ° {output_file}")
    
    def save_non_ai_papers_csv(self, papers: List[Dict], output_filename: str = "non_ai_inference_papers.csv"):
        """ä¿å­˜éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡ä¸ºCSVæ ¼å¼"""
        output_file = os.path.join(self.output_dir, output_filename)
        
        if not papers:
            print("æ²¡æœ‰éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡æ•°æ®ï¼Œæ— æ³•ç”ŸæˆCSVæ–‡ä»¶")
            return
        
        with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            headers = [
                'åºå·', 'æ ‡é¢˜', 'æ–‡ä»¶å', 'ä½œè€…', 'ç»„ç»‡', 'åŒ¹é…åˆ†æ•°',
                'å¤§æ¨¡å‹æ€»ç»“', 'å¤§æ¨¡å‹ç›¸å…³æ€§åˆ¤æ–­', 'ä¸­æ–‡æ‘˜è¦ç¿»è¯‘', 'æ‘˜è¦é¢„è§ˆ'
            ]
            writer.writerow(headers)
            
            # å†™å…¥æ•°æ®
            for i, paper in enumerate(papers, 1):
                match_info = paper.get('match_info', {})
                
                # å¤„ç†ä½œè€…ä¿¡æ¯
                authors_str = ""
                organizations_str = ""
                if paper.get('authors'):
                    authors = [author.get('name', '') for author in paper['authors'] if author.get('name')]
                    authors_str = "; ".join(authors)
                    
                    # æ”¶é›†ç»„ç»‡ä¿¡æ¯
                    all_affiliations = set()
                    for author in paper['authors']:
                        if author.get('affiliation'):
                            for affiliation in author.get('affiliation', []):
                                if affiliation and affiliation.strip():
                                    all_affiliations.add(affiliation.strip())
                    organizations_str = "; ".join(sorted(list(all_affiliations)))
                
                # æ‘˜è¦é¢„è§ˆ
                abstract_preview = ""
                if paper.get('abstract'):
                    abstract_preview = paper['abstract'][:2000] + "..." if len(paper['abstract']) > 2000 else paper['abstract']
                    # æ¸…ç†æ‘˜è¦ä¸­çš„æ¢è¡Œç¬¦ï¼Œé¿å…CSVæ ¼å¼é—®é¢˜
                    abstract_preview = abstract_preview.replace('\n', ' ').replace('\r', ' ')
                
                # ä¸­æ–‡æ‘˜è¦ç¿»è¯‘ï¼ˆéAIç›¸å…³è®ºæ–‡é€šå¸¸ä¸ºç©ºï¼‰
                chinese_abstract = match_info.get('chinese_abstract', '')
                if chinese_abstract:
                    # æ¸…ç†ç¿»è¯‘ä¸­çš„æ¢è¡Œç¬¦ï¼Œé¿å…CSVæ ¼å¼é—®é¢˜
                    chinese_abstract = chinese_abstract.replace('\n', ' ').replace('\r', ' ')
                
                row = [
                    i,
                    paper.get('title', ''),
                    paper.get('filename', ''),
                    authors_str,
                    organizations_str,
                    match_info.get('keyword_count', 0),
                    match_info.get('llm_summary', ''),
                    match_info.get('llm_relevance', ''),
                    chinese_abstract,
                    abstract_preview
                ]
                writer.writerow(row)
        
        print(f"éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡CSVæ–‡ä»¶å·²ä¿å­˜åˆ° {output_file}")
    
    def save_statistics_csv(self, papers: List[Dict], output_filename: str = "match_statistics.csv"):
        """ç”ŸæˆåŒ¹é…ç»Ÿè®¡æŠ¥å‘ŠCSVæ ¼å¼"""
        if not papers:
            print("æ²¡æœ‰è®ºæ–‡æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆç»Ÿè®¡CSVæŠ¥å‘Š")
            return
        
        output_file = os.path.join(self.output_dir, output_filename)
        
        # æ”¶é›†ç»Ÿè®¡æ•°æ®
        category_stats = {}
        keyword_stats = {}
        priority_stats = {}
        
        for paper in papers:
            if 'match_info' not in paper:
                continue
            
            for kw_info in paper['match_info']['matched_keywords']:
                category = kw_info['category']
                keyword = kw_info['keyword']
                priority = kw_info.get('priority', 'unknown')
                
                category_stats[category] = category_stats.get(category, 0) + 1
                keyword_stats[keyword] = keyword_stats.get(keyword, 0) + 1
                priority_stats[priority] = priority_stats.get(priority, 0) + 1
        
        with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            writer.writerow(['ç»Ÿè®¡ç±»åˆ«', 'é¡¹ç›®', 'æ•°é‡'])
            writer.writerow(['æ€»ä½“', 'åŒ¹é…è®ºæ–‡æ€»æ•°', len(papers)])
            writer.writerow([])  # ç©ºè¡Œ
            
            # ä¼˜å…ˆçº§ç»Ÿè®¡
            writer.writerow(['ä¼˜å…ˆçº§ç»Ÿè®¡', '', ''])
            for priority in ['core', 'high', 'medium', 'supporting']:
                if priority in priority_stats:
                    writer.writerow(['ä¼˜å…ˆçº§', priority, priority_stats[priority]])
            writer.writerow([])  # ç©ºè¡Œ
            
            # ç±»åˆ«ç»Ÿè®¡
            writer.writerow(['ç±»åˆ«ç»Ÿè®¡', '', ''])
            for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
                writer.writerow(['ç±»åˆ«', category, count])
            writer.writerow([])  # ç©ºè¡Œ
            
            # æœ€å¸¸è§å…³é”®å­—
            writer.writerow(['å…³é”®å­—ç»Ÿè®¡ (å‰20ä¸ª)', '', ''])
            for keyword, count in sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True)[:20]:
                writer.writerow(['å…³é”®å­—', keyword, count])
        
        print(f"åŒ¹é…ç»Ÿè®¡CSVæŠ¥å‘Šå·²ä¿å­˜åˆ° {output_file}")
    
    def finalize_reports(self, ai_papers: List[Dict], non_ai_papers: List[Dict]):
        """ç”Ÿæˆæœ€ç»ˆçš„ç»Ÿè®¡æŠ¥å‘Š"""
        try:
            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            if ai_papers:
                statistics_file = os.path.join(self.output_dir, "match_statistics.txt")
                self.generate_statistics(ai_papers, "match_statistics.txt")
                
                statistics_csv_file = os.path.join(self.output_dir, "match_statistics.csv")
                self.save_statistics_csv(ai_papers, "match_statistics.csv")
            
            # ç”Ÿæˆæ±‡æ€»ä¿¡æ¯æ–‡ä»¶
            summary_file = os.path.join(self.output_dir, "analysis_summary.txt")
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("AIæ¨ç†åŠ é€Ÿè®ºæ–‡åˆ†ææ±‡æ€»\n")
                f.write("=" * 50 + "\n\n")
                f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»å¤„ç†è®ºæ–‡æ•°: {len(ai_papers) + len(non_ai_papers)}\n")
                f.write(f"AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {len(ai_papers)}\n")
                f.write(f"éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {len(non_ai_papers)}\n\n")
                
                if ai_papers:
                    f.write("AIç›¸å…³è®ºæ–‡åˆ—è¡¨:\n")
                    f.write("-" * 30 + "\n")
                    for i, paper in enumerate(ai_papers, 1):
                        match_score = paper.get('match_info', {}).get('keyword_count', 0)
                        f.write(f"{i}. {paper['title']} (åŒ¹é…åˆ†æ•°: {match_score})\n")
                    f.write("\n")
                
                f.write("æ–‡ä»¶è¯´æ˜:\n")
                f.write("-" * 30 + "\n")
                f.write("1. ai_inference_related_papers.txt - AIç›¸å…³è®ºæ–‡è¯¦ç»†ä¿¡æ¯\n")
                f.write("2. ai_inference_related_papers.csv - AIç›¸å…³è®ºæ–‡CSVæ ¼å¼\n")
                f.write("3. non_ai_inference_papers.txt - éAIç›¸å…³è®ºæ–‡è¯¦ç»†ä¿¡æ¯\n")
                f.write("4. non_ai_inference_papers.csv - éAIç›¸å…³è®ºæ–‡CSVæ ¼å¼\n")
                f.write("5. match_statistics.txt - åŒ¹é…ç»Ÿè®¡æŠ¥å‘Š\n")
                f.write("6. match_statistics.csv - åŒ¹é…ç»Ÿè®¡CSVæ ¼å¼\n")
                f.write("7. analysis_summary.txt - æœ¬æ±‡æ€»æ–‡ä»¶\n")
            
            print(f"åˆ†ææ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ° {summary_file}")
            
        except Exception as e:
            print(f"è­¦å‘Š: ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")


class AiAccelerationExtractor:
    """AIåŠ é€Ÿè®ºæ–‡æå–å™¨ä¸»ç±»"""
    
    def __init__(self, papers_dir: str, output_dir: str = ".", enable_llm_judge: bool = True):
        """
        åˆå§‹åŒ–AIåŠ é€Ÿè®ºæ–‡æå–å™¨
        
        Args:
            papers_dir: è®ºæ–‡PDFæ–‡ä»¶æ‰€åœ¨ç›®å½•
            output_dir: è¾“å‡ºæ–‡ä»¶ä¿å­˜ç›®å½•
            enable_llm_judge: æ˜¯å¦å¯ç”¨å¤§æ¨¡å‹åˆ¤åˆ«åŠŸèƒ½
        """
        self.papers_dir = papers_dir
        self.output_dir = output_dir
        self.enable_llm_judge = enable_llm_judge
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        self._analyzer = _PaperAnalyzer(enable_llm_judge)
        self._report_generator = _ReportGenerator(output_dir)
    
    def _get_paper_files(self, paper_filenames: List[str] = None, analyze_all: bool = False) -> List[str]:
        """è·å–è¦åˆ†æçš„è®ºæ–‡æ–‡ä»¶åˆ—è¡¨"""
        if analyze_all:
            if not os.path.exists(self.papers_dir):
                print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®ºæ–‡æ–‡ä»¶å¤¹ {self.papers_dir}")
                return []
            
            paper_files = [f for f in os.listdir(self.papers_dir) if f.lower().endswith('.pdf')]
            print(f"ä» {self.papers_dir} æ–‡ä»¶å¤¹åŠ è½½äº† {len(paper_files)} ä¸ªPDFè®ºæ–‡æ–‡ä»¶")
            return paper_files
        else:
            return paper_filenames or []
    
    def _print_analysis_progress(self, paper: Dict, processed_count: int, total_count: int, saved_to_disk: bool = False):
        """æ‰“å°åˆ†æè¿›åº¦ä¿¡æ¯"""
        filename = paper['filename']
        match_info = paper['match_info']
        
        print(f"æ­£åœ¨å¤„ç† ({processed_count}/{total_count}): {filename}")
        
        if match_info['is_match']:
            title_kw = ", ".join(match_info['title_keywords']) if match_info['title_keywords'] else "æ— "
            abstract_kw_count = len(match_info['abstract_keywords'])
            core_count = match_info.get('core_match_count', 0)
            high_count = match_info.get('high_match_count', 0)
            medium_count = match_info.get('medium_match_count', 0)
            
            print(f"  âœ“ å‘ç°AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {paper['title'][:60]}...")
            print(f"    æ ‡é¢˜å…³é”®å­—: {title_kw}")
            print(f"    æ‘˜è¦å…³é”®å­—æ•°é‡: {abstract_kw_count}")
            print(f"    æ€»åŒ¹é…åˆ†æ•°: {match_info['keyword_count']}")
            print(f"    å…³é”®å­—åˆ†å¸ƒ - æ ¸å¿ƒ:{core_count}, é«˜ç›¸å…³:{high_count}, ä¸­ç­‰:{medium_count}")
            
            # æ˜¾ç¤ºå¤§æ¨¡å‹åˆ¤åˆ«ç»“æœ
            if self.enable_llm_judge:
                if match_info.get('llm_summary'):
                    print(f"    å¤§æ¨¡å‹æ€»ç»“: {match_info['llm_summary']}")
                if match_info.get('llm_relevance'):
                    print(f"    å¤§æ¨¡å‹ç›¸å…³æ€§: {match_info['llm_relevance']}")
            
            if saved_to_disk:
                print(f"    âœ“ å·²ä¿å­˜åˆ°æ–‡ä»¶")
        else:
            print(f"  - éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡ (åŒ¹é…åˆ†æ•°: {match_info['keyword_count']})")
            if saved_to_disk:
                print(f"    âœ“ å·²ä¿å­˜åˆ°æ–‡ä»¶")
    
    def _analyze_papers(self, paper_filenames: List[str] = None, analyze_all: bool = True) -> Dict[str, List[Dict]]:
        """
        åˆ†æè®ºæ–‡ï¼Œç­›é€‰å‡ºä¸AIæ¨ç†åŠ é€Ÿç›¸å…³å’Œéç›¸å…³çš„è®ºæ–‡ï¼Œæ”¯æŒå¢é‡å†™å…¥å’Œé”™è¯¯æ¢å¤
        
        Args:
            paper_filenames: è¦åˆ†æçš„è®ºæ–‡æ–‡ä»¶ååˆ—è¡¨
            analyze_all: æ˜¯å¦åˆ†æpapers_dirä¸‹çš„å…¨é‡è®ºæ–‡
        
        Returns:
            åŒ…å«'ai_related'å’Œ'non_ai_related'ä¸¤ä¸ªé”®çš„å­—å…¸ï¼Œå€¼ä¸ºè®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        # è·å–è¦åˆ†æçš„æ–‡ä»¶åˆ—è¡¨
        files_to_analyze = self._get_paper_files(paper_filenames, analyze_all)
        
        if not files_to_analyze:
            if analyze_all:
                print("æœªæ‰¾åˆ°è¦åˆ†æçš„è®ºæ–‡æ–‡ä»¶")
            else:
                print("é”™è¯¯: éœ€è¦æä¾›è®ºæ–‡æ–‡ä»¶ååˆ—è¡¨æˆ–è®¾ç½®analyze_all=True")
            return {'ai_related': [], 'non_ai_related': []}
        
        # åˆ†æè®ºæ–‡å¹¶å¢é‡å†™å…¥
        ai_related_papers = []
        non_ai_related_papers = []
        processed_count = 0
        error_count = 0
        
        paper_type = "è®ºæ–‡" if analyze_all else "æŒ‡å®šè®ºæ–‡"
        print(f"\nå¼€å§‹åˆ†æ {len(files_to_analyze)} ä¸ª{paper_type}...")
        print(f"åˆ†æç»“æœå°†å®æ—¶ä¿å­˜åˆ°: {self._report_generator.output_dir}")
        
        for i, filename in enumerate(files_to_analyze, 1):
            saved_to_disk = False
            try:
                pdf_path = os.path.join(self.papers_dir, filename)
                paper_info = self._analyzer.analyze_paper(pdf_path)
                
                if paper_info is None:
                    print(f"  è­¦å‘Š: æ— æ³•æå–æ ‡é¢˜æˆ–æ‘˜è¦ï¼Œè·³è¿‡ {filename}")
                    error_count += 1
                    continue
                
                # ç«‹å³å†™å…¥ç£ç›˜
                if paper_info['match_info']['is_match']:
                    ai_related_papers.append(paper_info)
                    self._report_generator.append_ai_paper(paper_info)
                    saved_to_disk = True
                else:
                    non_ai_related_papers.append(paper_info)
                    self._report_generator.append_non_ai_paper(paper_info)
                    saved_to_disk = True
                
                processed_count += 1
                
                # æ‰“å°è¿›åº¦ä¿¡æ¯
                self._print_analysis_progress(paper_info, i, len(files_to_analyze), saved_to_disk)
                
                # æ¯å¤„ç†10ç¯‡è®ºæ–‡è¾“å‡ºä¸€æ¬¡è¿›åº¦æ€»ç»“å¹¶æ›´æ–°è¿›åº¦æ–‡ä»¶
                if processed_count % 10 == 0:
                    self._report_generator.update_progress(error_count)
                    print(f"\nğŸ“Š è¿›åº¦æ€»ç»“ ({processed_count}/{len(files_to_analyze)} å·²å¤„ç†):")
                    print(f"    âœ… æˆåŠŸå¤„ç†: {processed_count} ç¯‡")
                    print(f"    âŒ å¤„ç†å¤±è´¥: {error_count} ç¯‡") 
                    print(f"    ğŸ¯ AIç›¸å…³: {len(ai_related_papers)} ç¯‡")
                    print(f"    ğŸ“„ å…¶ä»–: {len(non_ai_related_papers)} ç¯‡")
                    print(f"    ğŸ’¾ æ‰€æœ‰ç»“æœå·²å®æ—¶ä¿å­˜åˆ°ç£ç›˜\n")
                
            except Exception as e:
                error_count += 1
                print(f"  âŒ é”™è¯¯: å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}")
                print(f"    å°†è·³è¿‡æ­¤æ–‡ä»¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")
                
                # å†™å…¥é”™è¯¯æ—¥å¿—å¹¶æ›´æ–°è¿›åº¦
                try:
                    error_log_file = os.path.join(self._report_generator.output_dir, "error_log.txt")
                    with open(error_log_file, 'a', encoding='utf-8') as f:
                        f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - å¤„ç†æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}\n")
                    self._report_generator.update_progress(error_count)
                except:
                    pass  # å¿½ç•¥æ—¥å¿—å†™å…¥é”™è¯¯
                
                continue
        
        # æœ€ç»ˆè¿›åº¦æ›´æ–°
        self._report_generator.update_progress(error_count)
        
        # æ‰“å°æœ€ç»ˆæ€»ç»“
        print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {len(files_to_analyze)}")
        print(f"âœ… æˆåŠŸå¤„ç†: {processed_count} ç¯‡")
        print(f"âŒ å¤„ç†å¤±è´¥: {error_count} ç¯‡")
        print(f"ğŸ¯ AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {len(ai_related_papers)} ç¯‡")
        print(f"ğŸ“„ éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {len(non_ai_related_papers)} ç¯‡")
        print(f"ğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self._report_generator.output_dir}")
        
        return {
            'ai_related': ai_related_papers,
            'non_ai_related': non_ai_related_papers
        }
    
    def parse(self, paper_filenames: List[str] = None, analyze_all: bool = True,
              output_format: str = "both"):
        """
        è§£æè®ºæ–‡å¹¶ä¿å­˜ç»“æœ
        
        Args:
            paper_filenames: è¦åˆ†æçš„è®ºæ–‡æ–‡ä»¶ååˆ—è¡¨
            analyze_all: æ˜¯å¦åˆ†æå…¨é‡è®ºæ–‡
            output_format: è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰ "txt", "csv", "both"
        """
        print(f"\nğŸš€ å¼€å§‹AIæ¨ç†åŠ é€Ÿè®ºæ–‡åˆ†æ...")
        print(f"ğŸ“ åˆ†æç›®å½•: {self.papers_dir}")
        print(f"ğŸ“¤ è¾“å‡ºåŸºç¡€ç›®å½•: {self.output_dir}")
        print(f"ğŸ“¤ ç»“æœä¿å­˜ç›®å½•: {self._report_generator.output_dir}")
        print(f"ğŸ“Š è¾“å‡ºæ ¼å¼: {output_format}")
        print(f"ğŸ¯ åŒ¹é…é˜ˆå€¼: æƒé‡>=5åˆ†å³åŒ¹é…æˆåŠŸ")
        print(f"ğŸ” åŒ¹é…é€»è¾‘: çº¯å…³é”®è¯æƒé‡åŒ¹é…ï¼Œæ— æ’é™¤æœºåˆ¶")
        print(f"ğŸ¤– å¤§æ¨¡å‹åˆ¤åˆ«: {'å¯ç”¨' if self.enable_llm_judge else 'ç¦ç”¨'}")
        if self.enable_llm_judge:
            print(f"    å¯¹äºåˆç­›ç›¸å…³çš„è®ºæ–‡ï¼Œå°†è°ƒç”¨è±†åŒ…APIè¿›è¡Œæ€»ç»“ã€ç›¸å…³æ€§åˆ¤æ–­å’Œæ‘˜è¦ç¿»è¯‘")
        
        # åˆ†æè®ºæ–‡ï¼ˆç»“æœå·²åœ¨åˆ†æè¿‡ç¨‹ä¸­å®æ—¶å†™å…¥ç£ç›˜ï¼‰
        results = self._analyze_papers(paper_filenames, analyze_all)
        
        ai_papers = results['ai_related']
        non_ai_papers = results['non_ai_related']
        
        # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Šï¼ˆè®ºæ–‡è¯¦æƒ…å·²åœ¨åˆ†æè¿‡ç¨‹ä¸­å†™å…¥ï¼‰
        print(f"\nğŸ“Š ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š...")
        self._report_generator.finalize_reports(ai_papers, non_ai_papers)
        
        if ai_papers:
            # æ‰“å°ç®€è¦ç»Ÿè®¡
            print(f"\nâœ¨ å‘ç°çš„AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡:")
            for i, paper in enumerate(ai_papers, 1):
                match_score = paper.get('match_info', {}).get('keyword_count', 0)
                print(f"{i}. {paper['title']} (åŒ¹é…åˆ†æ•°: {match_score})")
        else:
            print("\nğŸ“­ æœªå‘ç°AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡")
        
        if non_ai_papers:
            print(f"\nğŸ“„ éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {len(non_ai_papers)} ç¯‡")
        else:
            print("\nğŸ¯ æ‰€æœ‰è®ºæ–‡éƒ½ä¸AIæ¨ç†åŠ é€Ÿç›¸å…³")
        
        # æ‰“å°æ€»ç»“ä¿¡æ¯
        print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        print(f"ğŸ“‚ æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ°: {self._report_generator.output_dir}")
        print(f"ğŸ” å…±å¤„ç† {len(ai_papers) + len(non_ai_papers)} ç¯‡è®ºæ–‡")
        print(f"âœ¨ AIæ¨ç†åŠ é€Ÿç›¸å…³: {len(ai_papers)} ç¯‡")
        print(f"ğŸ“„ å…¶ä»–è®ºæ–‡: {len(non_ai_papers)} ç¯‡")
        print(f"ğŸ’¾ æ‰€æœ‰ç»“æœåœ¨å¤„ç†è¿‡ç¨‹ä¸­å·²å®æ—¶ä¿å­˜ï¼Œå³ä½¿å‡ºç°ä¸­æ–­ä¹Ÿä¸ä¼šä¸¢å¤±æ•°æ®")


def ai_acceleration_parse(papers_dir: str, output_dir: str = ".", 
                         paper_filenames: List[str] = None, analyze_all: bool = True,
                         output_format: str = "both", enable_llm_judge: bool = True):
    """
    å¯¹å¤–æä¾›çš„AIæ¨ç†åŠ é€Ÿè®ºæ–‡è§£æå‡½æ•°
    
    Args:
        papers_dir: è®ºæ–‡PDFæ–‡ä»¶æ‰€åœ¨ç›®å½•
        output_dir: è¾“å‡ºæ–‡ä»¶ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
        paper_filenames: è¦åˆ†æçš„è®ºæ–‡æ–‡ä»¶ååˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneä¸”analyze_allä¸ºTrueåˆ™åˆ†ææ‰€æœ‰è®ºæ–‡
        analyze_all: æ˜¯å¦åˆ†æpapers_dirä¸‹çš„å…¨é‡è®ºæ–‡ï¼Œé»˜è®¤ä¸ºFalse
        output_format: è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰ "txt"ï¼ˆé»˜è®¤ï¼‰, "csv", "both"
        enable_llm_judge: æ˜¯å¦å¯ç”¨å¤§æ¨¡å‹åˆ¤åˆ«åŠŸèƒ½ï¼Œé»˜è®¤ä¸ºTrue
    
    Returns:
        None
    """
    extractor = AiAccelerationExtractor(papers_dir, output_dir, enable_llm_judge)
    extractor.parse(paper_filenames, analyze_all, output_format)
   