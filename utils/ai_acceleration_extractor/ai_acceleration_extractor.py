"""
é‡æ„åçš„AIæ¨ç†åŠ é€Ÿè®ºæ–‡æå–å™¨ä¸»æ¨¡å—
"""

import os
import logging
from typing import List, Dict, Optional, Union
from datetime import datetime
from dataclasses import dataclass

from .models import AnalysisConfig, AnalysisResult, PaperInfo, ProgressInfo
from .paper_analyzer import PaperAnalyzer
from .report_generator import ReportGenerator
from .exceptions import AIAccelerationExtractorError, ConfigurationError


@dataclass
class AnalysisParams:
    """åˆ†æå‚æ•°æ•°æ®ç±»"""
    paper_filenames: Optional[List[str]] = None
    analyze_all: bool = True
    output_format: str = "both"
    paper_infos: Optional[List[Dict]] = None


class AnalysisLogger:
    """åˆ†ææ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, config: AnalysisConfig):
        self.config = config
        self._setup_logging()
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('analysis.log', encoding='utf-8')
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def print_header(self, mode_name: str = ""):
        """æ‰“å°åˆ†æå¤´éƒ¨ä¿¡æ¯"""
        mode_suffix = f" ({mode_name}æ¨¡å¼)" if mode_name else ""
        self.logger.info(f"ğŸš€ å¼€å§‹AIæ¨ç†åŠ é€Ÿè®ºæ–‡åˆ†æ{mode_suffix}...")
        
        if self.config.papers_dir:
            self.logger.info(f"ğŸ“ åˆ†æç›®å½•: {self.config.papers_dir}")
        self.logger.info(f"ğŸ“¤ è¾“å‡ºåŸºç¡€ç›®å½•: {self.config.output_dir}")
        self.logger.info(f"ğŸ¯ åŒ¹é…é˜ˆå€¼: æƒé‡>={self.config.match_threshold}åˆ†å³åŒ¹é…æˆåŠŸ")
        self.logger.info(f"ğŸ” åŒ¹é…é€»è¾‘: çº¯å…³é”®è¯æƒé‡åŒ¹é…ï¼Œæ— æ’é™¤æœºåˆ¶")
        self.logger.info(f"ğŸ¤– å¤§æ¨¡å‹åˆ¤åˆ«: {'å¯ç”¨' if self.config.enable_llm_judge else 'ç¦ç”¨'}")
        if self.config.enable_llm_judge:
            self.logger.info(f"    å¯¹äºåˆç­›ç›¸å…³çš„è®ºæ–‡ï¼Œå°†è°ƒç”¨è±†åŒ…APIè¿›è¡Œæ€»ç»“ã€ç›¸å…³æ€§åˆ¤æ–­å’Œæ‘˜è¦ç¿»è¯‘")
    
    def print_progress(self, paper: PaperInfo, processed_count: int, 
                      total_count: int, saved_to_disk: bool = False):
        """æ‰“å°åˆ†æè¿›åº¦ä¿¡æ¯"""
        self.logger.info(f"æ­£åœ¨å¤„ç† ({processed_count}/{total_count}): {paper.filename}")
        
        if paper.match_info and paper.match_info.is_match:
            self._print_ai_paper_info(paper, saved_to_disk)
        else:
            self._print_non_ai_paper_info(paper, saved_to_disk)
    
    def _print_ai_paper_info(self, paper: PaperInfo, saved_to_disk: bool):
        """æ‰“å°AIç›¸å…³è®ºæ–‡ä¿¡æ¯"""
        title_kw = ", ".join(paper.match_info.title_keywords) if paper.match_info.title_keywords else "æ— "
        abstract_kw_count = len(paper.match_info.abstract_keywords)
        core_count = paper.match_info.core_match_count
        high_count = paper.match_info.high_match_count
        medium_count = paper.match_info.medium_match_count
        
        self.logger.info(f"  âœ“ å‘ç°AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {paper.title}...")
        self.logger.info(f"    æ ‡é¢˜å…³é”®å­—: {title_kw}")
        self.logger.info(f"    æ‘˜è¦å…³é”®å­—æ•°é‡: {abstract_kw_count}")
        self.logger.info(f"    æ€»åŒ¹é…åˆ†æ•°: {paper.match_info.keyword_count}")
        self.logger.info(f"    å…³é”®å­—åˆ†å¸ƒ - æ ¸å¿ƒ:{core_count}, é«˜ç›¸å…³:{high_count}, ä¸­ç­‰:{medium_count}")
        
        if self.config.enable_llm_judge:
            if paper.match_info.llm_summary:
                self.logger.info(f"    å¤§æ¨¡å‹æ€»ç»“: {paper.match_info.llm_summary}")
            if paper.match_info.llm_relevance:
                self.logger.info(f"    å¤§æ¨¡å‹ç›¸å…³æ€§: {paper.match_info.llm_relevance}")
        
        if saved_to_disk:
            self.logger.info(f"    âœ“ å·²ä¿å­˜åˆ°æ–‡ä»¶")
    
    def _print_non_ai_paper_info(self, paper: PaperInfo, saved_to_disk: bool):
        """æ‰“å°éAIç›¸å…³è®ºæ–‡ä¿¡æ¯"""
        match_score = paper.match_info.keyword_count if paper.match_info else 0
        self.logger.info(f"  - éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡ (åŒ¹é…åˆ†æ•°: {match_score})")
        if saved_to_disk:
            self.logger.info(f"    âœ“ å·²ä¿å­˜åˆ°æ–‡ä»¶")
    
    def print_progress_summary(self, progress: ProgressInfo):
        """æ‰“å°è¿›åº¦æ€»ç»“"""
        self.logger.info(f"\nğŸ“Š è¿›åº¦æ€»ç»“ ({progress.current}/{progress.total} å·²å¤„ç†):")
        self.logger.info(f"    âœ… æˆåŠŸå¤„ç†: {progress.success_count} ç¯‡")
        self.logger.info(f"    âŒ å¤„ç†å¤±è´¥: {progress.error_count} ç¯‡") 
        self.logger.info(f"    ğŸ¯ AIåŠ é€Ÿç›¸å…³: {progress.ai_related_count} ç¯‡")
        self.logger.info(f"    ğŸ“„ å…¶ä»–: {progress.non_ai_related_count} ç¯‡")
        self.logger.info(f"    ğŸ’¾ æ‰€æœ‰ç»“æœå·²å®æ—¶ä¿å­˜åˆ°ç£ç›˜\n")
    
    def print_final_summary(self, analysis_result: AnalysisResult, output_dir: str):
        """æ‰“å°æœ€ç»ˆæ€»ç»“"""
        self.logger.info(f"\nğŸ‰ åˆ†æå®Œæˆ!")
        self.logger.info(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {analysis_result.total_count}")
        self.logger.info(f"âœ… æˆåŠŸå¤„ç†: {analysis_result.processed_count} ç¯‡")
        self.logger.info(f"âŒ å¤„ç†å¤±è´¥: {analysis_result.error_count} ç¯‡")
        self.logger.info(f"ğŸ¯ AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {analysis_result.ai_related_count} ç¯‡")
        self.logger.info(f"ğŸ“„ éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {analysis_result.non_ai_related_count} ç¯‡")
        self.logger.info(f"ğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def print_analysis_results(self, analysis_result: AnalysisResult, output_dir: str):
        """æ‰“å°åˆ†æç»“æœ"""
        if analysis_result.ai_related_papers:
            self.logger.info(f"\nâœ¨ å‘ç°çš„AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡:")
            for i, paper in enumerate(analysis_result.ai_related_papers, 1):
                match_score = paper.match_info.keyword_count if paper.match_info else 0
                self.logger.info(f"{i}. {paper.title} (åŒ¹é…åˆ†æ•°: {match_score})")
        else:
            self.logger.info("\nğŸ“­ æœªå‘ç°AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡")
        
        if analysis_result.non_ai_related_papers:
            self.logger.info(f"\nğŸ“„ éAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {len(analysis_result.non_ai_related_papers)} ç¯‡")
        else:
            self.logger.info("\nğŸ¯ æ‰€æœ‰è®ºæ–‡éƒ½ä¸AIæ¨ç†åŠ é€Ÿç›¸å…³")
        
        self.logger.info(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        self.logger.info(f"ğŸ“‚ æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")
        self.logger.info(f"ğŸ” å…±å¤„ç† {analysis_result.total_count} ç¯‡è®ºæ–‡")
        self.logger.info(f"âœ¨ AIæ¨ç†åŠ é€Ÿç›¸å…³: {analysis_result.ai_related_count} ç¯‡")
        self.logger.info(f"ğŸ“„ å…¶ä»–è®ºæ–‡: {analysis_result.non_ai_related_count} ç¯‡")
        self.logger.info(f"ğŸ’¾ æ‰€æœ‰ç»“æœåœ¨å¤„ç†è¿‡ç¨‹ä¸­å·²å®æ—¶ä¿å­˜ï¼Œå³ä½¿å‡ºç°ä¸­æ–­ä¹Ÿä¸ä¼šä¸¢å¤±æ•°æ®")


class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, total: int):
        self.progress = ProgressInfo(
            current=0,
            total=total,
            success_count=0,
            error_count=0,
            ai_related_count=0,
            non_ai_related_count=0,
            start_time=datetime.now()
        )
    
    def update(self, paper_info: Optional[PaperInfo], is_error: bool = False):
        """æ›´æ–°è¿›åº¦"""
        if is_error:
            self.progress.error_count += 1
        elif paper_info:
            self.progress.success_count += 1
            if paper_info.match_info and paper_info.match_info.is_match:
                self.progress.ai_related_count += 1
            else:
                self.progress.non_ai_related_count += 1
        
        self.progress.current += 1
        self.progress.last_update = datetime.now()
    
    def get_progress(self) -> ProgressInfo:
        """è·å–å½“å‰è¿›åº¦"""
        return self.progress


class AiAccelerationExtractor:
    """AIæ¨ç†åŠ é€Ÿè®ºæ–‡æå–å™¨ä¸»ç±»"""
    
    def __init__(self, papers_dir: str = None, output_dir: str = ".", 
                 enable_llm_judge: bool = True, match_threshold: int = 5,
                 analysis_mode: str = "pdf"):
        """
        åˆå§‹åŒ–AIæ¨ç†åŠ é€Ÿè®ºæ–‡æå–å™¨
        
        Args:
            papers_dir: è®ºæ–‡PDFæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆä»…åœ¨PDFæ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
            output_dir: è¾“å‡ºæ–‡ä»¶ä¿å­˜ç›®å½•
            enable_llm_judge: æ˜¯å¦å¯ç”¨å¤§æ¨¡å‹åˆ¤åˆ«åŠŸèƒ½
            match_threshold: åŒ¹é…é˜ˆå€¼
            analysis_mode: åˆ†ææ¨¡å¼ï¼Œ"pdf"æˆ–"paper_copilot"
        """
        self._setup_config(papers_dir, output_dir, enable_llm_judge, match_threshold, analysis_mode)
        self._setup_components()
        self.logger = AnalysisLogger(self.config)
    
    def _setup_config(self, papers_dir: str, output_dir: str, enable_llm_judge: bool, 
                     match_threshold: int, analysis_mode: str):
        """è®¾ç½®é…ç½®"""
        self.config = AnalysisConfig(
            enable_llm_judge=enable_llm_judge,
            analysis_mode=analysis_mode,
            papers_dir=papers_dir,
            output_dir=output_dir,
            match_threshold=match_threshold
        )
        
        try:
            self.config.validate()
        except ValueError as e:
            raise ConfigurationError(str(e)) from e
    
    def _setup_components(self):
        """è®¾ç½®ç»„ä»¶"""
        if not os.path.exists(self.config.output_dir):
            os.makedirs(self.config.output_dir)
        
        self.report_generator = ReportGenerator(self.config.output_dir)
        self.analyzer = PaperAnalyzer(self.config)
    
    def _get_paper_files(self, paper_filenames: List[str] = None, 
                         analyze_all: bool = False) -> List[str]:
        """è·å–è¦åˆ†æçš„è®ºæ–‡æ–‡ä»¶åˆ—è¡¨"""
        if analyze_all:
            if not os.path.exists(self.config.papers_dir):
                raise ConfigurationError(f"æ‰¾ä¸åˆ°è®ºæ–‡æ–‡ä»¶å¤¹ {self.config.papers_dir}")
            
            paper_files = [f for f in os.listdir(self.config.papers_dir) 
                          if f.lower().endswith('.pdf')]
            self.logger.logger.info(f"ä» {self.config.papers_dir} æ–‡ä»¶å¤¹åŠ è½½äº† {len(paper_files)} ä¸ªPDFè®ºæ–‡æ–‡ä»¶")
            return paper_files
        else:
            return paper_filenames or []
    
    def _process_single_paper(self, paper_input: Union[str, Dict], analysis_mode: str, 
                             i: int, total: int) -> Optional[PaperInfo]:
        """å¤„ç†å•ç¯‡è®ºæ–‡"""
        try:
            paper_info = self.analyzer.analyze_paper(paper_input)
            
            if paper_info is None:
                filename = self._get_filename(paper_input, analysis_mode)
                self.logger.logger.warning(f"æ— æ³•æå–æ ‡é¢˜æˆ–æ‘˜è¦ï¼Œè·³è¿‡ {filename}")
                return None
            
            # ç«‹å³å†™å…¥ç£ç›˜
            self._save_paper_to_disk(paper_info)
            
            # æ‰“å°è¿›åº¦ä¿¡æ¯
            self.logger.print_progress(paper_info, i, total, saved_to_disk=True)
            
            return paper_info
            
        except Exception as e:
            filename = self._get_filename(paper_input, analysis_mode)
            self.logger.logger.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}")
            self._log_error(filename, str(e))
            return None
    
    def _get_filename(self, paper_input: Union[str, Dict], analysis_mode: str) -> str:
        """è·å–æ–‡ä»¶å"""
        if analysis_mode == "pdf":
            return paper_input
        else:
            return paper_input.get('filename', 'unknown')
    
    def _save_paper_to_disk(self, paper_info: PaperInfo):
        """ä¿å­˜è®ºæ–‡åˆ°ç£ç›˜"""
        if paper_info.match_info and paper_info.match_info.is_match:
            self.report_generator.append_ai_paper(paper_info)
        else:
            self.report_generator.append_non_ai_paper(paper_info)
    
    def _log_error(self, filename: str, error_msg: str):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        try:
            error_log_file = os.path.join(self.report_generator.output_dir, "error_log.txt")
            with open(error_log_file, 'a', encoding='utf-8') as f:
                f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - å¤„ç†æ–‡ä»¶ {filename} å¤±è´¥: {error_msg}\n")
        except Exception:
            pass  # å¿½ç•¥æ—¥å¿—å†™å…¥é”™è¯¯
    
    def _analyze_papers(self, papers_to_analyze: List, analysis_mode: str = "pdf") -> AnalysisResult:
        """åˆ†æè®ºæ–‡çš„æ ¸å¿ƒæ–¹æ³•"""
        if not papers_to_analyze:
            self.logger.logger.warning("æœªæ‰¾åˆ°è¦åˆ†æçš„è®ºæ–‡")
            return AnalysisResult()
        
        # æ›´æ–°é…ç½®
        self.config.analysis_mode = analysis_mode
        
        # åˆå§‹åŒ–åˆ†æç»“æœå’Œè¿›åº¦è·Ÿè¸ª
        analysis_result = AnalysisResult(
            total_count=len(papers_to_analyze),
            analysis_time=datetime.now()
        )
        progress_tracker = ProgressTracker(len(papers_to_analyze))
        
        # æ‰“å°å¼€å§‹ä¿¡æ¯
        paper_type = "è®ºæ–‡" if analysis_mode == "pdf" else "paper_copilotè®ºæ–‡"
        self.logger.logger.info(f"\nå¼€å§‹åˆ†æ {len(papers_to_analyze)} ä¸ª{paper_type}...")
        self.logger.logger.info(f"åˆ†æç»“æœå°†å®æ—¶ä¿å­˜åˆ°: {self.report_generator.output_dir}")
        
        # å¤„ç†æ¯ç¯‡è®ºæ–‡
        for i, paper_input in enumerate(papers_to_analyze, 1):
            paper_info = self._process_single_paper(paper_input, analysis_mode, i, len(papers_to_analyze))
            
            # æ›´æ–°è¿›åº¦
            is_error = paper_info is None
            progress_tracker.update(paper_info, is_error)
            
            # æ›´æ–°åˆ†æç»“æœ
            if paper_info:
                if paper_info.match_info and paper_info.match_info.is_match:
                    analysis_result.ai_related_papers.append(paper_info)
                else:
                    analysis_result.non_ai_related_papers.append(paper_info)
            
            # å®šæœŸè¾“å‡ºè¿›åº¦æ€»ç»“
            if i % 10 == 0:
                self.logger.print_progress_summary(progress_tracker.get_progress())
        
        # æ›´æ–°æœ€ç»ˆç»“æœ
        progress = progress_tracker.get_progress()
        analysis_result.processed_count = progress.success_count
        analysis_result.error_count = progress.error_count
        
        # æœ€ç»ˆè¿›åº¦æ›´æ–°å’Œæ€»ç»“
        self.report_generator.update_progress(progress.error_count)
        self.logger.print_final_summary(analysis_result, self.report_generator.output_dir)
        
        return analysis_result
    
    def analyze(self, params: AnalysisParams, mode: str = "pdf") -> AnalysisResult:
        """
        ç»Ÿä¸€çš„è®ºæ–‡åˆ†ææ–¹æ³•
        
        Args:
            params: åˆ†æå‚æ•°
            mode: åˆ†ææ¨¡å¼ï¼Œ"pdf"æˆ–"paper_copilot"
        
        Returns:
            åˆ†æç»“æœ
        """
        # æ‰“å°åˆ†æå¤´éƒ¨ä¿¡æ¯
        self.logger.print_header(mode.upper())
        
        # è·å–è¦åˆ†æçš„æ•°æ®
        if mode == "pdf":
            papers_to_analyze = self._get_paper_files(params.paper_filenames, params.analyze_all)
            if not papers_to_analyze:
                if params.analyze_all:
                    self.logger.logger.warning("æœªæ‰¾åˆ°è¦åˆ†æçš„è®ºæ–‡æ–‡ä»¶")
                else:
                    self.logger.logger.error("é”™è¯¯: éœ€è¦æä¾›è®ºæ–‡æ–‡ä»¶ååˆ—è¡¨æˆ–è®¾ç½®analyze_all=True")
                return AnalysisResult()
        else:  # paper_copilot mode
            papers_to_analyze = params.paper_infos or []
            if not papers_to_analyze:
                self.logger.logger.warning("æœªæ‰¾åˆ°è¦åˆ†æçš„è®ºæ–‡æ•°æ®")
                return AnalysisResult()
        
        # æ‰§è¡Œåˆ†æ
        analysis_result = self._analyze_papers(papers_to_analyze, mode)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.report_generator.finalize_reports(analysis_result)
        
        # æ‰“å°åˆ†æç»“æœ
        self.logger.print_analysis_results(analysis_result, self.report_generator.output_dir)
        
        return analysis_result
    
    def parse(self, paper_filenames: List[str] = None, analyze_all: bool = True,
              output_format: str = "both") -> AnalysisResult:
        """
        è§£æPDFè®ºæ–‡å¹¶ä¿å­˜ç»“æœ
        
        Args:
            paper_filenames: è¦åˆ†æçš„è®ºæ–‡æ–‡ä»¶ååˆ—è¡¨
            analyze_all: æ˜¯å¦åˆ†æå…¨é‡è®ºæ–‡
            output_format: è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰ "txt", "csv", "both"
        
        Returns:
            åˆ†æç»“æœ
        """
        params = AnalysisParams(
            paper_filenames=paper_filenames,
            analyze_all=analyze_all,
            output_format=output_format
        )
        return self.analyze(params, "pdf")
    
    def parse_paper_copilot(self, paper_infos: List[Dict], 
                           output_format: str = "both") -> AnalysisResult:
        """
        è§£æpaper_copilotè®ºæ–‡æ•°æ®å¹¶ä¿å­˜ç»“æœ
        
        Args:
            paper_infos: paper_copilotè®ºæ–‡çš„è§£æç»“æœåˆ—è¡¨
            output_format: è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰ "txt", "csv", "both"
        
        Returns:
            åˆ†æç»“æœ
        """
        params = AnalysisParams(
            paper_infos=paper_infos,
            output_format=output_format
        )
        return self.analyze(params, "paper_copilot")


class ExtractorFactory:
    """æå–å™¨å·¥å‚ç±»"""
    
    @staticmethod
    def create_extractor(**kwargs) -> AiAccelerationExtractor:
        """åˆ›å»ºæå–å™¨å®ä¾‹"""
        return AiAccelerationExtractor(**kwargs)
    
    @staticmethod
    def create_and_analyze(extractor_params: Dict, analysis_params: AnalysisParams, 
                          mode: str) -> AnalysisResult:
        """åˆ›å»ºæå–å™¨å¹¶æ‰§è¡Œåˆ†æ"""
        extractor = ExtractorFactory.create_extractor(**extractor_params)
        return extractor.analyze(analysis_params, mode)


def ai_acceleration_parse(papers_dir: str, output_dir: str = ".", 
                         paper_filenames: List[str] = None, analyze_all: bool = True,
                         output_format: str = "both", enable_llm_judge: bool = True,
                         match_threshold: int = 5) -> AnalysisResult:
    """
    å¯¹å¤–æä¾›çš„AIæ¨ç†åŠ é€Ÿè®ºæ–‡è§£æå‡½æ•°
    
    Args:
        papers_dir: è®ºæ–‡PDFæ–‡ä»¶æ‰€åœ¨ç›®å½•
        output_dir: è¾“å‡ºæ–‡ä»¶ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
        paper_filenames: è¦åˆ†æçš„è®ºæ–‡æ–‡ä»¶ååˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneä¸”analyze_allä¸ºTrueåˆ™åˆ†ææ‰€æœ‰è®ºæ–‡
        analyze_all: æ˜¯å¦åˆ†æpapers_dirä¸‹çš„å…¨é‡è®ºæ–‡ï¼Œé»˜è®¤ä¸ºTrue
        output_format: è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰ "txt", "csv", "both"
        enable_llm_judge: æ˜¯å¦å¯ç”¨å¤§æ¨¡å‹åˆ¤åˆ«åŠŸèƒ½ï¼Œé»˜è®¤ä¸ºTrue
        match_threshold: åŒ¹é…é˜ˆå€¼ï¼Œé»˜è®¤ä¸º5
    
    Returns:
        åˆ†æç»“æœ
    """
    extractor_params = {
        'papers_dir': papers_dir,
        'output_dir': output_dir,
        'enable_llm_judge': enable_llm_judge,
        'match_threshold': match_threshold
    }
    analysis_params = AnalysisParams(
        paper_filenames=paper_filenames,
        analyze_all=analyze_all,
        output_format=output_format
    )
    return ExtractorFactory.create_and_analyze(extractor_params, analysis_params, "pdf")


def ai_acceleration_parse_paper_copilot(paper_infos: List[Dict], 
                                       output_dir: str = ".", 
                                       output_format: str = "both", 
                                       enable_llm_judge: bool = True,
                                       match_threshold: int = 5) -> AnalysisResult:
    """
    å¯¹å¤–æä¾›çš„AIæ¨ç†åŠ é€Ÿè®ºæ–‡è§£æå‡½æ•°ï¼ˆpaper_copilotæ¨¡å¼ï¼‰
    
    Args:
        paper_infos: paper_copilotè®ºæ–‡çš„è§£æç»“æœ
        output_dir: è¾“å‡ºæ–‡ä»¶ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
        output_format: è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰ "txt", "csv", "both"
        enable_llm_judge: æ˜¯å¦å¯ç”¨å¤§æ¨¡å‹åˆ¤åˆ«åŠŸèƒ½ï¼Œé»˜è®¤ä¸ºTrue
        match_threshold: åŒ¹é…é˜ˆå€¼ï¼Œé»˜è®¤ä¸º5
    
    Returns:
        åˆ†æç»“æœ
    """
    extractor_params = {
        'output_dir': output_dir,
        'enable_llm_judge': enable_llm_judge,
        'match_threshold': match_threshold,
        'analysis_mode': 'paper_copilot'
    }
    analysis_params = AnalysisParams(
        paper_infos=paper_infos,
        output_format=output_format
    )
    return ExtractorFactory.create_and_analyze(extractor_params, analysis_params, "paper_copilot") 