# PDFå†…å®¹æå–å™¨

> ğŸ“„ åŸºäºGrobidçš„é«˜ç²¾åº¦PDFå­¦æœ¯è®ºæ–‡å†…å®¹æå–å·¥å…·

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

PDFå†…å®¹æå–å™¨æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºä»å­¦æœ¯è®ºæ–‡PDFä¸­æå–ç»“æ„åŒ–ä¿¡æ¯çš„å·¥å…·ï¼ŒåŸºäºå¼ºå¤§çš„Grobidæœºå™¨å­¦ä¹ åº“å®ç°é«˜ç²¾åº¦çš„å†…å®¹è§£æã€‚

### æ ¸å¿ƒåŠŸèƒ½
- ğŸ“– **æ ‡é¢˜æå–**ï¼šå‡†ç¡®è¯†åˆ«è®ºæ–‡ä¸»æ ‡é¢˜
- ğŸ‘¥ **ä½œè€…ä¿¡æ¯**ï¼šæå–ä½œè€…å§“åã€é‚®ç®±å’Œæ‰€å±æœºæ„
- ğŸ“ **æ‘˜è¦æå–**ï¼šå®Œæ•´æå–è®ºæ–‡æ‘˜è¦å†…å®¹
- ğŸ¢ **æœºæ„è¯†åˆ«**ï¼šè§£æä½œè€…æ‰€å±çš„ç ”ç©¶æœºæ„
- ğŸ’¾ **ç¼“å­˜æœºåˆ¶**ï¼šé¿å…é‡å¤å¤„ç†ï¼Œæé«˜æ•ˆç‡

## ğŸ“ æ–‡ä»¶ç»“æ„

```
pdf_extractor/
â”œâ”€â”€ __init__.py                # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ pdf_extractor.py           # ä¸»è¦æå–å™¨å®ç°
â”œâ”€â”€ config.json                # GrobidæœåŠ¡é…ç½®
â””â”€â”€ README.md                  # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

1. **å¯åŠ¨GrobidæœåŠ¡**ï¼ˆå¿…éœ€ï¼‰ï¼š
   ```bash
   # ä½¿ç”¨Dockerå¯åŠ¨GrobidæœåŠ¡
   docker run --rm -it -p 8070:8070 lfoppiano/grobid:0.8.3
   
   # æˆ–åå°è¿è¡Œ
   docker run -d --name grobid -p 8070:8070 lfoppiano/grobid:0.8.3
   ```

2. **å®‰è£…Pythonä¾èµ–**ï¼š
   ```bash
   pip install grobid-client beautifulsoup4 lxml
   ```

### åŸºæœ¬ä½¿ç”¨

```python
from utils.pdf_extractor import extract_paper_abstract

# æå–å•ä¸ªPDFçš„ä¿¡æ¯
paper_info = extract_paper_abstract("path/to/paper.pdf")

# æ‰“å°æå–ç»“æœ
print(f"æ ‡é¢˜: {paper_info['title']}")
print(f"ä½œè€…æ•°é‡: {len(paper_info['authors'])}")
print(f"æ‘˜è¦é•¿åº¦: {len(paper_info['abstract'])}")
```

## ğŸ“Š æ•°æ®æ ¼å¼

### è¾“å…¥æ ¼å¼
- **PDFæ–‡ä»¶è·¯å¾„**ï¼šæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
- **æ–‡ä»¶è¦æ±‚**ï¼šæ ‡å‡†çš„å­¦æœ¯è®ºæ–‡PDFæ ¼å¼

### è¾“å‡ºæ ¼å¼

```python
{
    'title': 'è®ºæ–‡æ ‡é¢˜',
    'authors': [
        {
            'name': 'ä½œè€…å…¨å',
            'email': 'ä½œè€…é‚®ç®±ï¼ˆå¦‚æœæœ‰ï¼‰',
            'affiliation': ['æ‰€å±æœºæ„åˆ—è¡¨']
        }
        # ... æ›´å¤šä½œè€…
    ],
    'abstract': 'å®Œæ•´çš„è®ºæ–‡æ‘˜è¦æ–‡æœ¬',
    'affiliations': []  # ä¿ç•™å­—æ®µ
}
```

### å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `title` | str | è®ºæ–‡ä¸»æ ‡é¢˜ï¼Œå»é™¤å¤šä½™ç©ºæ ¼ |
| `authors` | list | ä½œè€…ä¿¡æ¯åˆ—è¡¨ |
| `authors[].name` | str | ä½œè€…å§“åï¼ˆå + å§“ï¼‰ |
| `authors[].email` | str | ä½œè€…é‚®ç®±åœ°å€ï¼ˆå¯é€‰ï¼‰ |
| `authors[].affiliation` | list | ä½œè€…æ‰€å±æœºæ„åˆ—è¡¨ |
| `abstract` | str | è®ºæ–‡æ‘˜è¦å®Œæ•´æ–‡æœ¬ |
| `affiliations` | list | æœºæ„ä¿¡æ¯ï¼ˆä¿ç•™å­—æ®µï¼‰ |

## âš™ï¸ é…ç½®è¯´æ˜

### Grobidé…ç½® (`config.json`)

```json
{
    "grobid": {
        "server": "http://localhost:8070",
        "sleep_time": 5,
        "timeout": 60,
        "coordinates": ["persName", "figure", "ref", "biblStruct", "formula"]
    }
}
```

**é…ç½®å‚æ•°**ï¼š
- `server`: GrobidæœåŠ¡å™¨åœ°å€
- `sleep_time`: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
- `timeout`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- `coordinates`: éœ€è¦æå–çš„åæ ‡ä¿¡æ¯ç±»å‹

### è‡ªå®šä¹‰é…ç½®

```python
# ä¿®æ”¹config.jsonåä½¿ç”¨è‡ªå®šä¹‰é…ç½®
import os
from utils.pdf_extractor import extract_paper_abstract

# é…ç½®æ–‡ä»¶ä¼šè‡ªåŠ¨åŠ è½½
paper_info = extract_paper_abstract("paper.pdf")
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### æ‰¹é‡å¤„ç†PDFæ–‡ä»¶

```python
import os
from utils.pdf_extractor import extract_paper_abstract

def batch_extract_papers(pdf_directory):
    """æ‰¹é‡æå–PDFä¿¡æ¯"""
    papers_info = []
    
    for filename in os.listdir(pdf_directory):
        if filename.endswith('.pdf'):
            pdf_path = os.path.join(pdf_directory, filename)
            try:
                paper_info = extract_paper_abstract(pdf_path)
                paper_info['filename'] = filename
                papers_info.append(paper_info)
                print(f"âœ… æˆåŠŸå¤„ç†: {filename}")
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {filename} - {e}")
    
    return papers_info

# ä½¿ç”¨ç¤ºä¾‹
papers = batch_extract_papers("papers_directory")
print(f"æˆåŠŸå¤„ç† {len(papers)} ç¯‡è®ºæ–‡")
```

### é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
import time
from utils.pdf_extractor import extract_paper_abstract

def robust_extract(pdf_path, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„æå–"""
    for attempt in range(max_retries):
        try:
            return extract_paper_abstract(pdf_path)
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"é‡è¯• {attempt + 1}/{max_retries}: {e}")
                time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
            else:
                print(f"æœ€ç»ˆå¤±è´¥: {e}")
                raise

# ä½¿ç”¨ç¤ºä¾‹
try:
    paper_info = robust_extract("difficult_paper.pdf")
    print("æå–æˆåŠŸ")
except Exception:
    print("æå–å¤±è´¥")
```

### ç¼“å­˜æœºåˆ¶åˆ©ç”¨

```python
# æå–å™¨è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜æœºåˆ¶
# å¦‚æœå¯¹åº”çš„.grobid.tei.xmlæ–‡ä»¶å­˜åœ¨ï¼Œå°†ç›´æ¥è¯»å–è€Œä¸é‡æ–°å¤„ç†

import os
from utils.pdf_extractor import extract_paper_abstract

pdf_path = "paper.pdf"
xml_cache_path = "paper.grobid.tei.xml"

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šä½¿ç”¨Grobidå¤„ç†PDFï¼Œç”ŸæˆXMLç¼“å­˜
paper_info1 = extract_paper_abstract(pdf_path)

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šç›´æ¥è¯»å–XMLç¼“å­˜ï¼Œé€Ÿåº¦æ›´å¿«
paper_info2 = extract_paper_abstract(pdf_path)

print(f"XMLç¼“å­˜å­˜åœ¨: {os.path.exists(xml_cache_path)}")
```

## ğŸ” æŠ€æœ¯å®ç°

### Grobidé›†æˆ

```python
from grobid_client.grobid_client import GrobidClient

# åˆ›å»ºGrobidå®¢æˆ·ç«¯
client = GrobidClient(config_path="config.json")

# å¤„ç†PDFæ–‡æ¡£å¤´éƒ¨ä¿¡æ¯
result = client.process_pdf(
    "processHeaderDocument",
    pdf_file_path,
    None, None, None, None, None, None, None
)[2]
```

### XMLè§£æ

```python
from bs4 import BeautifulSoup

# è§£æGrobidè¿”å›çš„TEI XML
soup = BeautifulSoup(result, 'xml')

# æå–æ ‡é¢˜
title = soup.find('title', {'type': 'main'})

# æå–ä½œè€…
authors = soup.find_all('author')

# æå–æ‘˜è¦
abstract = soup.find('abstract')
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ç­–ç•¥
- **XMLç¼“å­˜**ï¼šè‡ªåŠ¨ä¿å­˜Grobidå¤„ç†ç»“æœä¸ºXMLæ–‡ä»¶
- **é¿å…é‡å¤å¤„ç†**ï¼šæ£€æŸ¥ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ€§
- **å¿«é€Ÿè¯»å–**ï¼šç›´æ¥è§£æXMLè€Œéé‡æ–°è°ƒç”¨Grobid

### å¤„ç†é€Ÿåº¦
- **é¦–æ¬¡å¤„ç†**ï¼š~2-5ç§’/PDFï¼ˆå–å†³äºæ–‡ä»¶å¤§å°å’Œå¤æ‚åº¦ï¼‰
- **ç¼“å­˜è¯»å–**ï¼š~0.1-0.5ç§’/PDF
- **æ‰¹é‡å¤„ç†**ï¼šå»ºè®®æ·»åŠ é€‚å½“å»¶è¿Ÿé¿å…æœåŠ¡å™¨è¿‡è½½

### å†…å­˜ä½¿ç”¨
- **æµå¼å¤„ç†**ï¼šé€ä¸ªå¤„ç†PDFæ–‡ä»¶
- **åŠæ—¶é‡Šæ”¾**ï¼šå¤„ç†å®Œæˆåé‡Šæ”¾å†…å­˜
- **é…ç½®ä¼˜åŒ–**ï¼šè°ƒæ•´Grobidè¶…æ—¶è®¾ç½®

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **GrobidæœåŠ¡æœªå“åº”**
   ```bash
   # æ£€æŸ¥æœåŠ¡çŠ¶æ€
   curl http://localhost:8070
   
   # é‡å¯GrobidæœåŠ¡
   docker restart grobid_container
   ```

2. **PDFå¤„ç†å¤±è´¥**
   ```python
   # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸå
   try:
       with open("paper.pdf", "rb") as f:
           f.read(1024)  # å°è¯•è¯»å–å‰1KB
   except Exception as e:
       print(f"PDFæ–‡ä»¶å¯èƒ½æŸå: {e}")
   ```

3. **æå–ç»“æœä¸ºç©º**
   - æ£€æŸ¥PDFæ˜¯å¦ä¸ºå›¾ç‰‡æ‰«æç‰ˆæœ¬
   - ç¡®è®¤PDFåŒ…å«å¯æå–çš„æ–‡æœ¬å†…å®¹
   - éªŒè¯è®ºæ–‡æ ¼å¼æ˜¯å¦ç¬¦åˆå­¦æœ¯æ ‡å‡†

### è°ƒè¯•æ¨¡å¼

```python
import logging
from utils.pdf_extractor import extract_paper_abstract

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)

# å¤„ç†PDFå¹¶æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
paper_info = extract_paper_abstract("paper.pdf")

# æ£€æŸ¥æå–è´¨é‡
if not paper_info['title']:
    print("âš ï¸ è­¦å‘Š: æœªæå–åˆ°æ ‡é¢˜")
if not paper_info['abstract']:
    print("âš ï¸ è­¦å‘Š: æœªæå–åˆ°æ‘˜è¦")
if not paper_info['authors']:
    print("âš ï¸ è­¦å‘Š: æœªæå–åˆ°ä½œè€…ä¿¡æ¯")
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´å·¥ä½œæµç¨‹

```python
import os
from utils.pdf_extractor import extract_paper_abstract

def analyze_paper_content(pdf_path):
    """åˆ†æè®ºæ–‡å†…å®¹"""
    try:
        # æå–è®ºæ–‡ä¿¡æ¯
        paper_info = extract_paper_abstract(pdf_path)
        
        # åŸºæœ¬ä¿¡æ¯æ£€æŸ¥
        print(f"ğŸ“„ è®ºæ–‡æ ‡é¢˜: {paper_info['title']}")
        print(f"ğŸ‘¥ ä½œè€…æ•°é‡: {len(paper_info['authors'])}")
        print(f"ğŸ“ æ‘˜è¦é•¿åº¦: {len(paper_info['abstract'])} å­—ç¬¦")
        
        # ä½œè€…ä¿¡æ¯è¯¦ç»†æ˜¾ç¤º
        for i, author in enumerate(paper_info['authors'], 1):
            print(f"  {i}. {author['name']}")
            if author.get('email'):
                print(f"     ğŸ“§ {author['email']}")
            if author.get('affiliation'):
                print(f"     ğŸ¢ {', '.join(author['affiliation'])}")
        
        # æ‘˜è¦é¢„è§ˆ
        if paper_info['abstract']:
            preview = paper_info['abstract'][:200] + "..." if len(paper_info['abstract']) > 200 else paper_info['abstract']
            print(f"ğŸ“„ æ‘˜è¦é¢„è§ˆ: {preview}")
        
        return paper_info
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    paper_info = analyze_paper_content("example_paper.pdf")
```

---

> ğŸ’¡ **æç¤º**ï¼šå»ºè®®åœ¨æ‰¹é‡å¤„ç†å‰å…ˆç”¨å°‘é‡æ–‡ä»¶æµ‹è¯•ï¼Œç¡®ä¿GrobidæœåŠ¡é…ç½®æ­£ç¡®ä¸”è¿è¡Œç¨³å®šã€‚ 