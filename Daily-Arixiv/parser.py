"""
arXiv Dailyè®ºæ–‡è§£æå™¨
è§£æarxiv_daily.txtæ–‡ä»¶ï¼Œæå–è®ºæ–‡ä¿¡æ¯å¹¶ç”Ÿæˆai_acceleration_parse_paper_copilotå‡½æ•°çš„è¾“å…¥æ ¼å¼
"""

import re
import os
import sys
import random
import argparse
from typing import List, Dict, Optional
from datetime import datetime

# æ·»åŠ utilsè·¯å¾„ä»¥ä¾¿å¯¼å…¥ai_acceleration_extractor
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'utils'))
from utils.ai_acceleration_extractor.ai_acceleration_extractor import ai_acceleration_parse_paper_copilot


def parse_arxiv_daily(file_path: str) -> List[Dict]:
    """
    è§£æarxiv_daily.txtæ–‡ä»¶ï¼Œæå–è®ºæ–‡ä¿¡æ¯
    
    Args:
        file_path: arxiv_daily.txtæ–‡ä»¶è·¯å¾„
    
    Returns:
        List[Dict]: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ï¼Œæ ¼å¼é€‚ç”¨äºai_acceleration_parse_paper_copilotå‡½æ•°
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")
    
    papers = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²è®ºæ–‡æ¡ç›®
    # æ¯ä¸ªè®ºæ–‡æ¡ç›®ä»¥ arXiv: å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€ä¸ª arXiv: æˆ–æ–‡ä»¶ç»“å°¾ç»“æŸ
    # å…ˆæŒ‰ arXiv: åˆ†å‰²ï¼Œç„¶åå¤„ç†æ¯ä¸ªéƒ¨åˆ†
    sections = re.split(r'(?=arXiv:\d{4}\.\d+)', content)
    
    matches = []
    for section in sections[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºæ®µ
        if section.strip():
            matches.append(section)
    
    for match in matches:
        paper_info = _parse_single_paper(match)
        if paper_info:
            papers.append(paper_info)
    
    return papers


def _parse_single_paper(paper_text: str) -> Optional[Dict]:
    """
    è§£æå•ç¯‡è®ºæ–‡çš„æ–‡æœ¬ä¿¡æ¯
    
    Args:
        paper_text: å•ç¯‡è®ºæ–‡çš„æ–‡æœ¬å†…å®¹
    
    Returns:
        Dict: è®ºæ–‡ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å›None
    """
    try:
        lines = paper_text.strip().split('\n')
        
        # æå–arXiv ID
        arxiv_id_match = re.search(r'arXiv:(\d{4}\.\d+)', lines[0])
        if not arxiv_id_match:
            return None
        arxiv_id = arxiv_id_match.group(1)
        
        # åˆå§‹åŒ–è®ºæ–‡ä¿¡æ¯
        paper_info = {
            'filename': f"arxiv_{arxiv_id}.pdf",
            'arxiv_id': arxiv_id,
            'title': '',
            'authors': '',
            'categories': '',
            'abstract': '',
            'date': '',
            'url': f"https://arxiv.org/abs/{arxiv_id}"
        }
        
        # è§£æå„ä¸ªå­—æ®µ
        current_field = None
        abstract_lines = []
        
        for line in lines[1:]:  # è·³è¿‡ç¬¬ä¸€è¡Œï¼ˆå·²å¤„ç†çš„arXiv IDï¼‰
            line = line.strip()
            
            if line.startswith('Date:'):
                # æå–æ—¥æœŸ
                date_match = re.search(r'Date:\s*(.+?GMT)', line)
                if date_match:
                    paper_info['date'] = date_match.group(1).strip()
                continue
            
            elif line.startswith('Title:'):
                # æå–æ ‡é¢˜
                title = line.replace('Title:', '').strip()
                paper_info['title'] = title
                current_field = 'title'
                continue
            
            elif line.startswith('Authors:'):
                # æå–ä½œè€…
                authors = line.replace('Authors:', '').strip()
                paper_info['authors'] = authors
                current_field = 'authors'
                continue
            
            elif line.startswith('Categories:'):
                # æå–ç±»åˆ«
                categories = line.replace('Categories:', '').strip()
                paper_info['categories'] = categories
                current_field = 'categories'
                continue
            
            elif line.startswith('Comments:'):
                # è·³è¿‡è¯„è®ºè¡Œ
                current_field = None
                continue
            
            elif line.startswith('DOI:'):
                # è·³è¿‡DOIè¡Œ
                current_field = None
                continue
            
            elif line.startswith('ACM-class:'):
                # è·³è¿‡ACMåˆ†ç±»è¡Œ
                current_field = None
                continue
            
            elif line.startswith('\\\\'):
                # å¼€å§‹æ‘˜è¦éƒ¨åˆ†
                if len(line) > 2:  # å¦‚æœ\\ åé¢è¿˜æœ‰å†…å®¹ï¼Œé‚£å°±æ˜¯æ‘˜è¦çš„ä¸€éƒ¨åˆ†
                    abstract_content = line[2:].strip()
                    if abstract_content and not abstract_content.startswith('( https://arxiv.org/abs/'):
                        abstract_lines.append(abstract_content)
                current_field = 'abstract'
                continue
            
            else:
                # å¤„ç†å¤šè¡Œå†…å®¹
                if current_field == 'title' and line and not line.startswith('Authors:') and not line.startswith('Categories:'):
                    # æ ‡é¢˜å¯èƒ½è·¨è¡Œ
                    paper_info['title'] += ' ' + line
                elif current_field == 'authors' and line and not line.startswith('Categories:') and not line.startswith('Comments:') and not line.startswith('DOI:') and not line.startswith('ACM-class:'):
                    # ä½œè€…å¯èƒ½è·¨è¡Œ
                    paper_info['authors'] += ' ' + line
                elif current_field == 'abstract' and line:
                    # æ‘˜è¦å†…å®¹ - è¿‡æ»¤æ‰URLå’Œåˆ†å‰²çº¿
                    if not (line.startswith('( https://arxiv.org/abs/') or 
                           line.startswith('----------') or
                           'https://arxiv.org/abs/' in line):
                        abstract_lines.append(line)
        
        # æ¸…ç†å’Œç»„è£…æ‘˜è¦
        if abstract_lines:
            paper_info['abstract'] = ' '.join(abstract_lines).strip()
            # ç§»é™¤æ‘˜è¦ä¸­çš„å¤šä½™åˆ†å‰²çº¿
            paper_info['abstract'] = re.sub(r'-{5,}', '', paper_info['abstract']).strip()
        
        # æ¸…ç†æ ‡é¢˜å’Œä½œè€…å­—æ®µ
        paper_info['title'] = ' '.join(paper_info['title'].split())
        paper_info['authors'] = ' '.join(paper_info['authors'].split())
        
        # éªŒè¯å’Œä¿®æ­£URL
        if paper_info['arxiv_id'] and not paper_info['url'].endswith(paper_info['arxiv_id']):
            paper_info['url'] = f"https://arxiv.org/abs/{paper_info['arxiv_id']}"
        
        # éªŒè¯å¿…è¦å­—æ®µ
        if not paper_info['title']:
            print(f"è­¦å‘Š: è®ºæ–‡ {arxiv_id} ç¼ºå°‘æ ‡é¢˜")
            return None
        
        if not paper_info['abstract']:
            print(f"è­¦å‘Š: è®ºæ–‡ {arxiv_id} ç¼ºå°‘æ‘˜è¦")
            
        return paper_info
    
    except Exception as e:
        print(f"è§£æè®ºæ–‡æ—¶å‡ºé”™: {e}")
        print(f"é—®é¢˜æ–‡æœ¬å‰100å­—ç¬¦: {paper_text[:100]}...")
        return None


def load_papers_for_ai_analysis(arxiv_daily_file: str) -> List[Dict]:
    """
    åŠ è½½arXiv dailyæ–‡ä»¶å¹¶è¿”å›é€‚ç”¨äºAIåŠ é€Ÿåˆ†æçš„è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
    
    Args:
        arxiv_daily_file: arxiv_daily.txtæ–‡ä»¶è·¯å¾„
    
    Returns:
        List[Dict]: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨ï¼Œå¯ç›´æ¥ç”¨äºai_acceleration_parse_paper_copilotå‡½æ•°
    """
    print(f"æ­£åœ¨è§£æ {arxiv_daily_file}...")
    papers = parse_arxiv_daily(arxiv_daily_file)
    print(f"æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
    
    return papers


def analyze_ai_acceleration_papers(papers: List[Dict], parse_dir: str = None) -> None:
    """
    åˆ†æAIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡
    
    Args:
        papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        parse_dir: è§£æç›®å½•è·¯å¾„ï¼ˆå¦‚ "2025-09/09-01"ï¼‰ï¼Œai_acceleration_resultså°†ä¿å­˜åœ¨æ­¤ç›®å½•ä¸‹
    """
    if not papers:
        print("âŒ æ²¡æœ‰è®ºæ–‡æ•°æ®éœ€è¦åˆ†æ")
        return
    

    # ä½¿ç”¨æŒ‡å®šçš„è§£æç›®å½•ä½œä¸ºåŸºç¡€ï¼Œåœ¨å…¶ä¸‹åˆ›å»ºai_acceleration_resultsæ–‡ä»¶å¤¹
    current_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(current_dir, parse_dir)

    print(f"\nğŸš€ å¼€å§‹AIæ¨ç†åŠ é€Ÿè®ºæ–‡ç­›é€‰åˆ†æ...")
    print(f"ğŸ“Š æ€»è®ºæ–‡æ•°: {len(papers)}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        # è°ƒç”¨ai_acceleration_parse_paper_copilotè¿›è¡Œåˆ†æ
        analysis_result = ai_acceleration_parse_paper_copilot(
            paper_infos=papers,
            output_dir=output_dir,
            output_format="both",
            enable_llm_judge=True,
            match_threshold=5
        )
        
        print(f"\nâœ… AIæ¨ç†åŠ é€Ÿè®ºæ–‡ç­›é€‰å®Œæˆ!")
        print(f"ğŸ¯ æ‰¾åˆ°AIæ¨ç†åŠ é€Ÿç›¸å…³è®ºæ–‡: {analysis_result.ai_related_count} ç¯‡")
        print(f"ğŸ“„ å…¶ä»–è®ºæ–‡: {analysis_result.non_ai_related_count} ç¯‡")
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
        return analysis_result
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(
        description="arXiv Dailyè®ºæ–‡è§£æå™¨ - è§£æarxiv_daily.txtæ–‡ä»¶å¹¶è¿›è¡ŒAIæ¨ç†åŠ é€Ÿè®ºæ–‡ç­›é€‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python parser.py --parse_dir "2025-09/09-01"
  python parser.py --parse_dir "2025-09/09-01" --sample_size 50
        """
    )
    
    parser.add_argument(
        "--parse_dir",
        required=True,
        help="æŒ‡å®šè¦è§£æçš„ç›®å½•è·¯å¾„ï¼ˆç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„ä½ç½®ï¼‰ï¼Œä¾‹å¦‚: '2025-09/09-01'"
    )
    
    parser.add_argument(
        "--sample_size",
        type=int,
        default=30,
        help="éšæœºæŠ½æ£€å±•ç¤ºçš„è®ºæ–‡æ•°é‡ï¼ˆé»˜è®¤: 30ï¼‰"
    )
    

    
    return parser.parse_args()


def main():
    """
    ä¸»å‡½æ•° - è§£æarxiv_daily.txtæ–‡ä»¶å¹¶è¿›è¡ŒAIæ¨ç†åŠ é€Ÿè®ºæ–‡ç­›é€‰
    """
    args = parse_args()
    
    # æ„å»ºarxiv_daily.txtæ–‡ä»¶è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parse_dir_path = os.path.join(current_dir, args.parse_dir)
    arxiv_daily_file = os.path.join(parse_dir_path, "arxiv_daily.txt")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(arxiv_daily_file):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {arxiv_daily_file}")
        print(f"è¯·ç¡®ä¿åœ¨ç›®å½• '{args.parse_dir}' ä¸‹å­˜åœ¨ 'arxiv_daily.txt' æ–‡ä»¶")
        return
    
    print(f"ğŸ“‚ è§£æç›®å½•: {args.parse_dir}")
    print(f"ğŸ“„ ç›®æ ‡æ–‡ä»¶: {arxiv_daily_file}")
    
    # è§£æè®ºæ–‡
    papers = load_papers_for_ai_analysis(arxiv_daily_file)
    
    if not papers:
        print("âŒ æ²¡æœ‰æˆåŠŸè§£æåˆ°ä»»ä½•è®ºæ–‡")
        return
    
    # æ˜¾ç¤ºè§£æç»“æœç»Ÿè®¡
    print(f"\nğŸ“Š è§£æç»“æœç»Ÿè®¡:")
    print(f"æ€»è®ºæ–‡æ•°: {len(papers)}")
    
    # éšæœºæŠ½æ£€è®ºæ–‡ä¿¡æ¯
    if papers and args.sample_size > 0:
        sample_size = min(args.sample_size, len(papers))
        sample_papers = random.sample(papers, sample_size)
        
        print(f"\nğŸ” éšæœºæŠ½æ£€ {sample_size} ç¯‡è®ºæ–‡ä¿¡æ¯:")
        for i, paper in enumerate(sample_papers, 1):
            print(f"\n=== è®ºæ–‡ {i} ===")
            print(f"æ ‡é¢˜: {paper['title'][:100]}{'...' if len(paper['title']) > 100 else ''}")
            print(f"ä½œè€…: {paper['authors'][:80]}{'...' if len(paper['authors']) > 80 else ''}")
            print(f"ç±»åˆ«: {paper['categories']}")
            print(f"arXiv ID: {paper['arxiv_id']}")
            print(f"æ—¥æœŸ: {paper['date']}")
            print(f"æ‘˜è¦é•¿åº¦: {len(paper['abstract'])} å­—ç¬¦")
            if paper['abstract']:
                print(f"æ‘˜è¦å‰100å­—ç¬¦: {paper['abstract'][:100]}...")
            else:
                print("æ‘˜è¦: [ç¼ºå°‘æ‘˜è¦]")
            print(f"URL: {paper['url']}")
            print("=" * 50)
    
    # è¿›è¡ŒAIæ¨ç†åŠ é€Ÿè®ºæ–‡ç­›é€‰åˆ†æ
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹AIæ¨ç†åŠ é€Ÿè®ºæ–‡ç­›é€‰åˆ†æ...")
    print("="*60)
    analyze_ai_acceleration_papers(papers, args.parse_dir)


if __name__ == "__main__":
    main()
